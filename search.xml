<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[OpenCV简介及使用（一）]]></title>
    <url>%2Fblog%2FOpenCV%E7%AE%80%E4%BB%8B%E5%8F%8A%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言OpenCV (Open Source Computer Vision Library) 是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它主要由 C++ 语言编写，同时提供了Python、Ruby、MATLAB、Java等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法，同时一个使用CUDA的GPU接口也于2010年9月开始实现。 该库拥有 2500 多个优化算法，其中包括一套全面的经典和最先进的计算机视觉和机器学习算法。这些算法可用于检测和识别人脸、识别对象、对视频中的人类行为进行分类、跟踪摄像机移动、跟踪移动对象、提取对象的 3D 模型、从立体摄像机生成 3D 点云、将图像拼接在一起生成整个场景的高分辨率图像，从图像数据库中查找相似图像，从使用闪光灯拍摄的图像中删除红眼，跟踪眼睛运动，识别风景，并建立标记以覆盖其增强现实等。 除了谷歌、微软、英特尔、IBM、索尼、等老牌公司，还有许多初创公司，它们广泛使用OpenCV。 我们可以通过以下网址获取各个版本的 OpenCV ，OpenCV最新版本已经进入 4.x.x 了。 https://opencv.org/ OpenCV环境搭建下面分别展示了Windows和Mac环境下的OpenCV搭建，在Linux环境下搭建和Mac环境下类似，如有机会，我会在进行补充。 本文基于 Java + IDEA + OpenCV 环境的搭建和使用来进行说明。 Windows环境下的OpenCV环境搭建首先下载 OpenCV for Windows 版本，我这儿下载了4.1.1 版本的OpenCV。 我们将这个exe运行，将OpenCV安装在一个方便寻找的目录下即可。 至此，OpenCV算是在Windows环境上安装了，下面我们来进行开发环境搭建。 PS：如果想获取OpenCV最新版进行安装，可以通过OpenCV的 GitHub 获取最新版源码，在通过CMake工具进行编译生成OpenCV Lib，这块大家可以查阅相关资料，不在过多叙述。 我们打开OpenCV的安装目录，在 build/java 可以看到 opencv-411.jar 和 x64、x86 两个文件夹，这两个文件夹里有两个dll文件，都叫opencv_java411.dll，这分别是64位系统和32系统需要使用的动态链接库文件，我们如果想在Java环境下使用OpenCV，在JVM启动时，必须将dll文件加载进去，然后才能使用opencv-411.jar里提供的Java接口方法。 我们来看简单测试下，我们用IDEA新建一个Java项目test，同时需要引入上面的dll文件和jar文件。 新建一个Java项目： 引入必要文件： 这儿要注意两个文件同时选中，引入后如下图效果，Native Library 可以被正确加载到。否则在启动后会出现 java.lang.UnsatisfiedLinkError错误。 然后我们测试一下，代码如下： 1234567public class Main &#123; public static void main(String[] args) &#123; System.loadLibrary(Core.NATIVE_LIBRARY_NAME); Mat mat = new Mat(); System.out.println(mat.toString()); &#125;&#125; 可以看到正确输出内容。 这儿要注意System.loadLibrary(Core.NATIVE_LIBRARY_NAME);这句话，在使用OpenCV时，需要调用此语句以加载原生Library。 PS：上面的文件引入过程，dll和jar也是可以分别引入的，也是相当于引入到了JVM运行环境中。本质上是没有区别的，但一起引入方便理解，即opencv_java411.dll是为opencv-411.jar ”服务“ 的。 显然我们实际的项目是没有这么简单的，而且很多是Web （SpringBoot）项目，使用Maven管理，这种直接引入对我们的管理十分不方便。 对于jar包部分，我们可以使用如下配置引入： 123456&lt;!-- https://mvnrepository.com/artifact/org.bytedeco/opencv --&gt;&lt;dependency&gt; &lt;groupId&gt;org.bytedeco&lt;/groupId&gt; &lt;artifactId&gt;opencv&lt;/artifactId&gt; &lt;version&gt;4.1.0-1.5.1&lt;/version&gt;&lt;/dependency&gt; 对于dll文件，我们有三种引入方式： 按照上面直接添加lib到项目里 这种虽然可以正常使用，但是在Maven打包项目部署到服务器运行后仍会遇到连接不到的问题。 添加VM Options 我们可以将dll所在路径添加到VM Options里，-Djava.library.path=D:/opencv/build/java/x64。 使用程序在项目启动时加入 理论上JVM启动后就不能在向其中加入参数了，但我们可以利用反射机制，在JVM启动时将java.library.path参数添加进去，代码如下： 12345678910111213141516171819202122232425/** * 追加VM Options参数 * java.library.path * 需要在启动后直接调用 * @param libraryPath * @throws Exception */public static void addLibraryDir(String libraryPath) throws Exception &#123; Field userPathsField = ClassLoader.class.getDeclaredField("usr_paths"); userPathsField.setAccessible(true); String[] paths = (String[]) userPathsField.get(null); StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; paths.length; i++) &#123; if (libraryPath.equals(paths[i])) &#123; continue; &#125; sb.append(paths[i]).append(';'); &#125; sb.append(libraryPath); System.setProperty("java.library.path", sb.toString()); //系统变量设置为空，JVM会重新加载 sys_paths 和 usr_paths final Field sysPathsField = ClassLoader.class.getDeclaredField("sys_paths"); sysPathsField.setAccessible(true); sysPathsField.set(null, null);&#125; 这个方法在项目启动时调用即可。 这个方法在JDK9 及以上版本会出现警告，因为我们这样操作是不安全的，也是不建议的，未来JDK所有非法访问操作将被拒绝。 我们通常也建议使用第二种方法引入OpenCV动态链接库dll文件。 PS：有时候我们环境搭建好了，可能仍无法使用，出现动态库dll找不到的情况，如下： 这时候需要注意Core.NATIVE_LIBRARY_NAME这段代码，我们点进去，可以看到它使用的 LIBRARY_NAME 为 opencv_java410，加载不到的原因是我们安装的是 OpenCV 4.1.1 版本，这时候我们使用System.loadLibrary(“opencv_java411”);即可加载成功，出现这种情况的原因是我们上面Maven下载的jar包是 4.1.0 版本的（这也是目前Maven上的最高版本）。所以为防止出现问题，需要知道自己安装了什么版本的OpenCV，并建议设置为常量，方便引入。 Mac环境下的OpenCV环境搭建OpenCV未提供Mac版本的安装程序，我们通常有以下几种安装方式： 使用Homebrew进行安装 这是获取OpenCV最快的方式，安装Homebrew后，要检查Mac系统是否安装了 XCode Command Line Tools。 在Terminal下执行如下命令 xcode-select –install，如果系统要求安装此工具，则进行安装即可。 上述步骤完成后，我们可以直接使用brew install opencv安装OpenCV，安装好后的文件位于/usr/local/Cellar/opencv4文件夹下。 我们使用此种方式安装，可以发现得到的OpenCV安装文件里是不包含支持Java接口部分的编译文件的。 这对我们来说十分不友好。 使用Homebrew下载源码自动编译安装 对于OpenCV，Homebrew也是可以在线自动下载源码到本地并自动进行编译安装的。 在安装Homebrew和 XCode Command Line Tools 后，使用Homebrew安装cmake工具brew install cmake，这是OpenCV编译需要用到的工具。 同时我们需要安装Apache Ant工具，brew install ant，因为OpenCV编译生成Java接口文件需要用到。 然后我们使用指令brew edit opencv，可以打开查看opencv的编译项。如下图： 然后找到编译项里的 -DBUILD_opencv_java=OFF，将其改为 -DBUILD_opencv_java=ON，然后保存配置。 由上面的操作可以看到 Homebrew 的 OpenCV编译默认是不包含Java相关支持的。 我们使用brew install –build-from-source opencv下载源码到本地自动进行编译安装。 PS:我按照上述操作后确实会从GitHub上自动下载最新版源码并进行编译，但是在编译过程中遇到了一个”has no symbol”的错误。导致始终无法make成功，也未找到相关解决办法，于是放弃了该种安装办法。 错误详情如下： 1Linking CXX static library ../../lib/libopencv_core.a /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(hal_internal.cpp.o) has no symbols /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(opencl_clamdblas.cpp.o) has no symbols /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(opencl_clamdfft.cpp.o) has no symbols /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(opencl_core.cpp.o) has no symbols /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(hal_internal.cpp.o) has no symbols /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(opencl_clamdblas.cpp.o) has no symbols /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(opencl_clamdfft.cpp.o) has no symbols /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: ../../lib/libopencv_core.a(opencl_core.cpp.o) has no symbols 我在OpenCV官网上也看到别人遇到此种情况，尚未有回复。 https://answers.opencv.org/question/104758/has-no-symbol-error-on-mac/ 手动编译OpenCV 我们使用OpenCV的源码来编译OpenCV，同样我们需要cmake工具，使用brew install cmake进行安装。 安装Apache Ant工具，brew install ant。 从OpenCV官网上下载 OpenCV的源码 Sources，我下载的是opencv-4.1.1.zip。 我们将它解压，得到源码文件。 进入到opencv源码目录 cd /Users/xxx/Desktop/opencv-4.1.1。 我们在该目录下创建一个build文件夹用于存放编译后的文件mkdir build。 进入到build目录cd build。 在此目录下，我们配置cmake的编译参数cmake -DBUILD_SHARED_LIBS=OFF -D CMAKE_INSTALL_PREFIX=/Users/xxx/Applications/opencv-4.1.1 .. PS： CMAKE_INSTALL_PREFIX 指的是编译完成后安装的路径前缀，我们会把OpenCV安装到此目录下。 -DBUILD_SHARED_LIBS=OFF 指的是OpenCV作为一组静态库构建，不去动态依赖其它库，而是包含全部代码的编译。 我们可以使用brew info opencv查看OpenCV的依赖库，如果-DBUILD_SHARED_LIBS=ON 则OpenCV会依赖已经存在的库来进行编译，可能会产生问题。 构建完成后我们开始进行安装，使用make -j6来进行任务。 PS：-j6 指的是并行6个任务来进行构建，当然也可以 -j5 五个并行任务等等。 检查cmake的输出并确保java是“待构建”模块之一。如果不是的话，很可能你缺少了一个依赖关系您应该通过查看cmake输出中未找到的与java相关的工具并安装它们来进行故障排除。 完成后，我们使用sudo make install完成最后的安装任务。 这样，在我们上面提到的安装目录里，就会找到OpenCV的安装文件，当然，也能找到Java接口部分的文件。 位于 ${CMAKE_INSTALL_PREFIX} /share/java/opencv4文件夹下，如图： Linux环境下的OpenCV环境搭建Linux环境下不再过多叙述，有需要的可以查看如下相关文章。 installing-opencv-for-java 文章内也包含Windows和Mac的安装教程。 OpenCV的使用这儿我们先来了解下OpenCV的简单使用，后面在深入了解。 基础使用先来了解下OpenCV的基本类Mat，它是一个图像的数据格式矩阵。用来存储图像的数据结构。 我们知道图像是由R (Red)、 G (Green) 、 B (Blue) 三原色构成，那Mat存储这三原色数据，会是一个三维数组？ 其实不是的，Mat中是使用二维数组存储图像数据的，如何存储呢？如下图： 可以看到Mat彩色图像的存储形式是三列当做一列，由 BGR 三个通道，存储在一个平面内，这儿彩色图的一个像素会占用3个字节。 对于灰度图，由于没有颜色要求，因此Mat的存储灰度图的格式和彩色图略有些不同，如下图： 可以看到灰度图的一个像素在Mat中会占用1个字节。 我们使用OpenCV里的 Imgcodecs读取一张红色图片转为Mat，然后进行输出，如下： 1234567public static void main(String[] args) throws Exception&#123; addLibraryDir(vmOptions); System.loadLibrary(opencvLibName); Mat mat = Imgcodecs.imread("C:\\Users\\DELL-3020\\Desktop\\test12.jpg"); System.out.println(mat.dump()); Color color = new Color(250,0, 0);&#125; 我们dump后可以看到Mat的数据内容。 Scalar是OpenCV里的可认为是颜色的向量类，它最多可以有四个入参。 1Scalar scalar = new Scalar(255,0,0,0); 它的参数分别表示该颜色向量的 B G R 和 透明度，上面我们就创建了一个纯蓝色透明度为0（不透明）的颜色向量。 我们使用setTo方法，可以将原来Mat图的红色全部替换为蓝色，如下： 12345678910public static void main(String[] args) throws Exception&#123; addLibraryDir(vmOptions); System.loadLibrary(opencvLibName); Mat mat = Imgcodecs.imread("C:\\Users\\DELL-3020\\Desktop\\test13.jpg"); Scalar scalar = new Scalar(255,0,0,0); mat.setTo(scalar); System.out.println(mat.dump()); Imgcodecs.imwrite("C:\\Users\\DELL-3020\\Desktop\\test15.jpg",mat); Color color = new Color(0,0, 255 );&#125; 我们会生成一张蓝色图片。 人脸检测CascadeClassifier级联分类器是OpenCV里进行图片对象识别的检测器。 一个分类器的生成： 用一个对象的几百个样本（或者更多）作为正面例子，需要将它们缩放到相同大小；负面例子样本可用任意多张（非正面例子）和正面例子相同大小的图片即可。用它们完成分类器的训练。 而对于级联分类器，则是由若干个分类器组成，它们占用不同的权重组成，比如人脸级联分类器，要检测人脸，则需要有眼睛部分，则眼睛分类器可以作为人脸级联分类器的一部分。 我们训练特定样本后，可以得到一个分类器文件（xml），加载这个xml文件后，我们可以对一些测试样本进行检测，看它是否属于目标样本。 关于训练的内容我们后面在聊，现在OpenCV内置了一些分类器文件例子，我们来看下，如下图： 可以看到 opencv\build\etc文件夹下有haarcascades和lbpcascades两种模式的分类器，我们以Haar分类器来看。 可以看到它下面的一些分类器文件demo，根据英文名称还是比较好确认它们是对于哪种类型图片进行识别的。 根据以上，我们来看一个寻找图片人脸并进行裁剪的例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Slf4jpublic class FaceDetector &#123; /** * 人脸识别xml文件地址 */ public static final String CASCADE_FACE_FILENAME = "D:\\opencv\\sources\\data\\haarcascades\\haarcascade_frontalface_alt.xml"; /** * opencv 安装的版本号 */ public static final String opencvLibName = "opencv_java411"; /** * VM Options */ public static final String vmOptions = "D:/opencv/build/java/x64"; /** * 人脸裁剪 * @param sourceImage * @param targetFilePath * @return */ public static List&lt;String&gt; getFaceImages(String sourceImage,String targetFilePath)&#123; File file = new File(sourceImage); if(!file.exists())&#123; throw new RuntimeException("文件不存在！！"); &#125; CascadeClassifier faceDetector = new CascadeClassifier(CASCADE_FACE_FILENAME); if(faceDetector.empty())&#123; throw new RuntimeException("处理文件时发生异常！"); &#125; Mat image = Imgcodecs.imread(sourceImage); MatOfRect faceDetections = new MatOfRect(); // 进行人脸检测 faceDetector.detectMultiScale(image, faceDetections); Rect[] rects = faceDetections.toArray(); if(rects.length &lt;=0 )&#123; throw new RuntimeException("原图片上未检测到人脸！！"); &#125; log.info("检测到人脸数量：&#123;&#125;",rects.length); List&lt;String&gt; result = new ArrayList&lt;&gt;(); int i = 0; for (Rect rect : rects) &#123; // 进行图片裁剪 Mat mat = new Mat(image,new Rect(rect.x, rect.y,rect.width,rect.height)); // 输出人脸图片 String outFilePath = targetFilePath +"/"+ i+".jpg"; Imgcodecs.imwrite(outFilePath, mat); i++; &#125; return result; &#125; public static void main(String[] args) throws Exception&#123; addLibraryDir(vmOptions); System.loadLibrary(opencvLibName); getFaceImages("C:/Users/DELL-3020/Desktop/test.jpg","C:\\Users\\DELL-3020\\Desktop"); &#125;&#125; 我们寻找一张照片（包含人脸），可以看到成功生成人脸图。 其检测裁剪逻辑主要如下： 级联分类器加载样本分类器文件haarcascade_frontalface_alt.xml，检测该文件的正确性!faceDetector.empty()； 将待检测图片读为Mat，Imgcodecs.imread； 进行人脸检测faceDetector.detectMultiScale，该方法除了传入Mat外，还要传入一个MatOfRect，它是一个Rect数组，用来存放识别返回的”感兴趣”的区域（人脸区域）； “感兴趣”的区域可以有多个，如果一个也没有，我们就可以认为这张待检测图片上不存在人脸； 如果有的话，我们可以根据矩形区域裁剪得到人脸。 关于这部分我们先介绍到这儿，有兴趣的可以了解下例子文件里的其他级联分类器。 总结上面我们主要说了OpenCV的安装、开发环境的配置等内容。 对于OpenCV的安装，我们根据一些文档手册，安装还是比较简单的；开发环境的配置方面，由于OpenCV主要以C++编写、开发和使用，因此应用在Java上还是不那么方便的（需要加载原生Lib库）。 对于OpenCV的使用，这儿说的比较少，只是简单的介绍了它的一些使用，其原理也未作深入分析。 后面我们继续来看下OpenCV的一些有意思的地方（图片训练、图片识别等）。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>OpenCV</tag>
        <tag>人脸识别</tag>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL规范]]></title>
    <url>%2Fblog%2FMySQL%E8%A7%84%E8%8C%83.html</url>
    <content type="text"><![CDATA[建表规约 【强制】 表达是与否概念的字段，必须使用is_xxx 的方式命名，数据类型是unsigned tinyint (1表示是，0表示否)。 说明: 任何字段如果为非负数，必须是unsigned。 注意: POJO类中的任何布尔类型的变量，都不要加is前缀，所以，需要在设置 从is_xxx到Xxx的映射关系。数据库表示是与否的值，使用tinyint类型，坚持is_xxx的命名方式是为了明确其取值含义与取值范围。 正例: 表达逻辑删除的字段名is_deleted，1 表示删除，0 表示未删除。 【强制】 表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。 说明: MySQL在Windows下不区分大小写，但在Linux下默认是区分大小写。因此，数据库名、表名、字段名，都不允许出现任何大写字母，避免节外生枝。 正例: aliyun_admin，rdc_config，level3_name 反例:AliyunAdmin，rdcConfig，level_3_name 【强制】 表名不使用复数名词。 说明: 表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于DO类名也是单数形式，符合表达习惯。 【强制】 禁止保留字，如desc、range、match、delayed等，请参考MySQL官方保留字。 【强制】 主键索引名为pk_字段名;唯一索引名为uk_字段名;普通索引名则为 idx_字段名。 说明: pk_ 即 primary key； uk_ 即 unique key;idx_即index的简称。 【强制】 小数类型为decimal，禁止使用float和double。 说明: float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不正确的结果。如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。 【强制】 如果存储的字符串长度几乎相等，使用 char 定长字符串类型。 【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text，独立出来一张表，用主键来对应，避免影响其它字段索引效率。 【强制】 表必备三字段:id, gmt_create, gmt_modified。 说明: 其中id必为主键，类型为bigintunsigned、单表时自增、步长为1。gmt_create, gmt_modified 的类型均为 datetime 类型，前者现在时表示主动创建，后者过去分词表示被动更新。 【推荐】 表的命名最好是加上“业务名称_表的作用”。 正例: alipay_task / force_project / trade_config 【推荐】 库名与应用名称尽量一致。 【推荐】 如果修改字段含义或对字段表示的状态追加时，需要及时更新字段注释。 【推荐】 字段允许适当冗余，以提高查询性能，但必须考虑数据一致。 冗余字段应遵循: 不是频繁修改的字段。 不是 varchar 超长字段，更不能是 text 字段。 正例: 商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存储类目名称，避免关联查询。 【推荐】 单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。 说明: 如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。 【参考】 合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检索速度。 正例: 如下表，其中无符号值可以避免误存负数，且扩大了表示范围。 对象 年龄区间 类型 字节 表示范围 人 150岁之内 tinyint unsigned 1 无符号值:0 到 255 龟 数百岁 smallint unsigned 2 无符号值:0 到 65535 恐龙化石 数千万年 int unsigned 4 无符号值:0 到约 42.9 亿 太阳 约50亿年 bigint unsigned 8 无符号值:0 到约 10 的 19 次方 索引规约 【强制】 业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引。 说明: 不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的;另外，即使在应用层做了非常完善的校验控制，只要没有唯一索引，根据墨菲定律，必然有脏数据产生。 【强制】 超过三个表禁止 join。需要 join 的字段，数据类型必须绝对一致;多表关联查询时， 保证被关联的字段需要有索引。 说明: 即使双表 join 也要注意表索引、SQL 性能。 【强制】 在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度即可。 说明: 索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上，可以使用 count(distinct left(列名, 索引长度))/count(*)的区分度来确定。 【强制】 页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。 说明: 索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。 【推荐】 如果有 order by 的场景，请注意利用索引的有序性。order by 最后的字段是组合索引的一部分，并且放在索引组合顺序的最后，避免出现 file_sort 的情况，影响查询性能。 正例: where a=? and b=? order by c; 索引:a_b_c 反例: 索引中有范围查找，那么索引有序性无法利用，如:WHERE a&gt;10 ORDER BY b; 索引 a_b 无法排序。 【推荐】 利用覆盖索引来进行查询操作，避免回表。 * 说明: 如果一本书需要知道第 11 章是什么标题，会翻开第 11 章对应的那一页吗?目录浏览一下就好，这个目录就是起到覆盖索引的作用。 正例: 能够建立索引的种类分为主键索引、唯一索引、普通索引三种，而覆盖索引只是一种查询的一种效果，用explain的结果，extra列会出现:using index。 【推荐】 利用延迟关联或者子查询优化超多分页场景。 说明: MySQL 并不是跳过 offset 行，而是取 offset+N 行，然后返回放弃前 offset 行，返回 N 行，那当 offset 特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行 SQL 改写。 正例: 先快速定位需要获取的 id 段，然后再关联:SELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id 【推荐】 SQL 性能优化的目标:至少要达到 range 级别，要求是 ref 级别，如果可以是 consts 最好。 SELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id 说明: consts 单表中最多只有一个匹配行(主键或者唯一索引)，在优化阶段即可读取到数据。 ref 指的是使用普通的索引(normal index)。 range 对索引进行范围检索。 反例: explain 表的结果，type=index，索引物理文件全扫描，速度非常慢，这个 index 级别比较range 还低，与全表扫描是小巫见大巫。 【推荐】 建组合索引的时候，区分度最高的在最左边。 正例: 如果 where a=? and b=? ，如果 a 列的几乎接近于唯一值，那么只需要单建 idx_a 索引即可。 说明: 存在非等号和等号混合时，在建索引时，请把等号条件的列前置。如:where c&gt;? and d=? 那么即使 c 的区分度更高，也必须把 d 放在索引的最前列，即索引 idx_d_c。 【推荐】 防止因字段类型不同造成的隐式转换，导致索引失效。 【参考】 创建索引时避免有如下极端误解: 宁滥勿缺。认为一个查询就需要建一个索引。 宁缺勿滥。认为索引会消耗空间、严重拖慢更新和新增速度。 抵制唯一索引。认为业务的唯一性一律需要在应用层通过“先查后插”方式解决。 SQL 语句 【强制】 不要使用 count(列名)或 count(常量)来替代 count()，count()是 SQL92 定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。 说明: count(*)会统计值为 NULL 的行，而 count(列名)不会统计此列为 NULL 值的行。 【强制】 count(distinct col) 计算该列除 NULL 之外的不重复行数，注意 count(distinct col1, col2)如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。 【强制】 当某一列的值全是 NULL 时，count(col)的返回结果为 0，但sum(col)的返回结果为NULL，因此使用 sum()时需注意 NPE 问题。 正例: 可以使用如下方式来避免sum的NPE问题:SELECT IF(ISNULL(SUM(g)),0,SUM(g)) FROM table; 【强制】 使用 ISNULL()来判断是否为 NULL 值。 说明: NULL 与任何值的直接比较都为 NULL。 NULL&lt;&gt;NULL 的返回结果是NULL，而不是false。 NULL=NULL 的返回结果是NULL，而不是true。 NULL&lt;&gt;1 的返回结果是NULL，而不是true。 【强制】 在代码中写分页查询逻辑时，若 count 为 0 应直接返回，避免执行后面的分页语句。 【强制】 不得使用外键与级联，一切外键概念必须在应用层解决。 说明: 以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。 如果更新学生表中的student_id，同时触发成绩表中的 student_id 更新，即为级联更新。 外键与级联更新适用于单机低并发，不适合分布式、高并发集群; 级联更新是强阻塞，存在数据库更新风暴的风险; 外键影响数据库的插入速度。 【强制】 禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。 【强制】 数据订正(特别是删除、修改记录操作)时，要先 select，避免出现误删除，确认无误才能执行更新语句。 【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在1000 个之内。 【参考】 如果有国际化需要，所有的字符存储与表示，均以 utf-8 编码，注意字符统计函数的区别。 说明: SELECT LENGTH(“轻松工作”); 返回为12SELECT CHARACTER_LENGTH(“轻松工作”); 返回为4 如果需要存储表情，那么选择 utf8mb4 来进行存储，注意它与 utf-8 编码的区别。 【参考】 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但 TRUNCATE无事务且不触发 trigger，有可能造成事故，故不建议在开发代码中使用此语句。 说明: TRUNCATE TABLE 在功能上与不带 WHERE 子句的 DELETE 语句相同。 ORM 映射 【强制】 在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。 说明: 增加查询分析器解析成本。 增减字段容易与 resultMap 配置不一致。 无用字段增加网络消耗，尤其是 text 类型的字段。 【强制】 POJO 类的布尔属性不能加 is，而数据库字段必须加 is_，要求在 resultMap 中进行字段与属性之间的映射。 说明: 参见定义 POJO 类以及数据库字段定义规定，在中增加映射，是必须的。 在MyBatis Generator生成的代码中，需要进行对应的修改。 【强制】 不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义;反过来，每一个表也必然有一个 POJO 类与之对应。 说明: 配置映射关系，使字段与 DO 类解耦，方便维护。 【强制】 sql.xml 配置参数使用:#{}，#param# 不要使用${} 此种方式容易出现 SQL 注入。 【强制】 iBATIS自带的queryForList(String statementName,int start,int size)不推荐使用。 说明: 其实现方式是在数据库取到 statementName 对应的 SQL 语句的所有记录，再通过 subList取 start,size 的子集合。 正例: 123Map&lt; String, Object&gt; map = new HashMap&lt;&gt;();map.put("start", start);map.put("size", size); 【强制】 不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。 说明: resultClass=”Hashtable”，会置入字段名和属性值，但是值的类型不可控。 【强制】 更新数据表记录时，必须同时更新记录对应的 gmt_modified 字段值为当前时间。 【推荐】 不要写一个大而全的数据更新接口。传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3; 这是不对的。 执行 SQL 时，不要更新无改动的字段，一是易出错;二是效率低;三是增加 binlog 存储。 【参考】 @Transactional 事务不要滥用。事务会影响数据库的 QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。 【参考】 中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件;表示不为空且不为 null 时执行;表示不为 null 值时执行。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java图片处理相关]]></title>
    <url>%2Fblog%2FJava%E5%9B%BE%E7%89%87%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3.html</url>
    <content type="text"><![CDATA[前言最近总结了Java处理图片的一些方法，特此分享下。 其中主要涉及到2种不同的类包。 一种是Java自带的ImageIO、Graphics2D、BufferedImage等类的使用；另一种是使用了net.coobird.thumbnailator包下的图片相关类。 net.coobird.thumbnailator包中对图片的操作底层其实也使用了Java自带的图片操作类相关方法，这儿不做过多讨论，有兴趣的可以直接看下它的源码。 PS: net.coobird.thumbnailator这个工具包在2014年12月 0.4.8版本后就再也没有维护过了。 12345&lt;dependency&gt; &lt;groupId&gt;net.coobird&lt;/groupId&gt; &lt;artifactId&gt;thumbnailator&lt;/artifactId&gt; &lt;version&gt;0.4.8&lt;/version&gt;&lt;/dependency&gt; 正文我们来看一下相关的处理方法。 获取图片真实格式（与后缀无关）1234567891011121314151617public static String getExtension(String filePath) &#123; File file = new File(filePath); if (!file.exists()) &#123; throw new RuntimeException("图片地址不正确，请传入正确的图片地址！！！"); &#125; //图片真实格式 String format = ""; try (ImageInputStream iis = ImageIO.createImageInputStream(file)) &#123; Iterator&lt;ImageReader&gt; iter = ImageIO.getImageReaders(iis); if (iter.hasNext()) &#123; format = iter.next().getFormatName(); &#125; &#125; catch (IOException e) &#123; throw new RuntimeException("获取图片真实格式时出现异常！！" + e); &#125; return format;&#125; 该方法可以获得图片的真实格式，这个格式和图片的后缀无关。 其返回的格式为大写，如JPEG，PNG等。 PS: jpg和jpeg类型的图片返回的格式均为JPEG。 图片彩色转为黑白1234567891011121314151617181920/** * 图片彩色转为黑白 * * @param sourceImagePath 源图像地址 * @param targetImagePath 生成的目标图像地址（不包含文件名称） */public static String gray(String sourceImagePath, String targetImagePath) throws IOException &#123; //获取图片真实格式 String sourceFormat = getExtension(sourceImagePath).toLowerCase(); //黑白处理 BufferedImage src = ImageIO.read(new File(sourceImagePath)); ColorSpace cs = ColorSpace.getInstance(ColorSpace.CS_GRAY); ColorConvertOp op = new ColorConvertOp(cs, null); src = op.filter(src, null); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; ImageIO.write(src, sourceFormat, new File(targetFile)); return targetFile;&#125; 该方法可对彩色图片进行黑白处理。 图片格式转换A方法：使用Thumbnails进行图片格式转换 12345678910111213141516171819202122232425262728293031/** * 图片格式转换 A 方案 * * @param targetFormat 目标格式 jpg jpeg png 见枚举 * @param sourceFilePath 源文件路径 * @param targetFilePath 生成目标文件路径 * @return * @throws IOException */public static String convertA(String targetFormat, String sourceFilePath, String targetFilePath) throws IOException &#123; //校验 if (isBlank(targetFormat) || isBlank(sourceFilePath) || isBlank(targetFilePath)) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceFilePath).toLowerCase(); //获取图片后缀格式 String suffixFormat = sourceFilePath.substring(sourceFilePath.lastIndexOf(".") + 1).toLowerCase(); //目标格式 String targetFormatStr = targetFormat.toLowerCase(); //如果是目标格式，就直接返回 if (sourceFormat.equals(targetFormatStr) &amp;&amp; suffixFormat.equals(targetFormatStr)) &#123; return sourceFilePath; &#125; //使用Thumbnails处理 String targetFile = targetFilePath + "/" + getUUID() + "." + targetFormatStr; Thumbnails.of(sourceFilePath).scale(1.0f).outputFormat(targetFormatStr).toFile(targetFile); return targetFile;&#125; B方法：使用ImageIO进行图片格式转换 12345678910111213141516171819202122232425262728293031323334/** * 图片格式转换 B 方案 * * @param targetFormat 目标格式 jpg jpeg png 见枚举 * @param sourceFilePath 源文件路径 * @param targetFilePath 生成目标文件路径 * @return * @throws IOException */public static String convertB(String targetFormat, String sourceFilePath, String targetFilePath) throws IOException &#123; //校验 if (isBlank(targetFormat) || isBlank(sourceFilePath) || isBlank(targetFilePath)) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceFilePath).toLowerCase(); //获取图片后缀格式 String suffixFormat = sourceFilePath.substring(sourceFilePath.lastIndexOf(".") + 1).toLowerCase(); //目标格式 String targetFormatStr = targetFormat.toLowerCase(); //如果是目标格式，就直接返回 if (sourceFormat.equals(targetFormatStr) &amp;&amp; suffixFormat.equals(targetFormatStr)) &#123; return sourceFilePath; &#125; //getExtension方法已经校验了文件存不存在，这儿不用校验 File file = new File(sourceFilePath); //使用ImageIO处理 String targetFile = targetFilePath + "/" + getUUID() + "." + targetFormatStr; BufferedImage src = ImageIO.read(file); ImageIO.write(src, targetFormatStr, new File(targetFile)); return targetFile;&#125; 两种方法都可以实现图片格式转换。 图片旋转相关12345678910111213141516171819/** * 图片旋转 * @param sourceImage 原图片 * @param targetImagePath 生成图片路径 * @param angle 旋转角度 正数为顺时针旋转，负数为逆时针旋转 * @return * @throws IOException */public static String rotate(String sourceImage, String targetImagePath,double angle) throws IOException&#123; if(isBlank(sourceImage)||isBlank(targetImagePath))&#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; Thumbnails.of(sourceImage).scale(1.0f).rotate(angle).outputFormat(sourceFormat).toFile(targetFile); return targetFile;&#125; 这儿我们使用Thumbnails工具来实现图片的旋转。 图片缩放相关图片缩放有两种类型，一种是根据比例进行缩放，另一种是按照大小进行缩放。 我们分别来看一下。 A方法：使用Thumbnails进行图片比例缩放和大小缩放 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 图片缩放 * * @param sourceImage 原图片 * @param targetImagePath 生成的图片地址 * @param scaleWidth 宽度缩放比例 0&lt; scaleWidth &lt;1 表示缩小 scaleWidth &gt;1 表示放大 * @param scaleHeight 高度缩放比例 0&lt; scaleWidth &lt;1 表示缩小 scaleWidth &gt;1 表示放大 （宽高缩放比相同图片将进行等比缩放） * @return * @throws IOException */public static String scale(String sourceImage, String targetImagePath, double scaleWidth, double scaleHeight) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || scaleWidth &lt;= 0 || scaleHeight &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; Thumbnails.of(sourceImage).scale(scaleWidth, scaleHeight).toFile(targetFile); return targetFile;&#125;/** * 将图片调整到指定大小 * * @param sourceImage 原图片 * @param targetImagePath 生成目标图片地址 * @param width 调整后的宽度 * @param height 调整后的高度 * @param keep 是否保持宽高比 * keep = false * 比如照片长宽为 100*97 ，则 * size(200,500) 后照片变为 200*198 * size(300,198)后照片变为200*198 * size(50,100)后照片变为50*49 * size(80,49)后照片变为50*49 * 可见该种缩放会保持长宽比，可能导致长或宽有一项看起来不符合我们的预设值 * 设置了keep = true ，就会按照指定的宽高变化了 * @return * @throws IOException */public static String size(String sourceImage, String targetImagePath, int width, int height, boolean keep) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || width &lt;= 0 || height &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; Thumbnails.of(sourceImage).size(width, height).keepAspectRatio(keep).toFile(targetFile); return targetFile;&#125; B方法：使用ImageIO进行图片比例缩放和大小缩放 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * 图片缩放，不常用，使用ImageIO进行的缩放 * * @param sourceImage 原图片 * @param targetImagePath 生成目标图片地址 * @param scaleWidth 宽度缩放比例 0&lt; scaleWidth &lt;1 表示缩小 scaleWidth &gt;1 表示放大 * @param scaleHeight 高度缩放比例 0&lt; scaleWidth &lt;1 表示缩小 scaleWidth &gt;1 表示放大 * @return * @throws IOException */public static String scaleB(String sourceImage, String targetImagePath, double scaleWidth, double scaleHeight) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || scaleWidth &lt;= 0 || scaleHeight &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; //使用ImageIO处理 BufferedImage src = ImageIO.read(new File(sourceImage)); int width = src.getWidth(); int height = src.getHeight(); width = (int) (width * scaleWidth); height = (int) (height * scaleHeight); Image image = src.getScaledInstance(width, height, Image.SCALE_DEFAULT); BufferedImage tag = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics g = tag.getGraphics(); g.drawImage(image, 0, 0, null); g.dispose(); ImageIO.write(tag, sourceFormat, new File(targetFile)); return targetFile;&#125;/** * 缩放图像（按照高度和宽度），不推荐使用 * * @param sourceImage 原图像 * @param targetImagePath 生成的图像地址 * @param width 要缩放到的宽 * @param height 要缩放到的高 * @param keep 比例不对时是否需要补白：true为补白; false为不补白 * @return * @throws IOException */public static String sizeB(String sourceImage, String targetImagePath, int width, int height, boolean keep) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || width &lt;= 0 || height &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; //缩放比例 double ratio = 0.0; BufferedImage bi = ImageIO.read(new File(sourceImage)); Image image = bi.getScaledInstance(width, height, Image.SCALE_SMOOTH); // 计算比例 if ((bi.getHeight() &gt; height) || (bi.getWidth() &gt; width)) &#123; if (bi.getHeight() &gt; bi.getWidth()) &#123; ratio = (double) (height / bi.getHeight()); &#125; else &#123; ratio = (double) (width / bi.getWidth()); &#125; AffineTransformOp op = new AffineTransformOp(AffineTransform.getScaleInstance(ratio, ratio), null); image = op.filter(bi, null); &#125; //是否需要填充白色块 if (keep) &#123; BufferedImage temp = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g = temp.createGraphics(); g.setColor(Color.WHITE); g.fillRect(0, 0, width, height); if (width == image.getWidth(null)) &#123; g.drawImage(image, 0, (height - image.getHeight(null)) / 2, image.getWidth(null), image.getHeight(null),Color.WHITE, null); &#125; else &#123; g.drawImage(image, (width - image.getWidth(null)) / 2, 0, image.getWidth(null), image.getHeight(null),Color.WHITE, null); &#125; g.dispose(); image = temp; &#125; ImageIO.write((BufferedImage) image, sourceFormat, new File(targetFile)); return targetFile;&#125; 我们一般使用Thumbnails工具类提供的缩放来处理即可，这是比较常用的。 图片压缩相关图片压缩也大致有两种形式，一种是指定图片质量系数进行压缩，另一种是根据大小进行压缩。 PS：这儿对于根据大小进行压缩的意思是图片压缩后大小不会超过指定值，而不是压缩到指定值，因为理论上压缩到指定值是十分困难的，也是不必要的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 图片压缩 * * @param sourceImage 原图片 * @param targetImagePath 生成的图片地址 * @param quality 图片质量系数 [ 0,1] * @return * @throws IOException */public static String compress(String sourceImage, String targetImagePath, float quality) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || quality &lt; 0 || quality &gt; 1) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; //控制图片生成质量，实际会压缩图片 Thumbnails.of(sourceImage).scale(1.0f).outputQuality(quality).outputFormat(sourceFormat).toFile(targetFile); return targetFile;&#125;/** * 压缩图片文件 （文件大小不会超过指定值） * * @param sourceImage 原文件 * @param targetImagePath 生成的文件地址 * @param maxSize 文件被压缩后允许的最大大小，单位 byte * @return * @throws IOException */public static String compress(String sourceImage, String targetImagePath, long maxSize) throws IOException&#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || maxSize &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; File source = new File(sourceImage); long size = source.length(); //如果图片本身大小就不超过，就直接返回图片地址 if (size &lt;= maxSize) &#123; return sourceImage; &#125; //计算缩放比例 double scale = (double) (maxSize / size); //图片尺寸不变，压缩图片大小 Thumbnails.of(source).scale(1.0f).outputQuality(scale).outputFormat(sourceFormat).toFile(targetFile); return targetFile;&#125; 我们同样使用了Thumbnails工具类进行处理。 图片切割相关切割主要有两种形式，一种是在原图指定位置切割一张指定大小的图片，另一种是把图片切割成若干份。 A形式：从原图指定位置切割（裁剪）一张小图片 可以通过Thumbnails工具类或者ImageIO实现，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * 从原图指定位置裁剪一张小图片 * * @param sourceImage 原图 * @param targetImagePath 生成的图片位置 * @param positions 裁剪位置 * @param width 裁剪宽 * @param height 裁剪高 * @return * @throws IOException */public static String cutA(String sourceImage, String targetImagePath, Positions positions, int width, int height) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || positions == null || width &lt;= 0 || height &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; Thumbnails.of(sourceImage).scale(1.0f).sourceRegion(positions, width, height).outputFormat(sourceFormat).toFile(targetFile); return targetFile;&#125;/** * 从原图指定位置裁剪一张小图片 * * @param sourceImage 原图 * @param targetImagePath 生成的图片位置 * @param x 裁剪位置x坐标 * @param y 裁剪位置y坐标 * @param width 裁剪宽 * @param height 裁剪高 * @return * @throws IOException */public static String cutB(String sourceImage, String targetImagePath, int x, int y, int width, int height) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || x &lt; 0 || y &lt; 0 || width &lt;= 0 || height &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; Thumbnails.of(sourceImage).scale(1.0f).sourceRegion(x, y, width, height).outputFormat(sourceFormat).toFile(targetFile); return targetFile;&#125;/** * 使用ImageIO进行的切割，切割指定位置的一张图片 * * @param sourceImage 原图 * @param targetImagePath 生成图片位置 * @param x 切割图片的位置x * @param y 切割图片的位置y * @param width 切割图片的宽 * @param height 切割图片的高 * @return * @throws IOException */public static String cutC(String sourceImage, String targetImagePath, int x, int y, int width, int height) throws IOException &#123; if (isBlank(sourceImage) || isBlank(targetImagePath) || x &lt; 0 || y &lt; 0 || width &lt;= 0 || height &lt;= 0) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetImagePath + "/" + getUUID() + "." + sourceFormat; // 使用ImageIO处理 BufferedImage bi = ImageIO.read(new File(sourceImage)); int srcWidth = bi.getHeight(); int srcHeight = bi.getWidth(); Image image = bi.getScaledInstance(srcWidth, srcHeight,Image.SCALE_DEFAULT); // 四个参数分别为图像起点坐标和宽高 // 即: CropImageFilter(int x,int y,int width,int height) ImageFilter cropFilter = new CropImageFilter(x, y, width, height); Image img = Toolkit.getDefaultToolkit().createImage(new FilteredImageSource(image.getSource(), cropFilter)); BufferedImage tag = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics g = tag.getGraphics(); g.drawImage(img, 0, 0, width, height, null); g.dispose(); // 输出为文件 ImageIO.write(tag, sourceFormat, new File(targetFile)); return targetFile;&#125; B形式：将原图分割成若干小图片 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116/** * 分割图片 * * @param filePath 图片地址 * @param rows 纵向分割份数 * @param cols 横向分割份数 * rows = 2 cols = 2 则图片会分成4个相等大小的图片 * rows = 2 cols = 1 则图片会分成2个大小相等的图片，分割为高度切割为2半 * @param splitImageBasePath 输出小图片的基础路径 * @return * @throws IOException */public static String[][] splitImage(String filePath, int rows, int cols, String splitImageBasePath) throws IOException &#123; if (rows &lt;= 0 || cols &lt;= 0 || isBlank(splitImageBasePath) || isBlank(filePath)) &#123; throw new RuntimeException("参数错误！！"); &#125; String imageType = getExtension(filePath); imageType = imageType.toLowerCase(); if (!(JPG.equals(imageType) || JPEG.equals(imageType) || PNG.equals(imageType))) &#123; throw new RuntimeException("请使用jpg、jpeg、png类型的图片"); &#125; // 读入文件 File file = new File(filePath); String fileName = file.getName(); String fileNamePrefix = fileName.substring(0, fileName.lastIndexOf(".")); FileInputStream fis = new FileInputStream(file); BufferedImage image = ImageIO.read(fis); // 计算每个小图的宽度和高度 int chunkWidth = image.getWidth() / cols; int chunkHeight = image.getHeight() / rows; BufferedImage[][] imgs = new BufferedImage[rows][cols]; for (int x = 0; x &lt; rows; x++) &#123; for (int y = 0; y &lt; cols; y++) &#123; //设置小图的大小和类型 imgs[x][y] = new BufferedImage(chunkWidth, chunkHeight, image.getType()); //写入图像内容 Graphics2D gr = imgs[x][y].createGraphics(); gr.drawImage(image, 0, 0, chunkWidth, chunkHeight, chunkWidth * y, chunkHeight * x, chunkWidth * y + chunkWidth, chunkHeight * x + chunkHeight, null); gr.dispose(); &#125; &#125; String[][] splitImages = new String[rows][cols]; // 输出小图 for (int i = 0; i &lt; imgs.length; i++) &#123; for (int j = 0; j &lt; imgs[i].length; j++) &#123; String splitImagePath = splitImageBasePath + "/" + fileNamePrefix + i + "-" + j + "." + imageType; splitImages[i][j] = splitImagePath; ImageIO.write(imgs[i][j], imageType, new File(splitImagePath)); &#125; &#125; return splitImages;&#125;/** * 图片分割 * * @param filePath * @param rows * @param cols * @param splitImageBasePath * @return * @throws IOException */public static List&lt;String&gt; splitImageReturnList(String filePath, int rows, int cols, String splitImageBasePath) throws IOException &#123; String[][] strings = splitImage(filePath, rows, cols, splitImageBasePath); List&lt;String&gt; list = new ArrayList&lt;&gt;(rows * cols); for (String[] strs : strings) &#123; list.addAll(Arrays.asList(strs)); &#125; return list;&#125;/** * 根据单位图片大小切割图片 * @param filePath 原图片 * @param imageWidth 单位图片宽度 * @param imageHeight 单位图片高度 * @param splitImageBasePath 生成图片基础路径 * @return * @throws IOException */public static String[][] splitImageB(String filePath, int imageWidth, int imageHeight, String splitImageBasePath) throws IOException &#123; if(isBlank(filePath)||imageWidth&lt;=0||imageHeight&lt;=0||isBlank(splitImageBasePath))&#123; throw new RuntimeException("参数错误！"); &#125; BufferedImage bufferedImage = ImageIO.read(new File(filePath)); int srcWidth = bufferedImage.getHeight(); int srcHeight = bufferedImage.getWidth(); if(srcWidth&lt;imageWidth||srcHeight&lt;imageHeight)&#123; throw new RuntimeException("基础图片长宽不符合要求！"); &#125; int cols; int rows; // 计算切片的横向和纵向数量 if (srcWidth % imageWidth == 0) &#123; cols = srcWidth / imageWidth; &#125; else &#123; cols = (int) Math.floor((double)(srcWidth / imageWidth)) + 1; &#125; if (srcHeight % imageHeight == 0) &#123; rows = srcHeight / imageHeight; &#125; else &#123; rows = (int) Math.floor((double)(srcHeight / imageHeight)) + 1; &#125; return splitImage(filePath,rows,cols,splitImageBasePath);&#125; 上述图片分割代码我们可以把一张图片分成9块来制造九宫格图片等。 图片合并相关图片合并可以将两张或者多张图片按照顺序合成一整张图片。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/** * 合并图片 * * @param images 图片列表 * [1.jpg 2.jpg 3.jpg] * [4.jpg 5.jpg 6.jpg] * [7.jpg 8.jpg 9.jpg] * @param finalImageBasePath 生成的图片要存放的位置 * @return * @throws IOException */public static String mergeImage(String[][] images, String finalImageBasePath) throws IOException &#123; if (images == null || images.length == 0 || isBlank(finalImageBasePath)) &#123; throw new RuntimeException("参数不正确！！"); &#125; int rows = 0; int cols = 0; for (int i = 0; i &lt; images.length; i++) &#123; rows++; int temp = 0; for (int j = 0; j &lt; images[i].length; j++) &#123; temp++; &#125; if (i == 0) &#123; cols = temp; &#125; else &#123; if (cols != temp) &#123; throw new RuntimeException("需要保证a * b 格式的图片"); &#125; &#125; &#125; String imageType = getExtension(images[0][0]); imageType = imageType.toLowerCase(); if (!(JPG.equals(imageType) || JPEG.equals(imageType) || PNG.equals(imageType))) &#123; throw new RuntimeException("请使用jpg、jpeg、png类型的图片"); &#125; int chunkWidth, chunkHeight; int type; //创建BufferedImage BufferedImage[][] buffImages = new BufferedImage[rows][cols]; for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; cols; j++) &#123; getExtension(images[i][j]); buffImages[i][j] = ImageIO.read(new File(images[i][j])); &#125; &#125; type = buffImages[0][0].getType(); chunkWidth = buffImages[0][0].getWidth(); chunkHeight = buffImages[0][0].getHeight(); //设置拼接后图的大小和类型 BufferedImage finalImg = new BufferedImage(chunkWidth * cols, chunkHeight * rows, type); //写入图像内容 for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; cols; j++) &#123; finalImg.createGraphics().drawImage(buffImages[i][j], chunkWidth * j, chunkHeight * i, null); &#125; &#125; String filePath = finalImageBasePath + "/" + getUUID() + "." + imageType; //输出拼接后的图像 ImageIO.write(finalImg, imageType, new File(filePath)); return filePath;&#125;/** * 合并图片 * * @param imageList * @param rows * @param finalImageBasePath * @return * @throws IOException */public static String mergeImage(List&lt;String&gt; imageList, int rows, String finalImageBasePath) throws IOException &#123; if (isEmpty(imageList)) &#123; throw new RuntimeException("参数错误"); &#125; if (rows &lt; 0 || imageList.size() % rows != 0) &#123; throw new RuntimeException("参数错误"); &#125; int cols = imageList.size() / rows; String[][] strings = new String[rows][cols]; int temp = 0; for (int i = 0; i &lt; strings.length; i++) &#123; for (int j = 0; j &lt; strings[i].length; j++) &#123; strings[i][j] = imageList.get(temp); temp++; &#125; &#125; return mergeImage(strings, finalImageBasePath);&#125; 图片水印相关根据文字生成相应图片12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 根据文字生成JPG图片 * * @param text 要生成图片的文字 * @param picFontSize 字体大小设置 * @param bgColor 图片整体背景色 * @param fontColor 文字颜色 * @param font 字体 * @param basePath 生成的图片地址 * @return * @throws IOException */public static String createImageByText(String text, int picFontSize, Color bgColor, Color fontColor, Font font, String basePath) throws IOException &#123; //校验 if (isBlank(text) || picFontSize &lt;= 0 || bgColor == null || fontColor == null || font == null || isBlank(basePath)) &#123; throw new RuntimeException("参数错误！"); &#125; //用于字体适应的单位长度 float beautify = picFontSize / 20.0f; //图片宽高 int width = text.length() * picFontSize + (int) (5 * beautify); int height = picFontSize + (int) (5 * beautify); //生成图片 BufferedImage bufferedImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g = bufferedImage.createGraphics(); //背景颜色 g.setColor(bgColor); g.fillRect(0, 0, width, height); //抗锯齿设置 g.setRenderingHint(RenderingHints.KEY_ANTIALIASING, RenderingHints.VALUE_ANTIALIAS_ON); //字体颜色 g.setColor(fontColor); g.setFont(font.deriveFont((float) picFontSize)); //在指定坐标除添加文字 g.drawString(text, beautify, picFontSize); g.dispose(); String fileName = basePath + "/" + getUUID() + "." + PictureConstants.JPG; try (FileOutputStream out = new FileOutputStream(fileName)) &#123; ImageIO.write(bufferedImage, PictureConstants.JPEG, out); &#125; return fileName;&#125; 该方法可以根据输入的文字、字体、颜色等生成一张图片。 为图片添加图片水印12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 为图片在指定位置添加水印图片 * * @param watermark 水印图片 * @param sourceImage 原图片 * @param positions 位置 * @param opacity 透明度 [0.0 - 1.0] * @param targetFilePath 目标文件生成路径 * @return * @throws IOException */public static String watermarkPic(String watermark, String sourceImage, Positions positions, float opacity, String targetFilePath) throws IOException &#123; if (isBlank(watermark) || isBlank(sourceImage) || isBlank(targetFilePath) || positions == null || opacity &lt; 0 || opacity &gt; 1) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetFilePath + "/" + getUUID() + "." + sourceFormat; Thumbnails.of(sourceImage).scale(1.0f).watermark(positions, ImageIO.read(new File(watermark)), opacity).outputFormat(sourceFormat).toFile(targetFile); return targetFile;&#125;/** * 为图片在指定位置添加水印图片 * * @param watermark 水印图片 * @param sourceImage 原图片 * @param x 水印位置（距原图片中心 x方向的偏移量） x&gt;0 在图片中心下方 * @param y 水印位置（距原图片中心 y方向的偏移量） y&gt;0 在图片中心右侧 * @param opacity 透明度 [0.0 - 1.0] * @param targetFilePath 目标文件生成路径 * @return * @throws IOException */public static String watermarkPic(String watermark, String sourceImage, int x, int y, float opacity, String targetFilePath) throws IOException &#123; if (isBlank(watermark) || isBlank(sourceImage) || isBlank(targetFilePath) || opacity &lt; 0 || opacity &gt; 1) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetFilePath + "/" + getUUID() + "." + sourceFormat; Image src = ImageIO.read(new File(sourceImage)); int width = src.getWidth(null); int height = src.getHeight(null); BufferedImage image = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g = image.createGraphics(); g.drawImage(src, 0, 0, width, height, null); // 水印文件 Image waterPic = ImageIO.read(new File(watermark)); int waterWidth = waterPic.getWidth(null); int waterHeight = waterPic.getHeight(null); g.setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_ATOP, opacity)); g.drawImage(waterPic, (width - waterWidth) / 2 + x, (height - waterHeight) / 2 + y, waterWidth, waterHeight, null); // 水印文件结束 g.dispose(); ImageIO.write(image, sourceFormat, new File(targetFile)); return targetFile;&#125; 上述方法分别使用了Thumbnails工具类和ImageIO类为原图片添加水印图片。 为图片添加文字水印123456789101112131415161718192021222324252627282930313233343536373839404142/** * 添加文字水印 * * @param text 水印文字 * @param sourceImage 原图片 * @param fontColor 字体颜色 * @param font 字体 * @param x 字体水印位置（距原图片中心 x方向的偏移量） x&gt;0 在图片中心下方 * @param y 字体水印位置（距原图片中心 x方向的偏移量） y&gt;0 在图片中心右侧 * @param opacity 透明度 [0.0 - 1.0] * @param targetFilePath 目标文件生成路径 * @return * @throws IOException */public static String watermarkText(String text, String sourceImage, Color fontColor, Font font, int x, int y, float opacity, String targetFilePath) throws IOException &#123; if (isBlank(text) || isBlank(sourceImage) || fontColor == null || font == null || isBlank(targetFilePath) || opacity &lt; 0 || opacity &gt; 1) &#123; throw new RuntimeException("参数错误！"); &#125; //获取图片真实格式 String sourceFormat = getExtension(sourceImage).toLowerCase(); String targetFile = targetFilePath + "/" + getUUID() + "." + sourceFormat; Image src = ImageIO.read(new File(sourceImage)); int width = src.getWidth(null); int height = src.getHeight(null); BufferedImage image = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g = image.createGraphics(); g.drawImage(src, 0, 0, width, height, null); g.setColor(fontColor); g.setFont(font); g.setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_ATOP, opacity)); //抗锯齿设置 g.setRenderingHint(RenderingHints.KEY_ANTIALIASING, RenderingHints.VALUE_ANTIALIAS_ON); // 在指定坐标绘制水印文字 g.drawString(text, (width - (getLength(text) * font.getSize())) / 2 + x, (height - font.getSize()) / 2 + y); g.dispose(); ImageIO.write(image, sourceFormat, new File(targetFile)); return targetFile;&#125; 上述方法可为图片在指定位置添加文字水印。 PS： 这儿说下图片去水印的问题，一般情况下，对于一个项目，是保留原图，需要的时候再添加水印，从而实现添加水印和去除水印的效果。 而如果就是想给一张有水印的图片去除水印，可以使用[OpenCV](https://opencv.org/)等相关进行处理。 相关处理在这儿就不过多介绍了。 Java在图片深度处理这方面是很一般的，一般进行图片深度处理时，都会调用其它语言的API。 如上面提到的OpenCV，其核心算法就是由C++语言实现的，同时提供了其它语言的接口。 GIF图片的生成与转换GIF相关操作可以使用com.madgag.animated-gif-lib相关Jar包，这个工具类Maven地址如下： 12345&lt;dependency&gt; &lt;groupId&gt;com.madgag&lt;/groupId&gt; &lt;artifactId&gt;animated-gif-lib&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt;&lt;/dependency&gt; 这个工具包在 2017年7月维护到最新版本1.4后就再也没有维护过了。 这个工具包里有4个类AnimatedGifEncoder.java、GifDecoder.java、LZWEncoder.java、NeuQuant.java。 将传入的若干图片生成gif1234567891011121314151617181920212223242526272829303132333435363738/** * 根据传入的图片生成指定间隔的gif图像 * * @param imageList 待处理图片 * @param basePath 生成gif文件路径 * @param delay 每张图片间隔 * @return */public static String imageToGif(List&lt;String&gt; imageList, String basePath, int delay) throws IOException &#123; //校验 if(isEmpty(imageList) || isBlank(basePath) || delay &lt; 0)&#123; throw new RuntimeException("参数错误！"); &#125; String fileName = basePath + "/" + getUUID() + "." + PictureConstants.GIF; File file = new File(fileName); try ( ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); FileOutputStream fileOutputStream = new FileOutputStream(file); ) &#123; //生成GIF AnimatedGifEncoder e = new AnimatedGifEncoder(); e.start(byteArrayOutputStream); //设置延迟时间 e.setDelay(delay); for (String imagePath : imageList) &#123; e.addFrame(ImageIO.read(new FileInputStream(imagePath))); //e.setDelay(delay);//可以设置不同的延迟时间 &#125; e.finish(); //写入到文件 byte[] byteArray = byteArrayOutputStream.toByteArray(); fileOutputStream.write(byteArray); fileOutputStream.flush(); &#125; return fileName;&#125; 调用此方法，可以将传入的一系列图片生成一张gif，我们应尽量保证传入的图片格式一致。 将gif图片拆分我们这里以jpg为例，将gif图片拆分成一张张jpg格式的图片。 12345678910111213141516171819202122232425262728293031323334353637/** * GIF 图片转 jpg * @param gifPath GIF图片地址 * @param basePath 生成图片地址 * @return * @throws IOException */public static List&lt;String&gt; gifToImage(String gifPath, String basePath) throws IOException &#123; //校验 if(isBlank(gifPath) || isBlank(basePath))&#123; throw new RuntimeException("参数错误！"); &#125; File file = new File(gifPath); String fileName = file.getName(); String fileNamePrefix = fileName.substring(0, fileName.lastIndexOf(".")); String imagePath = basePath + "/" + fileNamePrefix + "_%s." + PictureConstants.JPG; //GIF处理 GifDecoder gifDecoder = new GifDecoder(); gifDecoder.read(gifPath); //frame个数 int count = gifDecoder.getFrameCount(); List&lt;String&gt; imageList = new ArrayList&lt;&gt;(count); for (int i = 0; i &lt; count; i++) &#123; //得到帧 BufferedImage bufferedImage = gifDecoder.getFrame(i); //int delay = gifDecoder.getDelay(i);//延迟时间 String imageFileName = String.format(imagePath, i); //生成jpg文件 try (FileOutputStream fileOutputStream = new FileOutputStream(imageFileName)) &#123; ImageIO.write(bufferedImage, PictureConstants.JPEG, fileOutputStream); &#125; imageList.add(imageFileName); &#125; return imageList;&#125; 总结将一些需要的代码记录下来形成自己的工具类包，提高自身经验总结，对以后是蛮有帮助的。 源码源码详见我的 Github]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>图片处理</tag>
        <tag>Thumbnailator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化]]></title>
    <url>%2Fblog%2FMySQL%E4%BC%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[几个问题 如何发现慢查询？ 为什么不建议使用订单号作为主键？ 为什么要在需要排序的字段上加索引？ 为什么字段加上函数或者计算就不走索引了？ 为什么不建议字段允许为NULL？ 理论MySQL逻辑架构MySQL逻辑架构整体分为三层，如下图： 客户端： 如，连接处理、授权认证、安全等功能 核心服务： MySQL大多数核心服务均在这一层 包括查询解析、分析、优化、缓存、内置函数（如，时间、数学、加密等） 所有的跨存储引擎的功能也在这一层，如，存储过程、触发器、视图等 存储引擎： 负责MySQL中的数据存储和读取 中间的服务层通过API与存储引擎通信，这些API屏蔽了不同存储引擎间的差异 重点解释下查询缓存： 对于select语句，在解析查询之前，服务器会先检查查询缓存（Query Cache）。 如果命中，服务器便不再执行查询解析、优化和执行的过程，而是直接返回缓存中的结果集。 MySQL执行过程下图是MySQL执行一个查询的过程。实际上每一步都比想象中的复杂，尤其优化器，更复杂也更难理解。 MySQL查询过程如下： 客户端将查询发送到MySQL服务器； 服务器先检查查询缓存，如果命中，立即返回缓存中的结果，否则进入下一阶段； 服务器对SQL进行解析、预处理，再由优化器生成对象的执行计划； MySQL根据优化器生成的执行计划，调用存储引擎API来执行查询； 服务器将结果返回给客户端，同时缓存查询结果。 InnoDB存储引擎 InnoDB索引结构在MySQL中索引是在存储引擎层实现的，而不是在服务器层实现的。不同存储引擎实现索引的方式也各有不同。 Index_Type： 主键 普通索引 唯一索引 Index_Method： HASH： 在HEAP表中，如果存储的数据重复度很低（也就是说基数很大），对该列数据以等值查询为主，没有范围查询、没有排序的时候，特别适合采用哈希索引。 例如这种SQL： 1SELECT … FROM t WHERE C1 = ?; — 仅等值查询 BTREE（B+树）： 在大多数场景下，都会有范围查询、排序、分组等查询特征，用B+树索引就可以了。B+树索引结构适用于绝大多数场景。 为什么使用B+树？ 一、从内存⻆度上说，数据库中的索引一般时在磁盘上，数据量大的情况可能无法一次性装入内存，B+树的设计可以允许数据分批加载。 二、从业务场景上说，如果只选择一个数据那确实是hash更快，但是数据库中经常会选中多条，这时候由于B+树索引有序，并且又有链表相连，它的查询效率比hash就快很多了。 B-树 为了描述B-Tree，首先定义一条数据记录为一个二元组[key, data]，key为记录的键值，对于不同数据记录，key是互不相同的；data为数据记录除key外的数据。 那么B-Tree是满足下列条件的数据结构： d为大于1的一个正整数，称为B-Tree的深度。 h为一个正整数，称为B-Tree的高度。 每个非叶子节点由n-1个key和n个指针组成，其中d&lt;=n&lt;=2d。 每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。 所有叶节点具有相同的深度，等于树高h。 key和指针互相间隔，节点两端是指针。 一个节点中的key从左到右非递减排列。 所有节点组成树结构。 每个指针要么为null，要么指向另外一个节点。 如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key_1)，其中v(key_1)为node的第一个key的值。 如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(key_m)，其中v(key_m)为node的最后一个key的值。 如果某个指针在节点node的左右相邻key分别是key_i和key{i+1}且不为null，则其指向节点的所有key小于v(key{i+1})且大于v(key_i)。 下图是一个d=2的B-Tree示意图。 B+Tree B-Tree有许多变种，其中最常用的是B+Tree，例如MySQL就普遍使⽤B+Tree实现其索引结构。 与B-Tree相比，B+Tree有以下不同点： 每个节点的指针上限为2d而不是2d+1； 内节点不存储data，只存储key； 叶子节点不存储指针。 下图是一个简单的B+Tree示意图。 一般来说，B+Tree比B-Tree更适合实现外存储索引结构，具体原因与外存储器原理及计算机存取原理有关。 带有顺序访问指针的B+Tree 一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 如图所示，在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 InnoDB索引实现 聚簇索引 在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 上图是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。 定义：数据行的物理顺序与列值（一般是主键那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。 辅助索引 InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在Col3上的一个辅助索引： 聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 实战慢查询我们总是希望MySQL能够获得更高的查询性能，最好的办法是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行而已。 slow_query_log 这个参数设置为ON，可以捕获执行时间超过一定数值的SQL语句。 long_query_time 当SQL语句执行时间超过此数值时，就会被记录到日志中，建议设置为1或者更短。 slow_query_log_file 记录日志的文件路径以及文件名。 log_queries_not_using_indexes 这个参数设置为ON，可以捕获到所有未使用索引的SQL语句，尽管这个SQL语句有可能执行得很快。 慢查询解读1234567891011121314User@Host: test[test] @ [127.0.0.1] Id: 85Query_time: 3.299343 Lock_time: 0.001484Rows_sent: 10 Rows_examined: 4844057SET timestamp=1557113299;SELECT *FROMsalariesORDER BYfrom_date,salaryLIMIT 2000000, 10; 从慢查询日志里面摘选一条慢查询日志，数据组成如下： 用户名 、用户的IP信息、线程ID号 执行花费的时间【单位：毫秒】 执行获得锁的时间 获得的结果行数 扫描的数据行数 这SQL执行的具体时间 具体的SQL语句 执行计划简要说明 列名 表格查询的顺序编号 降序查看，id相同的从上到下查查看。id可以为null ，当table为(union ,m,n )类型的时候，id为null，这个时候，id的顺序为 m跟 n的后面。 id 表查询的顺序编号 id相同，执行顺序由上至下；id不同，id值越大优先级越高，越先被执行 select_type 查询的方式 下文详细说明 table 表格名称 表名，别名，( union m,n ） partitions 分区名称 查询使用到表分区的分区名 type 表连接的类型 下文详细说明 possible_keys 可能使用到的索引 这儿的索引只是可能会有到，实际不一定会用到 key 使用到的索引 实际使用的索引 key_len 使用到索引的长度 比如多列索引，只用到最左的一列，那么使用到索引的长度则为该列的长度，故该值不一定等于 key 列索引的长度 ref 谓词的关联信息 当 join type 为 const、eq_ref 或者 ref 时，谓词的关联信息。可能为 ：null（非 const \ eq_ref \ ref join type 时）、const（常量）、关联的谓词列名。显示该表的索引字段关联了哪张表的哪个字段 rows 扫描的行数 该表格扫描到的行数。这儿注意在mysql里边是嵌套链接，所以，需要把所有rows相乘就会得到查询数据行关联的次数 filtered 实际显示行数占扫描rows的比例 实际显示的行数 = rows * filtered / 100 extra 特性使用 select_type SIMPLE，简单查询方式，不使用UNION跟子查询； PRIMARY，该表格位于最外层开始查询，通常会跟其他查询方式组合； SUBQUERY，子查询内层查询的第一个SELECT，结果不依赖于外部查询结果集（不会被数据库引擎改写的情况）； UNION，UNION 第一个SELECT 为PRIMARY，第二个及之后的所有SELECT 为 UNION SELECT TYPE； UNION RESULT，每个结果集的取出来后，会做合并操作，这个操作就是 UNION RESULT； DEPENDENT UNION，子查询中的UNION操作，从UNION 第二个及之后的所有SELECT语句的SELECT TYPE为 DEPENDENT UNION，这个一般跟DEPENDENT SUBQUERY⼀起结合应用，子查询中UNION 的第一个为DEPENDENT SUBQUERY； DEPENDENT SUBQUERY，子查询中内层的第一个SELECT，依赖于外部查询的结果集； DERIVED，查询使用内联视图； MATERIALIZED，子查询物化，表出现在非相关子查询中，并且需要进行物化时会出现MATERIALIZED关键词； UNCACHEABLE SUBQUERY，结果集无法缓存的子查询，需要逐次查询； UNCACHEABLE UNION，表示子查询不可被物化，需要逐次运行。 type性能排序：system-&gt;const-&gt;eq-ref-&gt;ref-&gt;fulltext-&gt;ref_or_null-&gt;index_merge-&gt;unique_subquery-&gt;index_subquery-&gt;range-&gt;index-&gt;ALL，一般来说，得保证查询至少达到range级别，最好能达到ref。 system：表只有一行记录，相当于系统表。 const：通过索引一次就找到，只匹配一行数据。主键或者唯一索引的常量查询。 eq_ref：join查询中，关联条件为主键或者唯一索引。 ref：非唯一性索引扫描，返回匹配某个单独值的所有行，用于=、&lt;或&gt;操作符带索引的列。 range：只检索给定范围的行，使用一个索引来选择行。一般使用between、&lt;、&gt;。 index：只遍历索引树。使用到索引，但不是索引查找。 all：全表扫描，性能最差。 extra重点以下几种情况： Using filesort：MySQL会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。若出现有该值，应该优化SQL语句。 Using temporary：使用临时表保存中间结果，比如MySQL在对查询结果排序时使用临时表，常见于group by和order by。若出现有该值，应该优化SQL语句。 Using index：索引覆盖，不止用到索引，而且没有回表。避免了访问表的数据行，效率不错。 优化案例优化案例可以参考这篇文章 MySql 使用Explain分析SQL语句及索引 优化口诀 全职匹配我最爱，最左前缀要遵守； 带头大哥不能死，中间兄弟不能断； 索引列上少计算，范围之后全失效； Like百分写最右，覆盖索引不写星； 不等空值还有or，索引失效要少用。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使Java项目生成可执行的exe文件]]></title>
    <url>%2Fblog%2F%E5%A6%82%E4%BD%95%E4%BD%BFJava%E9%A1%B9%E7%9B%AE%E7%94%9F%E6%88%90%E5%8F%AF%E6%89%A7%E8%A1%8C%E7%9A%84exe%E6%96%87%E4%BB%B6.html</url>
    <content type="text"><![CDATA[前言前些天，有个朋友问我可否使用Java制作一个exe程序，要求运行程序可以实现某些功能。 我出于好奇了解了一下，是可以使用Java制作一些客户端exe程序的。 过程感觉还是比较有意思的，特此记录下来。 正文因为这类程序一般都为客户端程序，涉及到较古老的JFrame，我就简单的使用一个例子来说明下。 我们比如说想制作一个读取文件大小的exe文件。 构建Java项目我们首先构建一个普通的Java Maven项目，如下图。 随便给项目起个名称。 这儿我们不需要任何maven依赖，如果项目较复杂可以引入需要的maven依赖等。 我们新建一个Frame实现文件读取大小功能。 12345678910111213141516171819202122232425262728293031323334353637383940public class FileHelper extends JFrame implements ActionListener &#123; JButton open=null; public static void main(String[] args) &#123; new FileHelper(); &#125; public FileHelper()&#123; open=new JButton("请选择要处理的文件"); this.add(open); this.setBounds(400, 200, 1000, 500); this.setVisible(true); this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); open.addActionListener(this); &#125; @Override public void actionPerformed(ActionEvent e) &#123; JFileChooser jfc=new JFileChooser(); jfc.setFileSelectionMode(JFileChooser.FILES_AND_DIRECTORIES ); jfc.showDialog(new JLabel(), "选择"); File file=jfc.getSelectedFile(); if(file.isDirectory())&#123; JOptionPane.showMessageDialog(null, "请选择一个文件", "信息", JOptionPane.INFORMATION_MESSAGE); &#125;else if(file.isFile())&#123; if(file.canRead())&#123; long bytes = file.length(); String size = ""; if(bytes &lt; 1024)&#123; size = bytes + "B"; &#125;else if(bytes &lt; 1048576)&#123; size = bytes/1024 + "KB"; &#125;else&#123; size = bytes/1048576 + "MB"; &#125; String message = "该文件大小为:"+size + "\r\n文件路径:"+file.getPath(); JOptionPane.showMessageDialog(null, message, "信息", JOptionPane.INFORMATION_MESSAGE); &#125;else&#123; JOptionPane.showMessageDialog(null, "该文件不可读", "信息", JOptionPane.INFORMATION_MESSAGE); &#125; &#125; &#125;&#125; 我们运行一下，可以成功启动此项目。 那我们如何将它转换为exe呢？ 生成可执行jar文件在生成exe之前，我们需要先生成可执行jar文件，再将jar文件转换成exe文件。 我们打开IDEA项目结构（File -&gt; Project Structure -&gt; Artifacts），如下： 使项目可以生成一个可执行的jar包。 这儿要注意选择好项目和启动Main入口后，MANIFSET.MF一定到生成到src文件夹下。 完成后会看到我们项目的一些信息，这儿我们配置生成的jar输出到项目目录out下，点击OK即可。 这时候在项目src目录下可以看到META-INF文件夹及文件夹里的MANIFSET.MF文件，没有就说明配置错了。 在IDEA里找到 Build -&gt; Build Artifacts，选择这个项目，点击Build。 完成后会看到我们生成的jar包。 我们copy出来并使用Java运行一下。 1java -jar framework-exe-app.jar 可以看到成功运行。 显然我们只得到了一个只能在有JRT环境上运行的jar包，下一步我们来把它转成exe文件。 生成exe文件将jar文件转换为exe文件，需要下载exe4j工具，exe4j。 下载完成后安装，会打开welcome界面，这个界面是用来输注册码用的，如果不输入，是可以直接next进行下去的。但若是不输入，双击生成的exe时，会alert出一段话，如图。必须点“确定”才可以执行程序。 关于License Key，网上可以找到，这儿也提供了一些。 12345678A-XVK258563F-1p4lv7mg7savA-XVK209982F-1y0i3h4ywx2h1A-XVK267351F-dpurrhnyarvaA-XVK204432F-1kkoilo1jy2h3rA-XVK246130F-1l7msieqiwqnqA-XVK249554F-pllh351kcke50A-XVK238729F-25yn13iea25iA-XVK222711F-134h5ta8yxbm0 配置后，直接点击Next，选择”JAR” in “EXE” mode，如下图。 点击Next，我们输入应用名（这儿我起名MyApp），然后文件输出路径我们新建一个文件夹，然后输出到这个文件夹下。 点击Next，在这个页面我们需要配置名称，应用图标可配置可以不配置，如果应用不允许多开则需要勾选 Allow 那个选项。 这儿在Advance Options里，还有一些其它可选项，有兴趣的童鞋可以自己看一下。 如果不想生成错误日志，可以去掉如图选项。 这儿我们在这里面选中生成64-bit的应用程序。 在第5步Java invocation页面，VM Parameters输入-Dappdir=${EXE4J_EXEDIR}，然后我们将我们生成的jar包引入，如下图： Class Path —&gt; + —&gt; Archive —&gt; 选择jar包 —&gt; OK 然后配置我们的程序入口。 点击Next，到达第6步，需要设置jre版本，如下图： 需要设置最小的jre版本，然后在Advance Options里，选择Search Sequence，我们设置使用自己的JRE，这样生成的应用程序可以在没有安装Java的系统上使用。 在这之前，我们需要一份JRE（Java Runtime Environment）文件，可以在Java安装目录里找到，或者在官网上下载。比如 jre-8u231。 将它复制到我们之前创建的MyApp目录里，在Search Sequence里选择它。（之前Search Sequence的默认的3个直接删除即可）。 点击Next，到达第7步，这儿是设置App启动图，可以不设置。 第8步默认即可。 点击Next，到达最后一步，可以点击Click Here to start Application按钮测试一下应用程序。 成功说明没有问题，点击Exit按钮即可。 最后它会问你要不要保存配置信息，保存的话可以下次使用这个配置，这个按自己需要来即可。 退出后我们在我们的MyApp文件夹下可以找到刚才生成的App，可以运行一下是没有问题的。 一些问题我们虽然制作完成了该App，可是我们发现，我们把应用程序exe文件单独移动到一个地方，运行时，就会报找不到JRE环境。 这是比较容易理解的，我们使用Java编译的class文件，最终转换成了exe文件，但是仍不能离开JRE环境运行，也就是需要将整个MyApp文件夹进行移动，或者作为一个项目（App）。 这显然和在一台机器上安装JRE没有本质区别…… 就是变成了我们使用了项目自带的JRE环境。 这也是没有办法的事，因为Java的平台无关性，就是基于它的JRE环境，脱离JRE环境也无法运行class文件，我们的exe其实只是一层包装而已。 可安装版本的exe文件即使是一层包装，能不能使它在优化下呢？起码别人一看起来不立马觉得别扭？ 其实我们可以继续对exe文件和jre部分继续进行包装，使它成为一个安装版本的可执行文件。 这儿我们需要使用到 Inno Setup，这儿我们可以下载它 Inno Setup Downloads 下载安装完成（过程略）后打开，如下图： 我们选择使用生成向导来一步步引导我们生成exe文件。 我们可以设置自己的App名字及一些基本信息。 项目的安装路径，和是否允许用户更改安装路径参数设置我们直接取默认就好。 找到我们刚生成的exe文件，然后添加必要文件（jre文件），注意需要将jre文件夹下的全部文件添加上。 PS: 这儿我们需要新建一个temp文件夹，将jre移动进来，然后选择temp，这样生成的MyApp.exe会和jre文件夹在同一级，不然MyApp.exe找不到JRE环境。 然后下一步，是配置应用图标创建显示的，如果允许用户不在桌面创建图标，可以如下配置： 下一步关于License File 我们直接跳过就行。 下一步是安装后使用者权限，我们默认安装后所有人都可以使用即可。 下一步设置安装语言，用默认English即可。 然后设置生成的文件路径，还有应用图标等。 然后下一步到完成。 编译时会问到是否保存编译脚本，依照自己选择即可。 编译效果图如下： 最后会生成可安装的exe文件，我们运行下，把它安装一下。 我们安装完成可以看一下它的目录，其实和我们刚才的一模一样，只不过现在变成了一整个安装exe，而且安装包体积也被压缩了。 总结这篇文章没什么特别要总结的，主要就是相关工具的使用。 可以看到我们一个小小的功能，但生成的exe是巨大的，但是随着项目文件变多，jre占用比例减小，就是逐渐可以接受的了。 现在Java已经很少在桌面客户端进行一些开发工作了，但了解一下Java-exe的生成过程也是比较有意思的。 今天的内容就到这里吧。 源码源码详见于我的 GitHub]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>exe</tag>
        <tag>exe4j</tag>
        <tag>Inno Setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java sort和parallelSort接口及其实现]]></title>
    <url>%2Fblog%2FJava-sort%E5%92%8CparallelSort%E6%8E%A5%E5%8F%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0.html</url>
    <content type="text"><![CDATA[前言上篇文章我们介绍了 排序算法（九）-Java源码中的DualPivotQuicksort，今天我们来看下sort接口的实现，看看JDK对数据排序这块到底做了哪些优化。 正文sort接口sort接口有多个重载的方法，我们整理下后，它们分别如下： 在 java.util.Arrays类里，调用 Array.sort(a)方法，可以对数组a进行排序，它有18个重载方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102//整个int数组排序public static void sort(int[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1, null, 0, 0);&#125;//对int数组from到to的位置进行排序public static void sort(int[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); DualPivotQuicksort.sort(a, fromIndex, toIndex - 1, null, 0, 0);&#125;//long数组public static void sort(long[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1, null, 0, 0);&#125;public static void sort(long[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); DualPivotQuicksort.sort(a, fromIndex, toIndex - 1, null, 0, 0);&#125;//short数组public static void sort(short[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1, null, 0, 0);&#125;public static void sort(short[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); DualPivotQuicksort.sort(a, fromIndex, toIndex - 1, null, 0, 0);&#125;//char数组public static void sort(char[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1, null, 0, 0);&#125;public static void sort(char[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); DualPivotQuicksort.sort(a, fromIndex, toIndex - 1, null, 0, 0);&#125;//byte数组public static void sort(byte[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1);&#125;public static void sort(byte[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); DualPivotQuicksort.sort(a, fromIndex, toIndex - 1);&#125;//float数组public static void sort(float[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1, null, 0, 0);&#125;public static void sort(float[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); DualPivotQuicksort.sort(a, fromIndex, toIndex - 1, null, 0, 0);&#125;//double数组public static void sort(double[] a) &#123; DualPivotQuicksort.sort(a, 0, a.length - 1, null, 0, 0);&#125;public static void sort(double[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); DualPivotQuicksort.sort(a, fromIndex, toIndex - 1, null, 0, 0);&#125;//Object数组public static void sort(Object[] a) &#123; if (LegacyMergeSort.userRequested) legacyMergeSort(a); else ComparableTimSort.sort(a, 0, a.length, null, 0, 0);&#125;public static void sort(Object[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) legacyMergeSort(a, fromIndex, toIndex); else ComparableTimSort.sort(a, fromIndex, toIndex, null, 0, 0);&#125;//泛型数组public static &lt;T&gt; void sort(T[] a, Comparator&lt;? super T&gt; c) &#123; if (c == null) &#123; sort(a); &#125; else &#123; if (LegacyMergeSort.userRequested) legacyMergeSort(a, c); else TimSort.sort(a, 0, a.length, c, null, 0, 0); &#125;&#125;public static &lt;T&gt; void sort(T[] a, int fromIndex, int toIndex, Comparator&lt;? super T&gt; c) &#123; if (c == null) &#123; sort(a, fromIndex, toIndex); &#125; else &#123; rangeCheck(a.length, fromIndex, toIndex); if (LegacyMergeSort.userRequested) legacyMergeSort(a, fromIndex, toIndex, c); else TimSort.sort(a, fromIndex, toIndex, c, null, 0, 0); &#125;&#125; 源码分析可以看到对于基本数据类型，因为排序稳定性不会对数据造成影响（两个一样的数据谁前谁后都可以），故使用了DualPivotQuicksort排序算法。 对于Object数组(没有继承Comparator接口的数据类型)，会先判断一个LegacyMergeSort.userRequested的值是否为真，如果为真就使用legacyMergeSort排序算法，否则就使用ComparableTimSort排序算法。 对于泛型数组T []，如果比较器Comparator为空，就按照Object []方式进行处理；如果有比较器的话，照样先判断LegacyMergeSort.userRequested的值是否为真，是的话就用legacyMergeSort排序算法，否则就使用TimSort排序算法。 其他地方的sort最终会调用Array.sort(a)方法。如java.util.Collections类里的sort(List list)方法，最终调用了Array.sort(T[] a)。 根据上面的分析，我们先来看看LegacyMergeSort.userRequested这个参数吧，因为它决定非基本数据类型数组到底是使用legacyMergeSort还是TimSort（ComparableTimSort是TimSort的Object []版本，也相当于TimSort）。 追踪源码，LegacyMergeSort.userRequested赋值过程如下： 123456static final class LegacyMergeSort &#123; private static final boolean userRequested = java.security.AccessController.doPrivileged( new sun.security.action.GetBooleanAction( "java.util.Arrays.useLegacyMergeSort")).booleanValue(); &#125; 可以看到它是可以通过系统设置进行配置的，java -Djava.util.Arrays.useLegacyMergeSort=true，可以设置使用老的归并排序。 这个值默认是false，即不使用归并排序，Java之所以有这部分判断，完全是为了兼容老版本，同时归并排序这部分将在未来移除（当前介绍版本为JDK1.8，在JDK11中发现已经移除）。 legacyMergeSort这个方法涉及到的就是归并排序，关于这部分，我们不再展示源码（Java未来版本也会移除），有兴趣的可以看看我之前的文章归并排序(MergeSort)部分。这两者唯一不同的是Java的legacyMergeSort在排序部分长度小于 INSERTIONSORT_THRESHOLD = 7 的时候，会使用插入排序，相当于提高了普通归并的效率。 TimSort或者ComparableTimSort我在之前文章中也有分析了，有兴趣的可以看看，这儿不过多介绍。排序算法（六）- TimSort。 关于Java源码里的DualPivotQuicksort内容详见这篇文章排序算法（九）-Java源码中的DualPivotQuicksort。 parallelSort接口看完串行排序接口，我们再来看下Java自带排序的并行版本parallelSort接口，看看它是如何实现并行排序的。 先看看它的几个重载方法,由于基本数据类型数组的parallelSort都是类似的，这儿我只拿int[]进行举例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384//int数据并行排序方法（其它基本数据类型数组和其类似，这儿代码就不在展示）public static void parallelSort(int[] a) &#123; int n = a.length, p, g; if (n &lt;= MIN_ARRAY_SORT_GRAN || (p = ForkJoinPool.getCommonPoolParallelism()) == 1) DualPivotQuicksort.sort(a, 0, n - 1, null, 0, 0); else new ArraysParallelSortHelpers.FJInt.Sorter (null, a, new int[n], 0, n, 0, ((g = n / (p &lt;&lt; 2)) &lt;= MIN_ARRAY_SORT_GRAN) ? MIN_ARRAY_SORT_GRAN : g).invoke();&#125; public static void parallelSort(int[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); int n = toIndex - fromIndex, p, g; if (n &lt;= MIN_ARRAY_SORT_GRAN || (p = ForkJoinPool.getCommonPoolParallelism()) == 1) DualPivotQuicksort.sort(a, fromIndex, toIndex - 1, null, 0, 0); else new ArraysParallelSortHelpers.FJInt.Sorter (null, a, new int[n], fromIndex, n, 0, ((g = n / (p &lt;&lt; 2)) &lt;= MIN_ARRAY_SORT_GRAN) ? MIN_ARRAY_SORT_GRAN : g).invoke();&#125;//泛型数组并行排序方法（无比较器）public static &lt;T extends Comparable&lt;? super T&gt;&gt; void parallelSort(T[] a) &#123; int n = a.length, p, g; if (n &lt;= MIN_ARRAY_SORT_GRAN || (p = ForkJoinPool.getCommonPoolParallelism()) == 1) TimSort.sort(a, 0, n, NaturalOrder.INSTANCE, null, 0, 0); else new ArraysParallelSortHelpers.FJObject.Sorter&lt;T&gt; (null, a, (T[])Array.newInstance(a.getClass().getComponentType(), n), 0, n, 0, ((g = n / (p &lt;&lt; 2)) &lt;= MIN_ARRAY_SORT_GRAN) ? MIN_ARRAY_SORT_GRAN : g, NaturalOrder.INSTANCE).invoke();&#125;public static &lt;T extends Comparable&lt;? super T&gt;&gt;void parallelSort(T[] a, int fromIndex, int toIndex) &#123; rangeCheck(a.length, fromIndex, toIndex); int n = toIndex - fromIndex, p, g; if (n &lt;= MIN_ARRAY_SORT_GRAN || (p = ForkJoinPool.getCommonPoolParallelism()) == 1) TimSort.sort(a, fromIndex, toIndex, NaturalOrder.INSTANCE, null, 0, 0); else new ArraysParallelSortHelpers.FJObject.Sorter&lt;T&gt; (null, a, (T[])Array.newInstance(a.getClass().getComponentType(), n), fromIndex, n, 0, ((g = n / (p &lt;&lt; 2)) &lt;= MIN_ARRAY_SORT_GRAN) ? MIN_ARRAY_SORT_GRAN : g, NaturalOrder.INSTANCE).invoke();&#125;//泛型数组并行排序方法（有比较器）public static &lt;T&gt; void parallelSort(T[] a, Comparator&lt;? super T&gt; cmp) &#123; if (cmp == null) cmp = NaturalOrder.INSTANCE; int n = a.length, p, g; if (n &lt;= MIN_ARRAY_SORT_GRAN || (p = ForkJoinPool.getCommonPoolParallelism()) == 1) TimSort.sort(a, 0, n, cmp, null, 0, 0); else new ArraysParallelSortHelpers.FJObject.Sorter&lt;T&gt; (null, a, (T[])Array.newInstance(a.getClass().getComponentType(), n), 0, n, 0, ((g = n / (p &lt;&lt; 2)) &lt;= MIN_ARRAY_SORT_GRAN) ? MIN_ARRAY_SORT_GRAN : g, cmp).invoke();&#125;public static &lt;T&gt; void parallelSort(T[] a, int fromIndex, int toIndex, Comparator&lt;? super T&gt; cmp) &#123; rangeCheck(a.length, fromIndex, toIndex); if (cmp == null) cmp = NaturalOrder.INSTANCE; int n = toIndex - fromIndex, p, g; if (n &lt;= MIN_ARRAY_SORT_GRAN || (p = ForkJoinPool.getCommonPoolParallelism()) == 1) TimSort.sort(a, fromIndex, toIndex, cmp, null, 0, 0); else new ArraysParallelSortHelpers.FJObject.Sorter&lt;T&gt; (null, a, (T[])Array.newInstance(a.getClass().getComponentType(), n), fromIndex, n, 0, ((g = n / (p &lt;&lt; 2)) &lt;= MIN_ARRAY_SORT_GRAN) ? MIN_ARRAY_SORT_GRAN : g, cmp).invoke();&#125; 源码分析对于基本类型数组数据，并行排序会判断排序长度n（或者数组长度）是否小于 MIN_ARRAY_SORT_GRAN = 1 &lt;&lt; 13 = 8192，如果小于或者p = ForkJoinPool.getCommonPoolParallelism()) == 1的时候，就会使用DualPivotQuicksort排序算法；否则它创建了一个ArraysParallelSortHelpers.FJInt.Sorter类进行并行排序。 对于Object数组或者泛型数组T[]的排序，可以看到与基本数据类型相似，只是最后的排序算法使用的是稳定的TimSort，并行帮助类使用的是ArraysParallelSortHelpers.FJObject.Sorter，这个类底层串行排序也是基于TimSort。 我们来分析下并行排序源码： 对于长度小于8192很好理解，就是数据长度小的时候，使用串行排序就可以了，即DualPivotQuicksort，并没有使用并行排序。 而ForkJoinPool.getCommonPoolParallelism()是返回公共线程池的并行级别，即允许多少个线程并行，如果是1的话说明禁用了线程，那么就无法使用多线程，也就只能使用串行排序，关于这个值和ForkJoinPool相关，后面我们会看下这个类，来了解一下它的实现，这儿就不过多叙述。 我们重点来看下ArraysParallelSortHelpers.FJInt.Sorter这个类，这个是针对于int数组的并行工具类，当然我们还可以看到其它数据类型的并行工具类，如ArraysParallelSortHelpers.FJByte.Sorter，他们都在ArraysParallelSortHelpers这个类里。 这个类的并行实现是根据Cilk算法来实现的。 Cilk是一种多线程算法语言。Cilk背后的理念是，程序员应该集中精力构建程序，以暴露并行性和利用局部性，让Cilk的运行时系统负责调度计算，以便在给定平台上高效运行。因此，Cilk运行时系统负责诸如负载平衡、分页和通信协议等细节。然而，与其他多线程语言不同，Cilk是算法语言，因为运行时系统保证了高效和可预测的性能。 算法内容大致如下： 如果数组长度 n 过小（小于临界值 threshold ），就使用串行排序； 否则，将数组分为两半： 将一半数组再分为两半（n/4），对于每一半，继续分割下去，直到数组长度小于临界值threshold，不再进行分割； 对前一半串行排序，对后一半串行排序，两半排序是并行进行的； 需要注意的是 n/2排序时需要保证两个n/4的并行排序合并完成，以此类推，n/4排序时需要保证两个n/8的并行排序合并完成…… 将两部分合并 其伪代码大致如下： 1234567891011121314void parallelSort(int[] a,int low,int high)&#123; int n = high - low +1; if(n &lt; threshold)&#123; sort(a); &#125;else&#123; int half = n &gt;&gt;&gt;1; ForkJoinTaskTest task1 = ForkJoinTaskTest(a,low,half-1); task1.fork(); ForkJoinTaskTest task2 = ForkJoinTaskTest(a,half,high-1); task2.compute(); task1.join(); merge(a,low,half,high-1); &#125;&#125; 可以看到这个分割过程和我们之前说到过的 双调排序的并行版本有些许类似。 我们先不看Java源码的相关实现，我们想，如果我们自己实现一个并行版本的排序如何实现呢？ 我们需要使用到ForkJoinPool，我们可以参考我的另一篇文章一道Java试题引发的思考。 这篇文章里使用了分支/合并框架（ForkJoinPool）来使用并行处理累加数据，我们参照这个模式，可以根据伪代码写出使用 Java DualPivotQuicksort的并行版本，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class ParallelSort &#123; public static void main(String[] args) throws Exception &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100000000); &#125; int[] b = a.clone(); System.out.println("ParallelSort排序开始："); long start = System.currentTimeMillis(); ForkJoinParallelSort task = new ForkJoinParallelSort(a); new ForkJoinPool().invoke(task); System.out.println("ParallelSort耗时：" + (System.currentTimeMillis() - start) + "ms"); System.out.println("ParallelSort排序完成！"); System.out.println("数组是否有序：" + isOrdered(a)); System.out.println("ArrayParallelSort排序开始："); long start1 = System.currentTimeMillis(); Arrays.parallelSort(b); System.out.println("ArrayParallelSort耗时：" + (System.currentTimeMillis() - start1) + "ms"); System.out.println("ArrayParallelSort排序完成！"); System.out.println("数组是否有序：" + isOrdered(b)); &#125;&#125;class ForkJoinParallelSort extends RecursiveTask&lt;Void&gt; &#123; /** * 要排序的数组 */ private final int[] array; /** * 数组起始下标 */ private final int start; /** * 数组结束下标 */ private final int end; /** * 数组最小长度 */ public static final int THRESHOLD = 8192; public ForkJoinParallelSort(int[] array) &#123; this(array, 0, array.length - 1); &#125; private ForkJoinParallelSort(int[] array, int start, int end) &#123; this.array = array; this.start = start; this.end = end; &#125; @Override protected Void compute() &#123; int length = end - start + 1; int half = length &gt;&gt;&gt; 1; //小于等于阈值，串行排序处理 if (length &lt;= THRESHOLD) &#123; computeSequentially(array, start, end); return null; &#125; //创建一个子任务来为数组的前一半排序 ForkJoinParallelSort leftTask = new ForkJoinParallelSort(array, start, start + half - 1); //利用另一个ForkJoinPool线程异步执行新创建的子任务 leftTask.fork(); //创建一个任务为数组的后一半排序 ForkJoinParallelSort rightTask = new ForkJoinParallelSort(array, start + half, end); //同步执行第二个子任务，有可能允许进一步递归划分 rightTask.compute(); //读取第一个子任务的结果，没有完成就等待 leftTask.join(); //合并结果 //long startTime = System.currentTimeMillis(); merge(array, start, start + half - 1, start + half, end); //System.out.println("耗时："+(System.currentTimeMillis()-startTime)+"ms"); return null; &#125; /** * 使用JavaDualPivotQuickSort串行处理较小的数组 * * @return */ private void computeSequentially(int[] array, int start, int end) &#123; //直接调用Arrays.sort 也可以 JavaDualPivotQuicksort.sort(array, start, end, null, 0, 0); &#125; /** * 快速合并两个有序数组 O(min(m,n)) * * @param array * @param leftStart * @param leftEnd * @param rightStart * @param rightEnd */ private void merge(int[] array, int leftStart, int leftEnd, int rightStart, int rightEnd) &#123; if (array[leftEnd] &lt;= array[rightStart]) &#123; return; &#125; int i = leftStart; int j = rightStart; int k = 0; int len1 = leftEnd - leftStart + 1; int len2 = rightEnd - rightStart + 1; int[] temp = new int[len1 + len2]; while (i &lt;= leftEnd &amp;&amp; j &lt;= rightEnd) &#123; if (array[i] &lt;= array[j]) &#123; temp[k++] = array[i++]; &#125; else &#123; temp[k++] = array[j++]; &#125; &#125; if (i == leftEnd + 1 &amp;&amp; j &lt;= rightEnd) &#123; System.arraycopy(array, j, temp, k, rightEnd - j + 1); &#125; if (j == rightEnd + 1 &amp;&amp; i &lt;= leftEnd) &#123; System.arraycopy(array, i, temp, k, leftEnd - i + 1); &#125; //处理完后将temp赋值给array System.arraycopy(temp, 0, array, leftStart, len1); System.arraycopy(temp, len1, array, rightStart, len2); &#125;&#125; 上述代码是比较好理解的，其要注意的地方是在排完序的两个有序数组合并上。 运行一下可以看到对于1亿数据量该方法耗时稳定在6~7s，Java源码的ParallelSort方法耗时在3~4s左右。 它们结果如下图： 可以看到Java源码的排序处理速度要比我们实现的更高效的，速度差异主要在哪儿呢？ 其实Java并行源码中借鉴了Cilk算法，但是有些不同的地方是，会把原数组分成四份进行并行排序。 算法说明如下： 将数组分成4个子数组。 对前面两个子数组进行排序然后合并。 对后面的两个进行排序然后合并。上面着几个步骤会重复递归，每个子数组都要求容量小于上面计算出来的临界值。 我们回到ArraysParallelSortHelpers这个类从它里面的FJInt这个类入手，其他的类的实现和其类似。 根据上图的一些介绍，我再简单说明下。 其实上面图中Java源码这段代码是相当晦涩的，我们如何看出它每次是拆分成4个子任务并处理的呢？ 我们可以根据第一次调用来看，这时候代码中的b = this.base = 0，wb = this.wbase = 0，则三个Sorter如下： 123new Sorter(rc, a, w, b+u, n-u, wb+u, g).fork();new Sorter(rc, a, w, b+h, q, wb+h, g).fork();;new Sorter(bc, a, w, b+q, h-q, wb+q, g).fork(); 代入b =0 可以看到，它们分别处理了[ u , u + n - u ],[ h , h + q ],[ q , q + h -q ] 三部分，正好是[ 3/4 , 1 ],[ 1/2 , 3/4 ],[ 1/4 , 1/2 ] 三部分。 而对于剩下的1/4 ，直接在当前线程处理（不需要fork），代码如下： 1DualPivotQuicksort.sort(a, b, b + n - 1, w, wb, n);//注：此时 n = q （对于当前线程）,可以看源码的赋值过程 分别排序完了，需要进行合并，如下： 12345678910111213while (n &gt; g) &#123; int h = n &gt;&gt;&gt; 1, q = h &gt;&gt;&gt; 1, u = h + q; // quartiles Relay fc = new Relay(new Merger(s, w, a, wb, h, wb+h, n-h, b, g)); Relay rc = new Relay(new Merger(fc, a, w, b+h, q,b+u, n-u, wb+h, g)); new Sorter(rc, a, w, b+u, n-u, wb+u, g).fork(); new Sorter(rc, a, w, b+h, q, wb+h, g).fork();; Relay bc = new Relay(new Merger(fc, a, w, b, q,b+q, h-q, wb, g)); new Sorter(bc, a, w, b+q, h-q, wb+q, g).fork(); s = new EmptyCompleter(bc); n = q;&#125;DualPivotQuicksort.sort(a, b, b + n - 1, w, wb, n);s.tryComplete(); 这段Relay的依存关系是 rc （合并后1/2部分）和 bc （合并前1/2部分） 是并行的，fc 会合并rc和bc （排序好的数据）。 Java中把并行分成四份的优势在哪里呢？ 明显这段代码和使用我们Cilk算法每次分成两份的本质是一样的，而且分成4份代码变得更加晦涩。 具体原因就要说说这个Merger了，这个Merger是关于两个有序数组并行合并的实现，它的效率是非常高的，我们回到我们自己实现的那个ParallelSort类，可以看到我们设计的merge就是比较常规的合并，当两个数组数据量越大时，耗时越长，我在代码中注掉了耗时计算，有兴趣的童鞋可以打开观察下，在数据量较小情况下，其耗时基本是0~10ms，但是运行中随着两部分待合并的数据越来越大，耗时越来越大。 比如对于1亿数据的排序，其耗时主要消耗在2个5000w的数据合并成最终结果、4个2500w的数据两两合并成2个5000w数据、8个1250w的数据两两合并成4个2500w数据……的合并上。 Java中的这个Merger对合并进行了优化，使用了并行合并，其原理如下： 对于两个待合并数组A，B； 找到较大（或等于）的一个数组（比如A），如果长度小于阈值8192，就不分割了；如果大于8192，找到较大数组A的中点作为切割点M，使用二分法找到较小数组B中比这个切割点大的最小位置索引P； 这时候其实我们可以发现A中[lowA , M]和B中[lowB , P]位置数据合并后是始终 小于等于 A中[M , highA]和B中[P , highB]位置数据合并的，这就是分割合并有序的原则； 如果长度比较大，还会继续并行分割下去； 然后我们对上面拆分的数据两两合并，最终多线程执行完也就得到了有序数据。 有兴趣的童鞋可以参考原理结合上图看一下。 而我们在排序及合并时，会用到工作数组，分成4份后，可以保证最后的排序完成数组在原数组中，而不是在工作数组中，也避免了一次数据拷贝。 总结关于Java自带排序的内容就介绍到这儿，可以看到相比于串行排序，并行排序更加复杂，但是Cilk并行算法的原理还是比较简单的，Java并排代码之所以复杂是因为它尽可能的优化了算法耗时。 这也是软件开发者应当具有的品质：精益求精。 源码关于自写的ParallelSort排序可见于我的 GitHub。 关于 ArraysParallelSortHelpers相关代码可以参考JDK源码（1.8及以上版本）。 参考资料 JDK ArraysParallelSortHelpers源码 JDK Arrays源码]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
        <tag>并行排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令大全]]></title>
    <url>%2Fblog%2FLinux%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8.html</url>
    <content type="text"><![CDATA[前言Linux 命令大全，一些常用命令使用红色标记。 命令linux系统管理命令adduser功能说明：新增用户帐号。 语 法：adduser 补充说明：在Slackware中，adduser指令是个script程序，利用交谈的方式取得输入的用户帐号资料，然后再交由真正建立帐号的useradd指令建立新用户，如此可方便管理员建立用户帐号。在Red Hat Linux中，adduser指令则是useradd指令的符号连接，两者实际上是同一个指令。 chfn(change finger information)功能说明：改变finger指令显示的信息。 语 法：chfn [-f &lt;真实姓名&gt;][-h &lt;家中电话&gt;][-o &lt;办公地址&gt;][-p &lt;办公电话&gt;][-uv][帐号名称] 补充说明：chfn指令可用来更改执行finger指令时所显示的信息，这些信息都存放在/etc目录里的asswd文件里。若不指定任何参数，则chfn指令会进入问答式界面。 参 数： -f&lt;真实姓名&gt;或–full-name&lt;真实姓名&gt; 设置真实姓名。 -h&lt;家中电话&gt;或–home-phone&lt;家中电话&gt; 设置家中的电话号码。 -o&lt;办公地址&gt;或–office&lt;办公地址&gt; 设置办公室的地址。 -p&lt;办公电话&gt;或–office-phone&lt;办公电话&gt; 设置办公室的电话号码。 -u或–help 在线帮助。 -v或-version 显示版本信息。 chsh(change shell)功能说明：更换登入系统时使用的shell。 语 法：chsh [-luv][-s &lt;shell 名称&gt;][用户名称] 补充说明：每位用户在登入系统时，都会拥有预设的shell环境，这个指令可更改其预设值。若不指定任何参数与用户名称，则chsh会以应答的方式进行设置。 参 数： -s&lt;shell 名称&gt;或–shell&lt;shell 名称&gt; 更改系统预设的shell环境。 -l或–list-shells 列出目前系统可用的shell清单。 -u或–help 在线帮助。 -v或-version 显示版本信息。 date功能说明：显示或设置系统时间与日期。 语 法：date [-d &lt;字符串&gt;][-u][+%H%I%K%l%M%P%r%s%S%T%X%Z%a%A%b%B%c%d%D%j%m%U%w%x%y%Y%n%t] 或date [-s &lt;字符串&gt;][-u][MMDDhhmmCCYYss] 或 date [–help][–version] 补充说明：第一种语法可用来显示系统日期或时间，以%为开头的参数为格式参数，可指定日期或时间的显示格式。第二种语法可用来设置系统日期与时间。只有管理员才有设置日期与时间的权限。若不加任何参数，data会显示目前的日期与时间。 参 数： %H 小时(以00-23来表示)。 %I 小时(以01-12来表示)。 %K 小时(以0-23来表示)。 %l 小时(以0-12来表示)。 %M 分钟(以00-59来表示)。 %P AM或PM。 %r 时间(含时分秒，小时以12小时AM/PM来表示)。 %s 总秒数。起算时间为1970-01-01 00:00:00 UTC。 %S 秒(以本地的惯用法来表示)。 %T 时间(含时分秒，小时以24小时制来表示)。 %X 时间(以本地的惯用法来表示)。 %Z 市区。 %a 星期的缩写。 %A 星期的完整名称。 %b 月份英文名的缩写。 %B 月份的完整英文名称。 %c 日期与时间。只输入date指令也会显示同样的结果。 %d 日期(以01-31来表示)。 %D 日期(含年月日)。 %j 该年中的第几天。 %m 月份(以01-12来表示)。 %U 该年中的周数。 %w 该周的天数，0代表周日，1代表周一，异词类推。 %x 日期(以本地的惯用法来表示)。 %y 年份(以00-99来表示)。 %Y 年份(以四位数来表示)。 %n 在显示时，插入新的一行。 %t 在显示时，插入tab。 MM 月份(必要)。 DD 日期(必要)。 hh 小时(必要)。 mm 分钟(必要)。 CC 年份的前两位数(选择性)。 YY 年份的后两位数(选择性)。 ss 秒(选择性)。 -d&lt;字符串&gt; 显示字符串所指的日期与时间。字符串前后必须加上双引号。 -s&lt;字符串&gt; 根据字符串来设置日期与时间。字符串前后必须加上双引号。 -u 显示GMT。 –help 在线帮助。 –version 显示版本信息。 exit功能说明：退出目前的shell。 语 法：exit [状态值] 补充说明：执行exit可使shell以指定的状态值退出。若不设置状态值参数，则shell以预设值退出。状态值0代表执行成功，其他值代表执行失败。exit也可用在script，离开正在执行的script，回到shell。 finger功能说明：查找并显示用户信息。 语 法：finger [-lmsp][帐号名称…] 补充说明：finger指令会去查找，并显示指定帐号的用户相关信息，包括本地与远端主机的用户皆可，帐号名称没有大小写的差别。单独执行finger指令，它会显示本地主机现在所有的用户的登陆信息，包括帐号名称，真实姓名，登入终端机，闲置时间，登入时间以及地址和电话。 参 数： -l 列出该用户的帐号名称，真实姓名，用户专属目录，登入所用的Shell，登入时间，转信地址，电子邮件状态，还有计划文件和方案文件内容。 -m 排除查找用户的真实姓名。 -s 列出该用户的帐号名称，真实姓名，登入终端机，闲置时间，登入时间以及地址和电话。 -p 列出该用户的帐号名称，真实姓名，用户专属目录，登入所用的Shell，登入时间，转信地址，电子邮件状态，但不显示该用户的计划文件和方案文件内容。 free功能说明：显示内存状态。 语 法： free [-bkmotV][-s &lt;间隔秒数&gt;] 补充说明：free指令会显示内存的使用情况，包括实体内存，虚拟的交换文件内存，共享内存区段，以及系统核心使用的缓冲区等。 参 数： -b 以Byte为单位显示内存使用情况。 -k 以KB为单位显示内存使用情况。 -m 以MB为单位显示内存使用情况。 -o 不显示缓冲区调节列。 -s&lt;间隔秒数&gt; 持续观察内存使用状况。 -t 显示内存总和列。 -V 显示版本信息。 fwhois功能说明：查找并显示用户信息。 语 法：fwhios [帐号名称] 补充说明：本指令的功能有点类似finger指令，它会去查找并显示指定帐号的用户相关信息。不同之处在于fwhois指令是到Network Solutions的WHOIS数据库去查找，该帐号名称必须有在上面注册才能寻获，且名称没有大小写的差别 gitps(gnu interactive tools process status)功能说明：报告程序状况。 语 法：gitps [acefgjlnrsSTuvwxX][p &lt;程序识别码&gt;][t &lt;终端机编号&gt;][U &lt;帐号名称&gt;] 补充说明：gitps是用来报告并管理程序执行的指令，基本上它就是通过ps指令来报告，管理程序，也能通过gitps指令随时中断，删除不必要的程序。因为gitps指令会去执行ps指令，所以其参数和ps指令相当类似。 参 数： a 显示 现行终端机下的所有程序，包括其他用户的程序。 c 列出程序时，显示每个程序真正的指令名称，而不包含路径，参数或是常驻服务的标示． e 列出程序时，显示每个程序所使用的环境变量。 f 用ASCII字符显示树状结构，表达程序间的相互关系。 g 显示现行终端机下的所有程序，包括群组领导者的程序。 j 采用工作控制的格式来显示程序状况。 l 采用纤细的格式来显示程序状况。 n 以数字来表示USER和WCHAN栏位。 p&lt;程序识别码&gt; 指定程序识别码，并列出该程序的状况。 r 只列出现行终端机正在执行中的程序。 s 采用程序信号的格式显示程序状况。 S 列出程序时，包括已中断的子程序信息。 t&lt;终端机机标号&gt; 指定终端机编号，并列出属于该终端机的程序的状况。 T 显示现行终端机下的所有程序。 u 以用户为主的格式来显示程序状况。 U&lt;帐号名称&gt; 列出属于该用户的程序的状况。 v 采用虚拟内存的格式显示程序状况。 w 采用宽阔的格式来显示程序状况。 x 显示所有程序，不以终端机来区分。 X 采用旧试的Linux i386登陆格式显示程序状况。 groupdel(group delete)功能说明：删除群组。 语 法：groupdel [群组名称] 补充说明：需要从系统上删除群组时，可用groupdel指令来完成这项工作。倘若该群组中仍包括某些用户，则必须先删除这些用户后，方能删除群组。 groupmod(group modify)功能说明：更改群组识别码或名称。 语 法：groupmod [-g &lt;群组识别码&gt; &lt;-o&gt;][-n &lt;新群组名称&gt;][群组名称] 补充说明：需要更改群组的识别码或名称时，可用groupmod指令来完成这项工作。 参 数： -g &lt;群组识别码&gt; 设置欲使用的群组识别码。 -o 重复使用群组识别码。 -n &lt;新群组名称&gt; 设置欲使用的群组名称。 halt功能说明：关闭系统。 语 法：halt [-dfinpw] 补充说明：halt会先检测系统的runlevel。若runlevel为0或6，则关闭系统，否则即调用shutdown来关闭系统。 参 数： -d 不要在wtmp中记录。 -f 不论目前的runlevel为何，不调用shutdown即强制关闭系统。 -i 在halt之前，关闭全部的网络界面。 -n halt前，不用先执行sync。 -p halt之后，执行poweroff。 -w 仅在wtmp中记录，而不实际结束系统。 id功能说明：显示用户的ID，以及所属群组的ID。 语 法：id [-gGnru][–help][–version][用户名称] 补充说明：id会显示用户以及所属群组的实际与有效ID。若两个ID相同，则仅显示实际ID。若仅指定用户名称，则显示目前用户的ID。 参 数： -g或–group 显示用户所属群组的ID。 -G或–groups 显示用户所属附加群组的ID。 -n或–name 显示用户，所属群组或附加群组的名称。 -r或–real 显示实际ID。 -u或–user 显示用户ID。 -help 显示帮助。 -version 显示版本信息。 kill功能说明：删除执行中的程序或工作。 语 法：kill [-s &lt;信息名称或编号&gt;][程序] 或 kill [-l &lt;信息编号&gt;] 补充说明：kill可将指定的信息送至程序。预设的信息为SIGTERM(15)，可将指定程序终止。若仍无法终止该程序，可使用SIGKILL(9)信息尝试强制删除程序。程序或工作的编号可利用ps指令或jobs指令查看。 参 数： -l &lt;信息编号&gt; 若不加&lt;信息编号&gt;选项，则-l参数会列出全部的信息名称。 -s &lt;信息名称或编号&gt; 指定要送出的信息。 PS: [程序] : [程序]可以是程序的PID或是PGID，也可以是工作编号。 last功能说明：列出目前与过去登入系统的用户相关信息。 语 法：last [-adRx][-f &lt;记录文件&gt;][-n &lt;显示列数&gt;][帐号名称…][终端机编号…] 补充说明：单独执行last指令，它会读取位于/var/log目录下，名称为wtmp的文件，并把该给文件的内容记录的登入系统的用户名单全部显示出来。 参 数： -a 把从何处登入系统的主机名称或IP地址，显示在最后一行。 -d 将IP地址转换成主机名称。 -f &lt;记录文件&gt; 指定记录文件。 -n &lt;显示列数&gt;或-&lt;显示列数&gt; 设置列出名单的显示列数。 -R 不显示登入系统的主机名称或IP地址。 -x 显示系统关机，重新开机，以及执行等级的改变等信息。 lastb功能说明：列出登入系统失败的用户相关信息。 语 法：lastb [-adRx][-f &lt;记录文件&gt;][-n &lt;显示列数&gt;][帐号名称…][终端机编号…] 补充说明：单独执行lastb指令，它会读取位于/var/log目录下，名称为btmp的文件，并把该文件内容记录的登入失败的用户名单，全部显示出来。 参 数： -a 把从何处登入系统的主机名称或IP地址显示在最后一行。 -d 将IP地址转换成主机名称。 -f&lt;记录文件&gt; 指定记录文件。 -n&lt;显示列数&gt;或-&lt;显示列数&gt; 设置列出名单的显示列数。 -R 不显示登入系统的主机名称或IP地址。 -x 显示系统关机，重新开机，以及执行等级的改变等信息。 login功能说明：登入系统。 语 法：login 补充说明：login指令让用户登入系统，您亦可通过它的功能随时更换登入身份。在Slackware发行版中 ，您可在指令后面附加欲登入的用户名称，它会直接询问密码，等待用户输入。当/etc目录里含名称为nologin的文件时，系统只root帐号登入系统，其他用户一律不准登入。 logname功能说明：显示用户名称。 语 法：logname [–help][–version] 补充说明：执行logname指令，它会显示目前用户的名称。 参 数： –help 在线帮助。 –vesion 显示版本信息。 logout功能说明：退出系统。 语 法：logout 补充说明：logout指令让用户退出系统，其功能和login指令相互对应。 logrotate功能说明：管理记录文件。 语 法：logrotate [-?dfv][-s &lt;状态文件&gt;][–usage][配置文件] 补充说明：使用logrotate指令，可让你轻松管理系统所产生的记录文件。它提供自动替换，压缩，删除和邮寄记录文件，每个记录文件都可被设置成每日，每周或每月处理，也能在文件太大时立即处理。您必须自行编辑，指定配置文件，预设的配置文件存放在/etc目录下，文件名称为logrotate.conf。 参 数： -?或–help 在线帮助。 -d或–debug 详细显示指令执行过程，便于排错或了解程序执行的情况。 -f或–force 强行启动记录文件维护操作，纵使logrotate指令认为没有需要亦然。 -s&lt;状态文件&gt;或–state=&lt;状态文件&gt; 使用指定的状态文件。 -v或–version 显示指令执行过程。 -usage 显示指令基本用法。 newgrp功能说明：登入另一个群组。 语 法：newgrp [群组名称] 补充说明：newgrp指令类似login指令，当它是以相同的帐号，另一个群组名称，再次登入系统。欲使用newgrp指令切换群组，您必须是该群组的用户，否则将无法登入指定的群组。单一用户要同时隶属多个群组，需利用交替用户的设置。若不指定群组名称，则newgrp指令会登入该用户名称的预设群组。 nice功能说明：设置优先权。 语 法：nice [-n &lt;优先等级&gt;][–help][–version][执行指令] 补充说明：nice指令可以改变程序执行的优先权等级。 参 数： -n&lt;优先等级&gt;或-&lt;优先等级&gt;或–adjustment=&lt;优先等级&gt; 设置欲执行的指令的优先权等级。等级的范围从-20－19，其中-20最高，19最低，只有系统管理者可以设置负数的等级。 –help 在线帮助。 –version 显示版本信息。 procinfo(process information)功能说明：显示当前进程信息。 语 法：procinfo [-abdDfhimsSv][-F &lt;输出文件&gt;][-n &lt;间隔秒数&gt;] 补充说明：procinfo指令从/proc目录里读取相关数据，将数据妥善整理过后输出到标准输出设备。 参 数： -a 显示所有信息。 -b 显示磁盘设备的区块数目，而非存取数目。 -d 显示系统信息每秒间的变化差额，而非总和的数值。本参数必须配合”-f”参数使用 -D 此参数效果和指定”-d”参数类似，但内存和交换文件的信息为总和数值。 -f 进入全画面的互动式操作界面。 -F&lt;输出文件&gt; 把信息状态输出到文件保存起来，而非预设的标准输出设备。 -h 在线帮助。 -i 显示完整的IRP列表。 -m 显示系统模块和外围设备等相关信息。 -n间隔秒数&gt; 设置全画面互动模式的信息更新速度，单位以秒计算。 -s 显示系统的内存，磁盘空间，IRP和DMA等信息，此为预设值。 -S 搭配参数”-d”或”-D”使用时，每秒都会更新信息，不论是否有使用参数”-n”。 -v 显示版本信息。 ps(process status)功能说明：报告程序状况。 语 法：ps [-aAcdefHjlmNVwy][acefghLnrsSTuvxX][-C &lt;指令名称&gt;][-g &lt;群组名称&gt;][-G &lt;群组识别码&gt;][-p &lt;程序识别码&gt;][p &lt;程序识别码&gt;][-s &lt;阶段作业&gt;][-t &lt;终端机编号&gt;][t &lt;终端机编号&gt;][-u &lt;用户识别码&gt;][-U &lt;用户识别码&gt;][U &lt;用户名称&gt;][-&lt;程序识别码&gt;][–cols &lt;每列字符数&gt;][–columns &lt;每列字符数&gt;][–cumulative][–deselect][–forest][–headers][–help][–info][–lines &lt;显示列数&gt;][–no-headers][–group &lt;群组名称&gt;][-Group &lt;群组识别码&gt;][–pid &lt;程序识别码&gt;][–rows &lt;显示列数&gt;][–sid &lt;阶段作业&gt;][–tty &lt;终端机编号&gt;][–user &lt;用户名称&gt;][–User &lt;用户识别码&gt;][–version][–width &lt;每列字符数&gt;] 补充说明：ps是用来报告程序执行状况的指令，您可以搭配kill指令随时中断，删除不必要的程序。 参 数： -a 显示所有终端机下执行的程序，除了阶段作业领导者之外。 a 显示现行终端机下的所有程序，包括其他用户的程序。 -A 显示所有程序。 -c 显示CLS和PRI栏位。 c 列出程序时，显示每个程序真正的指令名称，而不包含路径，参数或常驻服务的标示。 -C&lt;指令名称&gt; 指定执行指令的名称，并列出该指令的程序的状况。 -d 显示所有程序，但不包括阶段作业领导者的程序。 -e 此参数的效果和指定”A”参数相同。 e 列出程序时，显示每个程序所使用的环境变量。 -f 显示UID,PPIP,C与STIME栏位。 f 用ASCII字符显示树状结构，表达程序间的相互关系。 -g&lt;群组名称&gt; 此参数的效果和指定”-G”参数相同，当亦能使用阶段作业领导者的名称来指定。 g 显示现行终端机下的所有程序，包括群组领导者的程序。 -G&lt;群组识别码&gt; 列出属于该群组的程序的状况，也可使用群组名称来指定。 h 不显示标题列。 -H 显示树状结构，表示程序间的相互关系。 -j或j 采用工作控制的格式显示程序状况。 -l或l 采用详细的格式来显示程序状况。 L 列出栏位的相关信息。 -m或m 显示所有的执行绪。 n 以数字来表示USER和WCHAN栏位。 -N 显示所有的程序，除了执行ps指令终端机下的程序之外。 -p&lt;程序识别码&gt; 指定程序识别码，并列出该程序的状况。 p&lt;程序识别码&gt; 此参数的效果和指定”-p”参数相同，只在列表格式方面稍有差异。 r 只列出现行终端机正在执行中的程序。 -s&lt;阶段作业&gt; 指定阶段作业的程序识别码，并列出隶属该阶段作业的程序的状况。 s 采用程序信号的格式显示程序状况。 S 列出程序时，包括已中断的子程序资料。 -t&lt;终端机编号&gt; 指定终端机编号，并列出属于该终端机的程序的状况。 t&lt;终端机编号&gt; 此参数的效果和指定”-t”参数相同，只在列表格式方面稍有差异。 -T 显示现行终端机下的所有程序。 -u&lt;用户识别码&gt; 此参数的效果和指定”-U”参数相同。 u 以用户为主的格式来显示程序状况。 -U&lt;用户识别码&gt; 列出属于该用户的程序的状况，也可使用用户名称来指定。 U&lt;用户名称&gt; 列出属于该用户的程序的状况。 v 采用虚拟内存的格式显示程序状况。 -V或V 显示版本信息。 -w或w 采用宽阔的格式来显示程序状况。 x 显示所有程序，不以终端机来区分。 X 采用旧式的Linux i386登陆格式显示程序状况。 -y 配合参数”-l”使用时，不显示F(flag)栏位，并以RSS栏位取代ADDR栏位 。 -&lt;程序识别码&gt; 此参数的效果和指定”p”参数相同。 –cols&lt;每列字符数&gt; 设置每列的最大字符数。 –columns&lt;每列字符数&gt; 此参数的效果和指定”–cols”参数相同。 –cumulative 此参数的效果和指定”S”参数相同。 –deselect 此参数的效果和指定”-N”参数相同。 –forest 此参数的效果和指定”f”参数相同。 –headers 重复显示标题列。 –help 在线帮助。 –info 显示排错信息。 –lines&lt;显示列数&gt; 设置显示画面的列数。 –no-headers 此参数的效果和指定”h”参数相同，只在列表格式方面稍有差异。 –group&lt;群组名称&gt; 此参数的效果和指定”-G”参数相同。 –Group&lt;群组识别码&gt; 此参数的效果和指定”-G”参数相同。 –pid&lt;程序识别码&gt; 此参数的效果和指定”-p”参数相同。 –rows&lt;显示列数&gt; 此参数的效果和指定”–lines”参数相同。 –sid&lt;阶段作业&gt; 此参数的效果和指定”-s”参数相同。 –tty&lt;终端机编号&gt; 此参数的效果和指定”-t”参数相同。 –user&lt;用户名称&gt; 此参数的效果和指定”-U”参数相同。 –User&lt;用户识别码&gt; 此参数的效果和指定”-U”参数相同。 –version 此参数的效果和指定”-V”参数相同。 –widty&lt;每列字符数&gt; 此参数的效果和指定”-cols”参数相同。 pstree(process status tree)功能说明：以树状图显示程序。 语 法：pstree [-acGhlnpuUV][-H &lt;程序识别码&gt;][&lt;程序识别码&gt;/&lt;用户名称&gt;] 补充说明：pstree指令用ASCII字符显示树状结构，清楚地表达程序间的相互关系。如果不指定程序识别码或用户名称，则会把系统启动时的第一个程序视为基层，并显示之后的所有程序。若指定用户名称，便会以隶属该用户的第一个程序当作基层，然后显示该用户的所有程序。 参 数： -a 显示每个程序的完整指令，包含路径，参数或是常驻服务的标示。 -c 不使用精简标示法。 -G 使用VT100终端机的列绘图字符。 -h 列出树状图时，特别标明现在执行的程序。 -H&lt;程序识别码&gt; 此参数的效果和指定”-h”参数类似，但特别标明指定的程序。 -l 采用长列格式显示树状图。 -n 用程序识别码排序。预设是以程序名称来排序。 -p 显示程序识别码。 -u 显示用户名称。 -U 使用UTF-8列绘图字符。 -V 显示版本信息。 reboot功能说明：重新开机。 语 法：dreboot [-dfinw] 补充说明：执行reboot指令可让系统停止运作，并重新开机。 参 数： -d 重新开机时不把数据写入记录文件/var/tmp/wtmp。本参数具有”-n”参数的效果。 -f 强制重新开机，不调用shutdown指令的功能。 -i 在重开机之前，先关闭所有网络界面。 -n 重开机之前不检查是否有未结束的程序。 -w 仅做测试，并不真的将系统重新开机，只会把重开机的数据写入/var/log目录下的wtmp记录文件。 renice功能说明：调整优先权。 语 法：renice [优先等级][-g &lt;程序群组名称&gt;…][-p &lt;程序识别码&gt;…][-u &lt;用户名称&gt;…] 补充说明：renice指令可重新调整程序执行的优先权等级。预设是以程序识别码指定程序调整其优先权，您亦可以指定程序群组或用户名称调整优先权等级，并修改所有隶属于该程序群组或用户的程序的优先权。等级范围从-20–19，只有系统管理者可以改变其他用户程序的优先权，也仅有系统管理者可以设置负数等级。 参 数： -g &lt;程序群组名称&gt; 使用程序群组名称，修改所有隶属于该程序群组的程序的优先权。 -p &lt;程序识别码&gt; 改变该程序的优先权等级，此参数为预设值。 -u &lt;用户名称&gt; 指定用户名称，修改所有隶属于该用户的程序的优先权。 rlogin(remote login)功能说明：远端登入。 语 法：rlogin [-8EL][-e &lt;脱离字符&gt;][-l &lt;用户名称&gt;][主机名称或IP地址] 补充说明：执行rlogin指令开启终端机阶段操作，并登入远端主机。 参 数： -8 允许输入8位字符数据。 -e脱离字符&gt; 设置脱离字符。 -E 滤除脱离字符。 -l用户名称&gt; 指定要登入远端主机的用户名称。 -L 使用litout模式进行远端登入阶段操作。 rsh(remote shell)功能说明：远端登入的Shell。 语 法：rsh [-dn][-l &lt;用户名称&gt;][主机名称或IP地址][执行指令] 补充说明：rsh提供用户环境，也就是Shell，以便指令能够在指定的远端主机上执行。 参 数： -d 使用Socket层级的排错功能。 -l&lt;用户名称&gt; 指定要登入远端主机的用户名称。 -n 把输入的指令号向代号为/dev/null的特殊外围设备。 rwho功能说明：查看系统用户。 语 法：rwho [-a] 补充说明：rwho指令的效果类似who指令，但它会显示局域网里所有主机的用户。主机必须提供rwhod常驻服务的功能，方可使用rwho指令。 参 数： -a 列出所有的用户，包括闲置时间超过1个小时以上的用户。 screen功能说明：多重视窗管理程序。 语 法：screen [-AmRvx -ls -wipe][-d &lt;作业名称&gt;][-h &lt;行数&gt;][-r &lt;作业名称&gt;][-s &lt;shell&gt;][-S &lt;作业名称&gt;] 补充说明：screen为多重视窗管理程序。此处所谓的视窗，是指一个全屏幕的文字模式画面。通常只有在使用telnet登入主机或是使用老式的终端机时，才有可能用到screen程序。 参 数： -A 将所有的视窗都调整为目前终端机的大小。 -d&lt;作业名称&gt; 将指定的screen作业离线。 -h&lt;行数&gt; 指定视窗的缓冲区行数。 -m 即使目前已在作业中的screen作业，仍强制建立新的screen作业。 -r&lt;作业名称&gt; 恢复离线的screen作业。 -R 先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。 -s&lt;shell&gt; 指定建立新视窗时，所要执行的shell。 -S&lt;作业名称&gt; 指定screen作业的名称。 -v 显示版本信息。 -x 恢复之前离线的screen作业。 -ls或–list 显示目前所有的screen作业。 -wipe 检查目前所有的screen作业，并删除已经无法使用的screen作业。 shutdown功能说明：系统关机指令。 语 法：shutdown [-efFhknr][-t 秒数][时间][警告信息] 补充说明：shutdown指令可以关闭所有程序，并依用户的需要，进行重新开机或关机的动作。 参 数： -c 当执行”shutdown -h 11:50”指令时，只要按+键就可以中断关机的指令。 -f 重新启动时不执行fsck。 -F 重新启动时执行fsck。 -h 将系统关机。 -k 只是送出信息给所有用户，但不会实际关机。 -n 不调用init程序进行关机，而由shutdown自己进行。 -r shutdown之后重新启动。 -t&lt;秒数&gt; 送出警告信息和删除信息之间要延迟多少秒。 PS : [时间 ] : 设置多久时间后执行shutdown指令。 [警告信息 ] : 要传送给所有登入用户的信息。 sliplogin功能说明：将SLIP接口加入标准输入。 语 法：sliplogin [用户名称] 补充说明：sliplogin可将SLIP接口加入标准输入，把一般终端机的连线变成SLIP连线。通常可用来建立SLIP服务器，让远端电脑以SLIP连线到服务器。sliplogin活去检查/etc/slip/slip.hosts文件中是否有相同的用户名称。通过检查后，sliplogin会调用执行shell script来设置IP地址，子网掩码等网络界面环境。此shell script通常是/etc/slip/slip.login。 su(super user)功能说明：变更用户身份。 语 法：su [-flmp][–help][–version][-][-c &lt;指令&gt;][-s &lt;shell&gt;][用户帐号] 补充说明：su可让用户暂时变更登入的身份。变更时须输入所要变更的用户帐号与密码。 参 数： -c&lt;指令&gt;或–command=&lt;指令&gt; 执行完指定的指令后，即恢复原来的身份。 -f或–fast 适用于csh与tsch，使shell不用去读取启动文件。 -.-l或–login 改变身份时，也同时变更工作目录，以及HOME,SHELL,USER,LOGNAME。此外，也会变更PATH变量。 -m,-p或–preserve-environment 变更身份时，不要变更环境变量。 -s&lt;shell&gt;或–shell=&lt;shell&gt; 指定要执行的shell。 –help 显示帮助。 –version 显示版本信息。 PS: [用户帐号 ] : 指定要变更的用户。若不指定此参数，则预设变更为root。 sudo功能说明：以其他身份来执行指令。 语 法：sudo [-bhHpV][-s &lt;shell&gt;][-u &lt;用户&gt;][指令] 或 sudo [-klv] 补充说明：sudo可让用户以其他的身份来执行指定的指令，预设的身份为root。在/etc/sudoers中设置了可执行sudo指令的用户。若其未经授权的用户企图使用sudo，则会发出警告的邮件给管理员。用户使用sudo时，必须先输入密码，之后有5分钟的有效期限，超过期限则必须重新输入密码。 参 数： -b 在后台执行指令。 -h 显示帮助。 -H 将HOME环境变量设为新身份的HOME环境变量。 -k 结束密码的有效期限，也就是下次再执行sudo时便需要输入密码。 -l 列出目前用户可执行与无法执行的指令。 -p 改变询问密码的提示符号。 -s&lt;shell&gt; 执行指定的shell。 -u&lt;用户&gt; 以指定的用户作为新的身份。若不加上此参数，则预设以root作为新的身份。 -v 延长密码有效期限5分钟。 -V 显示版本信息。 suspend功能说明：暂停执行shell。 语 法：suspend [-f] 补充说明：suspend为shell内建指令，可暂停目前正在执行的shell。若要恢复，则必须使用SIGCONT信息。 参 数： -f 若目前执行的shell为登入的shell，则suspend预设无法暂停此shell。若要强迫暂停登入的shell，则必须使用-f参数。 swatch(simple watcher)功能说明：系统监控程序。 语 法：swatch [-A &lt;分隔字符&gt;][-c &lt;设置文件&gt;][-f &lt;记录文件&gt;][-I &lt;分隔字符&gt;][-P &lt;分隔字符&gt;][-r &lt;时间&gt;][-t &lt;记录文件&gt;] 补充说明：swatch可用来监控系统记录文件，并在发现特定的事件时，执行指定的动作。swatch所监控的事件以及对应事件的动作都存放在swatch的配置文件中。预设的配置文件为拥护根目录下的.swatchrc。然而在Red Hat Linux的预设用户根目录下并没有.swatchrc配置文件，您可将/usr/doc/swatch-2.2/config_files/swatchrc.personal文件复制到用户根目录下的.swatchrc，然后修改.swatchrc所要监控的事件及执行的动作。 参 数： -A&lt;分隔字符&gt; 预设配置文件中，动作的分隔字符，预设为逗号。 -c设置文件&gt; 指定配置文件，而不使用预设的配置文件。 -f记录文件&gt; 检查指定的记录文件，检查完毕后不会继续监控该记录文件。 -I分隔字符&gt; 指定输入记录的分隔字符，预设为换行字符。 -P分隔字符&gt; 指定配置文件中，事件的分隔字符，预设为逗号。 -r时间&gt; 在指定的时间重新启动。 -t&lt;记录文件&gt; 检查指定的记录文件，并且会监控加入记录文件中的后继记录。 tload功能说明：显示系统负载状况。 语 法：tload [-V][-d &lt;间隔秒数&gt;][-s &lt;刻度大小&gt;][终端机编号] 补充说明：tload指令使用ASCII字符简单地以文字模式显示系统负载状态。假设不给予终端机编号，则会在执行tload指令的终端机显示负载情形。 参 数： -d&lt;间隔秒数&gt; 设置tload检测系统负载的间隔时间，单位以秒计算。 -s&lt;刻度大小&gt; 设置图表的垂直刻度大小，单位以列计算。 -V 显示版本信息。 top功能说明：显示，管理执行中的程序。 语 法：top [bciqsS][d &lt;间隔秒数&gt;][n &lt;执行次数&gt;] 补充说明：执行top指令可显示目前正在系统中执行的程序，并通过它所提供的互动式界面，用热键加以管理。 参 数： b 使用批处理模式。 c 列出程序时，显示每个程序的完整指令，包括指令名称，路径和参数等相关信息。 d&lt;间隔秒数&gt; 设置top监控程序执行状况的间隔时间，单位以秒计算。 i 执行top指令时，忽略闲置或是已成为Zombie的程序。 n&lt;执行次数&gt; 设置监控信息的更新次数。 q 持续监控程序执行的状况。 s 使用保密模式，消除互动模式下的潜在危机。 S 使用累计模式，其效果类似ps指令的”-S”参数。 uname功能说明：显示系统信息。 语 法：uname [-amnrsv][–help][–version] 补充说明：uname可显示电脑以及操作系统的相关信息。 参 数： -a或–all 显示全部的信息。 -m或–machine 显示电脑类型。 -n或-nodename 显示在网络上的主机名称。 -r或–release 显示操作系统的发行编号。 -s或–sysname 显示操作系统名称。 -v 显示操作系统的版本。 –help 显示帮助。 –version 显示版本信息。 useradd功能说明：建立用户帐号。 语 法：useradd [-mMnr][-c &lt;备注&gt;][-d &lt;登入目录&gt;][-e &lt;有效期限&gt;][-f &lt;缓冲天数&gt;][-g &lt;群组&gt;][-G &lt;群组&gt;][-s &lt;shell&gt;][-u &lt;uid&gt;][用户帐号] 或 useradd -D [-b][-e &lt;有效期限&gt;][-f &lt;缓冲天数&gt;][-g &lt;群组&gt;][-G &lt;群组&gt;][-s &lt;shell&gt;] 补充说明：useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 参 数： -c&lt;备注&gt; 加上备注文字。备注文字会保存在passwd的备注栏位中。 -d&lt;登入目录&gt; 指定用户登入时的启始目录。 -D 变更预设值． -e&lt;有效期限&gt; 指定帐号的有效期限。 -f&lt;缓冲天数&gt; 指定在密码过期后多少天即关闭该帐号。 -g&lt;群组&gt; 指定用户所属的群组。 -G&lt;群组&gt; 指定用户所属的附加群组。 -m 自动建立用户的登入目录。 -M 不要自动建立用户的登入目录。 -n 取消建立以用户名称为名的群组． -r 建立系统帐号。 -s&lt;shell&gt; 指定用户登入后所使用的shell。 -u&lt;uid&gt; 指定用户ID。 userconf功能说明：用户帐号设置程序。 语 法：userconf [–addgroup &lt;群组&gt;][–adduser &lt;用户ID&gt;&lt;群组&gt;&lt;用户名称&gt;&lt;shell&gt;][–delgroup &lt;群组&gt;][–deluser &lt;用户ID&gt;][–help] 补充说明：userconf实际上为linuxconf的符号连接，提供图形界面的操作方式，供管理员建立与管理各类帐号。若不加任何参数，即进入图形界面。 参 数： –addgroup&lt;群组&gt; 新增群组。 –adduser&lt;用户ID&gt;&lt;群组&gt;&lt;用户名称&gt;&lt;shell&gt; 新增用户帐号。 –delgroup&lt;群组&gt; 删除群组。 –deluser&lt;用户ID&gt; 删除用户帐号。 –help 显示帮助。 userdel功能说明：删除用户帐号。 语 法：userdel [-r][用户帐号] 补充说明：userdel可删除用户帐号与相关的文件。若不加参数，则仅删除用户帐号，而不删除相关文件。 参 数： -f 删除用户登入目录以及目录中所有文件。 usermod功能说明：修改用户帐号。 语 法：usermod [-LU][-c &lt;备注&gt;][-d &lt;登入目录&gt;][-e &lt;有效期限&gt;][-f &lt;缓冲天数&gt;][-g &lt;群组&gt;][-G &lt;群组&gt;][-l &lt;帐号名称&gt;][-s &lt;shell&gt;][-u &lt;uid&gt;][用户帐号] 补充说明：usermod可用来修改用户帐号的各项设定。 参 数： -c&lt;备注&gt; 修改用户帐号的备注文字。 -d登入目录&gt; 修改用户登入时的目录。 -e&lt;有效期限&gt; 修改帐号的有效期限。 -f&lt;缓冲天数&gt; 修改在密码过期后多少天即关闭该帐号。 -g&lt;群组&gt; 修改用户所属的群组。 -G&lt;群组&gt; 修改用户所属的附加群组。 -l&lt;帐号名称&gt; 修改用户帐号名称。 -L 锁定用户密码，使密码无效。 -s&lt;shell&gt; 修改用户登入后所使用的shell。 -u&lt;uid&gt; 修改用户ID。 -U 解除密码锁定。 vlock(virtual console lock)功能说明：锁住虚拟终端。 语 法：vlock [-achv] 补充说明：执行vlock指令可锁住虚拟终端，避免他人使用。 参 数： -a或–all 锁住所有的终端阶段作业，如果您在全屏幕的终端中使用本参数，则会将用键盘切换终端机的功能一并关闭。 -c或–current 锁住目前的终端阶段作业，此为预设值。 -h或–help 在线帮助。 -v或–version 显示版本信息。 w功能说明：显示目前登入系统的用户信息。 语 法：w [-fhlsuV][用户名称] 补充说明：执行这项指令可得知目前登入系统的用户有那些人，以及他们正在执行的程序。单独执行w指令会显示所有的用户，您也可指定用户名称，仅显示某位用户的相关信息。 参 数： -f 开启或关闭显示用户从何处登入系统。 -h 不显示各栏位的标题信息列。 -l 使用详细格式列表，此为预设值。 -s 使用简洁格式列表，不显示用户登入时间，终端机阶段作业和程序所耗费的CPU时间。 -u 忽略执行程序的名称，以及该程序耗费CPU时间的信息。 -V 显示版本信息。 who功能说明：显示目前登入系统的用户信息。 语 法：who [-Himqsw][–help][–version][am i][记录文件] 补充说明：执行这项指令可得知目前有那些用户登入系统，单独执行who指令会列出登入帐号，使用的 终端机，登入时间以及从何处登入或正在使用哪个X显示器。 参 数： -H或–heading 显示各栏位的标题信息列。 -i或-u或–idle 显示闲置时间，若该用户在前一分钟之内有进行任何动作，将标示成”.”号，如果该用户已超过24小时没有任何动作，则标示出”old”字符串。 -m 此参数的效果和指定”am i”字符串相同。 -q或–count 只显示登入系统的帐号名称和总人数。 -s 此参数将忽略不予处理，仅负责解决who指令其他版本的兼容性问题。 -w或-T或–mesg或–message或–writable 显示用户的信息状态栏。 –help 在线帮助。 –version 显示版本信息。 whoami功能说明：显示用户名称。 语 法：whoami [–help][–version] 补充说明：显示自身的用户名称，本指令相当于执行”id -un”指令。 参 数： –help 在线帮助。 –version 显示版本信息。 whois功能说明：查找并显示用户信息。 语 法：whois [帐号名称] 补充说明：whois指令会去查找并显示指定帐号的用户相关信息，因为它是到Network Solutions的WHOIS数据库去查找，所以该帐号名称必须在上面注册方能寻获，且名称没有大小写的差别。 linux系统设置命令alias功能说明：设置指令的别名。 语 法：alias[别名]=[指令名称] 补充说明：用户可利用alias，自定指令的别名。若仅输入alias，则可列出目前所有的别名设置。 alias的效力仅及于该次登入的操作。若要每次登入是即自动设好别名，可在.profile或.cshrc中设定指令的别名。 参 数：若不加任何参数，则列出目前所有的别名设置。 apmd(advanced power management BIOS daemon)功能说明：进阶电源管理服务程序。 语 法：apmd [-u v V W][-p &lt;百分比变化量&gt;][-w &lt;百分比值&gt;] 补充说明：apmd负责BIOS进阶电源管理(APM)相关的记录，警告与管理工作。 参 数： -p&lt;百分比变化量&gt;或–percentage&lt;百分比变化量&gt; 当电力变化的幅度超出设置的百分比变化量，即记录事件百分比变化量的预设值为5，若设置值超过100，则关闭此功能。 -u或–utc 将BIOS时钟设为UTC，以便从悬待模式恢复时，将-u参数传送至clock或hwclock程序。 -v或–verbose 记录所有的APM事件。 -V或–version 显示版本信息。 -w&lt;百分比值&gt;或–warn&lt;百分比值&gt; 当电池不在充电状态时，且电池电量低于设置的百分比值，则在syslog(2)的ALERT层记录警告信息。百分比值的预设置为10，若设置为0，则关闭此功能。 -W或–wall 发出警告信息给所有人。 aumix(audio mixer)功能说明：设置音效装置。 语 法：aumix [-123bcilmoprstvwWx][(+/-)强度][PqR][-dfhILqS] 补充说明：设置各项音效装置的信号强度以及指定播放与录音的装置。 参 数： [-123bcilmoprstvwWx]为频道参数，用来指定装置的频道；[PqR]可用来指定播放或录音装置；[-dfhILqS] 则为指令参数。若不加任何参数，aumix会显示简单的图形界面供调整设置频道参数： -1 输入信号线１。 -2 输入信号线２。 -3 输入信号线３。 -b 低音。 -c CD。 -i 输入信号强度。 -m 麦克风。 -o 输出信号强度。 -p PC喇叭。 -r 录音。 -s 合成器。 -t 高音。 -v 主音量。 -w PCM。 -W PCM2。 -x 混音器。 (+/-)强度 出现(+/-)时，代表在原有的强度上加减指定值。若未使用(+/-)，则直接将强度设为指定值。 指定音效装置 P 指定播放装置。 q 显示频道设置。 R 指定录音装置。 指令参数 -d 指定音效装置的名称。 -f 指定存储或载入设置的文件。 -h 在使用时显示信息。 -I 以图形界面方式来执行aumix。 -L 从$HOME/.aumixrc或/etc/aumixrc载入设置。 -q 显示所有频道的设置值。 -S 将设置值保存至/HOME/.aumixrc。 bind功能说明：显示或设置键盘按键与其相关的功能。 语 法：bind [-dlv][-f &lt;按键配置文件&gt;][-m &lt;按键配置&gt;][-q &lt;功能&gt;] 补充说明：您可以利用bind命令了解有哪些按键组合与其功能，也可以自行指定要用哪些按键组合。 参 数： -d 显示按键配置的内容。 -f&lt;按键配置文件&gt; 载入指定的按键配置文件。 -l 列出所有的功能。 -m&lt;按键配置&gt; 指定按键配置。 -q&lt;功能&gt; 显示指定功能的按键。 -v 列出目前的按键配置与其功能。 chkconfig(check config)功能说明：检查，设置系统的各种服务。 语 法：chkconfig [–add][–del][–list][系统服务] 或 chkconfig [–level &lt;等级代号&gt;][系统服务][on/off/reset] 补充说明：这是Red Hat公司遵循GPL规则所开发的程序，它可查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。 参 数： –add 增加所指定的系统服务，让chkconfig指令得以管理它，并同时在系统启动的叙述文件内增加相关数据。 –del 删除所指定的系统服务，不再由chkconfig指令管理，并同时在系统启动的叙述文件内删除相关数据。 –level&lt;等级代号&gt; 指定读系统服务要在哪一个执行等级中开启或关毕 chroot(change root)功能说明：改变根目录。 语 法：chroot [–help][–version][目的目录][执行指令…] 补充说明：把根目录换成指定的目的目录。 参 数： –help 在线帮助。 –version 显示版本信息。 clock功能说明：调整 RTC 时间。 语 法：clock [–adjust][–debug][–directisa][–getepoch][–hctosys][–set –date=”&lt;日期时间&gt;”][–setepoch –epoch=&lt;年份&gt;][–show][–systohc][–test][–utc][–version] 补充说明：RTC 是电脑内建的硬件时间，执行这项指令可以显示现在时刻，调整硬件时钟的时间，将系统时间设成与硬件时钟之时间一致，或是把系统时间回存到硬件时钟。 参 数： –adjust 第一次使用”–set”或”–systohc”参数设置硬件时钟，会在/etc目录下产生一个名称为adjtime的文件。当再次使用这两个参数调整硬件时钟，此文件便会记录两次调整间之差异，日后执行clock指令加上”–adjust”参数时，程序会自动根 据记录文件的数值差异，计算出平均值，自动调整硬件时钟的时间。 –debug 详细显示指令执行过程，便于排错或了解程序执行的情形。 –directisa 告诉clock指令不要通过/dev/rtc设备文件，直接对硬件时钟进行存取。这个参数适用于仅有ISA总线结构的老式电脑。 –getepoch 把系统核心内的硬件时钟新时代数值，呈现到标准输出设备。 –hctosys Hardware Clock to System Time，把系统时间设成和硬件时钟一致。由于这个动作将会造成系统全面更新文件的存取时间，所以最好在系统启动时就执行它。 –set–date 设置硬件时钟的日期和时间。 –setepoch–epoch=&lt;年份&gt; 设置系统核心之硬件时钟的新时代数值，年份以四位树字表示。 –show 读取硬件时钟的时间，并将其呈现至标准输出设备。 –systohc System Time to Hardware Clock，将系统时间存回硬件时钟内。 –test 仅作测试，并不真的将时间写入硬件时钟或系统时间。 –utc 把硬件时钟上的时间时为CUT，有时也称为UTC或UCT。 –version 显示版本信息。 crontab功能说明：设置计时器。 语 法：crontab [-u &lt;用户名称&gt;][配置文件] 或 crontab [-u &lt;用户名称&gt;][-elr] 补充说明：cron是一个常驻服务，它提供计时器的功能，让用户在特定的时间得以执行预设的指令或程序。只要用户会编辑计时器的配置文件，就可以使用计时器的功能。其配置文件格式如下： Minute Hour Day Month DayOFWeek Command 参 数： -e 编辑该用户的计时器设置。 -l 列出该用户的计时器设置。 -r 删除该用户的计时器设置。 -u&lt;用户名称&gt; 指定要设定计时器的用户名称。 declare功能说明：声明 shell 变量。 语 法：declare [+/-][rxi][变量名称＝设置值] 或 declare -f 补充说明：declare为shell指令，在第一种语法中可用来声明变量并设置变量的属性([rix]即为变量的属性），在第二种语法中可用来显示shell函数。若不加上任何参数，则会显示全部的shell变量与函数(与执行set指令的效果相同)。 参 数： +/- “-“可用来指定变量的属性，”+”则是取消变量所设的属性。 -f 仅显示函数。 r 将变量设置为只读。 x 指定的变量会成为环境变量，可供shell以外的程序来使用。 i [设置值]可以是数值，字符串或运算式。 depmod(depend module)功能说明：分析可载入模块的相依性。 语 法：depmod [-adeisvV][-m &lt;文件&gt;][–help][模块名称] 补充说明：depmod可检测模块的相依性，供modprobe在安装模块时使用。 参 数： -a或–all 分析所有可用的模块。 -d或debug 执行排错模式。 -e 输出无法参照的符号。 -i 不检查符号表的版本。 -m&lt;文件&gt;或system-map&lt;文件&gt; 使用指定的符号表文件。 -s或–system-log 在系统记录中记录错误。 -v或–verbose 执行时显示详细的信息。 -V或–version 显示版本信息。 –help 显示帮助。 dircolors功能说明：设置 ls 指令在显示目录或文件时所用的色彩。 语 法：dircolors [色彩配置文件] 或 dircolors [-bcp][–help][–version] 补充说明：dircolors可根据[色彩配置文件]来设置LS_COLORS环境变量或是显示设置LS_COLORS环境变量的shell指令。 参 数： -b或–sh或–bourne-shell 显示在Boume shell中，将LS_COLORS设为目前预设置的shell指令。 -c或–csh或–c-shell 显示在C shell中，将LS_COLORS设为目前预设置的shell指令。 -p或–print-database 显示预设置 -help 显示帮助。 -version 显示版本信息。 dmesg功能说明：显示开机信息。 语 法：dmesg [-cn][-s &lt;缓冲区大小&gt;] 补充说明：kernel会将开机信息存储在ring buffer中。您若是开机时来不及查看信息，可利用dmesg来查看。开机信息亦保存在/var/log目录中，名称为dmesg的文件里。 参 数： -c 显示信息后，清除ring buffer中的内容。 -s&lt;缓冲区大小&gt; 预设置为8196，刚好等于ring buffer的大小。 -n 设置记录信息的层级。 enable功能说明：启动或关闭 shell 内建指令。 语 法：enable [-n][-all][内建指令] 补充说明：若要执行的文件名称与shell内建指令相同，可用enable -n来关闭shell内建指令。若不加-n参数，enable可重新启动关闭的指令。 参 数： -n 关闭指定的shell内建指令。 -all 显示shell所有关闭与启动的指令。 eval功能说明：重新运算求出参数的内容。 语 法：eval [参数] 补充说明：eval可读取一连串的参数，然后再依参数本身的特性来执行。 参 数：参数不限数目，彼此之间用分号分开。 export功能说明：设置或显示环境变量。 语 法：export [-fnp][变量名称]=[变量设置值] 补充说明：在shell中执行程序时，shell会提供一组环境变量。export可新增，修改或删除环境变量，供后续执行的程序使用。export的效力仅及于该此登陆操作。 参 数： -f 代表[变量名称]中为函数名称。 -n 删除指定的变量。变量实际上并未删除，只是不会输出到后续指令的执行环境中。 -p 列出所有的shell赋予程序的环境变量。 fbset(frame buffer setup)功能说明：设置景框缓冲区。 语 法：fbset [-ahinsvVx][-db &lt;信息文件&gt;][-fb &lt;外围设备代号&gt;][–test][显示模式] 补充说明：fbset指令可用于设置景框缓冲区的大小，还能调整画面之分辨率，位置，高低宽窄，色彩 深度，并可决定是否启动先卡之各项硬件特性。 参 数： -a或–all 改变所有使用该设备之虚拟终端机的显示模式。 -db&lt;信息文件&gt; 指定显示模式的信息文件，预设值文件名称为fb.modes，存放在/etc目录下 -fb&lt;外围设备代号&gt; 指定用来做为输出景框缓冲区之外围设备，预设置为”/dev/fd0”。 -h或-help 在线帮助。 -i或–info 列出所有景框缓冲区之相关信息。 -ifb&lt;外围设备代号&gt; 使用另一个景框缓冲区外围设备之设置值。 -n或–now 马上改变显示模式。 -ofb&lt;外围设备代号&gt; 此参数效果和指定”-fb”参数相同。 -s或–show 列出目前显示模式之设置。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -x或–xfree86 使用XFree86兼容模式。 –test 仅做测试，并不改变现行的显示模式 grpconv(group convert to shadow password)功能说明：开启群组的投影密码。 语 法：grpconv 补充说明：Linux系统里的用户和群组密码，分别存放在/etc目录下的passwd和group文件中。因系统运作所需，任何人都得以读取它们，造成安全上的破绽。投影密码将文件内的密码改存在/etc目录下的shadow和gshadow文件内，只允许系统管理者读取，同时把原密码置换为”x”字符。投影密码的功能可随时开启或关闭，您只需执行grpconv指令就能开启群组投影密码。 grpunconv(group unconvert from shadow password)功能说明：关闭群组的投影密码。 语 法：grpunconv 补充说明：执行grpunconv指令可关闭群组投影密码，它会把密码从gshadow文件内，回存到group文件里。 hwclock(hardware clock)功能说明：显示与设定硬件时钟。 语 法：hwclock [–adjust][–debug][–directisa][–hctosys][–show][–systohc][–test][–utc][–version][–set –date=&lt;日期与时间&gt;] 补充说明：在Linux中有硬件时钟与系统时钟等两种时钟。硬件时钟是指主机板上的时钟设备，也就是通常可在BIOS画面设定的时钟。系统时钟则是指kernel中的时钟。当Linux启动时，系统时钟会去读取硬件时钟的设定，之后系统时钟即独立运作。所有Linux相关指令与函数都是读取系统时钟的设定。 参 数： –adjust hwclock每次更改硬件时钟时，都会记录在/etc/adjtime文件中。使用–adjust参数，可使hwclock根据先前的记录来估算硬件时钟的偏差，并用来校正目前的硬件时钟。 –debug 显示hwclock执行时详细的信息。 –directisa hwclock预设从/dev/rtc设备来存取硬件时钟。若无法存取时，可用此参数直接以I/O指令来存取硬件时钟。 –hctosys 将系统时钟调整为与目前的硬件时钟一致。 –set –date=&lt;日期与时间&gt; 设定硬件时钟。 –show 显示硬件时钟的时间与日期。 –systohc 将硬件时钟调整为与目前的系统时钟一致。 –test 仅测试程序，而不会实际更改硬件时钟。 –utc 若要使用格林威治时间，请加入此参数，hwclock会执行转换的工作。 –version 显示版本信息。 insmod(install module)功能说明：载入模块。 语 法：insmod [-fkmpsvxX][-o &lt;模块名称&gt;][模块文件][符号名称 = 符号值] 补充说明：Linux有许多功能是通过模块的方式，在需要时才载入kernel。如此可使kernel较为精简，进而提高效率，以及保有较大的弹性。这类可载入的模块，通常是设备驱动程序。 参 数： -f 不检查目前kernel版本与模块编译时的kernel版本是否一致，强制将模块载入。 -k 将模块设置为自动卸除。 -m 输出模块的载入信息。 -o&lt;模块名称&gt; 指定模块的名称，可使用模块文件的文件名。 -p 测试模块是否能正确地载入kernel。 -s 将所有信息记录在系统记录文件中。 -v 执行时显示详细的信息。 -x 不要汇出模块的外部符号。 -X 汇出模块所有的外部符号，此为预设置 kbdconfig功能说明：设置键盘类型。 语 法：kbdconfig [–back][–test] 补充说明：kbdconfig(Red Hat Linux才有的指令)是一个用来设置键盘的程序，提供图形化的操作界面。kbdconfig实际上是修改/etc/sysconfig/keyboard的键盘配置文件。 参 数： –back 执行时将预设的Cancel按钮更改为Back按钮。 –test 仅作测试，不会实际更改设置。 lilo(linux loader)功能说明：安装核心载入，开机管理程序。 语 法：lilo [-clqtV][-b&lt;外围设备代号&gt;][-C&lt;配置文件&gt;][-d&lt;延迟时间&gt;][－D&lt;识别标签&gt;][-f&lt;几何参数文件&gt;][-i&lt;开机磁区文件&gt;][-I&lt;识别标签&gt;][-m&lt;映射文件&gt;][-P&lt;fix/ignore&gt;][-r&lt;根目录&gt;][-R&lt;执行指令&gt;…][-s&lt;备份文件&gt;][-S&lt;备份文件&gt;][-uU&lt;外围设备代号&gt;][-v…] 补充说明：lilo是个Linux系统核心载入程序，同时具备管理开机的功能。单独执行lilo指令，它会读取/etc/目录下的lilo.conf配置文件，然后根据其内容安装lilo。 参 数： -b&lt;外围设备代号&gt; 指定安装lilo之处的外围设备代号。 -c 使用紧致映射模式。 -C&lt;配置文件&gt; 指定lilo的配置文件。 -d&lt;延迟时间&gt; 设置开机延迟时间。 -D&lt;识别标签&gt; 指定开机后预设启动的操作系统，或系统核心识别标签。 -f&lt;几何参数文件&gt; 指定磁盘的几何参数配置文件。 -i&lt;开机磁区文件&gt; 指定欲使用的开机磁区文件，预设是/boot目录里的boot.b文件。 -I&lt;识别标签&gt; 显示系统核心存放之处。 -l 产生线形磁区地址。 -m&lt;映射文件&gt; 指定映射文件。 -P&lt;fix/ignore&gt; 决定要修复或忽略分区表的错误。 -q 列出映射的系统核心文件。 -r&lt;根目录&gt; 设置系统启动时欲挂入成为根目录的目录。 -R&lt;执行指令&gt; 设置下次启动系统时，首先执行的指令。 -s&lt;备份文件&gt; 指定备份文件。 -S&lt;备份文件&gt; 强制指定备份文件。 -t 不执行指令，仅列出实际执行会进行的动作。 -u&lt;外围色设备代号&gt; 删除lilo。 -U&lt;外围设备代号&gt; 此参数的效果和指定”-u”参数类似，当不检查时间戳记。 -v 显示指令执行过程。 -V 显示版本信息。 liloconfig功能说明：设置核心载入，开机管理程序。 语 法：liloconfig 补充说明：liloconfig是Slackware发行版专门用来调整lilo设置的程序。它通过互动式操作界面，让用户能够利用键盘上的方向键等，轻易地操控lilo的安装，设置作业，而无须下达各种参数或撰写配置文件。 lsmod(list modules)功能说明：显示已载入系统的模块。 语 法：lsmod 补充说明：执行lsmod指令，会列出所有已载入系统的模块。Linux操作系统的核心具有模块化的特性，应此在编译核心时，务须把全部的功能都放入核心。您可以将这些功能编译成一个个单独的模块，待需要时再分别载入。 minfo功能说明：显示MS-DOS文件系统的各项参数。 语 法：minfo [-v][驱动器代号] 补充说明：minfo为mtools工具指令，可显示MS-DOS系统磁盘的各项参数，包括磁区数，磁头数…等。 参 数： -v 除了一般信息外，并显示可开机磁区的内容。 mkkickstart功能说明：建立安装的组态文件。 语 法：mkkickstart [–bootp][–dhcp][–nonet][–nox][–version][–nfs &lt;远端电脑:路径&gt;] 补充说明：mkkickstart可根据目前系统的设置来建立组态文件，供其他电脑在安装时使用。组态文件的内容包括使用语言，网络环境，系统磁盘状态，以及X Windows的设置等信息。 参 数： –bootp 安装与开机时，使用BOOTP。 –dhcp 安装与开机时，使用DHCP。 –nfs&lt;远端电脑:路径&gt; 使用指定的网络路径安装。 –nonet 不要进行网络设置，即假设在没有网络环境的状态下。 –nox 不要进行X Windows的环境设置。 –version 显示版本信息。 modinfo(module infomation)功能说明：显示kernel模块的信息。 语 法：modinfo [-adhpV][模块文件] 补充说明：modinfo会显示kernel模块的对象文件，以显示该模块的相关信息。 参 数： -a或–author 显示模块开发人员。 -d或–description 显示模块的说明。 -h或–help 显示modinfo的参数使用方法。 -p或–parameters 显示模块所支持的参数。 -V或–version 显示版本信息。 modprobe(module probe)功能说明：自动处理可载入模块。 语 法：modprobe [-acdlrtvV][–help][模块文件][符号名称 = 符号值] 补充说明：modprobe可载入指定的个别模块，或是载入一组相依的模块。modprobe会根据depmod所产生的相依关系，决定要载入哪些模块。若在载入过程中发生错误，在modprobe会卸载整组的模块。 参 数： -a或–all 载入全部的模块。 -c或–show-conf 显示所有模块的设置信息。 -d或–debug 使用排错模式。 -l或–list 显示可用的模块。 -r或–remove 模块闲置不用时，即自动卸载模块。 -t或–type 指定模块类型。 -v或–verbose 执行时显示详细的信息。 -V或–version 显示版本信息。 -help 显示帮助。 mouseconfig功能说明：设置鼠标相关参数。 语 法：mouseconfig [–back][–emulthree][–help][–expert][–kickstart][–noprobe][–test][–device &lt;连接端口&gt;][鼠标类型] 补充说明：mouseconfig为鼠标设置程序，可自动设置相关参数，或者用户也可以利用所提供互动模式自行设置鼠标。mouseconfig是Red Hat Linux才有的命令。 参 数： –back 在设置画面上显示Back按钮，而取代预设的Cancel按钮。 –device&lt;连接端口&gt; 指定硬件连接端口。可用的选项有ttyS0，ttyS1，ttyS2，ttyS3与orpsaux。 –emulthree 将二钮鼠标模拟成三钮鼠标。 –help 显示帮助以及所有支持的鼠标类型。 –expert 程序预设可自动判断部分设置值。若要自行设置，请使用–expert参数。 –kickstart 让程序自动检测并保存所有的鼠标设置。 –noprobe 不要检测鼠标设备。 –test 测试模式，不会改变任何设置。 ntsysv功能说明：设置系统的各种服务。 语 法：ntsysv [–back][–level &lt;等级代号&gt;] 补充说明：这是Red Hat公司遵循GPL规则所开发的程序，它具有互动式操作界面，您可以轻易地利用方向键和空格键等，开启，关闭操作系统在每个执行等级中，所要执行的系统服务。 参 数： –back 在互动式界面里，显示Back钮，而非Cancel钮。 –level &lt;等级代号&gt; 在指定的执行等级中，决定要开启或关闭哪些系统服务。 passwd(password)功能说明：设置密码。 语 法：passwd [-dklS][-u &lt;-f&gt;][用户名称] 补充说明：passwd指令让用户可以更改自己的密码，而系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 参 数： -d 删除密码。本参数仅有系统管理者才能使用。 -f 强制执行。 -k 设置只有在密码过期失效后，方能更新。 -l 锁住密码。 -s 列出密码的相关信息。本参数仅有系统管理者才能使用。 -u 解开已上锁的帐号。 pwconv功能说明：开启用户的投影密码。 语 法：pwconv 补充说明：Linux系统里的用户和群组密码，分别存放在名称为passwd和group的文件中， 这两个文件位于/etc目录下。因系统运作所需，任何人都得以读取它们，造成安全上的破绽。投影密码将文件内的密码改存在/etc目录下的shadow和gshadow文件内，只允许系统管理者读取，同时把原密码置换为”x”字符，有效的强化了系统的安全性。 pwunconv功能说明：关闭用户的投影密码。 语 法：pwunconv 补充说明：执行pwunconv指令可以关闭用户投影密码，它会把密码从shadow文件内，重回存到passwd文件里。 rdate(receive date)功能说明：显示其他主机的日期与时间。 语 法：rdate [-ps][主机名称或IP地址…] 补充说明：执行rdate指令，向其他主机询问系统时间并显示出来。 参 数： -p 显示远端主机的日期与时间。 -s 把从远端主机收到的日期和时间，回存到本地主机的系统时间。 resize功能说明：设置终端机视窗的大小。 语 法：resize [-cu][-s &lt;列数&gt; &lt;行数&gt;] 补充说明：执行resize指令可设置虚拟终端机的视窗大小。 参 数： -c 就算用户环境并非C Shell，也用C Shell指令改变视窗大小。 -s &lt;列数&gt; &lt;行数&gt; 设置终端机视窗的垂直高度和水平宽度。 -u 就算用户环境并非Bourne Shell，也用Bourne Shell指令改变视窗大小。 rmmod(remove module)功能说明：删除模块。 语 法：rmmod [-as][模块名称…] 补充说明：执行rmmod指令，可删除不需要的模块。Linux操作系统的核心具有模块化的特性，应此在编译核心时，务须把全部的功能都放如核心。你可以将这些功能编译成一个个单独的模块，待有需要时再分别载入它们。 参 数： -a 删除所有目前不需要的模块。 -s 把信息输出至syslog常驻服务，而非终端机界面。 rpm(redhat package manager)功能说明：管理套件。 语 法：rpm [-acdhilqRsv][-b&lt;完成阶段&gt;&lt;套间档&gt;+][-e&lt;套件挡&gt;][-f&lt;文件&gt;+][-i&lt;套件档&gt;][-p&lt;套件档&gt;＋][-U&lt;套件档&gt;][-vv][–addsign&lt;套件档&gt;+][–allfiles][–allmatches][–badreloc][–buildroot&lt;根目录&gt;][–changelog][–checksig&lt;套件档&gt;+][–clean][–dbpath&lt;数据库目录&gt;][–dump][–excludedocs][–excludepath&lt;排除目录&gt;][–force][–ftpproxy&lt;主机名称或IP地址&gt;][–ftpport&lt;通信端口&gt;][–help][–httpproxy&lt;主机名称或IP地址&gt;][–httpport&lt;通信端口&gt;][–ignorearch][–ignoreos][–ignoresize][–includedocs][–initdb][justdb][–nobulid][–nodeps][–nofiles][–nogpg][–nomd5][–nopgp][–noorder][–noscripts][–notriggers][–oldpackage][–percent][–pipe&lt;执行指令&gt;][–prefix&lt;目的目录&gt;][–provides][–queryformat&lt;档头格式&gt;][–querytags][–rcfile&lt;配置档&gt;][–rebulid&lt;套件档&gt;][–rebuliddb][–recompile&lt;套件档&gt;][–relocate&lt;原目录&gt;=&lt;新目录&gt;][–replacefiles][–replacepkgs][–requires][–resign&lt;套件档&gt;+][–rmsource][–rmsource&lt;文件&gt;][–root&lt;根目录&gt;][–scripts][–setperms][–setugids][–short-circuit][–sign][–target=&lt;安装平台&gt;+][–test][–timecheck&lt;检查秒数&gt;][–triggeredby&lt;套件档&gt;][–triggers][–verify][–version][–whatprovides&lt;功能特性&gt;][–whatrequires&lt;功能特性&gt;] 补充说明：rmp原本是Red Hat Linux发行版专门用来管理Linux各项套件的程序，由于它遵循GPL规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM套件管理方式的出现，让Linux易于安装，升级，间接提升了Linux的适用度。 参 数： -a 查询所有套件。 -b&lt;完成阶段&gt;&lt;套件档&gt;+或-t &lt;完成阶段&gt;&lt;套件档&gt;+ 设置包装套件的完成阶段，并指定套件档的文件名称。 -c 只列出组态配置文件，本参数需配合”-l”参数使用。 -d 只列出文本文件，本参数需配合”-l”参数使用。 -e&lt;套件档&gt;或–erase&lt;套件档&gt; 删除指定的套件。 -f&lt;文件&gt;+ 查询拥有指定文件的套件。 -h或–hash 套件安装时列出标记。 -i 显示套件的相关信息。 -i&lt;套件档&gt;或–install&lt;套件档&gt; 安装指定的套件档。 -l 显示套件的文件列表。 -p&lt;套件档&gt;+ 查询指定的RPM套件档。 -q 使用询问模式，当遇到任何问题时，rpm指令会先询问用户。 -R 显示套件的关联性信息。 -s 显示文件状态，本参数需配合”-l”参数使用。 -U&lt;套件档&gt;或–upgrade&lt;套件档&gt; 升级指定的套件档。 -v 显示指令执行过程。 -vv 详细显示指令执行过程，便于排错。 -addsign&lt;套件档&gt;+ 在指定的套件里加上新的签名认证。 –allfiles 安装所有文件。 –allmatches 删除符合指定的套件所包含的文件。 –badreloc 发生错误时，重新配置文件。 –buildroot&lt;根目录&gt; 设置产生套件时，欲当作根目录的目录。 –changelog 显示套件的更改记录。 –checksig&lt;套件档&gt;+ 检验该套件的签名认证。 –clean 完成套件的包装后，删除包装过程中所建立的目录。 –dbpath&lt;数据库目录&gt; 设置欲存放RPM数据库的目录。 –dump 显示每个文件的验证信息。本参数需配合”-l”参数使用。 –excludedocs 安装套件时，不要安装文件。 –excludepath&lt;排除目录&gt; 忽略在指定目录里的所有文件。 –force 强行置换套件或文件。 –ftpproxy&lt;主机名称或IP地址&gt; 指定FTP代理服务器。 –ftpport&lt;通信端口&gt; 设置FTP服务器或代理服务器使用的通信端口。 –help 在线帮助。 –httpproxy&lt;主机名称或IP地址&gt; 指定HTTP代理服务器。 –httpport&lt;通信端口&gt; 设置HTTP服务器或代理服务器使用的通信端口。 –ignorearch 不验证套件档的结构正确性。 –ignoreos 不验证套件档的结构正确性。 –ignoresize 安装前不检查磁盘空间是否足够。 –includedocs 安装套件时，一并安装文件。 –initdb 确认有正确的数据库可以使用。 –justdb 更新数据库，当不变动任何文件。 –nobulid 不执行任何完成阶段。 –nodeps 不验证套件档的相互关联性。 –nofiles 不验证文件的属性。 –nogpg 略过所有GPG的签名认证。 –nomd5 不使用MD5编码演算确认文件的大小与正确性。 –nopgp 略过所有PGP的签名认证。 –noorder 不重新编排套件的安装顺序，以便满足其彼此间的关联性。 –noscripts 不执行任何安装Script文件。 –notriggers 不执行该套件包装内的任何Script文件。 –oldpackage 升级成旧版本的套件。 –percent 安装套件时显示完成度百分比。 –pipe&lt;执行指令&gt; 建立管道，把输出结果转为该执行指令的输入数据。 –prefix&lt;目的目录&gt; 若重新配置文件，就把文件放到指定的目录下。 –provides 查询该套件所提供的兼容度。 –queryformat&lt;档头格式&gt; 设置档头的表示方式。 –querytags 列出可用于档头格式的标签。 –rcfile&lt;配置文件&gt; 使用指定的配置文件。 –rebulid&lt;套件档&gt; 安装原始代码套件，重新产生二进制文件的套件。 –rebuliddb 以现有的数据库为主，重建一份数据库。 –recompile&lt;套件档&gt; 此参数的效果和指定”–rebulid”参数类似，当不产生套件档。 –relocate&lt;原目录&gt;=&lt;新目录&gt; 把本来会放到原目录下的文件改放到新目录。 –replacefiles 强行置换文件。 –replacepkgs 强行置换套件。 –requires 查询该套件所需要的兼容度。 –resing&lt;套件档&gt;+ 删除现有认证，重新产生签名认证。 –rmsource 完成套件的包装后，删除原始代码。 –rmsource&lt;文件&gt; 删除原始代码和指定的文件。 –root&lt;根目录&gt; 设置欲当作根目录的目录。 –scripts 列出安装套件的Script的变量。 –setperms 设置文件的权限。 –setugids 设置文件的拥有者和所属群组。 –short-circuit 直接略过指定完成阶段的步骤。 –sign 产生PGP或GPG的签名认证。 –target=&lt;安装平台&gt;+ 设置产生的套件的安装平台。 –test 仅作测试，并不真的安装套件。 –timecheck&lt;检查秒数&gt; 设置检查时间的计时秒数。 –triggeredby&lt;套件档&gt; 查询该套件的包装者。 –triggers 展示套件档内的包装Script。 –verify 此参数的效果和指定”-q”参数相同。 –version 显示版本信息。 –whatprovides&lt;功能特性&gt; 查询该套件对指定的功能特性所提供的兼容度。 –whatrequires&lt;功能特性&gt; 查询该套件对指定的功能特性所需要的兼容度。 set功能说明：设置shell。 语 法：set [+-abCdefhHklmnpPtuvx] 补充说明：set指令能设置所使用shell的执行方式，可依照不同的需求来做设置。 参 数： -a 标示已修改的变量，以供输出至环境变量。 -b 使被中止的后台程序立刻回报执行状态。 -C 转向所产生的文件无法覆盖已存在的文件。 -d Shell预设会用杂凑表记忆使用过的指令，以加速指令的执行。使用-d参数可取消。 -e 若指令传回值不等于0，则立即退出shell。 -f 取消使用通配符。 -h 自动记录函数的所在位置。 -H Shell 可利用”!”加&lt;指令编号&gt;的方式来执行history中记录的指令。 -k 指令所给的参数都会被视为此指令的环境变量。 -l 记录for循环的变量名称。 -m 使用监视模式。 -n 只读取指令，而不实际执行。 -p 启动优先顺序模式。 -P 启动-P参数后，执行指令时，会以实际的文件或目录来取代符号连接。 -t 执行完随后的指令，即退出shell。 -u 当执行时使用到未定义过的变量，则显示错误信息。 -v 显示shell所读取的输入值。 -x 执行指令后，会先显示该指令及所下的参数。 +&lt;参数&gt; 取消某个set曾启动的参数。 setconsole功能说明：设置系统终端。 语 法：setconsole [video][serial][ttya][ttyb] 补充说明：setconsole可用来指定系统终端。 参 数： serial 使用PROM终端。 ttya,cua0或ttyS0 使用第１个串口设备作为终端。 ttyb,cua1或ttyS1 使用第２个串口设备作为终端。 video 使用主机上的现卡作为终端。 setenv(set environment variable)功能说明：查询或显示环境变量。 语 法：setenv [变量名称][变量值] 补充说明：setenv为tsch中查询或设置环境变量的指令 setup功能说明：设置公用程序。 语 法：setup 补充说明：setup是一个设置公用程序，提供图形界面的操作方式。在setup中可设置7类的选项： １.登陆认证方式 ２.键盘组态设置 ３.鼠标组态设置 ４.开机时所要启动的系统服务 ５.声卡组态设置 ６.时区设置 ７.X Windows组态设置 sndconfig功能说明：设置声卡。 语 法：sndconfig [–help][–noautoconfig][–noprobe] 补充说明：sndconfig为声卡设置程序，支持PnP柚茫勺远觳獠⑸柚肞nP声卡。 参 数： –help 显示帮助。 –noautoconfig 不自动设置PnP的声卡。 –noprobe 不自动检测PnP声卡。 SVGAText Mode功能说明：加强文字模式的显示画面。 语 法：SVGATextMode [-acdfhmnrsv][-t &lt;配置文件&gt;][模式] 补充说明：SVGATextMode可用来设置文字模式下的显示画面，包括分辨率，字体和更新频率等。 参 数： -a 如果新显示模式的屏幕大小与原先不同时，SVGATextMode会执行必要的系统设置。 -c 维持原有的VGA时脉。 -d 执行时会显示详细的信息，供排错时参考。 -f 不要执行配置文件中有关字体载入的指令。 -h 显示帮助。 -m 允许1x1的方式来重设屏幕大小。 -n 仅测试指定的模式。 -r 通知或重设与屏幕大小相关的程序。 -s 显示配置文件中所有可用的模式。 -t&lt;配置文件&gt; 指定配置文件。 -v SVGATextMode在配置新的显示模式时，预设会先检查垂直与水平的更新更新频率是否在配置文件所指定的范围内，如果不在范围内，则不设置新的显示模式。 [模式］ [模式]参数必须是配置文件中模式的名称。 timeconfig功能说明：设置时区。 语 法：timeconfig [–arc][–back][–test][–utc][时区名称] 补充说明：这是Red Hat公司遵循GPL规则所开发的程序，它具有互动式操作界面，您可以轻易地利用方向键和空格键等，设置系统时间所属的时区。 参 数： –arc 使用Alpha硬件结构的格式存储系统时间。 –back 在互动式界面里，显示Back钮而非Cancel钮。 –test 仅作测试，并不真的改变系统的时区。 –utc 把硬件时钟上的时间视为CUT，有时也称为UTC或UCT。 ulimit功能说明：控制shell程序的资源。 语 法：ulimit [-aHS][-c &lt;core文件上限&gt;][-d &lt;数据节区大小&gt;][-f &lt;文件大小&gt;][-m &lt;内存大小&gt;][-n &lt;文件数目&gt;][-p &lt;缓冲区大小&gt;][-s &lt;堆叠大小&gt;][-t &lt;CPU时间&gt;][-u &lt;程序数目&gt;][-v &lt;虚拟内存大小&gt;] 补充说明：ulimit为shell内建指令，可用来控制shell执行程序的资源。 参 数： -a 显示目前资源限制的设定。 -c &lt;core文件上限&gt; 设定core文件的最大值，单位为区块。 -d &lt;数据节区大小&gt; 程序数据节区的最大值，单位为KB。 -f &lt;文件大小&gt; shell所能建立的最大文件，单位为区块。 -H 设定资源的硬性限制，也就是管理员所设下的限制。 -m &lt;内存大小&gt; 指定可使用内存的上限，单位为KB。 -n &lt;文件数目&gt; 指定同一时间最多可开启的文件数。 -p &lt;缓冲区大小&gt; 指定管道缓冲区的大小，单位512字节。 -s &lt;堆叠大小&gt; 指定堆叠的上限，单位为KB。 -S 设定资源的弹性限制。 -t &lt;CPU时间&gt; 指定CPU使用时间的上限，单位为秒。 -u &lt;程序数目&gt; 用户最多可开启的程序数目。 -v &lt;虚拟内存大小&gt; 指定可使用的虚拟内存上限，单位为KB。 unalias功能说明：删除别名。 语 法：unalias [-a][别名] 补充说明：unalias为shell内建指令，可删除别名设置。 参 数： -a 删除全部的别名。 unset功能说明：删除变量或函数。 语 法：unset [-fv][变量或函数名称] 补充说明：unset为shell内建指令，可删除变量或函数。 参 数： -f 仅删除函数。 -v 仅删除变量。 linux文档编辑命令col功能说明：过滤控制字符。 语 法：col [-bfx][-l&lt;缓冲区列数&gt;] 补充说明：在许多UNIX说明文件里，都有RLF控制字符。当我们运用shell特殊字符”&gt;”和”&gt;&gt;”，把说明文件的内容输出成纯文本文件时，控制字符会变成乱码，col指令则能有效滤除这些控制字符。 参 数： -b 过滤掉所有的控制字符，包括RLF和HRLF。 -f 滤除RLF字符，但允许将HRLF字符呈现出来。 -x 以多个空格字符来表示跳格字符。 -l&lt;缓冲区列数&gt; 预设的内存缓冲区有128列，您可以自行指定缓冲区的大小。 colrm(column remove)功能说明：滤掉指定的行。 语 法：colrm [开始行数编号&lt;结束行数编号&gt;] 补充说明：colrm指令从标准输入设备读取书记，转而输出到标准输出设备。如果不加任何参数，则该指令不会过滤任何一行。 comm(common)功能说明：比较两个已排过序的文件。 语 法：comm [-123][–help][–version][第1个文件][第2个文件] 补充说明：这项指令会一列列地比较两个已排序文件的差异，并将其结果显示出来，如果没有指定任何参数，则会把结果分成3行显示：第1行仅是在第1个文件中出现过的列，第2行是仅在第2个文件中出现过的列，第3行则是在第1与第2个文件里都出现过的列。若给予的文件名称为”-“，则comm指令会从标准输入设备读取数据。 参 数： -1 不显示只在第1个文件里出现过的列。 -2 不显示只在第2个文件里出现过的列。 -3 不显示只在第1和第2个文件里出现过的列。 –help 在线帮助。 –version 显示版本信息。 csplit(context split)功能说明：分割文件。 语 法：csplit [-kqsz][-b&lt;输出格式&gt;][-f&lt;输出字首字符串&gt;][-n&lt;输出文件名位数&gt;][–help][–version][文件][范本样式…] 补充说明：将文件依照指定的范本样式予以切割后，分别保存成名称为xx00,xx01,xx02…的文件。若给予的文件名称为”-“，则csplit指令会从标准输入设备读取数据。 参 数： -b&lt;输出格式&gt;或–suffix-format=&lt;输出格式&gt; 预设的输出格式其文件名称为xx00,xx01…等，您可以通过改变&lt;输出格式&gt;来改变输出的文件名。 -f&lt;输出字首字符串&gt;或–prefix=&lt;输出字首字符串&gt; 预设的输出字首字符串其文件名为xx00,xx01…等，如果你指定输出字首字符串为”hello”，则输出的文件名称会变成hello00,hello01…等。 -k或–keep-files 保留文件，就算发生错误或中断执行，也不能删除已经输出保存的文件。 -n&lt;输出文件名位数&gt;或–digits=&lt;输出文件名位数&gt; 预设的输出文件名位数其文件名称为xx00,xx01…等，如果你指定输出文件名位数为”3”，则输出的文件名称会变成xx000,xx001…等。 -q或-s或–quiet或–silent 不显示指令执行过程。 -z或–elide-empty-files 删除长度为0 Byte文件。 –help 在线帮助。 –version 显示版本信息。 ed(editor)功能说明：文本编辑器。 语 法：ed [-][-Gs][-p&lt;字符串&gt;][–help][–version][文件] 补充说明：ed是Linux中功能最简单的文本编辑程序，一次仅能编辑一行而非全屏幕方式的操作。 参 数： -G或–traditional 提供回兼容的功能。 -p&lt;字符串&gt; 指定ed在command mode的提示字符。 -s,-,–quiet或–silent 不执行开启文件时的检查功能。 –help 显示帮助。 –version 显示版本信息。 egrep功能说明：在文件内查找指定的字符串。egrep执行效果如grep -E，使用的语法及参数可参照grep指令，与grep不同点在于解读字符串的方法，egrep是用extended regular expression语法来解读，而grep则用basic regular expression语法，extended regular expression比basic regular expression有更完整的表达规范。 ex功能说明：在Ex模式下启动vim文本编辑器。ex执行效果如同vi -E，使用语法及参数可参照vi指令，如要从Ex模式回到普通模式，则在vim中输入:vi或:visual即可。 fgrep(fixed regexp)功能说明：查找文件里符合条件的字符串。 语 法：fgrep [范本样式][文件或目录…] 补充说明：本指令相当于执行grep指令加上参数”-F”，详见grep指令说明。 fmt(fromat)功能说明：编排文本文件。 语 法：fmt [-cstu][-p&lt;列起始字符串&gt;][-w&lt;每列字符数&gt;][–help][–version][文件…] 补充说明：fmt指令会从指定的文件里读取内容，将其依照指定格式重新编排后，输出到标准输出设备。若指定的文件名为”-“，则fmt指令会从标准输入设备读取数据。 参 数： -c或–crown-margin 每段前两列缩排。 -p&lt;列起始字符串&gt;或-prefix=&lt;列起始字符串&gt; 仅合并含有指定字符串的列，通常运用在程序语言的注解方面。 -s或–split-only 只拆开字数超出每列字符数的列，但不合并字数不足每列字符数的列。 -t或–tagged-paragraph 每列前两列缩排，但第1列和第2列的缩排格式不同。 -u或–uniform-spacing 每个字符之间都以一个空格字符间隔，每个句子之间则两个空格字符分隔。 -w&lt;每列字符数&gt;或–width=&lt;每列字符数&gt;或-&lt;每列字符数&gt; 设置每列的最大字符数。 –help 在线帮助。 –version 显示版本信息。 fold功能说明：限制文件列宽。 语 法：fold [-bs][-w&lt;每列行数&gt;][–help][–version][文件…] 补充说明：fold指令会从指定的文件里读取内容，将超过限定列宽的列加入增列字符后，输出到标准输出设备。若不指定任何文件名称，或是所给予的文件名为“-”，则fold指令会从标准输入设备读取数据。 参 数： -b或–bytes 以Byte为单位计算列宽，而非采用行数编号为单位。 -s或–spaces 以空格字符作为换列点。 -w&lt;每列行数&gt;或–width&lt;每列行数&gt; 设置每列的最大行数。 –help 在线帮助。 –version 显示版本信息。 grep功能说明：查找文件里符合条件的字符串。 语 法：grep [-abcEFGhHilLnqrsvVwxy][-A&lt;显示列数&gt;][-B&lt;显示列数&gt;][-C&lt;显示列数&gt;][-d&lt;进行动作&gt;][-e&lt;范本样式&gt;][-f&lt;范本文件&gt;][–help][范本样式][文件或目录…] 补充说明：grep指令用于查找内容包含指定的范本样式的文件，如果发现某文件的内容符合所指定的范本样式，预设grep指令会把含有范本样式的那一列显示出来。若不指定任何文件名称，或是所给予的文件名为“-”，则grep指令会从标准输入设备读取数据。 参 数： -a或–text 不要忽略二进制的数据。 -A&lt;显示列数&gt;或–after-context=&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之后的内容。 -b或–byte-offset 在显示符合范本样式的那一列之前，标示出该列第一个字符的位编号。 -B&lt;显示列数&gt;或–before-context=&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之前的内容。 -c或–count 计算符合范本样式的列数。 -C&lt;显示列数&gt;或–context=&lt;显示列数&gt;或-&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。 -d&lt;进行动作&gt;或–directories=&lt;进行动作&gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。 -e&lt;范本样式&gt;或–regexp=&lt;范本样式&gt; 指定字符串做为查找文件内容的范本样式。 -E或–extended-regexp 将范本样式为延伸的普通表示法来使用。 -f&lt;范本文件&gt;或–file=&lt;范本文件&gt; 指定范本文件，其内容含有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每列一个范本样式。 -F或–fixed-regexp 将范本样式视为固定字符串的列表。 -G或–basic-regexp 将范本样式视为普通的表示法来使用。 -h或–no-filename 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。 -H或–with-filename 在显示符合范本样式的那一列之前，表示该列所属的文件名称。 -i或–ignore-case 忽略字符大小写的差别。 -l或–file-with-matches 列出文件内容符合指定的范本样式的文件名称。 -L或–files-without-match 列出文件内容不符合指定的范本样式的文件名称。 -n或–line-number 在显示符合范本样式的那一列之前，标示出该列的列数编号。 -q或–quiet或–silent 不显示任何信息。 -r或–recursive 此参数的效果和指定“-d recurse”参数相同。 -s或–no-messages 不显示错误信息。 -v或–revert-match 反转查找。 -V或–version 显示版本信息。 -w或–word-regexp 只显示全字符合的列。 -x或–line-regexp 只显示全列符合的列。 -y 此参数的效果和指定“-i”参数相同。 –help 在线帮助。 ispell(interactive spelling checking)功能说明：拼字检查程序。 语 法：ispell [-aAbBClmMnNPStVx][-d&lt;字典文件&gt;][-L&lt;行数&gt;][-p&lt;字典文件&gt;][-w&lt;非字母字符&gt;][-W&lt;字符串长度&gt;][要检查的文件] 补充说明：ispell预设会使用/usr/lib/ispell/english.hash字典文件来检查文本文件。若在检查的文件中找到字典没有的词汇，ispell会建议使用的词汇，或是让你将新的词汇加入个人字典。 参 数： -a 当其他程序输出送到ispell时，必须使用此参数。 -A 读取到”&amp;Include File&amp;”字符串时，就去检查字符串后所指定文件的内容。 -b 产生备份文件，文件名为.bak。 -B 检查连字错误。 -C 不检查连字错误。 -d&lt;字典文件&gt; 指定字典文件。 -l 从标准输入设备读取字符串，结束后显示拼错的词汇。 -L&lt;行数&gt; 指定内文显示的行数。 -m 自动考虑字尾的变化。 -M 进入ispell后，在画面下方显示指令的按键。 -n 检查的文件为noff或troff的格式。 -N 进入ispell后，在画面下方不显示指令的按键。 -p&lt;字典文件&gt; 指定个人字典文件。 -P 不考虑字尾变化的情形。 -S 不排序建议取代的词汇。 -t 检查的文件为TeX或LaTeX的格式。 -V 非ANSI标准的字符会以”M-^”的方式来显示。 -w&lt;非字母字符&gt; 检查时，特别挑出含有指定的字符。 -W&lt;字符串长度&gt; 不检查指定长度的词汇。 -x 不要产生备份文件。 jed功能说明：编辑文本文件。 语 法：jed [-2n][-batch][-f&lt;函数&gt;][-g&lt;行数&gt;][-i&lt;文件&gt;][-I&lt;文件&gt;][-s&lt;字符串&gt;][文件] 补充说明：Jed是以Slang所写成的程序，适合用来编辑程序原始代码。 参 数： -2 显示上下两个编辑区。 -batch 以批处理模式来执行。 -f&lt;函数&gt; 执行Slang函数。 -g&lt;行数&gt; 移到缓冲区中指定的行数。 -i&lt;文件&gt; 将指定的文件载入缓冲区。 -i&lt;文件&gt; 载入Slang原始代码文件。 -n 不要载入jed.rc配置文件。 -s&lt;字符串&gt; 查找并移到指定的字符串。 joe功能说明：编辑文本文件。 语 法：joe [-asis][-beep][-csmode][-dopadding][-exask][-force][-help][-keepup][-lightoff][-arking][-mid][-nobackups][-nonotice][-nosta][-noxon][-orphan][-backpath&lt;目录&gt;][-columns&lt;栏位&gt;][-lines&lt;行数&gt;][-pg&lt;行数&gt;][-skiptop&lt;行数&gt;][-autoindent crlf linums overwrite rdonly wordwrap][+&lt;行数&gt;][-indentc&lt;缩排字符&gt;][-istep&lt;缩排字符数&gt;][-keymap&lt;按键配置文件&gt;][-lmargin&lt;栏数&gt;][-rmargin&lt;栏数&gt;][-tab&lt;栏数&gt;][要编辑的文件] 补充说明：Joe是一个功能强大的全屏幕文本编辑程序。操作的复杂度要比Pico高一点，但是功能较为齐全。Joe一次可开启多个文件，每个文件各放在一个编辑区内，并可在文件之间执行剪贴的动作。 参 数： 以下为程序参数 -asis 字符码超过127的字符不做任何处理。 -backpath&lt;目录&gt; 指定备份文件的目录。 -beep 编辑时，若有错误即发出哗声。 -columns&lt;栏位&gt; 设置栏数。 -csmode 可执行连续查找模式。 -dopadding 是程序跟tty间存在缓冲区。 -exask 在程序中，执行”Ctrl+k+x”时，会先确认是否要保存文件。 -force 强制在最后一行的结尾处加上换行符号。 -help 执行程序时一并显示帮助。 -keepup 在进入程序后，画面上方为状态列。 -lightoff 选取的区块在执行完区块命令后，就会回复成原来的状态。 -lines&lt;行数&gt; 设置行数。 -marking 在选取区块时，反白区块会随着光标移动。 -mid 当光标移出画面时，即自动卷页，使光标回到中央。 -nobackups 不建立备份文件。 -nonotice 程序执行时，不显示版权信息。 -nosta 程序执行时，不显示状态列。 -noxon 尝试取消“Ctrl+s”与“Ctrl+q”键的功能。 -orphan 若同时开启一个以上的文件，则其他文件会置于独立的缓冲区，而不会另外开启编辑区。 -pg&lt;行数&gt; 按“PageUp”或“PageDown”换页时，所要保留前一页的行数。 -skiptop&lt;行数&gt; 不使用屏幕上方指定的行数。 以下为文件参数 +&lt;行数&gt; 指定开启文件时，光标所在的行数。 -autoindent 自动缩排。 -crlf 在换行时，使用CR-LF字符。 -indentc&lt;缩排字符&gt; 执行缩排时，实际插入的字符。 -istep&lt;缩排字符数&gt; 每次执行缩排时，所移动的缩排字符数。 -keymap&lt;按键配置文件&gt; 使用不同的按键配置文件。 -linums 在每行前面加上行号。 -lmargin&lt;栏数&gt; 设置左侧边界。 -overwrite 设置覆盖模式。 -rmargin&lt;栏数&gt; 设置右侧边界。 -tab&lt;栏数&gt; 设置tab的宽度。 -rdonly 以只读的方式开启文件-wordwrap编辑时若超过右侧边界，则自动换行。 join功能说明：将两个文件中，指定栏位内容相同的行连接起来。 语 法：join [-i][-a&lt;1或2&gt;][-e&lt;字符串&gt;][-o&lt;格式&gt;][-t&lt;字符&gt;][-v&lt;1或2&gt;][-1&lt;栏位&gt;][-2&lt;栏位&gt;][–help][–version][文件1][文件2] 补充说明：找出两个文件中，指定栏位内容相同的行，并加以合并，再输出到标准输出设备。 参 数： -a&lt;1或2&gt; 除了显示原来的输出内容之外，还显示指令文件中没有相同栏位的行。 -e&lt;字符串&gt; 若[文件1]与[文件2]中找不到指定的栏位，则在输出中填入选项中的字符串。 -i或–igore-case 比较栏位内容时，忽略大小写的差异。 -o&lt;格式&gt; 按照指定的格式来显示结果。 -t&lt;字符&gt; 使用栏位的分隔字符。 -v&lt;1或2&gt; 跟-a相同，但是只显示文件中没有相同栏位的行。 -1&lt;栏位&gt; 连接[文件1]指定的栏位。 -2&lt;栏位&gt; 连接[文件2]指定的栏位。 –help 显示帮助。 –version 显示版本信息。 look功能说明：查询单字。 语 法：look [-adf][-t&lt;字尾字符串&gt;][字首字符串][字典文件] 补充说明：look指令用于英文单字的查询。您仅需给予它欲查询的字首字符串，它会显示所有开头字符串符合该条件的单字。 参 数： -a 使用另一个字典文件web2，该文件也位于/usr/dict目录下。 -d 只对比英文字母和数字，其余一慨忽略不予比对。 -f 忽略字符大小写差别。 -t&lt;字尾字符串&gt; 设置字尾字符串。 mtype功能说明：显示MS-DOS文件的内容。 语 法：mtype [-st][文件] 补充说明：mtype为mtools工具指令，模拟MS-DOS的type指令，可显示MS-DOS文件的内容。 参 数： -s 去除8位字符码集的第一个位，使它兼容于7位的ASCII。 -t 将MS-DOS文本文件中的“换行+光标移至行首”字符转换成Linux的换行字符。 pico功能说明：编辑文字文件。 语 法：pico [-bdefghjkmqtvwxz][-n&lt;间隔秒数&gt;][-o&lt;工作目录&gt;][-r&lt;编辑页宽&gt;][-s&lt;拼字检查器&gt;][+&lt;列数编号&gt;][文件] 补充说明：pico是个简单易用、以显示导向为主的文字编辑程序，它伴随着处理电子邮件和新闻组的程序pine而来。 参 数： -b 开启置换的功能。 -d 开启删除的功能。 -e 使用完整的文件名称。 -f 支持键盘上的F1、F2…等功能键。 -g 显示光标。 -h 在线帮助。 -j 开启切换的功能。 -k 预设pico在使用剪下命令时，会把光标所在的列的内容全部删除。 -m 开启鼠标支持的功能，您可用鼠标点选命令列表。 -n&lt;间隔秒数&gt; 设置多久检查一次新邮件。 -o&lt;工作目录&gt; 设置工作目录。 -q 忽略预设值。 -r&lt;编辑页宽&gt; 设置编辑文件的页宽。 -s&lt;拼字检查器&gt; 另外指定拼字检查器。 -t 启动工具模式。 -v 启动阅读模式，用户只能观看，无法编辑文件的内容。 -w 关闭自动换行，通过这个参数可以编辑内容很长的列。 -x 关闭换面下方的命令列表。 -z 让pico可被Ctrl+z中断，暂存在后台作业里。 +&lt;列数编号&gt; 执行pico指令进入编辑模式时，从指定的列数开始编辑。 rgrep(recursive grep)功能说明：递归查找文件里符合条件的字符串。 语 法：rgrep [-?BcDFhHilnNrv][-R&lt;范本样式&gt;][-W&lt;列长度&gt;][-x&lt;扩展名&gt;][–help][–version][范本样式][文件或目录…] 补充说明：rgrep指令的功能和grep指令类似，可查找内容包含指定的范本样式的文件，如果发现某文件的内容符合所指定的范本样式，预设rgrep指令会把含有范本样式的那一列显示出来。 参 数： -? 显示范本样式与范例的说明。 -B 忽略二进制的数据。 -c 计算符合范本样式的列数。 -D 排错模式，只列出指令搜寻的目录清单，而不会读取文件内容。 -F 当遇到符号连接时，rgrep预设是忽略不予处理，加上本参数后，rgrep指令就会读取该连接所指向的原始文件的内容。 -h 特别将符合范本样式的字符串标示出来。 -H 只列出符合范本样式的字符串，而非显示整列的内容。 -i 忽略字符大小写的差别。 -l 列出文件内容符合指定的范本样式的文件名称。 -n 在显示符合坊本样式的那一列之前，标示出该列的列数编号。 -N 不要递归处理。 -r 递归处理，将指定目录下的所有文件及子目录一并处理。 -R&lt;范本样式&gt; 此参数的效果和指定“-r”参数类似，但只主力符合范本样式文件名称的文件。 -v 反转查找。 -W&lt;列长度&gt; 限制符合范本样式的字符串所在列，必须拥有的字符数。 -x&lt;扩展名&gt; 只处理符合指定扩展名的文件名称的文件。 –help 在线帮助。 –version 显示版本信息。 sed(stream editor)功能说明：利用script来处理文本文件。 语 法：sed [-hnV][-e &lt;script&gt;][-f&lt;script文件&gt;][文本文件] 补充说明：sed可依照script的指令，来处理、编辑文本文件。 参 数： -e&lt;script&gt;或–expression=&lt;script&gt; 以选项中指定的script来处理输入的文本文件。 -f&lt;script文件&gt;或–file=&lt;script文件&gt; 以选项中指定的script文件来处理输入的文本文件。 -h或–help 显示帮助。 -n或–quiet或–silent 仅显示script处理后的结果。 -V或–version 显示版本信息。 sort功能说明：将文本文件内容加以排序。 语 法：sort [-bcdfimMnr][-o&lt;输出文件&gt;][-t&lt;分隔字符&gt;][+&lt;起始栏位&gt;-&lt;结束栏位&gt;][–help][–verison][文件] 补充说明：sort可针对文本文件的内容，以行为单位来排序。 参 数： -b 忽略每行前面开始出的空格字符。 -c 检查文件是否已经按照顺序排序。 -d 排序时，处理英文字母、数字及空格字符外，忽略其他的字符。 -f 排序时，将小写字母视为大写字母。 -i 排序时，除了040至176之间的ASCII字符外，忽略其他的字符。 -m 将几个排序好的文件进行合并。 -M 将前面3个字母依照月份的缩写进行排序。 -n 依照数值的大小排序。 -o&lt;输出文件&gt; 将排序后的结果存入指定的文件。 -r 以相反的顺序来排序。 -t&lt;分隔字符&gt; 指定排序时所用的栏位分隔字符。 +&lt;起始栏位&gt;-&lt;结束栏位&gt; 以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 –help 显示帮助。 –version 显示版本信息。 spell功能说明：拼字检查程序。 语 法：spell 补充说明：spell可从标准输入设备读取字符串，结束后显示拼错的词汇。 tr(translate character)功能说明：转换字符。 语 法：tr [-cdst][–help][–version][第一字符集][第二字符集] 补充说明：tr指令从标准输入设备读取数据，经过字符串转译后，输出到标准输出设备。 参 数： -c或–complerment 取代所有不属于第一字符集的字符。 -d或–delete 删除所有属于第一字符集的字符。 -s或–squeeze-repeats 把连续重复的字符以单独一个字符表示。 -t或–truncate-set1 先删除第一字符集较第二字符集多出的字符。 –help 在线帮助。 –version 显示版本信息。 uniq功能说明：检查及删除文本文件中重复出现的行列。 语 法：uniq [-cdu][-f&lt;栏位&gt;][-s&lt;字符位置&gt;][-w&lt;字符位置&gt;][–help][–version][输入文件][输出文件] 补充说明：uniq可检查文本文件中重复出现的行列。 参 数： -c或–count 在每列旁边显示该行重复出现的次数。 -d或–repeated 仅显示重复出现的行列。 -f&lt;栏位&gt;或–skip-fields=&lt;栏位&gt; 忽略比较指定的栏位。 -s&lt;字符位置&gt;或–skip-chars=&lt;字符位置&gt; 忽略比较指定的字符。 -u或–unique 仅显示出一次的行列。 -w&lt;字符位置&gt;或–check-chars=&lt;字符位置&gt; 指定要比较的字符。 –help 显示帮助。 –version 显示版本信息。 PS: [输入文件] 指定已排序好的文本文件。 [输出文件] 指定输出的文件。 wc(word count)功能说明：计算字数。 语 法：wc [-clw][–help][–version][文件…] 补充说明：利用wc指令我们可以计算文件的Byte数、字数、或是列数，若不指定文件名称、或是所给予的文件名为“-”，则wc指令会从标准输入设备读取数据。 参 数： -c或–bytes或–chars 只显示Bytes数。 -l或–lines 只显示列数。 -w或–words 只显示字数。 –help 在线帮助。 –version 显示版本信息。 linux压缩备份命令ar功能说明：建立或修改备存文件，或是从备存文件中抽取文件。 语 法：ar[-dmpqrtx][cfosSuvV][a&lt;成员文件&gt;][b&lt;成员文件&gt;][i&lt;成员文件&gt;][备存文件][成员文件] 补充说明：ar可让您集合许多文件，成为单一的备存文件。在备存文件中，所有成员文件皆保有原来的属性与权限。 参 数： 指令参数 -d 删除备存文件中的成员文件。 -m 变更成员文件在备存文件中的次序。 -p 显示备存文件中的成员文件内容。 -q 将问家附加在备存文件末端。 -r 将文件插入备存文件中。 -t 显示备存文件中所包含的文件。 -x 自备存文件中取出成员文件。 选项参数 a&lt;成员文件&gt; 将文件插入备存文件中指定的成员文件之后。 b&lt;成员文件&gt; 将文件插入备存文件中指定的成员文件之前。 c 建立备存文件。 f 为避免过长的文件名不兼容于其他系统的ar指令指令，因此可利用此参数，截掉要放入备存文件中过长的成员文件名称。 i&lt;成员文件&gt; 将问家插入备存文件中指定的成员文件之前。 o 保留备存文件中文件的日期。 s 若备存文件中包含了对象模式，可利用此参数建立备存文件的符号表。 S 不产生符号表。 u 只将日期较新文件插入备存文件中。 v 程序执行时显示详细的信息。 V 显示版本信息。 bunzip2功能说明：.bz2文件的解压缩程序。 语 法：bunzip2 [-fkLsvV][.bz2压缩文件] 补充说明：bunzip2可解压缩.bz2格式的压缩文件。bunzip2实际上是bzip2的符号连接，执行bunzip2与bzip2 -d的效果相同。 参 数： -f或–force 解压缩时，若输出的文件与现有文件同名时，预设不会覆盖现有的文件。若要覆盖，请使用此参数。 -k或–keep 在解压缩后，预设会删除原来的压缩文件。若要保留压缩文件，请使用此参数。 -s或–small 降低程序执行时，内存的使用量。 -v或–verbose 解压缩文件时，显示详细的信息。 -l,–license,-V或–version 显示版本信息。 bzip2功能说明：.bz2文件的压缩程序。 语 法：bzip2 [-cdfhkLstvVz][–repetitive-best][–repetitive-fast][- 压缩等级][要压缩的文件] 补充说明：bzip2采用新的压缩演算法，压缩效果比传统的LZ77/LZ78压缩演算法来得好。若没有加上任何参数，bzip2压缩完文件后会产生.bz2的压缩文件，并删除原始的文件。 参 数： -c或–stdout 将压缩与解压缩的结果送到标准输出。 -d或–decompress 执行解压缩。 -f或–force bzip2在压缩或解压缩时，若输出文件与现有文件同名，预设不会覆盖现有文件。若要覆盖，请使用此参数。 -h或–help 显示帮助。 -k或–keep bzip2在压缩或解压缩后，会删除原始的文件。若要保留原始文件，请使用此参数。 -s或–small 降低程序执行时内存的使用量。 -t或–test 测试.bz2压缩文件的完整性。 -v或–verbose 压缩或解压缩文件时，显示详细的信息。 -z或–compress 强制执行压缩。 -L,–license, -V或–version 显示版本信息。 –repetitive-best 若文件中有重复出现的资料时，可利用此参数提高压缩效果。 –repetitive-fast 若文件中有重复出现的资料时，可利用此参数加快执行速度。 -压缩等级 压缩时的区块大小。 bzip2recover功能说明：用来修复损坏的.bz2文件。 语 法：bzip2recover [.bz2 压缩文件] 补充说明：bzip2是以区块的方式来压缩文件，每个区块视为独立的单位。因此，当某一区块损坏时，便可利用bzip2recover，试着将文件中的区块隔开来，以便解压缩正常的区块。通常只适用在压缩文件很大的情况。 compress功能说明：压缩或解压文件。 语 法：compress [-cdfrvV][-b &lt;压缩效率&gt;][文件或目录…] 补充说明：compress是个历史悠久的压缩程序，文件经它压缩后，其名称后面会多出”.Z”的扩展名。当要解压缩时，可执行uncompress指令。事实上uncompress是指向compress的符号连接，因此不论是压缩或解压缩，都可通过compress指令单独完成。 参 数： -b&lt;压缩效率&gt; 压缩效率是一个介于9－16的数值，预设值为”16”，指定愈大的数值，压缩效率就愈高。 -c 把压缩后的文件输出到标准输出设备，不去更动原始文件。 -d 对文件进行解压缩而非压缩。 -f 强制保存压缩文件，不理会文件名称或硬连接是否存在，该文件是否为符号连接以及压缩效率高低的问题。 -r 递归处理，将指定目录下的所有文件及子目录一并处理。 -v 显示指令执行过程。 -V 显示指令版本及程序预设值。 cpio(copy in/out)功能说明：备份文件。 语 法：cpio [-0aABckLovV][-C &lt;输入/输出大小&gt;][-F &lt;备份档&gt;][-H &lt;备份格式&gt;][-O &lt;备份档&gt;][–block-size=&lt;区块大小&gt;][–force-local][–help][–quiet][–version] 或 cpio [-bBcdfikmnrsStuvV][-C &lt;输入/输出大小&gt;][-E &lt;范本文件&gt;][-F &lt;备份档&gt;][-H &lt;备份格式&gt;][-I &lt;备份档&gt;][-M &lt;回传信息&gt;][-R &lt;拥有者&gt;&lt;:/.&gt;&lt;所属群组&gt;][–block-size=&lt;区块大小&gt;][–force-local][–help][–no-absolute-filenames][–no-preserve-owner][–only-verify-crc][–quiet][–sparse][–version][范本样式…] 或 cpio [-0adkiLmpuvV][-R &lt;拥有者&gt;&lt;:/.&gt;&lt;所属群组&gt;][–help][–no-preserve-owner][–quiet][–sparse][–version][目的目] 补充说明：cpio是用来建立，还原备份档的工具程序，它可以加入，解开cpio或tra备份档内的文件。 参 数： -0或–null 接受新增列控制字符，通常配合find指令的”-print0”参数使用。 -a或–reset-access-time 重新设置文件的存取时间。 -A或–append 附加到已存在的备份档中，且这个备份档必须存放在磁盘上，而不能放置于磁带机里。 -b或–swap 此参数的效果和同时指定”-sS”参数相同。 -B 将输入/输出的区块大小改成5210 Bytes。 -c 使用旧ASCII备份格式。 -C&lt;区块大小&gt;或–io-size=&lt;区块大小&gt; 设置输入/输出的区块大小，单位是Byte。 -d或–make-directories 如有需要cpio会自行建立目录。 -E&lt;范本文件&gt;或–pattern-file=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让cpio解开符合范本条件的文件，格式为每列一个范本样式。 -f或–nonmatching 让cpio解开所有不符合范本条件的文件。 -F&lt;备份档&gt;或–file=&lt;备份档&gt; 指定备份档的名称，用来取代标准输入或输出，也能借此通过网络使用另一台主机的保存设备存取备份档。 -H&lt;备份格式&gt; 指定备份时欲使用的文件格式。 -i或–extract 执行copy-in模式，还原备份档。 -l&lt;备份档&gt; 指定备份档的名称，用来取代标准输入，也能借此通过网络使用另一台主机的保存设备读取备份档。 -k 此参数将忽略不予处理，仅负责解决cpio不同版本间的兼容性问题。 -l或–link 以硬连接的方式取代复制文件，可在copy-pass模式下运用。 -L或–dereference 不建立符号连接，直接复制该连接所指向的原始文件。 -m或preserve-modification-time 不去更换文件的更改时间。 -M&lt;回传信息&gt;或–message=&lt;回传信息&gt; 设置更换保存媒体的信息。 -n或–numeric-uid-gid 使用”-tv”参数列出备份档的内容时，若再加上参数”-n”，则会以用户识别码和群组识别码替代拥有者和群组名称列出文件清单。 -o或–create 执行copy-out模式，建立备份档。 -O&lt;备份档&gt; 指定备份档的名称，用来取代标准输出，也能借此通过网络 使用另一台主机的保存设备存放备份档。 -p或–pass-through 执行copy-pass模式，略过备份步骤，直接将文件复制到目的目录。 -r或–rename 当有文件名称需要更动时，采用互动模式。 -R&lt;拥有者&gt;&lt;:/.&gt;&lt;所属群组&gt;或 —-owner&lt;拥有者&gt;&lt;:/.&gt;&lt;所属群组&gt; 在copy-in模式还原备份档，或copy-pass模式复制文件时，可指定这些备份，复制的文件的拥有者与所属群组。 -s或–swap-bytes 交换每对字节的内容。 -S或–swap-halfwords 交换每半个字节的内容。 -t或–list 将输入的内容呈现出来。 -u或–unconditional 置换所有文件，不论日期时间的新旧与否，皆不予询问而直接覆盖。 -v或–verbose 详细显示指令的执行过程。 -V或–dot 执行指令时，在每个文件的执行程序前面加上”.”号 –block-size=&lt;区块大小&gt; 设置输入/输出的区块大小，假如设置数值为5，则区块大小为2500，若设置成10，则区块大小为5120，依次类推。 –force-local 强制将备份档存放在本地主机。 –help 在线帮助。 –no-absolute-filenames 使用相对路径建立文件名称。 –no-preserve-owner 不保留文件的拥有者，谁解开了备份档，那些文件就归谁所有。 -only-verify-crc 当备份档采用CRC备份格式时，可使用这项参数检查备份档内的每个文件是否正确无误。 –quiet 不显示复制了多少区块。 –sparse 倘若一个文件内含大量的连续0字节，则将此文件存成稀疏文件。 –version 显示版本信息。 dump功能说明：备份文件系统。 语 法：dump [-cnu][-0123456789][-b &lt;区块大小&gt;][-B &lt;区块数目&gt;][-d &lt;密度&gt;][-f &lt;设备名称&gt;][-h &lt;层级&gt;][-s &lt;磁带长度&gt;][-T &lt;日期&gt;][目录或文件系统] 或 dump [-wW] 补充说明：dump为备份工具程序，可将目录或整个文件系统备份至指定的设备，或备份成一个大文件。 参 数： -0123456789 备份的层级。 -b&lt;区块大小&gt; 指定区块的大小，单位为KB。 -B&lt;区块数目&gt; 指定备份卷册的区块数目。 -c 修改备份磁带预设的密度与容量。 -d&lt;密度&gt; 设置磁带的密度。单位为BPI。 -f&lt;设备名称&gt; 指定备份设备。 -h&lt;层级&gt; 当备份层级等于或大雨指定的层级时，将不备份用户标示为”nodump”的文件。 -n 当备份工作需要管理员介入时，向所有”operator”群组中的使用者发出通知。 -s&lt;磁带长度&gt; 备份磁带的长度，单位为英尺。 -T&lt;日期&gt; 指定开始备份的时间与日期。 -u 备份完毕后，在/etc/dumpdates中记录备份的文件系统，层级，日期与时间等。 -w 与-W类似，但仅显示需要备份的文件。 -W 显示需要备份的文件及其最后一次备份的层级，时间与日期。 gunzip(gnu unzip)功能说明：解压文件。 语 法：gunzip [-acfhlLnNqrtvV][-s &lt;压缩字尾字符串&gt;][文件…] 或 gunzip [-acfhlLnNqrtvV][-s &lt;压缩字尾字符串&gt;][目录] 补充说明：gunzip是个使用广泛的解压缩程序，它用于解开被gzip压缩过的文件，这些压缩文件预设最后的扩展名为”.gz”。事实上gunzip就是gzip的硬连接，因此不论是压缩或解压缩，都可通过gzip指令单独完成。 参 数： -a或–ascii 使用ASCII文字模式。 -c或–stdout或–to-stdout 把解压后的文件输出到标准输出设备。 -f或-force 强行解开压缩文件，不理会文件名称或硬连接是否存在以及该文件是否为符号连接。 -h或–help 在线帮助。 -l或–list 列出压缩文件的相关信息。 -L或–license 显示版本与版权信息。 -n或–no-name 解压缩时，若压缩文件内含有远来的文件名称及时间戳记，则将其忽略不予处理。 -N或–name 解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其回存到解开的文件上。 -q或–quiet 不显示警告信息。 -r或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -S&lt;压缩字尾字符串&gt;或–suffix&lt;压缩字尾字符串&gt; 更改压缩字尾字符串。 -t或–test 测试压缩文件是否正确无误。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 gzexe(gzip executable)功能说明：压缩执行文件。 语 法：gzexe [-d][执行文件…] 补充说明：gzexe是用来压缩执行文件的程序。当您去执行被压缩过的执行文件时，该文件会自动解压然后继续执行，和使用一般的执行文件相同。 参 数： -d 解开压缩文件。 gzip(gnu zip)功能说明：压缩文件。 语 法：gzip [-acdfhlLnNqrtvV][-S &lt;压缩字尾字符串&gt;][-&lt;压缩效率&gt;][–best/fast][文件…] 或 gzip [-acdfhlLnNqrtvV][-S &lt;压缩字尾字符串&gt;][-&lt;压缩效率&gt;][–best/fast][目录] 补充说明：gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多出”.gz”的扩展名。 参 数： -a或–ascii 使用ASCII文字模式。 -c或–stdout或–to-stdout 把压缩后的文件输出到标准输出设备，不去更动原始文件。 -d或–decompress或—-uncompress 解开压缩文件。 -f或–force 强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接。 -h或–help 在线帮助。 -l或–list 列出压缩文件的相关信息。 -L或–license 显示版本与版权信息。 -n或–no-name 压缩文件时，不保存原来的文件名称及时间戳记。 -N或–name 压缩文件时，保存原来的文件名称及时间戳记。 -q或–quiet 不显示警告信息。 -r或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -S&lt;压缩字尾字符串&gt;或—-suffix&lt;压缩字尾字符串&gt; 更改压缩字尾字符串。 -t或–test 测试压缩文件是否正确无误。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -&lt;压缩效率&gt; 压缩效率是一个介于1－9的数值，预设值为”6”，指定愈大的数值，压缩效率就会愈高。 –best 此参数的效果和指定”-9”参数相同。 –fast 此参数的效果和指定”-1”参数相同。 lha功能说明：压缩或解压缩文件。 语 法：lha [-acdfglmnpqtuvx][-a &lt;0/1/2&gt;/u&lt;/0/1/2&gt;][-&lt;a/c/u&gt;d][-&lt;e/x&gt;i][-&lt;a/u&gt;o][-&lt;e/x&gt;w=&lt;目的目录&gt;][-&lt;a/u&gt;z][压缩文件][文件…] 或 lha [-acdfglmnpqtuvx][-a &lt;0/1/2&gt;/u&lt;/0/1/2&gt;][-&lt;a/c/u&gt;d][-&lt;e/x&gt;i][-&lt;a/u&gt;o][-&lt;e/x&gt;w=&lt;目的目录&gt;][-&lt;a/u&gt;z][压缩文件][目录…] 补充说明：lha是从lharc演变而来的压缩程序，文件经它压缩后，会另外产生具有”.lzh”扩展名的压缩文件。 参 数： -a或a 压缩文件，并加入到压缩文件内。 -a&lt;0/1/2&gt;/u&lt;/0/1/2&gt; 压缩文件时，采用不同的文件头。 -c或c 压缩文件，重新建构新的压缩文件后，再将其加入。 -d或d 从压缩文件内删除指定的文件。 -&lt;a/c/u&gt;d或&lt;a/c/u&gt;d 压缩文件，然后将其加入，重新建构，更新压缩文件或，删除原始文件，也就是把文件移到压缩文件中。 -e或e 解开压缩文件。 -f或f 强制执行lha命令，在解压时会直接覆盖已有的文件而不加以询问。 -g或g 使用通用的压缩格式，便于解决兼容性的问题。 -&lt;e/x&gt;i或&lt;e/x&gt;i 解开压缩文件时，忽略保存在压缩文件内的文件路径，直接将其解压后存放在现行目录下或是指定的目录中。 -l或l 列出压缩文件的相关信息。 -m或m 此参数的效果和同时指定”-ad”参数相同。 -n或n 不执行指令，仅列出实际执行会进行的动作。 -&lt;a/u&gt;o或&lt;a/u&gt;o 采用lharc兼容格式，将压缩后的文件加入，更新压缩文件。 -p或p 从压缩文件内输出到标准输出设备。 -q或q 不显示指令执行过程。 -t或t 检查备份文件内的每个文件是否正确无误。 -u或u 更换较新的文件到压缩文件内。 -u&lt;/0/1/2&gt;或u&lt;/0/1/2&gt; 在文件压缩时采用不同的文件头，然后更新到压缩文件内。 -v或v 详细列出压缩文件的相关信息。 -&lt;e/x&gt;w=&lt;目的目录&gt;或&lt;e/x&gt;w=&lt;目的目录&gt; 指定解压缩的目录。 -x或x 解开压缩文件。 -&lt;a/u&gt;z或&lt;a/u&gt;z 不压缩文件，直接把它加入，更新压缩文件。 restore功能说明：还原(Restore)由倾倒(Dump)操作所备份下来的文件或整个文件系统(一个分区)。 语 法：restore [-cCvy][-b &lt;区块大小&gt;][-D &lt;文件系统&gt;][-f &lt;备份文件&gt;][-s &lt;文件编号&gt;] 或 restore [-chimvy][-b &lt;区块大小&gt;][-f &lt;备份文件&gt;][-s &lt;文件编号&gt;] 或 restore [-crvy][-b &lt;区块大小&gt;][-f &lt;备份文件&gt;][-s &lt;文件编号&gt;] 或 restore [-cRvy][-b &lt;区块大小&gt;][-D &lt;文件系统&gt;][-f &lt;备份文件&gt;][-s &lt;文件编号&gt;] 或 restore [chtvy][-b &lt;区块大小&gt;][-D &lt;文件系统&gt;][-f &lt;备份文件&gt;][-s &lt;文件编号&gt;][文件…] 或 restore [-chmvxy][-b &lt;区块大小&gt;][-D &lt;文件系统&gt;][-f &lt;备份文件&gt;][-s &lt;文件编号&gt;][文件…] 补充说明：restore 指令所进行的操作和dump指令相反，倾倒操作可用来备份文件，而还原操作则是写回这些已备份的文件。 参 数： -b&lt;区块大小&gt; 设置区块大小，单位是Byte。 -c 不检查倾倒操作的备份格式，仅准许读取使用旧格式的备份文件。 -C 使用对比模式，将备份的文件与现行的文件相互对比。 -D&lt;文件系统&gt; 允许用户指定文件系统的名称。 -f&lt;备份文件&gt; 从指定的文件中读取备份数据，进行还原操作。 -h 仅解出目录而不包括与该目录相关的所有文件。 -i 使用互动模式，在进行还原操作时，restore指令将依序询问用户。 -m 解开符合指定的inode编号的文件或目录而非采用文件名称指定。 -r 进行还原操作。 -R 全面还原文件系统时，检查应从何处开始进行。 -s&lt;文件编号&gt; 当备份数据超过一卷磁带时，您可以指定备份文件的编号。 -t 指定文件名称，若该文件已存在备份文件中，则列出它们的名称。 -v 显示指令执行过程。 -x 设置文件名称，且从指定的存储媒体里读入它们，若该文件已存在在备份文件中，则将其还原到文件系统内。 -y 不询问任何问题，一律以同意回答并继续执行指令。 tar(tape archive)功能说明：备份文件。 语 法：tar [-ABcdgGhiklmMoOpPrRsStuUvwWxzZ][-b &lt;区块数目&gt;][-C &lt;目的目录&gt;][-f &lt;备份文件&gt;][-F &lt;Script文件&gt;][-K &lt;文件&gt;][-L &lt;媒体容量&gt;][-N &lt;日期时间&gt;][-T &lt;范本文件&gt;][-V &lt;卷册名称&gt;][-X &lt;范本文件&gt;][-&lt;设备编号&gt;&lt;存储密度&gt;][–after-date=&lt;日期时间&gt;][–atime-preserve][–backuup=&lt;备份方式&gt;][–checkpoint][–concatenate][–confirmation][–delete][–exclude=&lt;范本样式&gt;][–force-local][–group=&lt;群组名称&gt;][–help][–ignore-failed-read][–new-volume-script=&lt;Script文件&gt;][–newer-mtime][–no-recursion][–null][–numeric-owner][–owner=&lt;用户名称&gt;][–posix][–erve][–preserve-order][–preserve-permissions][–record-size=&lt;区块数目&gt;][–recursive-unlink][–remove-files][–rsh-command=&lt;执行指令&gt;][–same-owner][–suffix=&lt;备份字尾字符串&gt;][–totals][–use-compress-program=&lt;执行指令&gt;][–version][–volno-file=&lt;编号文件&gt;][文件或目录…] 补充说明：tar是用来建立，还原备份文件的工具程序，它可以加入，解开备份文件内的文件。 参 数： -A或–catenate 新增温暖件到已存在的备份文件。 -b&lt;区块数目&gt;或–blocking-factor=&lt;区块数目&gt; 设置每笔记录的区块数目，每个区块大小为12Bytes。 -B或–read-full-records 读取数据时重设区块大小。 -c或–create 建立新的备份文件。 -C&lt;目的目录&gt;或–directory=&lt;目的目录&gt; 切换到指定的目录。 -d或–diff或–compare 对比备份文件内和文件系统上的文件的差异。 -f&lt;备份文件&gt;或–file=&lt;备份文件&gt; 指定备份文件。 -F&lt;Script文件&gt;或–info-script=&lt;Script文件&gt; 每次更换磁带时，就执行指定的Script文件。 -g或–listed-incremental 处理GNU格式的大量备份。 -G或–incremental 处理旧的GNU格式的大量备份。 -h或–dereference 不建立符号连接，直接复制该连接所指向的原始文件。 -i或–ignore-zeros 忽略备份文件中的0 Byte区块，也就是EOF。 -k或–keep-old-files 解开备份文件时，不覆盖已有的文件。 -K&lt;文件&gt;或–starting-file=&lt;文件&gt; 从指定的文件开始还原。 -l或–one-file-system 复制的文件或目录存放的文件系统，必须与tar指令执行时所处的文件系统相同，否则不予复制。 -L&lt;媒体容量&gt;或-tape-length=&lt;媒体容量&gt; 设置存放每体的容量，单位以1024 Bytes计算。 -m或–modification-time 还原文件时，不变更文件的更改时间。 -M或–multi-volume 在建立，还原备份文件或列出其中的内容时，采用多卷册模式。 -N&lt;日期格式&gt;或–newer=&lt;日期时间&gt; 只将较指定日期更新的文件保存到备份文件里。 -o或–old-archive或–portability 将资料写入备份文件时使用V7格式。 -O或–stdout 把从备份文件里还原的文件输出到标准输出设备。 -p或–same-permissions 用原来的文件权限还原文件。 -P或–absolute-names 文件名使用绝对名称，不移除文件名称前的”/“号。 -r或–append 新增文件到已存在的备份文件的结尾部分。 -R或–block-number 列出每个信息在备份文件中的区块编号。 -s或–same-order 还原文件的顺序和备份文件内的存放顺序相同。 -S或–sparse 倘若一个文件内含大量的连续0字节，则将此文件存成稀疏文件。 -t或–list 列出备份文件的内容。 -T&lt;范本文件&gt;或–files-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让tar解开或建立符合设置条件的文件。 -u或–update 仅置换较备份文件内的文件更新的文件。 -U或–unlink-first 解开压缩文件还原文件之前，先解除文件的连接。 -v或–verbose 显示指令执行过程。 -V&lt;卷册名称&gt;或–label=&lt;卷册名称&gt; 建立使用指定的卷册名称的备份文件。 -w或–interactive 遭遇问题时先询问用户。 -W或–verify 写入备份文件后，确认文件正确无误。 -x或–extract或–get 从备份文件中还原文件。 -X&lt;范本文件&gt;或–exclude-from=&lt;范本文件&gt; 指定范本文件，其内含有一个或多个范本样式，让ar排除符合设置条件的文件。 -z或–gzip或–ungzip 通过gzip指令处理备份文件。 -Z或–compress或–uncompress 通过compress指令处理备份文件。 -&lt;设备编号&gt;&lt;存储密度&gt; 设置备份用的外围设备编号及存放数据的密度。 –after-date=&lt;日期时间&gt; 此参数的效果和指定”-N”参数相同。 –atime-preserve 不变更文件的存取时间。 –backup=&lt;备份方式&gt;或–backup 移除文件前先进行备份。 –checkpoint 读取备份文件时列出目录名称。 –concatenate 此参数的效果和指定”-A”参数相同。 –confirmation 此参数的效果和指定”-w”参数相同。 –delete 从备份文件中删除指定的文件。 –exclude=&lt;范本样式&gt; 排除符合范本样式的问家。 –group=&lt;群组名称&gt; 把加入设备文件中的文件的所属群组设成指定的群组。 –help 在线帮助。 –ignore-failed-read 忽略数据读取错误，不中断程序的执行。 –new-volume-script=&lt;Script文件&gt; 此参数的效果和指定”-F”参数相同。 –newer-mtime 只保存更改过的文件。 –no-recursion 不做递归处理，也就是指定目录下的所有文件及子目录不予处理。 –null 从null设备读取文件名称。 –numeric-owner 以用户识别码及群组识别码取代用户名称和群组名称。 –owner=&lt;用户名称&gt; 把加入备份文件中的文件的拥有者设成指定的用户。 –posix 将数据写入备份文件时使用POSIX格式。 –preserve 此参数的效果和指定”-ps”参数相同。 –preserve-order 此参数的效果和指定”-A”参数相同。 –preserve-permissions 此参数的效果和指定”-p”参数相同。 –record-size=&lt;区块数目&gt; 此参数的效果和指定”-b”参数相同。 –recursive-unlink 解开压缩文件还原目录之前，先解除整个目录下所有文件的连接。 –remove-files 文件加入备份文件后，就将其删除。 –rsh-command=&lt;执行指令&gt; 设置要在远端主机上执行的指令，以取代rsh指令。 –same-owner 尝试以相同的文件拥有者还原问家你。 –suffix=&lt;备份字尾字符串&gt; 移除文件前先行备份。 –totals 备份文件建立后，列出文件大小。 –use-compress-program=&lt;执行指令&gt; 通过指定的指令处理备份文件。 –version 显示版本信息。 –volno-file=&lt;编号文件&gt; 使用指定文件内的编号取代预设的卷册编号。 unarj功能说明：解压缩.arj文件。 语 法：unarj [eltx][.arj压缩文件] 补充说明：unarj为.arj压缩文件的压缩程序。 参 数： e 解压缩.arj文件。 l 显示压缩文件内所包含的文件。 t 检查压缩文件是否正确。 x 解压缩时保留原有的路径。 unzip功能说明：解压缩zip文件 语 法：unzip [-cflptuvz][-agCjLMnoqsVX][-P &lt;密码&gt;][.zip文件][文件][-d &lt;目录&gt;][-x &lt;文件&gt;] 或 unzip [-Z] 补充说明：unzip为.zip压缩文件的解压缩程序。 参 数： -c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。 -f 更新现有的文件。 -l 显示压缩文件内所包含的文件。 -p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。 -t 检查压缩文件是否正确。 -u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。 -v 执行是时显示详细的信息。 -z 仅显示压缩文件的备注文字。 -a 对文本文件进行必要的字符转换。 -b 不要对文本文件进行字符转换。 -C 压缩文件中的文件名称区分大小写。 -j 不处理压缩文件中原有的目录路径。 -L 将压缩文件中的全部文件名改为小写。 -M 将输出结果送到more程序处理。 -n 解压缩时不要覆盖原有的文件。 -o 不必先询问用户，unzip执行后覆盖原有文件。 -P&lt;密码&gt; 使用zip的密码选项。 -q 执行时不显示任何信息。 -s 将文件名中的空白字符转换为底线字符。 -V 保留VMS的文件版本信息。 -X 解压缩时同时回存文件原来的UID/GID。 [.zip文件] 指定.zip压缩文件。 [文件] 指定要处理.zip压缩文件中的哪些文件。 -d&lt;目录&gt; 指定文件解压缩后所要存储的目录。 -x&lt;文件&gt; 指定不要处理.zip压缩文件中的哪些文件。 -Z unzip -Z等于执行zipinfo指令。 zip功能说明：压缩文件。 语 法：zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$][-b &lt;工作目录&gt;][-ll][-n &lt;字尾字符串&gt;][-t &lt;日期时间&gt;][-&lt;压缩效率&gt;][压缩文件][文件…][-i &lt;范本样式&gt;][-x &lt;范本样式&gt;] 补充说明：zip是个使用广泛的压缩程序，文件经它压缩后会另外产生具有”.zip”扩展名的压缩文件。 参 数： -A 调整可执行的自动解压缩文件。 -b&lt;工作目录&gt; 指定暂时存放文件的目录。 -c 替每个被压缩的文件加上注释。 -d 从压缩文件内删除指定的文件。 -D 压缩文件内不建立目录名称。 -f 此参数的效果和指定”-u”参数类似，但不仅更新既有文件，如果某些文件原本不存在于压缩文件内，使用本参数会一并将其加入压缩文件中。 -F 尝试修复已损坏的压缩文件。 -g 将文件压缩后附加在既有的压缩文件之后，而非另行建立新的压缩文件。 -h 在线帮助。 -i&lt;范本样式&gt; 只压缩符合条件的文件。 -j 只保存文件名称及其内容，而不存放任何目录名称。 -J 删除压缩文件前面不必要的数据。 -k 使用MS-DOS兼容格式的文件名称。 -l 压缩文件时，把LF字符置换成LF+CR字符。 -ll 压缩文件时，把LF+CR字符置换成LF字符。 -L 显示版权信息。 -m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。 -n&lt;字尾字符串&gt; 不压缩具有特定字尾字符串的文件。 -o 以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同。 -q 不显示指令执行过程。 -r 递归处理，将指定目录下的所有文件和子目录一并处理。 -S 包含系统和隐藏文件。 -t&lt;日期时间&gt; 把压缩文件的日期设成指定的日期。 -T 检查备份文件内的每个文件是否正确无误。 -u 更换较新的文件到压缩文件内。 -v 显示指令执行过程或显示版本信息。 -V 保存VMS操作系统的文件属性。 -w 在文件名称里假如版本编号，本参数仅在VMS操作系统下有效。 -x&lt;范本样式&gt; 压缩时排除符合条件的文件。 -X 不保存额外的文件属性。 -y 直接保存符号连接，而非该连接所指向的文件，本参数仅在UNIX之类的系统下有效。 -z 替压缩文件加上注释。 -$ 保存第一个被压缩文件所在磁盘的卷册名称。 -&lt;压缩效率&gt; 压缩效率是一个介于1-9的数值。 zipinfo功能说明：列出压缩文件信息。 语 法：zipinfo [-12hlmMstTvz][压缩文件][文件…][-x &lt;范本样式&gt;] 补充说明：执行zipinfo指令可得知zip压缩文件的详细信息。 参 数： -1 只列出文件名称。 -2 此参数的效果和指定”-1”参数类似，但可搭配”-h”,”-t”和”-z”参数使用。 -h 只列出压缩文件的文件名称。 -l 此参数的效果和指定”-m”参数类似，但会列出原始文件的大小而非每个文件的压缩率。 -m 此参数的效果和指定”-s”参数类似，但多会列出每个文件的压缩率。 -M 若信息内容超过一个画面，则采用类似more指令的方式列出信息。 -s 用类似执行”ls -l”指令的效果列出压缩文件内容。 -t 只列出压缩文件内所包含的文件数目，压缩前后的文件大小及压缩率。 -T 将压缩文件内每个文件的日期时间用年，月，日，时，分，秒的顺序列出。 -v 详细显示压缩文件内每一个文件的信息。 -x&lt;范本样式&gt; 不列出符合条件的文件的信息。 -z 如果压缩文件内含有注释，就将注释显示出来。 linux文件管理命令diff(differential)功能说明：比较文件的差异。 语 法：diff [-abBcdefHilnNpPqrstTuvwy][-&lt;行数&gt;][-C &lt;行数&gt;][-D &lt;巨集名称&gt;][-I &lt;字符或字符串&gt;][-S &lt;文件&gt;][-W &lt;宽度&gt;][-x &lt;文件或目录&gt;][-X &lt;文件&gt;][–help][–left-column][–suppress-common-line][文件或目录1][文件或目录2] 补充说明：diff以逐行的方式，比较文本文件的异同处。所是指定要比较目录，则diff会比较目录中相同文件名的文件，但不会比较其中子目录。 参 数： -&lt;行数&gt; 指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。 -a或–text diff预设只会逐行比较文本文件。 -b或–ignore-space-change 不检查空格字符的不同。 -B或–ignore-blank-lines 不检查空白行。 -c 显示全部内文，并标出不同之处。 -C&lt;行数&gt;或–context&lt;行数&gt; 与执行”-c-&lt;行数&gt;”指令相同。 -d或–minimal 使用不同的演算法，以较小的单位来做比较。 -D&lt;巨集名称&gt;或ifdef&lt;巨集名称&gt; 此参数的输出格式可用于前置处理器巨集。 -e或–ed 此参数的输出格式可用于ed的script文件。 -f或-forward-ed 输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。 -H或–speed-large-files 比较大文件时，可加快速度。 -l&lt;字符或字符串&gt;或–ignore-matching-lines&lt;字符或字符串&gt; 若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。 -i或–ignore-case 不检查大小写的不同。 -l或–paginate 将结果交由pr程序来分页。 -n或–rcs 将比较结果以RCS的格式来显示。 -N或–new-file 在比较目录时，若文件A仅出现在某个目录中，预设会显示： Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。 -p 若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。 -P或–unidirectional-new-file 与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。 -q或–brief 仅显示有无差异，不显示详细的信息。 -r或–recursive 比较子目录中的文件。 -s或–report-identical-files 若没有发现任何差异，仍然显示信息。 -S&lt;文件&gt;或–starting-file&lt;文件&gt; 在比较目录时，从指定的文件开始比较。 -t或–expand-tabs 在输出时，将tab字符展开。 -T或–initial-tab 在每行前面加上tab字符以便对齐。 -u,-U&lt;列数&gt;或–unified=&lt;列数&gt; 以合并的方式来显示文件内容的不同。 -v或–version 显示版本信息。 -w或–ignore-all-space 忽略全部的空格字符。 -W&lt;宽度&gt;或–width&lt;宽度&gt; 在使用-y参数时，指定栏宽。 -x&lt;文件名或目录&gt;或–exclude&lt;文件名或目录&gt; 不比较选项中所指定的文件或目录。 -X&lt;文件&gt;或–exclude-from&lt;文件&gt; 您可以将文件或目录类型存成文本文件，然后在=&lt;文件&gt;中指定此文本文件。 -y或–side-by-side 以并列的方式显示文件的异同之处。 –help 显示帮助。 –left-column 在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。 –suppress-common-lines 在使用-y参数时，仅显示不同之处。 diffstat(differential status)功能说明：根据diff的比较结果，显示统计数字。 语 法：diff [-wV][-n &lt;文件名长度&gt;][-p &lt;文件名长度&gt;] 补充说明：diffstat读取diff的输出结果，然后统计各文件的插入，删除，修改等差异计量。 参 数： -n&lt;文件名长度&gt; 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 -p&lt;文件名长度&gt; 与-n参数相同，但此处的&lt;文件名长度&gt;包括了文件的路径。 -w 指定输出时栏位的宽度。 -V 显示版本信息。 file功能说明：辨识文件类型。 语 法：file [-beLvz][-f &lt;名称文件&gt;][-m &lt;魔法数字文件&gt;…][文件或目录…] 补充说明：通过file指令，我们得以辨识该文件的类型。 参 数： -b 列出辨识结果时，不显示文件名称。 -c 详细显示指令执行过程，便于排错或分析程序执行的情形。 -f&lt;名称文件&gt; 指定名称文件，其内容有一个或多个文件名称呢感，让file依序辨识这些文件，格式为每列一个文件名称。 -L 直接显示符号连接所指向的文件的类别。 -m&lt;魔法数字文件&gt; 指定魔法数字文件。 -v 显示版本信息。 -z 尝试去解读压缩文件的内容。 find功能说明：查找文件或目录。 语 法：find [目录…][-amin &lt;分钟&gt;][-anewer &lt;参考文件或目录&gt;][-atime &lt;24小时数&gt;][-cmin &lt;分钟&gt;][-cnewer &lt;参考文件或目录&gt;][-ctime &lt;24小时数&gt;][-daystart][-depyh][-empty][-exec &lt;执行指令&gt;][-false][-fls &lt;列表文件&gt;][-follow][-fprint &lt;列表文件&gt;][-fprint0 &lt;列表文件&gt;][-fprintf &lt;列表文件&gt;&lt;输出格式&gt;][-fstype &lt;文件系统类型&gt;][-gid &lt;群组识别码&gt;][-group &lt;群组名称&gt;][-help][-ilname &lt;范本样式&gt;][-iname &lt;范本样式&gt;][-inum &lt;inode编号&gt;][-ipath &lt;范本样式&gt;][-iregex &lt;范本样式&gt;][-links &lt;连接数目&gt;][-lname &lt;范本样式&gt;][-ls][-maxdepth &lt;目录层级&gt;][-mindepth &lt;目录层级&gt;][-mmin &lt;分钟&gt;][-mount][-mtime &lt;24小时数&gt;][-name &lt;范本样式&gt;][-newer &lt;参考文件或目录&gt;][-nogroup][noleaf] [-nouser][-ok &lt;执行指令&gt;][-path &lt;范本样式&gt;][-perm &lt;权限数值&gt;][-print][-print0][-printf &lt;输出格式&gt;][-prune][-regex &lt;范本样式&gt;][-size &lt;文件大小&gt;][-true][-type &lt;文件类型&gt;][-uid &lt;用户识别码&gt;][-used &lt;日数&gt;][-user &lt;拥有者名称&gt;][-version][-xdev][-xtype &lt;文件类型&gt;] 补充说明：find指令用于查找符合条件的文件。任何位于参数之前的字符串都将被视为欲查找的目录。 参 数： -amin&lt;分钟&gt; 查找在指定时间曾被存取过的文件或目录，单位以分钟计算。 -anewer&lt;参考文件或目录&gt; 查找其存取时间较指定文件或目录的存取时间更接近现在的文件或目录。 -atime&lt;24小时数&gt; 查找在指定时间曾被存取过的文件或目录，单位以24小时计算。 -cmin&lt;分钟&gt; 查找在指定时间之时被更改的文件或目录。 -cnewer&lt;参考文件或目录&gt; 查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录。 -ctime&lt;24小时数&gt; 查找在指定时间之时被更改的文件或目录，单位以24小时计算。 -daystart 从本日开始计算时间。 -depth 从指定目录下最深层的子目录开始查找。 -expty 寻找文件大小为0 Byte的文件，或目录下没有任何子目录或文件的空目录。 -exec&lt;执行指令&gt; 假设find指令的回传值为True，就执行该指令。 -false 将find指令的回传值皆设为False。 -fls&lt;列表文件&gt; 此参数的效果和指定”-ls”参数类似，但会把结果保存为指定的列表文件。 -follow 排除符号连接。 -fprint&lt;列表文件&gt; 此参数的效果和指定”-print”参数类似，但会把结果保存成指定的列表文件。 -fprint0&lt;列表文件&gt; 此参数的效果和指定”-print0”参数类似，但会把结果保存成指定的列表文件。 -fprintf&lt;列表文件&gt;&lt;输出格式&gt; 此参数的效果和指定”-printf”参数类似，但会把结果保存成指定的列表文件。 -fstype&lt;文件系统类型&gt; 只寻找该文件系统类型下的文件或目录。 -gid&lt;群组识别码&gt; 查找符合指定之群组识别码的文件或目录。 -group&lt;群组名称&gt; 查找符合指定之群组名称的文件或目录。 -help或–help 在线帮助。 -ilname&lt;范本样式&gt; 此参数的效果和指定”-lname”参数类似，但忽略字符大小写的差别。 -iname&lt;范本样式&gt; 此参数的效果和指定”-name”参数类似，但忽略字符大小写的差别。 -inum&lt;inode编号&gt; 查找符合指定的inode编号的文件或目录。 -ipath&lt;范本样式&gt; 此参数的效果和指定”-ipath”参数类似，但忽略字符大小写的差别。 -iregex&lt;范本样式&gt; 此参数的效果和指定”-regexe”参数类似，但忽略字符大小写的差别。 -links&lt;连接数目&gt; 查找符合指定的硬连接数目的文件或目录。 -iname&lt;范本样式&gt; 指定字符串作为寻找符号连接的范本样式。 -ls 假设find指令的回传值为True，就将文件或目录名称列出到标准输出。 -maxdepth&lt;目录层级&gt; 设置最大目录层级。 -mindepth&lt;目录层级&gt; 设置最小目录层级。 -mmin&lt;分钟&gt; 查找在指定时间曾被更改过的文件或目录，单位以分钟计算。 -mount 此参数的效果和指定”-xdev”相同。 -mtime&lt;24小时数&gt; 查找在指定时间曾被更改过的文件或目录，单位以24小时计算。 -name&lt;范本样式&gt; 指定字符串作为寻找文件或目录的范本样式。 -newer&lt;参考文件或目录&gt; 查找其更改时间较指定文件或目录的更改时间更接近现在的文件或目录。 -nogroup 找出不属于本地主机群组识别码的文件或目录。 -noleaf 不去考虑目录至少需拥有两个硬连接存在。 -nouser 找出不属于本地主机用户识别码的文件或目录。 -ok&lt;执行指令&gt; 此参数的效果和指定”-exec”参数类似，但在执行指令之前会先询问用户，若回答”y”或”Y”，则放弃执行指令。 -path&lt;范本样式&gt; 指定字符串作为寻找目录的范本样式。 -perm&lt;权限数值&gt; 查找符合指定的权限数值的文件或目录。 -print 假设find指令的回传值为True，就将文件或目录名称列出到标准输出。格式为每列一个名称，每个名称之前皆有”./“字符串。 -print0 假设find指令的回传值为True，就将文件或目录名称列出到标准输出。格式为全部的名称皆在同一行。 -printf&lt;输出格式&gt; 假设find指令的回传值为True，就将文件或目录名称列出到标准输出。格式可以自行指定。 -prune 不寻找字符串作为寻找文件或目录的范本样式。 -regex&lt;范本样式&gt; 指定字符串作为寻找文件或目录的范本样式。 -size&lt;文件大小&gt; 查找符合指定的文件大小的文件。 -true 将find指令的回传值皆设为True。 -typ&lt;文件类型&gt; 只寻找符合指定的文件类型的文件。 -uid&lt;用户识别码&gt; 查找符合指定的用户识别码的文件或目录。 -used&lt;日数&gt; 查找文件或目录被更改之后在指定时间曾被存取过的文件或目录，单位以日计算。 -user&lt;拥有者名称&gt; 查找符合指定的拥有者名称的文件或目录。 -version或–version 显示版本信息。 -xdev 将范围局限在先行的文件系统中。 -xtype&lt;文件类型&gt; 此参数的效果和指定”-type”参数类似，差别在于它针对符号连接检查。 git(gnu interactive tools)功能说明：文字模式下的文件管理员。 语 法：git 补充说明：git是用来管理文件的程序，它十分类似DOS下的Norton Commander，具有互动式操作界面。它的操作方法和Norton Commander几乎一样，略诉如下： F1 ：执行info指令，查询指令相关信息，会要求您输入欲查询的名称。 F2 ：执行cat指令，列出文件内容。 F3 ：执行gitview指令，观看文件内容。 F4 ：执行vi指令，编辑文件内容。 F5 ：执行cp指令，复制文件或目录，会要求您输入目标文件或目录。 F6 ：执行mv指令，移动文件或目录，或是更改其名称，会要求您输入目标文件或目录。 F7 ：执行mkdir指令，建立目录。 F8 ：执行rm指令，删除文件或目录。 F9 ：执行make指令，批处理执行指令或编译程序时，会要求您输入相关命令。 F10 ：离开git文件管理员。 gitview(gnu interactie tools viewer)功能说明：Hex/ASCII的看文件程序。 语 法：gitview [-bchilv][文件] 补充说明：gitview指令可用于观看文件的内容，它会同时显示十六进制和ASCII格式的字码。 参 数： -b 单色模式，不使用ANSI控制码显示彩色。 -c 彩色模式，使用ANSI控制码显示色彩。 -h 在线帮助。 -i 显示存放gitview程序的所在位置。 -l 不使用先前的显示字符。 -v 显示版本信息。 ln(link)功能说明：连接文件或目录。 语 法：ln [-bdfinsv][-S &lt;字尾备份字符串&gt;][-V &lt;备份方式&gt;][–help][–version][源文件或目录][目标文件或目录] 或 ln [-bdfinsv][-S &lt;字尾备份字符串&gt;][-V &lt;备份方式&gt;][–help][–version][源文件或目录…][目的目录] 补充说明：ln指令用在连接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 参 数： -b或–backup 删除，覆盖目标文件之前的备份。 -d或-F或–directory 建立目录的硬连接。 -f或–force 强行建立文件或目录的连接，不论文件或目录是否存在。 -i或–interactive 覆盖既有文件之前先询问用户。 -n或–no-dereference 把符号连接的目的目录视为一般文件。 -s或–symbolic 对源文件建立符号连接，而非硬连接。 -S&lt;字尾备份字符串&gt;或–suffix=&lt;字尾备份字符串&gt; 用”-b”参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，预设的字尾备份字符串是符号”~”，您可通过”-S”参数来改变它。 -v或–verbose 显示指令执行过程。 -V&lt;备份方式&gt;或–version-control=&lt;备份方式&gt; 用”-b”参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，这个字符串不仅可用”-S”参数变更，当使用”-V”参数&lt;备份方式&gt;指定不同备份方式时，也会产生不同字尾的备份字符串。 –help 在线帮助。 –version 显示版本信息。 locate功能说明：查找文件。 语 法：locate [-d &lt;数据库文件&gt;][–help][–version][范本样式…] 补充说明：locate指令用于查找符合条件的文件，它会去保存文件与目录名称的数据库内，查找合乎范本样式条件的文件或目录。 参 数： -d&lt;数据库文件&gt;或–database=&lt;数据库文件&gt; 设置locate指令使用的数据库。locate指令预设的数据库位于/var/lib/slocate目录里，文件名为slocate.db，您可使用这个参数另行指定。 –help 在线帮助。 –version 显示版本信息。 lsattr(list attribute)功能说明：显示文件属性。 语 法：lsattr [-adlRvV][文件或目录…] 补充说明：用chattr执行改变文件或目录的属性，可执行lsattr指令查询其属性。 参 数： -a 显示所有文件和目录，包括以”.”为名称开头字符的额外内建，现行目录”.”与上层目录”..”。 -d 显示，目录名称，而非其内容。 -l 此参数目前没有任何作用。 -R 递归处理，将指定目录下的所有文件及子目录一并处理。 -v 显示文件或目录版本。 -V 显示版本信息。 mattrib功能说明：变更或显示MS-DOS文件的属性。 语 法：mattrib [+/-][ahrs][-/X][文件] 或 mattrib [-/X][文件] 补充说明：mattrib为mtools工具指令，模拟MS-DOS的attrib指令，可变更MS-DOS文件的属性。 参 数： +/- “+”代表开启指定的文件属性。”-“代表关闭指定的文件属性。 a 备份属性。使备份程序可用来判断文件是否已经备份过。 h 隐藏属性。 r 只读属性。 s 系统属性。 -/ 列出指定目录及子目录下所有文件的属性。 -X 显示文件属性时，中间不输入空格。 mc(midnight commander)功能说明：提供一个菜单式的文件管理程序。 语 法：mc [-abcdfhkPstuUVx][-C &lt;参数&gt;][-l &lt;文件&gt;][-v &lt;文件&gt;][目录] 补充说明：执行mc之后，将会看到菜单式的文件管理程序，共分成４个部分。 参 数： -a 当mc程序画线时不用绘图字符画线。 -b 使用单色模式显示。 -c 使用彩色模式显示。 -C&lt;参数&gt; 指定显示的颜色。 -d 不使用鼠标。 -f 显示mc函数库所在的目录。 -h 显示帮助。 -k 重设softkeys成预设置。 -l&lt;文件&gt; 在指定文件中保存ftpfs对话窗的内容。 -P 程序结束时，列出最后的工作目录。 -s 用慢速的终端机模式显示，在这模式下将减少大量的绘图及文字显示。 -t 使用TEMPCAP变量设置终端机，而不使用预设置。 -u 不用目前的shell程序。 -U 使用目前的shell程序。 -v&lt;文件&gt; 使用mc的内部编辑器来显示指定的文件。 -V 显示版本信息。 -x 指定以xterm模式显示。 mcopy功能说明：复制MS-DOS文件。 语 法：mcopy [-bnmpQt/][源文件][目标文件或目录] 补充说明：mcopy为mtools工具指令，可在MS-DOS文件系统中复制文件或是在MS-DOS与Linux的文件系统之间复制文件。mcopy会MS-DOS磁盘驱动器代号出现的位置来判断如何复制文件。 参 数： -b 批处理模式。 -n 覆盖其他文件时，不需进行确认而直接覆盖。 -m 将源文件修改时间设为目标文件的修改时间。 -p 将源文件属性设为目标文件的属性。 -Q 复制多个文件时，若发生错误，则立即结束程序。 -t 转换文本文件。 -/ 复制子目录以及其中的所有文件。 mdel功能说明：删除MS-DOS文件。 语 法：mdel [文件…] 补充说明：mdel为mtools工具指令，模拟MS-DOS的del指令，可删除MS-DOS文件系统中的文件。 mdir功能说明：显示MS-DOS目录。 语 法：mdir [-afwx/][目录] 补充说明：mdir为mtools工具指令，模拟MS-DOS的dir指令，可显示MS-DOS文件系统中的目录内容。 参 数： -/ 显示目录下所有子目录与文件。 -a 显示隐藏文件。 -f 不显示磁盘所剩余的可用空间。 -w 仅显示目录或文件名称，并以横排方式呈现，以便一次能显示较多的目录或文件。 -X 仅显示目录下所有子目录与文件的完整路径，不显示其他信息。 mktemp功能说明：建立暂存文件。 语 法：mktemp [-qu][文件名参数] 补充说明：mktemp可建立一个暂存文件，供shell script使用。 参 数： -q 执行时若发生错误，不会显示任何信息。 -u 暂存文件会在mktemp结束前先行删除。 [文件名参数] 文件名参数必须是以”自订名称.XXXXXX”的格式。 mmove功能说明：在MS-DOS文件系统中，移动文件或目录，或更改名称。 语 法：mmove [源文件或目录…][目标文件或目录] 补充说明：mmove为mtools工具指令，模拟MS-DOS的move指令，可在MS-DOS文件系统中移动现有的文件或目录，或是更改现有文件或目录的名称。 mread功能说明：将MS-DOS文件复制到Linux/Unix的目录中。 语 法：mread [MS-DOS文件…][Linux文件或目录] 补充说明：mread为mtools工具指令，可将MS-DOS文件复制到Linux的文件系统中。这个指令目前已经不常用，一般都使用mcopy指令来代替。 mren功能说明：更改MS-DOS文件或目录的名称，或是移动文件或目录。 语 法：mren [源文件或目录…][目标文件或目录] 补充说明：mren为MS-DOS工具指令，模拟MS-DOS的ren指令，可更改MS-DOS文件或目录名称。除此之外，ren也可移动文件或目录，但仅限于在同一设备内。 mshowfat功能说明：显示MS-DOS文件在FAT中的记录。 语 法：mshowfat [文件…] 补充说明：mshowfat为mtools工具指令，可显示MS-DOS文件在FAT中的记录编号。 mtools功能说明：显示mtools支持的指令。 语 法：mtools 补充说明：mtools为MS-DOS文件系统的工具程序，可模拟许多MS-DOS的指令。这些指令都是mtools的符号连接，因此会有一些共同的特性。 参 数： -a 长文件名重复时自动更改目标文件的长文件名。 -A 短文件名重复但长文件名不同时自动更改目标文件的短文件名。 -o 长文件名重复时，将目标文件覆盖现有的文件。 -O 短文件名重复但长文件名不同时，将目标文件覆盖现有的文件。 -r 长文件名重复时，要求用户更改目标文件的长文件名。 -R 短文件名重复但长文件名煌保笥没Ц哪勘晡募亩涛募?nbsp; -s 长文件名重复时，则不处理该目标文件。 -S 短文件名重复但长文件名不同时，则不处理该目标文件。 -v 执行时显示详细的说明。 -V 显示版本信息。 mtoolstest功能说明：测试并显示mtools的相关设置。 语 法：mtoolstest 补充说明：mtoolstest为mtools工具指令，可读取与分析mtools的配置文件，并在屏幕上显示结果 mv功能说明：移动或更名现有的文件或目录。 语 法：mv [-bfiuv][–help][–version][-S &lt;附加字尾&gt;][-V &lt;方法&gt;][源文件或目录][目标文件或目录] 补充说明：mv可移动文件或目录，或是更改文件或目录的名称。 参 数： -b或–backup 若需覆盖文件，则覆盖前先行备份。 -f或–force 若目标文件或目录与现有的文件或目录重复，则直接覆盖现有的文 件或目录。 -i或–interactive 覆盖前先行询问用户。 -S&lt;附加字尾&gt;或 –suffix=&lt;附加字尾&gt; 与-b参数一并使用，可指定备份文件的所要附加的字尾。 -u或–update 在移动或更改文件名时，若目标文件已存在，且其文件日期比源文件新，则不覆盖目标文件。 -v或–verbose 执行时显示详细的信息。 -V=&lt;方法&gt;或 –version-control=&lt;方法&gt; 与-b参数一并使用，可指定备份的方法。 –help 显示帮助。 –version 显示版本信息。 od(octal dump)功能说明：输出文件内容。 语 法：od [-abcdfhilovx][-A &lt;字码基数&gt;][-j &lt;字符数目&gt;][-N &lt;字符数目&gt;][-s &lt;字符串字符数&gt;][-t &lt;输出格式&gt;][-w &lt;每列字符数&gt;][–help][–version][文件…] 补充说明：od指令会读取所给予的文件的内容，并将其内容以八进制字码呈现出来。 参 数： -a 此参数的效果和同时指定”-ta”参数相同。 -A&lt;字码基数&gt; 选择要以何种基数计算字码。 -b 此参数的效果和同时指定”-toC”参数相同。 -c 此参数的效果和同时指定”-tC”参数相同。 -d 此参数的效果和同时指定”-tu2”参数相同。 -f 此参数的效果和同时指定”-tfF”参数相同。 -h 此参数的效果和同时指定”-tx2”参数相同。 -i 此参数的效果和同时指定”-td2”参数相同。 -j&lt;字符数目&gt;或–skip-bytes=&lt;字符数目&gt; 略过设置的字符数目。 -l 此参数的效果和同时指定”-td4”参数相同。 -N&lt;字符数目&gt;或–read-bytes=&lt;字符数目&gt; 到设置的字符数目为止。 -o 此参数的效果和同时指定”-to2”参数相同。 -s&lt;字符串字符数&gt;或–strings=&lt;字符串字符数&gt; 只显示符合指定的字符数目的字符串。 -t&lt;输出格式&gt;或–format=&lt;输出格式&gt; 设置输出格式。 -v或–output-duplicates 输出时不省略重复的数据。 -w&lt;每列字符数&gt;或–width=&lt;每列字符数&gt; 设置每列的最大字符数。 -x 此参数的效果和同时指定”-h”参数相同。 –help 在线帮助。 –version 显示版本信息。 paste功能说明：合并文件的列。 语 法：paste [-s][-d &lt;间隔字符&gt;][–help][–version][文件…] 补充说明：paste指令会把每个文件以列对列的方式，一列列地加以合并。 参 数： -d&lt;间隔字符&gt;或–delimiters=&lt;间隔字符&gt; 用指定的间隔字符取代跳格字符。 -s或–serial 串列进行而非平行处理。 –help 在线帮助。 –version 显示帮助信息。 patch功能说明：修补文件。 语 法：patch [-bceEflnNRstTuvZ][-B &lt;备份字首字符串&gt;][-d &lt;工作目录&gt;][-D &lt;标示符号&gt;][-F &lt;监别列数&gt;][-g &lt;控制数值&gt;][-i &lt;修补文件&gt;][-o &lt;输出文件&gt;][-p &lt;剥离层级&gt;][-r &lt;拒绝文件&gt;][-V &lt;备份方式&gt;][-Y &lt;备份字首字符串&gt;][-z &lt;备份字尾字符串&gt;][–backup-if -mismatch][–binary][–help][–nobackup-if-mismatch][–verbose][原始文件 &lt;修补文件&gt;] 或 path [-p &lt;剥离层级&gt;] &lt; [修补文件] 补充说明：patch指令让用户利用设置修补文件的方式，修改，更新原始文件。倘若一次仅修改一个文件，可直接在指令列中下达指令依序执行。如果配合修补文件的方式则能一次修补大批文件，这也是Linux系统核心的升级方法之一。 参 数： -b或–backup 备份每一个原始文件。 -B&lt;备份字首字符串&gt;或–prefix=&lt;备份字首字符串&gt; 设置文件备份时，附加在文件名称前面的字首字符串，该字符串可以是路径名称。 -c或–context 把修补数据解译成关联性的差异。 -d&lt;工作目录&gt;或–directory=&lt;工作目录&gt; 设置工作目录。 -D&lt;标示符号&gt;或–ifdef=&lt;标示符号&gt; 用指定的符号把改变的地方标示出来。 -e或–ed 把修补数据解译成ed指令可用的叙述文件。 -E或–remove-empty-files 若修补过后输出的文件其内容是一片空白，则移除该文件。 -f或–force 此参数的效果和指定”-t”参数类似，但会假设修补数据的版本为新 版本。 -F&lt;监别列数&gt;或–fuzz&lt;监别列数&gt; 设置监别列数的最大值。 -g&lt;控制数值&gt;或–get=&lt;控制数值&gt; 设置以RSC或SCCS控制修补作业。 -i&lt;修补文件&gt;或–input=&lt;修补文件&gt; 读取指定的修补问家你。 -l或–ignore-whitespace 忽略修补数据与输入数据的跳格，空格字符。 -n或–normal 把修补数据解译成一般性的差异。 -N或–forward 忽略修补的数据较原始文件的版本更旧，或该版本的修补数据已使 用过。 -o&lt;输出文件&gt;或–output=&lt;输出文件&gt; 设置输出文件的名称，修补过的文件会以该名称存放。 -p&lt;剥离层级&gt;或–strip=&lt;剥离层级&gt; 设置欲剥离几层路径名称。 -f&lt;拒绝文件&gt;或–reject-file=&lt;拒绝文件&gt; 设置保存拒绝修补相关信息的文件名称，预设的文件名称为.rej。 -R或–reverse 假设修补数据是由新旧文件交换位置而产生。 -s或–quiet或–silent 不显示指令执行过程，除非发生错误。 -t或–batch 自动略过错误，不询问任何问题。 -T或–set-time 此参数的效果和指定”-Z”参数类似，但以本地时间为主。 -u或–unified 把修补数据解译成一致化的差异。 -v或–version 显示版本信息。 -V&lt;备份方式&gt;或–version-control=&lt;备份方式&gt; 用”-b”参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，这个字符串不仅可用”-z”参数变更，当使用”-V”参数指定不同备份方式时，也会产生不同字尾的备份字符串。 -Y&lt;备份字首字符串&gt;或–basename-prefix=–&lt;备份字首字符串&gt; 设置文件备份时，附加在文件基本名称开头的字首字符串。 -z&lt;备份字尾字符串&gt;或–suffix=&lt;备份字尾字符串&gt; 此参数的效果和指定”-B”参数类似，差别在于修补作业使用的路径与文件名若为src/linux/fs/super.c，加上”backup/“字符串后，文件super.c会备份于/src/linux/fs/backup目录里。 -Z或–set-utc 把修补过的文件更改，存取时间设为UTC。 –backup-if-mismatch 在修补数据不完全吻合，且没有刻意指定要备份文件时，才备份文件。 –binary 以二进制模式读写数据，而不通过标准输出设备。 –help 在线帮助。 –nobackup-if-mismatch 在修补数据不完全吻合，且没有刻意指定要备份文件时，不要备份文件。 –verbose 详细显示指令的执行过程。 rcp(remote copy)功能说明：远端复制文件或目录。 语 法：rcp [-pr][源文件或目录][目标文件或目录] 或 rcp [-pr][源文件或目录…][目标文件] 补充说明：rcp指令用在远端复制文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则它灰把前面指定的所有文件或目录复制到该目录中。 参 数： -p 保留源文件或目录的属性，包括拥有者，所属群组，权限与时间。 -r 递归处理，将指定目录下的文件与子目录一并处理。 rhmask功能说明：产生与还原加密文件。 语 法：rhmask [加密文件][输出文件] 或 rhmask [-d][加密文件][源文件][输出文件] 补充说明：执行rhmask指令可制作加密过的文件，方便用户在公开的网络上传输该文件，而不至于被任意盗用。 参 数： -d 产生加密过的文件。 rm(remove)功能说明：删除文件或目录。 语 法：rm [-dfirv][–help][–version][文件或目录…] 补充说明：执行rm指令可删除文件或目录，如欲删除目录必须加上参数”-r”，否则预设仅会删除文件。 参 数： -d或–directory 直接把欲删除的目录的硬连接数据删成0，删除该目录。 -f或–force 强制删除文件或目录。 -i或–interactive 删除既有文件或目录之前先询问用户。 -r或-R或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -v或–verbose 显示指令执行过程。 –help 在线帮助。 –version 显示版本信息。 slocate(secure locate)功能说明：查找文件或目录。 语 法：slocate [-u][–help][–version][-d &lt;目录&gt;][查找的文件] 补充说明：slocate本身具有一个数据库，里面存放了系统中文件与目录的相关信息。 参 数： -d&lt;目录&gt;或–database=&lt;目录&gt; 指定数据库所在的目录。 -u 更新slocate数据库。 –help 显示帮助。 –version 显示版本信息。 split功能说明：切割文件。 语 法：split [–help][–version][-&lt;行数&gt;][-b &lt;字节&gt;][-C &lt;字节&gt;][-l &lt;行数&gt;][要切割的文件][输出文件名] 补充说明：split可将文件切成较小的文件，预设每1000行会切成一个小文件。 参 数： -&lt;行数&gt;或-l&lt;行数&gt; 指定每多少行就要切成一个小文件。 -b&lt;字节&gt; 指定每多少字就要切成一个小文件。 -C&lt;字节&gt; 与-b参数类似，但切割时尽量维持每行的完整性。 –help 显示帮助。 –version 显示版本信息。 [输出文件名] 设置切割后文件的前置文件名，split会自动在前置文件名后再加上编号。 tee功能说明：读取标准输入的数据，并将其内容输出成文件。 语 法：tee [-ai][–help][–version][文件…] 补充说明：tee指令会从标准输入设备读取数据，将其内容输出到标准输出设备，同时保存成文件。 参 数： -a或–append 附加到既有文件的后面，而非覆盖它． -i-i或–ignore-interrupts 忽略中断信号。 –help 在线帮助。 –version 显示版本信息。 tmpwatch(temporary watch)功能说明：删除暂存文件。 语 法：tmpwatch [-afqv][–test][超期时间][目录…] 补充说明：执行tmpwatch指令可删除不必要的暂存文件，您可以设置文件超期时间，单位以小时计算。 参 数： -a或–all 删除任何类型的文件。 -f或–force 强制删除文件或目录，其效果类似rm指令的”-f”参数。 -q或–quiet 不显示指令执行过程。 -v或–verbose 详细显示指令执行过程。 -test 仅作测试，并不真的删除文件或目录。 touch功能说明：改变文件或目录时间。 语 法：touch [-acfm][-d &lt;日期时间&gt;][-r &lt;参考文件或目录&gt;][-t &lt;日期时间&gt;][–help] [–version][文件或目录…] 或 touch [-acfm][–help][–version][日期时间][文件或目录…] 补充说明：使用touch指令可更改文件或目录的日期时间，包括存取时间和更改时间。 参 数： -a或–time=atime或–time=access或–time=use 只更改存取时间。 -c或–no-create 不建立任何文件。 -d&lt;时间日期&gt; 使用指定的日期时间，而非现在的时间。 -f 此参数将忽略不予处理，仅负责解决BSD版本touch指令的兼容性问题。 -m或–time=mtime或–time=modify 只更改变动时间。 -r&lt;参考文件或目录&gt; 把指定文件或目录的日期时间，统统设成和参考文件或目录的日期时间相同。 -t&lt;日期时间&gt; 使用指定的日期时间，而非现在的时间。 –help 在线帮助。 –version 显示版本信息。 umask功能说明：指定在建立文件时预设的权限掩码。 语 法：umask [-S][权限掩码] 补充说明：umask可用来设定[权限掩码]。[权限掩码]是由3个八进制的数字所组成，将现有的存取权限减掉权限掩码后，即可产生建立文件时预设的权限。 参 数： -S 以文字的方式来表示权限掩码。 whereis功能说明：查找文件。 语 法：whereis [-bfmsu][-B &lt;目录&gt;…][-M &lt;目录&gt;…][-S &lt;目录&gt;…][文件…] 补充说明：whereis指令会在特定目录中查找符合条件的文件。这些文件的烈性应属于原始代码，二进制文件，或是帮助文件。 参 数： -b 只查找二进制文件。 -B&lt;目录&gt; 只在设置的目录下查找二进制文件。 -f 不显示文件名前的路径名称。 -m 只查找说明文件。 -M&lt;目录&gt; 只在设置的目录下查找说明文件。 -s 只查找原始代码文件。 -S&lt;目录&gt; 只在设置的目录下查找原始代码文件。 -u 查找不包含指定类型的文件。 which功能说明：查找文件。 语 法：which [文件…] 补充说明：which指令会在环境变量$PATH设置的目录里查找符合条件的文件。 参 数： -n&lt;文件名长度&gt; 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 -p&lt;文件名长度&gt; 与-n参数相同，但此处的&lt;文件名长度&gt;包括了文件的路径。 -w 指定输出时栏位的宽度。 -V 显示版本信息。 cat使用权限：所有使用者 使用方式：cat [-AbeEnstTuv] [–help] [–version] fileName 说明：把档案串连接后传到基本输出（萤幕或加 &gt; fileName 到另一个档案） 参数： -n 或 –number 由 1 开始对所有输出的行数编号 -b 或 –number-nonblank 和 -n 相似，只不过对于空白行不编号 -s 或 –squeeze-blank 当遇到有连续两行以上的空白行，就代换为一行的空白行 -v 或 –show-nonprinting 范例： cat -n textfile1 &amp;gt; textfile2 把 textfile1 的档案内容加上行号后输入 textfile2 这个档案里 cat -b textfile1 textfile2 &amp;gt;&amp;gt; textfile3 把 textfile1 和 textfile2 的档案内容加上行号（空白行不加）之后将内容附加到 textfile3 里。 chattr(change attribute)功能说明：改变文件属性。 语 法：chattr [-RV][-v&lt;版本编号&gt;][+/-/=&lt;属性&gt;][文件或目录…] 补充说明：这项指令可改变存放在ext2文件系统上的文件或目录属性，这些属性共有以下8种模式： a：让文件或目录仅供附加用途。 b：不更新文件或目录的最后存取时间。 c：将文件或目录压缩后存放。 d：将文件或目录排除在倾倒操作之外。 i：不得任意更动文件或目录。 s：保密性删除文件或目录。 S：即时更新文件或目录。 u：预防以外删除。 参 数： -R 递归处理，将指定目录下的所有文件及子目录一并处理。 -v&lt;版本编号&gt; 设置文件或目录版本。 -V 显示指令执行过程。 +&lt;属性&gt; 开启文件或目录的该项属性。 -&lt;属性&gt; 关闭文件或目录的该项属性。 =&lt;属性&gt; 指定文件或目录的该项属性。 chgrp(change group)功能说明：变更文件或目录的所属群组。 语 法：chgrp [-cfhRv][–help][–version][所属群组][文件或目录…] 或 chgrp [-cfhRv][–help][–reference=&lt;参考文件或目录&gt;][–version][文件或目录…] 补充说明：在UNIX系统家族里，文件或目录权限的掌控以拥有者及所属群组来管理。您可以使用chgrp指令去变更文件与目录的所属群组，设置方式采用群组名称或群组识别码皆可。 参 数： -c或–changes 效果类似”-v”参数，但仅回报更改的部分。 -f或–quiet或–silent 不显示错误信息。 -h或–no-dereference 只对符号连接的文件作修改，而不更动其他任何相关文件。 -R或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -v或–verbose 显示指令执行过程。 –help 在线帮助。 –reference=&lt;参考文件或目录&gt; 把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同。 –version 显示版本信息。 chmod(change mode)功能说明：变更文件或目录的权限。 语 法：chmod [-cfRv][–help][–version][&lt;权限范围&gt;+/-/=&lt;权限设置…&gt;][文件或目录…] 或 chmod [-cfRv][–help][–version][数字代号][文件或目录…] 或 chmod [-cfRv][–help][–reference=&lt;参考文件或目录&gt;][–version][文件或目录…] 补充说明：在UNIX系统家族里，文件或目录权限的控制分别以读取，写入，执行3种一般权限来区分，另有3种特殊权限可供运用，再搭配拥有者与所属群组管理权限范围。您可以使用chmod指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果您对符号连接修改权限，其改变会作用在被连接的原始文件。权限范围的表示法如下： u：User，即文件或目录的拥有者。 g：Group，即文件或目录的所属群组。 o：Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围。 a：All，即全部的用户，包含拥有者，所属群组以及其他用户。 有关权限代号的部分，列表于下： r：读取权限，数字代号为&quot;4&quot;。 w：写入权限，数字代号为&quot;2&quot;。 x：执行或切换权限，数字代号为&quot;1&quot;。 -：不具任何权限，数字代号为&quot;0&quot;。 s：特殊?b&amp;gt;功能说明：变更文件或目录的权限。 参 数： -c或–changes 效果类似”-v”参数，但仅回报更改的部分。 -f或–quiet或–silent 不显示错误信息。 -R或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -v或–verbose 显示指令执行过程。 –help 在线帮助。 –reference=&lt;参考文件或目录&gt; 把指定文件或目录的权限全部设成和参考文件或目录的权限相同 –version 显示版本信息。 &lt;权限范围&gt;+&lt;权限设置&gt; 开启权限范围的文件或目录的该项权限设置。 &lt;权限范围&gt;-&lt;权限设置&gt; 关闭权限范围的文件或目录的该项权限设置。 &lt;权限范围&gt;=&lt;权限设置&gt; 指定权限范围的文件或目录的该项权限设置。 如chmod a+x … chown(change owner)功能说明：变更文件或目录的拥有者或所属群组。 语 法：chown [-cfhRv][–dereference][–help][–version][拥有者.&lt;所属群组&gt;][文件或目录..] 或chown [-chfRv][–dereference][–help][–version][.所属群组][文件或目录… …] 或chown [-cfhRv][–dereference][–help][–reference=&lt;参考文件或目录&gt;][–version][文件或目录…] 补充说明：在UNIX系统家族里，文件或目录权限的掌控以拥有者及所属群组来管理。您可以使用chown指令去变更文件与目录的拥有者或所属群组，设置方式采用用户名称或用户识别码皆可，设置群组则用群组名称或群组识别码。 参 数： -c或–changes 效果类似”-v”参数，但仅回报更改的部分。 -f或–quite或–silent 不显示错误信息。 -h或–no-dereference 之对符号连接的文件作修改，而不更动其他任何相关文件。 -R或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -v或–version 显示指令执行过程。 –dereference 效果和”-h”参数相同。 –help 在线帮助。 –reference=&lt;参考文件或目录&gt; 把指定文件或目录的拥有者与所属群组全部设成和参考文件或目 录的拥有者与所属群组相同。 –version 显示版本信息。 cksum(check sum)功能说明：检查文件的CRC是否正确。 语 法：cksum [–help][–version][文件…] 补充说明：CRC是一种排错检查方式，该演算法的标准由CCITT所指定，至少可检测到99.998%的已知错误。指定文件交由cksum演算，它会回报计算结果，供用户核对文件是否正确无误。若不指定任何文件名称或是所给予的文件名为”-“，则cksum指令会从标准输入设备读取数据。 参 数： –help 在线帮助。 –version 显示版本信息。 cmp(compare)功能说明：比较两个文件是否有差异。 语 法：cmp [-clsv][-i &lt;字符数目&gt;][–help][第一个文件][第二个文件] 补充说明：当相互比较的两个文件完全一样时，则该指令不会显示任何信息。若发现有所差异，预设会标示出第一个不同之处的字符和列数编号。若不指定任何文件名称或是所给予的文件名为”-“，则cmp指令会从标准输入设备读取数据。 参 数： -c或–print-chars 除了标明差异处的十进制字码之外，一并显示该字符所对应字符。 -i&lt;字符数目&gt;或–ignore-initial=&lt;字符数目&gt; 指定一个数目。 -l或–verbose 标示出所有不一样的地方。 -s或–quiet或–silent 不显示错误信息。 -v或–version 显示版本信息。 –help 在线帮助。 cp(copy)功能说明：复制文件或目录。 语 法：cp [-abdfilpPrRsuvx][-S &lt;备份字尾字符串&gt;][-V &lt;备份方式&gt;][–help][–spares=&lt;使用时机&gt;][–version][源文件或目录][目标文件或目录] [目的目录] 补充说明：cp指令用在复制文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则它会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，而最后的目的地并非是一个已存在的目录，则会出现错误信息。 参 数： -a或–archive 此参数的效果和同时指定”-dpR”参数相同。 -b或–backup 删除，覆盖目标文件之前的备份，备份文件会在字尾加上一个备份字符串。 -d或–no-dereference 当复制符号连接时，把目标文件或目录也建立为符号连接，并指向与源文件或目录连接的原始文件或目录。 -f或–force 强行复制文件或目录，不论目标文件或目录是否已存在。 -i或–interactive 覆盖既有文件之前先询问用户。 -l或–link 对源文件建立硬连接，而非复制文件。 -p或–preserve 保留源文件或目录的属性。 -P或–parents 保留源文件或目录的路径。 -r 递归处理，将指定目录下的文件与子目录一并处理。 -R或–recursive 递归处理，将指定目录下的所有文件与子目录一并处理。 -s或–symbolic-link 对源文件建立符号连接，而非复制文件。 -S&lt;备份字尾字符串&gt;或–suffix=&lt;备份字尾字符串&gt; 用”-b”参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，预设的备份字尾字符串是符号”~”。 -u或–update 使用这项参数后只会在源文件的更改时间较目标文件更新时或是 名称相互对应的目标文件并不存在，才复制文件。 -v或–verbose 显示指令执行过程。 -V&lt;备份方式&gt;或–version-control=&lt;备份方式&gt; 用”-b”参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，这字符串不仅可用”-S”参数变更，当使用”-V”参数指定不同备份方式时，也会产生不同字尾的备份字串。 -x或–one-file-system 复制的文件或目录存放的文件系统，必须与cp指令执行时所处的文件系统相同，否则不予复制。 –help 在线帮助。 –sparse=&lt;使用时机&gt; 设置保存稀疏文件的时机。 –version 显示版本信息。 cut使用权限：所有使用者 用法：cut -cnum1-num2 filename 说明：显示每行从开头算起 num1 到 num2 的文字。 范例： shell&amp;gt;&amp;gt; cat example test2 this is test1 shell&amp;gt;&amp;gt; cut -c0-6 example ## print 开头算起前 6 个字元 test2 this i indent功能说明：调整C原始代码文件的格式。 语 法：indent [参数][源文件] 或 indent [参数][源文件][-o 目标文件] 补充说明：indent可辨识C的原始代码文件，并加以格式化，以方便程序设计师阅读。 参 数： -bad或–blank-lines-after-declarations 在声明区段或加上空白行。 -bap或–blank-lines-after-procedures 在程序或加上空白行。 -bbb或–blank-lines-after-block-comments 在注释区段后加上空白行。 -bc或–blank-lines-after-commas 在声明区段中，若出现逗号即换行。 -bl或–braces-after-if-line if(或是else,for等等)与后面执行区段的”{“不同行，且”}”自成一行。 -bli&lt;缩排格数&gt;或–brace-indent&lt;缩排格数&gt; 设置{ }缩排的格数。 -br或–braces-on-if-line if(或是else,for等等)与后面执行跛段的”{“不同行，且”}”自成一行。 -bs或–blank-before-sizeof 在sizeof之后空一格。 -c&lt;栏数&gt;或–comment-indentation&lt;栏数&gt; 将注释置于程序码右侧指定的栏位。 -cd&lt;栏数&gt;或–declaration-comment-column&lt;栏数&gt; 将注释置于声明右侧指定的栏位。 -cdb或–comment-delimiters-on-blank-lines 注释符号自成一行。 -ce或–cuddle-else 将else置于”}”(if执行区段的结尾)之后。 -ci&lt;缩排格数&gt;或–continuation-indentation&lt;缩排格数&gt; 叙述过长而换行时，指定换行后缩排的格数。 -cli&lt;缩排格数&gt;或–case-indentation-&lt;缩排格数&gt; 使用case时，switch缩排的格数。 -cp&lt;栏数&gt;或-else-endif-column&lt;栏数&gt; 将注释置于else与elseif叙述右侧定的栏位。 -cs或–space-after-cast 在cast之后空一格。 -d&lt;缩排格数&gt;或-line-comments-indentation&lt;缩排格数&gt; 针对不是放在程序码右侧的注释，设置其缩排格数。 -di&lt;栏数&gt;或–declaration-indentation&lt;栏数&gt; 将声明区段的变量置于指定的栏位。 -fc1或–format-first-column-comments 针对放在每行最前端的注释，设置其格式。 -fca或–format-all-comments 设置所有注释的格式。 -gnu或–gnu-style 指定使用GNU的格式，此为预设值。 -i&lt;格数&gt;或–indent-level&lt;格数&gt; 设置缩排的格数。 -ip&lt;格数&gt;或–parameter-indentation&lt;格数&gt; 设置参数的缩排格数。 -kr或–k-and-r-style 指定使用Kernighan&amp;Ritchie的格式。 -lp或–continue-at-parentheses 叙述过长而换行，且叙述中包含了括弧时，将括弧中的每行起始栏位内容垂直对其排列。 -nbad或–no-blank-lines-after-declarations 在声明区段后不要加上空白行。 -nbap或–no-blank-lines-after-procedures 在程序后不要加上空白行。 -nbbb或–no-blank-lines-after-block-comments 在注释区段后不要加上空白行。 -nbc或–no-blank-lines-after-commas 在声明区段中，即使出现逗号，仍旧不要换行。 -ncdb或–no-comment-delimiters-on-blank-lines 注释符号不要自成一行。 -nce或–dont-cuddle-else 不要将else置于”}”之后。 -ncs或–no-space-after-casts 不要在cast之后空一格。 -nfc1或–dont-format-first-column-comments 不要格式化放在每行最前端的注释。 -nfca或–dont-format-comments 不要格式化任何的注释。 -nip或–no-parameter-indentation 参数不要缩排。 -nlp或–dont-line-up-parentheses 叙述过长而换行，且叙述中包含了括弧时，不用将括弧中的每行起始栏位垂直对其排列。 -npcs或–no-space-after-function-call-names 在调用的函数名称之后，不要加上空格。 -npro或–ignore-profile 不要读取indent的配置文件.indent.pro。 -npsl或–dont-break-procedure-type 程序类型与程序名称放在同一行。 -nsc或–dont-star-comments 注解左侧不要加上星号(*)。 -nsob或–leave-optional-semicolon 不用处理多余的空白行。 -nss或–dont-space-special-semicolon 若for或while区段仅有一行时，在分号前不加上空格。 -nv或–no-verbosity 不显示详细的信息。 -orig或–original 使用Berkeley的格式。 -pcs或–space-after-procedure-calls 在调用的函数名称与”{“之间加上空格。 -psl或–procnames-start-lines 程序类型置于程序名称的前一行。 -sc或–start-left-side-of-comments 在每行注释左侧加上星号(*)。 -sob或–swallow-optional-blank-lines 删除多余的空白行。 -ss或–space-special-semicolon 若for或swile区段今有一行时，在分号前加上空格。 -st或–standard-output 将结果显示在标准输出设备。 -T 数据类型名称缩排。 -ts&lt;格数&gt;或–tab-size&lt;格数&gt; 设置tab的长度。 -v或–verbose 执行时显示详细的信息。 -version 显示版本信息。 linux文件传输命令bye功能说明：中断FTP连线并结束程序。 语 法：bye 补充说明：在ftp模式下，输入bye即可中断目前的连线作业，并结束ftp的执行。 ftp(file transfer protocol)功能说明：设置文件系统相关功能。 语 法：ftp [-dignv][主机名称或IP地址] 补充说明：FTP是ARPANet的标准文件传输协议，该网络就是现今Internet的前身。 参 数： -d 详细显示指令执行过程，便于排错或分析程序执行的情形。 -i 关闭互动模式，不询问任何问题。 -g 关闭本地主机文件名称支持特殊字符的扩充特性。 -n 不使用自动登陆。 -v 显示指令执行过程。 ftpcount功能说明：显示目前以FTP登入的用户人数。 语 法：ftpcount 补充说明：执行这项指令可得知目前用FTP登入系统的人数以及FTP登入人数的上限。 ftpshut(ftp shutdown)功能说明：在指定的时间关闭FTP服务器。 语 法：ftpshut [-d&lt;分钟&gt;][-l&lt;分钟&gt;][关闭时间][“警告信息”] 补充说明：本指令提供系统管理者在设置的时间关闭FTP服务器，且能在关闭之前发出警告信息通知用户。关闭时间若设置后为”none”，则会马上关闭服务器。如果采用”+30”的方式来设置表示服务器在30分钟之后关闭。依次类推，假设使用”1130”的格式则代表服务器会在每日的11时30分关闭，时间格式为24小时制。FTP服务器关闭后，在/etc目录下会产生一个名称为shutmsg的文件，把它删除后即可再度启动FTP服务器的功能。 参 数： -d&lt;分钟&gt; 切断所有FTP连线时间。 -l&lt;分钟&gt; 停止接受FTP登入的时间。 ftpwho功能说明：显示目前所有以FTP登入的用户信息。 语 法：ftpwho 补充说明：执行这项指令可得知目前用FTP登入系统的用户有那些人，以及他们正在进行的操作。 ncftp(nc file transfer protocol)功能说明：传输文件。 语 法：ncftp [主机或IP地址] 补充说明：FTP让用户得以下载存放于服务器主机的文件，也能将文件上传到远端主机放置。NcFTP是文字模式FTP程序的佼佼者，它具备多样特色， 包括显示传输速率，下载进度，自动续传，标住书签，可通过防火墙和代理服务器等。 tftp(trivial file transfer protocol)功能说明：传输文件。 语 法：tftp [主机名称或IP地址] 补充说明：FTP让用户得以下载存放于远端主机的文件，也能将文件上传到远端主机放置。tftp是简单的文字模式ftp程序，它所使用的指令和FTP类似。 uucico功能说明：UUCP文件传输服务程序。 语 法：uucico [-cCDefqvwz][-i&lt;类型&gt;][-I&lt;文件&gt;][-p&lt;连接端口号码&gt;][-][-rl][-s&lt;主机&gt;][-S&lt;主机&gt;][-u&lt;用户&gt;][-x&lt;类型&gt;][–help] 补充说明：uucico是用来处理uucp或uux送到队列的文件传输工具。uucico有两种工作模式：主动模式和附属模式。当在主动模式下时，uucico会调用远端主机；在附属模式下时，uucico则接受远端主机的调用。 参 数： -c或–quiet 当不执行任何工作时，不要更改记录文件的内容及更新目前的状态。 -C或–ifwork 当有工作要执行时，才调用-s或-S参数所指定主机。 -D或–nodetach 不要与控制终端机离线。 -e或–loop 在附属模式下执行，并且出现要求登入的提示画面。 -f或–force 当执行错误时，不等待任何时间即重新调用主机。 -i&lt;类型&gt;或–stdin&lt;类型&gt; 当使用到标准输入设备时，指定连接端口的类型。 -I&lt;文件&gt;–config&lt;文件&gt; 指定使用的配置文件。 -l或–prompt 出现要求登入的提示画面。 -p&lt;连接端口号码&gt;或-port&lt;连接端口号码&gt; 指定连接端口号码。 -q或–quiet 不要启动uuxqt服务程序。 -r0或–slave 以附属模式启动。 -s&lt;主机&gt;或–system&lt;主机&gt; 调用指定的主机。 -u&lt;用户&gt;或–login&lt;用户&gt; 指定登入的用户帐号，而不允许输入任意的登入帐号。 -v或–version 显示版本信息，并且结束程序。 -w或–wait 在主动模式下，当执行调用动作时，则出现要求登入的提示画面。 -x&lt;类型&gt;或-X&lt;类型&gt;或outgoing-debug&lt;类型&gt; 启动指定的排错模式。 -z或–try-next 当执行不成功时，尝试下一个选择而不结束程序。 –help 显示帮助，并且结束程序。 uucp功能说明：在Unix系统之间传送文件。 语 法：uucp [-cCdfjmrRtvW][-g&lt;等级&gt;][-I&lt;配置文件&gt;][-n&lt;用户&gt;][-x&lt;类型&gt;][–help][…来源][目的] 补充说明：UUCP为Unix系统之间，通过序列线来连线的协议。uucp使用UUCP协议，主要的功能为传送文件。 参 数： -c或–nocopy 不用将文件复制到缓冲区。 -C或–copy 将文件复制到缓冲区。 -d或–directiories 在传送文件时，自动在[目的]建立必要的目录。 -f或–nodirectiories 在传送文件时，若需要在[目的]建立目录，则放弃执行该作业。 -g&lt;等级&gt;或–grade&lt;等级&gt; 指定文件传送作业的优先顺序。 -I&lt;配置文件&gt;或–config&lt;配置文件&gt; 指定uucp配置文件。 -j或–jobid 显示作业编号。 -m或–mail 作业结束后，以电子邮件报告作业是否顺利完成。 -n&lt;用户&gt;或–notify&lt;用户&gt; 作业结束后，以电子邮件向指定的用户报告作业是否顺利完成。 -r或–nouucico 不要立即启动uucico服务程序，仅将作业送到队列中，待稍后再执行。 -R或–recursive 若[来源]为目录，则将整个目录包含子目录复制到[目的]。 -t或–uuto 将最后一个参数视为”主机名!用户”。 -v或–version 显示版本信息。 -W或–noexpand 不要将目前所在的目录加入路径。 -x&lt;类型&gt;或–debug&lt;类型&gt;启动指定的排错模式。 –help 显示帮助。 [源…] 指定源文件或路径。 [目的] 指定目标文件或路径。 uupick功能说明：处理传送进来的文件。 语 法：uupick [-v][-I&lt;配置文件&gt;][-s&lt;主机&gt;][-x&lt;层级&gt;][–help] 补充说明：当其他主机通过UUCP将文件传送进来时，可利用uupick指令取出这些文件。 参 数： -I&lt;配置文件&gt;或–config&lt;配置文件&gt; 指定配置文件。 -s&lt;主机&gt;或–system&lt;主机&gt; 处理由指定主机传送过来的文件。 -v或–version 显示版本信息。 –help 显示帮助。 uuto功能说明：将文件传送到远端的UUCP主机。 语 法：uuto [文件][目的] 补充说明：uuto为script文件，它实际上会执行uucp，用来将文件传送到远端UUCP主机，并在完成工作后，以邮件通知远端主机上的用户。 参 数： 相关参数请参考uucp指令。 linux磁盘管理命令cd(change directory)功能说明：切换目录。 语 法：cd [目的目录] 补充说明：cd指令可让用户在不同的目录间切换，但该用户必须拥有足够的权限进入目的目录。 df(disk free)功能说明：显示磁盘的相关信息。 语 法：df [-ahHiklmPT][–block-size=&lt;区块大小&gt;][-t &lt;文件系统类型&gt;][-x &lt;文件系统类型&gt;][–help][–no-sync][–sync][–version][文件或设备] 补充说明：df可显示磁盘的文件系统与使用情形。 参 数： -a或–all 包含全部的文件系统。 –block-size=&lt;区块大小&gt; 以指定的区块大小来显示区块数目。 -h或–human-readable 以可读性较高的方式来显示信息。 -H或–si 与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes。 -i或–inodes 显示inode的信息。 -k或–kilobytes 指定区块大小为1024字节。 -l或–local 仅显示本地端的文件系统。 -m或–megabytes 指定区块大小为1048576字节。 –no-sync 在取得磁盘使用信息前，不要执行sync指令，此为预设值。 -P或–portability 使用POSIX的输出格式。 –sync 在取得磁盘使用信息前，先执行sync指令。 -t&lt;文件系统类型&gt;或–type=&lt;文件系统类型&gt; 仅显示指定文件系统类型的磁盘信息。 -T或–print-type 显示文件系统的类型。 -x&lt;文件系统类型&gt;或–exclude-type=&lt;文件系统类型&gt; 不要显示指定文件系统类型的磁盘信息。 –help 显示帮助。 –version 显示版本信息。 [文件或设备] 指定磁盘设备。 dirs功能说明：显示目录记录。 语 法：dirs [+/-n -l] 补充说明：显示目录堆叠中的记录。 参 数： +n 显示从左边算起第n笔的目录。 -n 显示从右边算起第n笔的目录。 -l 显示目录完整的记录。 du(disk usage)功能说明：显示目录或文件的大小。 语 法：du [-abcDhHklmsSx][-L &lt;符号连接&gt;][-X &lt;文件&gt;][–block-size][–exclude=&lt;目录或文件&gt;][–max-depth=&lt;目录层数&gt;][–help][–version][目录或文件] 补充说明：du会显示指定的目录或文件所占用的磁盘空间。 参 数： -a或-all 显示目录中个别文件的大小。 -b或-bytes 显示目录或文件大小时，以byte为单位。 -c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。 -D或–dereference-args 显示指定符号连接的源文件大小。 -h或–human-readable 以K，M，G为单位，提高信息的可读性。 -H或–si 与-h参数相同，但是K，M，G是以1000为换算单位。 -k或–kilobytes 以1024 bytes为单位。 -l或–count-links 重复计算硬件连接的文件。 -L&lt;符号连接&gt;或–dereference&lt;符号连接&gt; 显示选项中所指定符号连接的源文件大小。 -m或–megabytes 以1MB为单位。 -s或–summarize 仅显示总计。 -S或–separate-dirs 显示个别目录的大小时，并不含其子目录的大小。 -x或–one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。 -X&lt;文件&gt;或–exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件。 –exclude=&lt;目录或文件&gt; 略过指定的目录或文件。 –max-depth=&lt;目录层数&gt; 超过指定层数的目录后，予以忽略。 –help 显示帮助。 –version 显示版本信息。 edquota(edit quota)功能说明：编辑用户或群组的quota。 语 法：edquota [-p &lt;源用户名称&gt;][-ug][用户或群组名称…] 或 edquota [-ug] -t 补充说明：edquota预设会使用vi来编辑使用者或群组的quota设置。 参 数： -u 设置用户的quota，这是预设的参数。 -g 设置群组的quota。 -p&lt;源用户名称&gt; 将源用户的quota设置套用至其他用户或群组。 -t 设置宽限期限。 eject功能说明：退出抽取式设备。 语 法：eject [-dfhnqrstv][-a &lt;开关&gt;][-c &lt;光驱编号&gt;][设备] 补充说明：若设备已挂入，则eject会先将该设备卸除再退出。 参 数： [设备] 设备可以是驱动程序名称，也可以是挂入点。 -a&lt;开关&gt;或–auto&lt;开关&gt; 控制设备的自动退出功能。 -c&lt;光驱编号&gt;或–changerslut&lt;光驱编号&gt; 选择光驱柜中的光驱。 -d或–default 显示预设的设备，而不是实际执行动作。 -f或–floppy 退出抽取式磁盘。 -h或–help 显示帮助。 -n或–noop 显示指定的设备。 -q或–tape 退出磁带。 -r或–cdrom 退出光盘。 -s或–scsi 以SCSI指令来退出设备。 -t或–trayclose 关闭光盘的托盘。 -v或–verbose 执行时，显示详细的说明。 lndir(link directory)功能说明：连接目录内容。 语 法：lndir [-ignorelinks][-silent][源目录][目的目录] 补充说明：执行lndir指令，可一口气把源目录底下的文件和子目录统统建立起相互对应的符号连接。 参 数： -ignorelinks 直接建立符号连接的符号连接。 -silent 不显示指令执行过程。 ls(list)功能说明：列出目录内容。 语 法：ls [-1aAbBcCdDfFgGhHiklLmnNopqQrRsStuUvxX][-I &lt;范本样式&gt;][-T &lt;跳格字数&gt;][-w &lt;每列字符数&gt;][–block-size=&lt;区块大小&gt;][–color=&lt;使用时机&gt;][–format=&lt;列表格式&gt;][–full-time][–help][–indicator-style=&lt;标注样式&gt;][–quoting-style=&lt;引号样式&gt;][–show-control-chars][–sort=&lt;排序方式&gt;][–time=&lt;时间戳记&gt;][–version][文件或目录…] 补充说明：执行ls指令可列出目录的内容，包括文件和子目录的名称。 参 数： -1 每列仅显示一个文件或目录名称。 -a或–all 下所有文件和目录。 -A或–almost-all 显示所有文件和目录，但不显示现行目录和上层目录。 -b或–escape 显示脱离字符。 -B或–ignore-backups 忽略备份文件和目录。 -c 以更改时间排序，显示文件和目录。 -C 以又上至下，从左到右的直行方式显示文件和目录名称。 -d或–directory 显示目录名称而非其内容。 -D或–dired 用Emacs的模式产生文件和目录列表。 -f 此参数的效果和同时指定”aU”参数相同，并关闭”lst”参数的效果。 -F或–classify 在执行文件，目录，Socket，符号连接，管道名称后面，各自加上”*”,”/“,”=”,”@”,”|”号。 -g 次参数将忽略不予处理。 -G或–no-group 不显示群组名称。 -h或–human-readable 用”K”,”M”,”G”来显示文件和目录的大小。 -H或–si 此参数的效果和指定”-h”参数类似，但计算单位是1000Bytes而非1024Bytes。 -i或–inode 显示文件和目录的inode编号。 -I&lt;范本样式&gt;或–ignore=&lt;范本样式&gt; 不显示符合范本样式的文件或目录名称。 -k或–kilobytes 此参数的效果和指定”block-size=1024”参数相同。 -l 使用详细格式列表。 -L或–dereference 如遇到性质为符号连接的文件或目录，直接列出该连接所指向的原始文件或目录。 -m 用”,”号区隔每个文件和目录的名称。 -n或–numeric-uid-gid 以用户识别码和群组识别码替代其名称。 -N或–literal 直接列出文件和目录名称，包括控制字符。 -o 此参数的效果和指定”-l” 参数类似，但不列出群组名称或识别码。 -p或–file-type 此参数的效果和指定”-F”参数类似，但不会在执行文件名称后面加上”*”号。 -q或–hide-control-chars 用”?”号取代控制字符，列出文件和目录名称。 -Q或–quote-name 把文件和目录名称以””号标示起来。 -r或–reverse 反向排序。 -R或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -s或–size 显示文件和目录的大小，以区块为单位。 -S 用文件和目录的大小排序。 -t 用文件和目录的更改时间排序。 -T&lt;跳格字符&gt;或–tabsize=&lt;跳格字数&gt; 设置跳格字符所对应的空白字符数。 -u 以最后存取时间排序，显示文件和目录。 -U 列出文件和目录名称时不予排序。 -v 文件和目录的名称列表以版本进行排序。 -w&lt;每列字符数&gt;或–width=&lt;每列字符数&gt; 设置每列的最大字符数。 -x 以从左到右，由上至下的横列方式显示文件和目录名称。 -X 以文件和目录的最后一个扩展名排序。 –block-size=&lt;区块大小&gt; 指定存放文件的区块大小。 –color=&lt;列表格式&gt; 培植文件和目录的列表格式。 –full-time 列出完整的日期与时间。 –help 在线帮助。 –indicator-style=&lt;标注样式&gt; 在文件和目录等名称后面加上标注，易于辨识该名称所属的类型。 –quoting-syte=&lt;引号样式&gt; 把文件和目录名称以指定的引号样式标示起来。 –show-control-chars 在文件和目录列表时，使用控制字符。 –sort=&lt;排序方式&gt; 配置文件和目录列表的排序方式。 –time=&lt;时间戳记&gt; 用指定的时间戳记取代更改时间。 –version 显示版本信息。 mcd功能说明：在MS-DOS文件系统中切换工作目录 语 法：mcd [目录名称] 补充说明：mcd为mtools工具指令，可在MS-DOS文件系统中切换工作目录。若不加任何参数，则显示目前所在的磁盘与工作目录。 mdeltree功能说明：删除MS-DOS目录 语 法：mdeltree [目录…] 补充说明：mdel为mtools工具指令，模拟MS-DOS的deltree指令，可删除MS-DOS文件系统中的目录及目录下所有子目录与文件。 mdu功能说明：显示MS-DOS目录所占用的磁盘空间。 语 法：mdu [-as][目录] 补充说明：mdu为mstools工具指令，可显示MS-DOS文件系统中目录所占用的磁盘空间。 参 数： -a 显示每个文件及整个目录所占用的空间。 -s 仅显示整个目录所占用的空间。 mkdir(make directories)功能说明：建立目录 语 法：mkdir [-p][–help][–version][-m &lt;目录属性&gt;][目录名称] 补充说明：mkdir可建立目录并同时设置目录的权限。 参 数： -m&lt;目录属性&gt;或–mode&lt;目录属性&gt; 建立目录时同时设置目录的权限。 -p或–parents 若所要建立目录的上层目录目前尚未建立，则会一并建立上层目录。 –help 显示帮助。 –verbose 执行时显示详细的信息。 –version 显示版本信息。 mlabel功能说明：显示或设置MS-DOS磁盘驱动器的标签名称。 语 法：mlabel [-csvV][驱动器代号][标签名称] 补充说明：mlabel为mtools工具指令，模拟MS-DOS的label指令，可显示或设置MS-DOS磁盘驱动器的标签名称。 参 数： -c 清楚标签名称 -s 显示标签名称 -v 执行时显示详细的信息。 -V 显示版本信息。 mmd功能说明：在MS-DOS文件系统中建立目录。 语 法：mmd [目录…] 补充说明：mmd为mtools工具指令，模拟MS-DOS的md指令，可在MS-DOS的文件系统中建立目录。 mmount功能说明：挂入MS-DOS文件系统。 语 法：mmount [驱动器代号][mount参数] 补充说明：mmount为mtools工具指令，可根据[mount参数]中的设置，将磁盘内容挂入到Linux目录中。 参 数： [mount参数]的用法请参考mount指令。 mrd功能说明：删除MS-DOS文件系统中的目录。 语 法：mrd [目录…] 补充说明：mrd为mtools工具指令，模拟MS-DOS的rd指令，可删除MS-DOS的目录。 mzip功能说明：Zip/Jaz磁盘驱动器控制指令。 语 法：mzip [-efpqruwx] 补充说明：mzip为mtools工具指令，可设置Zip或Jaz磁盘驱动区的保护模式以及执行退出磁盘的动作。 参 数： -e 退出磁盘。 -f 与-e参数一并使用，不管是否已经挂入磁盘中的文件系统，一律强制退出磁盘。 -p 设置磁盘的写入密码。 -q 显示目前的状态。 -r 将磁盘设为防写状态。 -u 退出磁盘以前，暂时解除磁盘的保护状态。 -w 将磁盘设为可写入状态。 -x 设置磁盘的密码。 pwd(print working directory)功能说明：显示工作目录。 语 法：pwd [–help][–version] 补充说明：执行pwd指令可立刻得知您目前所在的工作目录的绝对路径名称。 参 数： –help 在线帮助。 –version 显示版本信息。 quota功能说明：显示磁盘已使用的空间与限制。 语 法：quota [-quvV][用户名称…] 或 quota [-gqvV][群组名称…] 补充说明：执行quota指令，可查询磁盘空间的限制，并得知已使用多少空间。 参 数： -g 列出群组的磁盘空间限制。 -q 简明列表，只列出超过限制的部分。 -u 列出用户的磁盘空间限制。 -v 显示该用户或群组，在所有挂入系统的存储设备的空间限制。 -V 显示版本信息。 quotacheck功能说明：检查磁盘的使用空间与限制。 语 法：quotacheck [-adgRuv][文件系统…] 补充说明：执行quotacheck指令，扫描挂入系统的分区，并在各分区的文件系统根目录下产生quota.user和quota.group文件，设置用户和群组的磁盘空间限制。 参 数： -a 扫描在/etc/fstab文件里，有加入quota设置的分区。 -d 详细显示指令执行过程，便于排错或了解程序执行的情形。 -g 扫描磁盘空间时，计算每个群组识别码所占用的目录和文件数目。 -R 排除根目录所在的分区。 -u 扫描磁盘空间时，计算每个用户识别码所占用的目录和文件数目。 -v 显示指令执行过程。 quotaoff功能说明：关闭磁盘空间限制。 语 法：quotaoff [-aguv][文件系统…] 补充说明：执行quotaoff指令可关闭用户和群组的磁盘空间限制。 参 数： -a 关闭在/etc/fstab文件里，有加入quota设置的分区的空间限制。 -g 关闭群组的磁盘空间限制。 -u 关闭用户的磁盘空间限制。 -v 显示指令执行过程。 quotaon功能说明：开启磁盘空间限制。 语 法：quotaon [-aguv][文件系统…] 补充说明：执行quotaon指令可开启用户和群组的才磅秒年空间限制，各分区的文件系统根目录必须有quota.user和quota.group配置文件。 参 数： -a 开启在/ect/fstab文件里，有加入quota设置的分区的空间限制。 -g 开启群组的磁盘空间限制。 -u 开启用户的磁盘空间限制。 -v 显示指令指令执行过程。 repquota(report quota)功能说明：检查磁盘空间限制的状态。 语 法：repquota [-aguv][文件系统…] 补充说明：执行repquota指令，可报告磁盘空间限制的状况，清楚得知每位用户或每个群组已使用多少空间。 参 数： -a 列出在/etc/fstab文件里，有加入quota设置的分区的使用状况，包括用户和群组。 -g 列出所有群组的磁盘空间限制。 -u 列出所有用户的磁盘空间限制。 -v 显示该用户或群组的所有空间限制。 rmdir(remove directory)功能说明：删除目录。 语 法：rmdir [-p][–help][–ignore-fail-on-non-empty][–verbose][–version][目录…] 补充说明：当有空目录要删除时，可使用rmdir指令。 参 数： -p或–parents 删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除。 –help 在线帮助。 –ignore-fail-on-non-empty 忽略非空目录的错误信息。 –verbose 显示指令执行过程。 –version 显示版本信息。 rmt(remote magnetic tape)功能说明：远端磁带传输协议模块。 语 法：rmt 补充说明：通过rmt指令，用户可通过IPC连线，远端操控磁带机的倾倒和还原操作。 stat(status)功能说明：显示inode内容。 语 法：stat [文件或目录] 补充说明：stat以文字的格式来显示inode的内容。 tree功能说明：以树状图列出目录的内容。 语 法：tree [-aACdDfFgilnNpqstux][-I &lt;范本样式&gt;][-P &lt;范本样式&gt;][目录…] 补充说明：执行tree指令，它会列出指定目录下的所有文件，包括子目录里的文件。 参 数： -a 显示所有文件和目录。 -A 使用ASNI绘图字符显示树状图而非以ASCII字符组合。 -C 在文件和目录清单加上色彩，便于区分各种类型。 -d 显示目录名称而非内容。 -D 列出文件或目录的更改时间。 -f 在每个文件或目录之前，显示完整的相对路径名称。 -F 在执行文件，目录，Socket，符号连接，管道名称名称，各自加上”*”,”/“,”=”,”@”,”|”号。 -g 列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码。 -i 不以阶梯状列出文件或目录名称。 -I&lt;范本样式&gt; 不显示符合范本样式的文件或目录名称。 -l 如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录。 -n 不在文件和目录清单加上色彩。 -N 直接列出文件和目录名称，包括控制字符。 -p 列出权限标示。 -P&lt;范本样式&gt; 只显示符合范本样式的文件或目录名称。 -q 用”?”号取代控制字符，列出文件和目录名称。 -s 列出文件或目录大小。 -t 用文件和目录的更改时间排序。 -u 列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码。 -x 将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该子目录予以排除在寻找范围外。 umount功能说明：卸除文件系统。 语 法：umount [-ahnrvV][-t &lt;文件系统类型&gt;][文件系统] 补充说明：umount可卸除目前挂在Linux目录中的文件系统。 参 数： -a 卸除/etc/mtab中记录的所有文件系统。 -h 显示帮助。 -n 卸除时不要将信息存入/etc/mtab文件中。 -r 若无法成功卸除，则尝试以只读的方式重新挂入文件系统。 -t&lt;文件系统类型&gt; 仅卸除选项中所指定的文件系统。 -v 执行时显示详细的信息。 -V 显示版本信息。 [文件系统] 除了直接指定文件系统外，也可以用设备名称或挂入点来表示文件系统。 linux磁盘维护命令badblocks功能说明：检查磁盘装置中损坏的区块。 语 法：badblocks [-svw][-b &lt;区块大小&gt;][-o &lt;输出文件&gt;][磁盘装置][磁盘区块数][启始区块] 补充说明：执行指令时须指定所要检查的磁盘装置，及此装置的磁盘区块数。 参 数： -b&lt;区块大小&gt; 指定磁盘的区块大小，单位为字节。 -o&lt;输出文件&gt; 将检查的结果写入指定的输出文件。 -s 在检查时显示进度。 -v 执行时显示详细的信息。 -w 在检查时，执行写入测试。 [磁盘装置] 指定要检查的磁盘装置。 [磁盘区块数] 指定磁盘装置的区块总数。 [启始区块] 指定要从哪个区块开始检查。 cfdisk功能说明：磁盘分区。 语 法：cfdisk [-avz][-c &lt;柱面数目&gt;-h &lt;磁头数目&gt;-s &lt;盘区数目&gt;][-P &lt;r,s,t&gt;][外围设备代号] 补充说明：cfdisk是用来磁盘分区的程序，它十分类似DOS的fdisk，具有互动式操作界面而非传统fdisk的问答式界面，您可以轻易地利用方向键来操控分区操作。 参 数： -a 在程序里不用反白代表选取，而以箭头表示。 -c&lt;柱面数目&gt; 忽略BIOS的数值，直接指定磁盘的柱面数目。 -h&lt;磁头数目&gt; 忽略BIOS的数值，直接指定磁盘的磁头数目。 -P&lt;r,s,t&gt; 显示分区表的内容，附加参数”r”会显示整个分区表的详细资料，附加参数”s”会依照磁区的顺序显示相关信息，附加参数”t”则会以磁头，磁区，柱面的方式来显示资料。 -s&lt;磁区数目&gt; 忽略BIOS的数值，直接指定磁盘的磁区数目。 -v 显示版本信息。 -z 不读取现有的分区，直接当作没有分区的新磁盘使用。 dd功能说明：读取，转换并输出数据。 语 法：dd [bs=&lt;字节数&gt;][cbs=&lt;字节数&gt;][conv=&lt;关键字&gt;][count=&lt;区块数&gt;][ibs=&lt;字节数&gt;][if=&lt;文件&gt;][obs=&lt;字节数&gt;][of=&lt;文件&gt;][seek=&lt;区块数&gt;][skip=&lt;区块数&gt;][–help][–version] 补充说明：dd可从标准输入或文件读取数据，依指定的格式来转换数据，再输出到文件，设备或标准输出。 参 数： bs=&lt;字节数&gt; 将ibs( 输入)与obs(输出)设成指定的字节数。 cbs=&lt;字节数&gt; 转换时，每次只转换指定的字节数。 conv=&lt;关键字&gt; 指定文件转换的方式。 count=&lt;区块数&gt; 仅读取指定的区块数。 ibs=&lt;字节数&gt; 每次读取的字节数。 if=&lt;文件&gt; 从文件读取。 obs=&lt;字节数&gt; 每次输出的字节数。 of=&lt;文件&gt; 输出到文件。 seek=&lt;区块数&gt; 一开始输出时，跳过指定的区块数。 skip=&lt;区块数&gt; 一开始读取时，跳过指定的区块数。 –help 帮助。 –version 显示版本信息。 e2fsck(ext2 file system check)功能说明：检查ext2文件系统的正确性。 语 法：e2fsck [-acCdfFnprsStvVy][-b &lt;superblock&gt;][-B &lt;区块大小&gt;][-l &lt;文件&gt;][-L &lt;文件&gt;][设备名称] 补充说明：e2fsck执行后的传回值及代表意义如下。 0 没有任何错误发生。 1 文件系统发生错误，并且已经修正。 2 文件系统发生错误，并且已经修正。 4 文件系统发生错误，但没有修正。 8 运作时发生错误。 16 使用的语法发生错误。 128 共享的函数库发生错误。 参 数： -a 不询问使用者意见，便自动修复文件系统。 -b&lt;superblock&gt; 指定superblock，而不使用预设的superblock。 -B&lt;区块大小&gt; 指定区块的大小，单位为字节。 -c 一并执行badblocks，以标示损坏的区块。 -C 将检查过程的信息完整记录在file descriptor中，使得整个检查过程都能完整监控。 -d 显示排错信息。 -f 即使文件系统没有错误迹象，仍强制地检查正确性。 -F 执行前先清除设备的缓冲区。 -l&lt;文件&gt; 将文件中指定的区块加到损坏区块列表。 -L&lt;文件&gt; 先清除损坏区块列表，再将文件中指定的区块加到损坏区块列表。因此损坏区块列表的区块跟文件中指定的区块是一样的。 -n 以只读模式开启文件系统，并采取非互动方式执行，所有的问题对话均设置以”no”回答。 -p 不询问使用者意见，便自动修复文件系统。 -r 此参数只为了兼容性而存在，并无实际作用。 -s 如果文件系统的字节顺序不适当，就交换字节顺序，否则不做任何动作。 -S 不管文件系统的字节顺序，一律交换字节顺序。 -t 显示时间信息。 -v 执行时显示详细的信息。 -V 显示版本信息。 -y 采取非互动方式执行，所有的问题均设置以”yes”回答。 ext2ed(ext2 file system editor)功能说明：ext2文件系统编辑程序。 语 法：ext2ed 补充说明：ext2ed可直接处理硬盘分区上的数据，这指令只有Red Hat Linux才提供。 参 数： 一般指令 setdevice[设备名称] 指定要处理的设备。 disablewrite 将ext2ed设为只读的状态。 enablewrite 将ext2ed设为可读写的状态。 help[指令] 显示个别指令的帮助。 next 移至下一个单位，单位会依目前所在的模式而异。 prev 移至前一个单位，单位会依目前所在的模式而异。 pgup 移至下一页。 pgdn 移至上一页。 set 修改目前的数据，参数会依目前所在的模式而异。 writedata 在执行此指令之后，才会实际修改分区中的数据。 ext2进入3种模式的指令 super 进入main superblock,即Superblock模式。 group&lt;编号&gt; 进入指定的group，即Group模式。 cd&lt;目录或文件&gt; 在inode模式下，进入指定的目录或文件，即Inode模式。 Superblock模式 gocopy&lt;备份编号&gt; 进入指定的superblock备份。 setactivecopy 将目前所在的superblock，复制到main superblock。 Group模式 blockbitmap 显示目前groupo的区块图。 inode 进入目前group的第一个inode。 inodebitmap 显示目前group的inode二进制码。 Inode模式 dir 进入目录模式。 file 进入文件模式。 fdisk功能说明：磁盘分区。 语 法：fdisk [-b &lt;分区大小&gt;][-uv][外围设备代号] 或 fdisk [-l][-b &lt;分区大小&gt;][-uv][外围设备代号…] 或 fdisk [-s &lt;分区编号&gt;] 补充说明：fdisk是用来磁盘分区的程序，它采用传统的问答式界面，而非类似DOS fdisk的cfdisk互动式操作界面，因此在使用上较为不便，但功能却丝毫不打折扣。 参 数： -b&lt;分区大小&gt; 指定每个分区的大小。 -l 列出指定的外围设备的分区表状况。 -s&lt;分区编号&gt; 将指定的分区大小输出到标准输出上，单位为区块。 -u 搭配”-l”参数列表，会用分区数目取代柱面数目，来表示每个分区的起始地址。 -v 显示版本信息。 fsck.ext2(file system check-second filesystem)功能说明：检查文件系统并尝试修复错误。 语 法：fsck.ext2 [-acdfFnprsStvVy][-b &lt;分区第一个磁区地址&gt;][-B &lt;区块大小&gt;][-C &lt;反叙述器&gt;][-I &lt;inode缓冲区块数&gt;][-l/L &lt;损坏区块文件&gt;][-P &lt;处理inode大小&gt;][外围设备代号] 补充说明：当ext2文件系统发生错误时，可用fsck.ext2指令尝试加以修复。 参 数： -a 自动修复文件系统，不询问任何问题。 -b&lt;分区第一个磁区地址&gt; 指定分区的第一个磁区的起始地址，也就是Super Block。 -B&lt;区块大小&gt; 设置该分区每个区块的大小。 -c 检查指定的文件系统内，是否存在有损坏的区块。 -C&lt;反叙述器&gt; 指定反叙述器，fsck.ext2指令会把全部的执行过程，都交由其逆向叙述，便于排错或监控程序执行的情形。 -d 详细显示指令执行过程，便于排错或分析程序执行的情形。 -f 强制对该文件系统进行完整检查，纵然该文件系统在慨略检查下没有问题。 -F 检查文件系统之前，先清理该保存设备块区内的数据。 -I&lt;inode缓冲区块数&gt; 设置欲检查的文件系统，其inode缓冲区的区块数目。 -l&lt;损坏区块文件&gt; 把文件中所列出的区块，视为损坏区块并将其标示出来，避免应用程序使用该区块。 -L&lt;损坏区块文件&gt; 此参数的效果和指定”-l”参数类似，但在参考损坏区块文件标示损坏区块之前，会先将原来标示成损坏区块者统统清楚，即全部重新设置，而非仅是加入新的损坏区块标示。 -n 把欲检查的文件系统设成只读，并关闭互动模式，否决所有询问的问题。 -p 此参数的效果和指定”-a”参数相同。 -P&lt;处理inode大小&gt; 设置fsck.ext2指令所能处理的inode大小为多少。 -r 此参数将忽略不予处理，仅负责解决兼容性的问题。 -s 检查文件系统时，交换每对字节的内容。 -S 此参数的效果和指定”-s”参数类似，但不论该文件系统是否已是标准位顺序，一律交换每对字节的内容。 -t 显示fsck.ext2指令的时序信息。 -v 详细显示指令执行过程。 -V 显示版本信息。 -y 关闭互动模式，且同意所有询问的问题。 fsck(file system check)功能说明：检查文件系统并尝试修复错误。 语 法：fsck [-aANPrRsTV][-t &lt;文件系统类型&gt;][文件系统…] 补充说明：当文件系统发生错误四化，可用fsck指令尝试加以修复。 参 数： -a 自动修复文件系统，不询问任何问题。 -A 依照/etc/fstab配置文件的内容，检查文件内所列的全部文件系统。 -N 不执行指令，仅列出实际执行会进行的动作。 -P 当搭配”-A”参数使用时，则会同时检查所有的文件系统。 -r 采用互动模式，在执行修复时询问问题，让用户得以确认并决定处理方式。 -R 当搭配”-A”参数使用时，则会略过/目录的文件系统不予检查。 -s 依序执行检查作业，而非同时执行。 -t&lt;文件系统类型&gt; 指定要检查的文件系统类型。 -T 执行fsck指令时，不显示标题信息。 -V 显示指令执行过程。 fsck.minix(file system check-minix filesystem)功能说明：检查文件系统并尝试修复错误。 语 法：fsck.minix [-aflmrsv][外围设备代号] 补充说明：当minix文件系统发生错误时，可用fsck.minix指令尝试加以参考。 参 数： -a 自动修复文件系统，不询问任何问题。 -f 强制对该文件系统进行完整检查，纵然该文件系统在慨略检查下没有问题。 -l 列出所有文件名称。 -m 使用类似MINIX操作系统的警告信息。 -r 采用互动模式，在执行修复时询问问题，让用户得以确认并决定处理方式。 -s 显示该分区第一个磁区的相关信息。 -v 显示指令执行过程。 fsconf(file system configurator)功能说明：设置文件系统相关功能。 语 法：fsconf [–check] 补充说明：fsconf是Red Hat Linux发行版专门用来调整Linux各项设置的程序。 参 数： –chedk 检查特定文件的权限。 hdparm(hard disk parameters)功能说明：显示与设定硬盘的参数。 语 法：hdparm [-CfghiIqtTvyYZ][-a &lt;快取分区&gt;][-A &lt;0或1&gt;][-c &lt;I/O模式&gt;][-d &lt;0或1&gt;][-k &lt;0或1&gt;][-K &lt;0或1&gt;][-m &lt;分区数&gt;][-n &lt;0或1&gt;][-p &lt;PIO模式&gt;][-P &lt;分区数&gt;][-r &lt;0或1&gt;][-S &lt;时间&gt;][-u &lt;0或1&gt;][-W &lt;0或1&gt;][-X &lt;传输模式&gt;][设备] 补充说明：hdparm可检测，显示与设定IDE或SCSI硬盘的参数。 参 数： -a&lt;快取分区&gt; 设定读取文件时，预先存入块区的分区数，若不加上&lt;快取分区&gt;选项，则显示目前的设定。 -A&lt;0或1&gt; 启动或关闭读取文件时的快取功能。 -c&lt;I/O模式&gt; 设定IDE32位I/O模式。 -C 检测IDE硬盘的电源管理模式。 -d&lt;0或1&gt; 设定磁盘的DMA模式。 -f 将内存缓冲区的数据写入硬盘，并清楚缓冲区。 -g 显示硬盘的磁轨，磁头，磁区等参数。 -h 显示帮助。 -i 显示硬盘的硬件规格信息，这些信息是在开机时由硬盘本身所提供。 -I 直接读取硬盘所提供的硬件规格信息。 -k&lt;0或1&gt; 重设硬盘时，保留-dmu参数的设定。 -K&lt;0或1&gt; 重设硬盘时，保留-APSWXZ参数的设定。 -m&lt;磁区数&gt; 设定硬盘多重分区存取的分区数。 -n&lt;0或1&gt; 忽略硬盘写入时所发生的错误。 -p&lt;PIO模式&gt; 设定硬盘的PIO模式。 -P&lt;磁区数&gt; 设定硬盘内部快取的分区数。 -q 在执行后续的参数时，不在屏幕上显示任何信息。 -r&lt;0或1&gt; 设定硬盘的读写模式。 -S&lt;时间&gt; 设定硬盘进入省电模式前的等待时间。 -t 评估硬盘的读取效率。 -T 平谷硬盘快取的读取效率。 -u&lt;0或1&gt; 在硬盘存取时，允许其他中断要求同时执行。 -v 显示硬盘的相关设定。 -W&lt;0或1&gt; 设定硬盘的写入快取。 -X&lt;传输模式&gt; 设定硬盘的传输模式。 -y 使IDE硬盘进入省电模式。 -Y 使IDE硬盘进入睡眠模式。 -Z 关闭某些Seagate硬盘的自动省电功能。 losetup(loop setup)功能说明：设置循环设备。 语 法：losetup [-d][-e &lt;加密方式&gt;][-o &lt;平移数目&gt;][循环设备代号][文件] 补充说明：循环设备可把文件虚拟成区块设备，籍以模拟整个文件系统，让用户得以将其视为硬盘驱动器，光驱或软驱等设备，并挂入当作目录来使用。 参 数： -d 卸除设备。 -e&lt;加密方式&gt; 启动加密编码。 -o&lt;平移数目&gt; 设置数据平移的数目。 mbadblocks功能说明：检查MS-DOS文件系统的磁盘是否有损坏的磁区。 语 法：mbadblocks [驱动器代号] 补充说明：mbadblocks为mtools工具指令，可用来扫描MS-DOS文件系统的磁盘驱动器，并标示出损坏的磁区。 mformat功能说明：对MS-DOS文件系统的磁盘进行格式化。 语 法：mformat [-1aCFIKX][-0 &lt;数据传输率&gt;][-2 &lt;磁区数&gt;][-A &lt;数据传输率&gt;][-B &lt;开机区文件&gt;][-c &lt;丛集大小&gt;][-h &lt;磁头数&gt;][-H &lt;隐藏磁区数&gt;][-l &lt;磁盘标签&gt;][-M &lt;磁区大小&gt;][-n &lt;序号&gt;][-r &lt;根目录大小&gt;][-s &lt;磁区数&gt;][-S &lt;磁区大小&gt;][-t &lt;柱面数&gt;][驱动器代号] 补充说明：mformat为mtools工具指令，模拟MS-DOS的format指令，可将指定的磁盘或硬盘分区格式化为MS-DOS文件系统。 参 数： -0&lt;数据传输率&gt; 指定第0磁轨的数据传输率。 -1 不使用2M格式。 -2&lt;磁区数&gt; 指定在第0磁轨的第0个磁头的磁区数，也就是所谓的2M格式。 -a 指定产生Atari格式的序号。 -A&lt;数据传输率&gt; 指定第0磁轨以外所有磁轨的数据传输率。 -B&lt;开机区文件&gt; 从指定的文件来建立开机区。 -c&lt;丛集大小&gt; 指定丛集大小，单位为磁区。 -C 建立磁盘MS-DOS文件系统的映像文件。 -F 以FAT32来格式化磁盘。 -H&lt;隐藏磁区数&gt; 指定隐藏磁区的数目。 -I 指定FAT32的frVersion编号。 -k 尽可能地保留原有的开机区。 -l&lt;磁盘标签&gt; 指定磁盘标签名称。 -M&lt;磁区大小&gt; 指定MS-DOS文件系统所使用的磁区大小，预设应该与硬体磁区大小相同，单位为字节。 -n&lt;序号&gt; 指定序号。 -r&lt;根目录大小&gt; 指定根目录的大小，单位为磁区。 -s&lt;磁区数&gt; 指定每一磁轨所包含的磁区数目。 -S&lt;磁区大小&gt; 指定硬件磁区大小。 -t&lt;柱面数&gt; 指定柱面数目。 -X 格式化XDF磁盘。 mkbootdisk(make boot disk)功能说明：建立目前系统的启动盘。 语 法：mkbootdisk [–noprompt][–verbose][–version][–device &lt;设备&gt;][–mkinitrdargs &lt;参数&gt;][kernel 版本] 补充说明：mkbootdisk可建立目前系统的启动盘。 参 数： –device&lt;设备&gt; 指定设备。 –mkinitrdargs&lt;参数&gt; 设置mkinitrd的参数。 –noprompt 不会提示用户插入磁盘。 –verbose 执行时显示详细的信息。 –version 显示版本信息。 mkdosfs(make Dos file system)功能说明：建立MS-DOS文件系统。 语 法：mkdosfs [-cv][-f &lt;FAT数目&gt;][-F &lt;FAT记录的单位&gt;][-i &lt;磁盘序号&gt;][-l &lt;文件名&gt;][-m &lt;信息文件&gt;][-n &lt;标签&gt;][-r &lt;根目录项目数&gt;][-s &lt;磁区数&gt;][设备名称][区块数] 补充说明：mkdosfs可在Linux下，将磁盘格式化为MS-DOS文件系统的格式。 参 数： -c 检查是否有损坏的区块。 -f&lt;FAT数目&gt; 指定FAT的数目，目前支持1与2两种选项。 -F&lt;FAT记录的单位&gt; 指定FAT记录的单位为12或16位。 -i&lt;磁盘序号&gt; 设置文件系统的磁盘序号。 -l&lt;文件名&gt; 从指定的文件中，读取文件系统中损坏区块的信息。 -m&lt;信息文件&gt; 若以次文件系统开机，而却因操作系统没有正常安装，则会显示选项中所指定的信息文件内容。 -n&lt;标签&gt; 设置文件系统的磁盘标签名称。 -r&lt;根目录项目数&gt; 设置根目录最多能记载项目数。 -s&lt;磁区数&gt; 指定每个丛集所包含的磁区数。 -v 执行时显示详细的信息。 mke2fs(make ext2 file system)功能说明：建立ext2文件系统。 语 法：mke2fs [-cFMqrSvV][-b &lt;区块大小&gt;][-f &lt;不连续区段大小&gt;][-i &lt;字节&gt;][-N &lt;inode数&gt;][-l &lt;文件&gt;][-L &lt;标签&gt;][-m &lt;百分比值&gt;][-R=&lt;区块数&gt;][ 设备名称][区块数] 补充说明：mke2fs可建立Linux的ext2文件系统。 参 数： -b&lt;区块大小&gt; 指定区块大小，单位为字节。 -c 检查是否有损坏的区块。 -f&lt;不连续区段大小&gt; 指定不连续区段的大小，单位为字节。 -F 不管指定的设备为何，强制执行mke2fs。 -i&lt;字节&gt; 指定”字节/inode”的比例。 -N&lt;inode数&gt; 指定要建立的inode数目。 -l&lt;文件&gt; 从指定的文件中，读取文件西中损坏区块的信息。 -L&lt;标签&gt; 设置文件系统的标签名称。 -m&lt;百分比值&gt; 指定给管理员保留区块的比例，预设为5%。 -M 记录最后一次挂入的目录。 -q 执行时不显示任何信息。 -r 指定要建立的ext2文件系统版本。 -R=&lt;区块数&gt; 设置磁盘阵列参数。 -S 仅写入superblock与group descriptors，而不更改inode able inode bitmap以及block bitmap。 -v 执行时显示详细信息。 -V 显示版本信息。 mkfs.ext2功能说明：与mke2fs相同。 mkfs(make file system)功能说明：建立各种文件系统。 语 法：mkfs [-vV][fs][-f &lt;文件系统类型&gt;][设备名称][区块数] 补充说明：mkfs本身并不执行建立文件系统的工作，而是去调用相关的程序来执行。 参 数： fs 指定建立文件系统时的参数。 -t&lt;文件系统类型&gt; 指定要建立何种文件系统。 -v 显示版本信息与详细的使用方法。 -V 显示简要的使用方法。 mkfs.minix功能说明：建立Minix文件系统。 语 法：mkfs.minix [-cv][-i &lt;inode数目&gt;][-l &lt;文件&gt;][-n &lt;文件名长度&gt;][设备名称][区块数] 补充说明：mkfs.minix可建立Minix文件系统。 参 数： -c 检查是否有损坏的区块。 -i&lt;inode数目&gt; 指定文件系统的inode总数。 -l&lt;文件&gt; 从指定的文件中，读取文件系统中损坏区块的信息。 -n&lt;文件名长度&gt; 指定文件名称长度的上限。 -v 建立第2版的Minix文件系统。 mkfs.msdos功能说明：与mkdosfs相同。 mkinitrd(make initial ramdisk images)功能说明：建立要载入ramdisk的映像文件。 语 法：mkinitrd [-fv][–omit-scsi-modules][–version][–preload=&lt;模块名称&gt;][–with=&lt;模块名称&gt;][映像文件][Kernel 版本] 补充说明：mkinitrd可建立映像文件，以供Linux开机时载入ramdisk。 参 数： -f 若指定的映像问家名称与现有文件重复，则覆盖现有的文件。 -v 执行时显示详细的信息。 –omit-scsi-modules 不要载入SCSI模块。 –preload=&lt;模块名称&gt; 指定要载入的模块。 –with=&lt;模块名称&gt; 指定要载入的模块。 –version 显示版本信息。 mkisofs(make iso file system)功能说明：建立ISO 9660映像文件。 语 法：mkisofs [-adDfhJlLNrRTvz][-print-size][-quiet][-A &lt;应用程序ID&gt;][-abstract &lt;摘要文件&gt;][-b &lt;开机映像文件&gt;][-biblio &lt;ISBN文件&gt;][-c &lt;开机文件名称&gt;][-C &lt;盘区编号，磁区编号&gt;][-copyright &lt;版权信息文件&gt;][-hide &lt;目录或文件名&gt;][-hide-joliet &lt;文件或目录名&gt;][-log-file &lt;记录文件&gt;][-m &lt;目录或文件名&gt;][-M &lt;开机映像文件&gt;][-o &lt;映像文件&gt;][-p &lt;数据处理人&gt;][-P &lt;光盘发行人&gt;][-sysid &lt;系统ID &gt;][-V &lt;光盘ID &gt;][-volset &lt;卷册集ID&gt;][-volset-size &lt;光盘总数&gt;][-volset-seqno &lt;卷册序号&gt;][-x &lt;目录&gt;][目录或文件] 补充说明：mkisofs可将指定的目录与文件做成ISO 9660格式的映像文件，以供刻录光盘。 参 数： -a或–all mkisofs通常不处理备份文件。使用此参数可以把备份文件加到映像文件中。 -A&lt;应用程序ID&gt;或-appid&lt;应用程序ID&gt; 指定光盘的应用程序ID。 -abstract&lt;摘要文件&gt; 指定摘要文件的文件名。 -b&lt;开机映像文件&gt;或-eltorito-boot&lt;开机映像文件&gt; 指定在制作可开机光盘时所需的开机映像文件。 -biblio&lt;ISBN文件&gt; 指定ISBN文件的文件名，ISBN文件位于光盘根目录下，记录光盘的ISBN。 -c&lt;开机文件名称&gt; 制作可开机光盘时，mkisofs会将开机映像文件中的全-eltorito-catalog&lt;开机文件名称&gt;全部内容作成一个文件。 -C&lt;盘区编号，盘区编号&gt; 将许多节区合成一个映像文件时，必须使用此参数。 -copyright&lt;版权信息文件&gt; 指定版权信息文件的文件名。 -d或-omit-period 省略文件后的句号。 -D或-disable-deep-relocation ISO 9660最多只能处理8层的目录，超过8层的部分，RRIP会自动将它们设置成ISO 9660兼容的格式。使用-D参数可关闭此功能。 -f或-follow-links 忽略符号连接。 -h 显示帮助。 -hide&lt;目录或文件名&gt; 使指定的目录或文件在ISO 9660或Rock RidgeExtensions的系统中隐藏。 -hide-joliet&lt;目录或文件名&gt; 使指定的目录或文件在Joliet系统中隐藏。 -J或-joliet 使用Joliet格式的目录与文件名称。 -l或-full-iso9660-filenames 使用ISO 9660 32字符长度的文件名。 -L或-allow-leading-dots 允许文件名的第一个字符为句号。 -log-file&lt;记录文件&gt; 在执行过程中若有错误信息，预设会显示在屏幕上。 -m&lt;目录或文件名&gt;或-exclude&lt;目录或文件名&gt; 指定的目录或文件名将不会房入映像文件中。 -M&lt;映像文件&gt;或-prev-session&lt;映像文件&gt; 与指定的映像文件合并。 -N或-omit-version-number 省略ISO 9660文件中的版本信息。 -o&lt;映像文件&gt;或-output&lt;映像文件&gt; 指定映像文件的名称。 -p&lt;数据处理人&gt;或-preparer&lt;数据处理人&gt; 记录光盘的数据处理人。 -print-size 显示预估的文件系统大小。 -quiet 执行时不显示任何信息。 -r或-rational-rock 使用Rock Ridge Extensions，并开放全部文件的读取权限。 -R或-rock 使用Rock Ridge Extensions。 -sysid&lt;系统ID&gt; 指定光盘的系统ID。 -T或-translation-table 建立文件名的转换表，适用于不支持Rock Ridge Extensions的系统上。 -v或-verbose 执行时显示详细的信息。 -V&lt;光盘ID&gt;或-volid&lt;光盘ID&gt; 指定光盘的卷册集ID。 -volset-size&lt;光盘总数&gt; 指定卷册集所包含的光盘张数。 -volset-seqno&lt;卷册序号&gt; 指定光盘片在卷册集中的编号。 -x&lt;目录&gt; 指定的目录将不会放入映像文件中。 -z 建立通透性压缩文件的SUSP记录，此记录目前只在Alpha机器上的Linux有效。 mkswap功能说明：设置交换区(swap area)。 语 法：mkswap [-cf][-v0][-v1][设备名称或文件][交换区大小] 补充说明：mkswap可将磁盘分区或文件设为Linux的交换区。 参 数： -c 建立交换区前，先检查是否有损坏的区块。 -f 在SPARC电脑上建立交换区时，要加上此参数。 -v0 建立旧式交换区，此为预设值。 -v1 建立新式交换区。 [交换区大小] 指定交换区的大小，单位为1024字节。 mpartition功能说明：建立或删除MS-DOS的分区。 语 法：mpartition [-acdfIprv][-b &lt;磁区数&gt;][-h &lt;磁头数&gt;][l &lt;磁区数&gt;][-s &lt;磁区数&gt;][-t &lt;柱面数&gt;][驱动器代号] 补充说明：mpartition为mtools工具指令，可建立或删除磁盘分区。 参 数： -a 将分区设置为可开机分区。 -b&lt;磁区数&gt; 建立分区时，指定要从第几个磁区开始建立分区。 -c 建立分区。 -d 将分区设置为无法开机的分区。 -f 强制地修改分区而不管检查时发生的错误信息。 -h&lt;磁头数&gt; 建立分区时，指定分区的磁头数。 -I 删除全部的分区。 -l&lt;磁区数&gt; 建立分区时，指定分区的容量大小，单位为磁区数。 -p 当要重新建立分区时，显示命令列。 -r 删除分区。 -s&lt;磁区数&gt; 建立分区时，指定每个磁轨的磁区数。 -t&lt;柱面数&gt; 建立分区时，指定分区的柱面数。 -v 与-p参数一并使用，若没有同时下达修改分区的命令，则显示目前分区的状态。 sfdisk功能说明：硬盘恩区工具程序。 语 法：sfdisk [-?Tvx][-d &lt;硬盘&gt;][-g &lt;硬盘&gt;][-l &lt;硬盘&gt;][-s &lt;分区&gt;][-V &lt;硬盘&gt;] 补充说明：sfdisk为硬盘分区工具程序，可显示分区的设置信息，并检查分区是否正常。 参 数： -?或–help 显示帮助。 -d&lt;硬盘&gt; 显示硬盘分区的设置。 -g&lt;硬盘&gt;或–show-geometry&lt;硬盘&gt; 显示硬盘的CHS参数。 -l&lt;硬盘&gt; 显示后硬盘分区的相关设置。 -s&lt;分区&gt; 显示分区的大小，单位为区块。 -T或–list-types 显示所有sfdisk能辨识的文件系统ID。 -v或–version 显示版本信息。 -V&lt;硬盘&gt;或–verify&lt;硬盘&gt; 检查硬盘分区是否正常。 -x或–show-extend 显示扩展分区中的逻辑分区。 swapoff功能说明：关闭系统交换区(swap area)。 语 法：swapoff [设备] 补充说明：swapoff实际上为swapon的符号连接，可用来关闭系统的交换区。 swapon功能说明：启动系统交换区(swap area)。 语 法：swapon [-ahsV][-p &lt;优先顺序&gt;][设备] 补充说明：Linux系统的内存管理必须使用交换区来建立虚拟内存。 参 数： -a 将/etc/fstab文件中所有设置为swap的设备，启动为交换区。 -h 显示帮助。 -p&lt;优先顺序&gt; 指定交换区的优先顺序。 -s 显示交换区的使用状况。 -V 显示版本信息。 symlinks(symbolic links)功能说明：维护符号连接的工具程序。 语 法：symlinks [-cdrstv][目录] 补充说明：symlinks可检查目录中的符号连接，并显示符号连接类型。以下为symlinks可判断的符号连接类型： absolute：符号连接使用了绝对路径。 dangling：原始文件已经不存在。 lengthy：符号连接的路径中包含了多余的”../“。 messy：符号连接的路径中包含了多余的”/“。 other_fs：原始文件位于其他文件系统中。 relative：符号连接使用了相对路径。 参 数： -c 将使用绝对路径的符号连接转换为相对路径。 -d 移除dangling类型的符号连接。 -r 检查目录下所有子目录中的符号连接。 -s 检查lengthy类型的符号连接。 -t 与-c一并使用时，会显示如何将绝对路径的符号连接转换为相对路径，但不会实际转换。 -v 显示所有类型的符号连接。 sync功能说明：将内存缓冲区内的数据写入磁盘。 语 法：sync [–help][–version] 补充说明：在Linux系统中，当数据需要存入磁盘时，通常会先放到缓冲区内，等到适当的时刻再写入磁盘，如此可提高系统的执行效率。 参 数： –help 显示帮助。 –version 显示版本信息。 linux网络通讯命令dip功能说明：IP拨号连接。 语 法：dip [-aikltv][-m&lt;MTU数目&gt;][-p&lt;协议&gt;][拨号script文件] 补充说明：dip可控制调制解调器，以拨号IP的方式建立对外的双向连接。 参 数： -a 询问用户名称与密码。 -i 启动拨号服务器功能。 -k 删除执行中的dip程序。 -l 指定要删除的连线，必须配合-k参数一起使用。 -m&lt;MTU数目&gt; 设置最大传输单位，预设值为296。 -p&lt;协议&gt; 设置通信协议。 -t 进入dip的指令模式。 -v 执行时显示详细的信息。 getty(get teletypewriter)功能说明：设置终端机模式，连线速率和管制线路。 语 法：getty [-h][-d&lt;组态配置文件&gt;][-r&lt;延迟秒数&gt;][-t&lt;超时秒数&gt;][-w&lt;等待字符串&gt;][终端机编号][连线速率&lt;终端机类型&gt;&lt;管制线路&gt;] 或 getty [-c&lt;定义配置文件&gt;] 补充说明：getty指令是UNIX之类操作系统启动时所必须的3个步骤之一。 参 数： -c&lt;定义配置文件&gt; 指定定义配置文件，预设为/etc/gettydefs。 -d&lt;组态配置文件&gt; 指定组态配置文件，预设为/etc/conf.getty。 -h 当传输速率为0时就强制断线。 -r&lt;延迟秒数&gt; 设置延迟时间。 -t&lt;超时秒数&gt; 设置等待登入的时间。 -w&lt;等待字符串&gt; 设置等待回应的字符串。 mingetty功能说明：精简版的getty。 语 法：mingetty [–long-hostname][–noclear][tty] 补充说明：mingetty适用于本机上的登入程序。 参 数： –long-hostname 显示完整的主机名称。 –noclear 在询问登入的用户名称之前不要清楚屏幕画面。 ppp-off功能说明：关闭ppp连线。 语 法：ppp-off 补充说明：这是Slackware发行版内附的程序，让用户切断PPP的网络连线。 smbd(samba daemon)功能说明：Samba服务器程序。 语 法：smbd [-aDhoP][-d&lt;排错层级&gt;][-i&lt;范围&gt;][-l&lt;记录文件&gt;][-O&lt;连接槽选项&gt;][-p&lt;连接端口编号&gt;][-s&lt;配置文件&gt;] 补充说明：smbd为Samba服务器程序，可分享文件与打印机等网络资源供Windows相关的用户端程序存取。 参 数： -a 所有的连线记录都会加到记录文件中。 -d&lt;排错层级&gt; 指定记录文件所记载事件的详细程度。 -D 使用此参数时，smbd会以服务程序的方式在后台执行。 -h 显示帮助。 -i&lt;范围&gt; 指定NetBIOS名称的范围。 -l&lt;记录文件&gt; 指定记录文件的名称。 -o 每次启动时，会覆盖原有的记录文件。 -O&lt;连接槽选项&gt; 设置连接槽选项。 -p&lt;连接端口编号&gt; 设置连接端口编号。 -P 仅用来测试smbd程序的正确性。 -s&lt;配置文件&gt; 指定smbd的设置文件。 telnet功能说明：远端登入。 语 法：telnet [-8acdEfFKLrx][-b&lt;主机别名&gt;][-e&lt;脱离字符&gt;][-k&lt;域名&gt;][-l&lt;用户名称&gt;][-n&lt;记录文件&gt;][-S&lt;服务类型&gt;][-X&lt;认证形态&gt;][主机名称或IP地址&lt;通信端口&gt;] 补充说明：执行telnet指令开启终端机阶段作业，并登入远端主机。 参 数： -8 允许使用8位字符资料，包括输入与输出。 -a 尝试自动登入远端系统。 -b&lt;主机别名&gt; 使用别名指定远端主机名称。 -c 不读取用户专属目录里的.telnetrc文件。 -d 启动排错模式。 -e&lt;脱离字符&gt; 设置脱离字符。 -E 滤除脱离字符。 -f 此参数的效果和指定”-F”参数相同。 -F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。 -k&lt;域名&gt; 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。 -K 不自动登入远端主机。 -l&lt;用户名称&gt; 指定要登入远端主机的用户名称。 -L 允许输出8位字符资料。 -n&lt;记录文件&gt; 指定文件记录相关信息。 -r 使用类似rlogin指令的用户界面。 -S&lt;服务类型&gt; 设置telnet连线所需的IP TOS信息。 -x 假设主机有支持数据加密的功能，就使用它。 -X&lt;认证形态&gt; 关闭指定的认证形态。 uulog功能说明：显示UUCP记录文件。 语 法：uulog [-DFISv][-&lt;行数&gt;][-f&lt;主机&gt;][-I&lt;配置文件&gt;][-n&lt;行数&gt;][-s&lt;主机&gt;][-u&lt;用户&gt;][-X&lt;层级&gt;][–help] 补充说明：uulog可用来显示UUCP记录文件中记录。 参 数： -D或–debuglog 显示排错记录。 -f&lt;主机&gt;或–follow&lt;主机&gt; 与-F参数类似，但仅显示与指定主机相关的记录。 -I&lt;配置文件&gt;或–config&lt;配置文件&gt; 指定程序的配置文件。 -&lt;行数&gt;,-n&lt;行数&gt;或–lines&lt;行数&gt; 显示记录文件中，从最后算起指定行数的数值。 -s&lt;主机&gt; 仅显示记录文件中，与指定文件相关的记录。 -S或–statslog 显示统计记录。 -u&lt;用户&gt;或–suer&lt;用户&gt; 仅显示记录文件中，与指定用户相关的记录。 -v或–version 显示版本信息。 -X&lt;层级&gt;或–debug&lt;层级&gt; 设定排错层级。 –help 显示帮助。 uustat功能说明：显示UUCP目前的状况。 语 法：uustat [-aeiKmMNpqQRv][-B&lt;行数&gt;][-c&lt;指令&gt;][-C&lt;指令&gt;][-I&lt;配置文件&gt;][-k&lt;工作&gt;][-o&lt;小时&gt;][-r&lt;工作&gt;][-s&lt;主机&gt;][-S&lt;主机&gt;][-u&lt;用户&gt;][-U&lt;用户&gt;][-W&lt;附注&gt;][-y&lt;小时&gt;][-x&lt;层级&gt;][–help] 补充说明：执行uucp与uux指令后，会先将工作送到队列，再由uucico来执行工作。uustat可显示，删除或启动队列中等待执行的工作。 参 数： -a或-all 显示全部的UUCP工作。 -B&lt;行数&gt;或–mail-lines&lt;行数&gt; 与-M或-N参数一并使用，用来指定邮件中要包含多少行的信息。 -c&lt;指令&gt;或–command&lt;指令&gt; 显示与&lt;指令&gt;有关的工作。 -C&lt;指令&gt;或–not-command&lt;指令&gt; 显示与&lt;指令&gt;无关的工作。 -e或–executions 仅显示待执行的工作。 -i或–prompt 针对队列中的每项工作，询问使用是否要删除工作。 -I&lt;配置文件&gt;或–config&lt;配置文件&gt; 指定配置文件。 -k&lt;工作&gt;或–kill&lt;工作&gt; 删除指定的工作。 -m或–status 删除全部的工作。 -M或-mail 将状态信息邮寄给UUCP管理员。 -N或–notify 将状态信息分别邮寄给提出该项工作的用户。 -o&lt;小时&gt;或–older-than&lt;小时&gt; 显示超过指定时数的工作。 -p或–ps 显示负责UUCP锁定的程序。 -q或–list 显示每台远端主机上所要执行工作的状态。 -Q或–no-list 不显示工作。 -r&lt;工作&gt;或–rejuvenate&lt;工作&gt; 重新启动指定的工作。 -R或–rejuvenate-all 重新启动全部的工作。 -s&lt;主机&gt;或–system&lt;主机&gt; 显示与&lt;主机&gt;有关的工作。 -S&lt;主机&gt;或–not-system&lt;主机&gt; 显示与&lt;主机&gt;无关的工作。 -v或–version 显示版本信息。 -u&lt;用户&gt;或–user&lt;用户&gt; 显示与&lt;用户&gt;有关的工作。 -U&lt;用户&gt;或–not-user&lt;用户&gt; 显示与&lt;用户&gt;无关的工作。 -W&lt;附注&gt;或–comment&lt;附注&gt; 要放在邮件信息中的附注。 -y&lt;小时&gt;或–younger-than&lt;小时&gt; 显示低于指定时数的工作。 -x&lt;层级&gt;或–debug&lt;层级&gt; 指定排错层级。 –help 显示帮助。 uux功能说明：在远端的UUCP主机上执行指令。 语 法：uux [-bcCIjlnrvz][-a&lt;地址&gt;][-g&lt;等级&gt;][-s&lt;文件&gt;][-x&lt;层级&gt;][–help][指令] 补充说明：uux可在远端的UUCP主机上执行指令或是执行本机上的指令，但在执行时会使用远端电脑的文件。 参 数： -或-p或–stdin 直接从键盘读取要执行的指令。 -a&lt;地址&gt;或–requestor&lt;地址&gt; 执行邮件地址，以便寄送状态信息。 -b或–erturn-stdin 在屏幕上显示状态信息。 -c或–nocopy 不用将文件复制到缓冲区。 -C或–copy 将文件复制到缓冲区。 -g&lt;等级&gt;或–grade&lt;等级&gt; 指定文件传送作业的优先顺序。 -I或–config file 指定uux配置文件。 -j或–jobid 显示作业编号。 -l或–link 将本机上的文件连接到缓冲区。 -n或–notification=no 无论发生任何状态，都不寄邮件通知用户。 -r或–nouucico 不要立即启动uucico服务程序，仅将作业送到队列中，然后再执行。 -s&lt;文件&gt;或–status&lt;文件&gt; 将完成状态保存为指定的文件。 -v或–version 显示版本信息。 -x&lt;层级&gt;或–debug&lt;层级&gt; 指定排错层级。 -z或–notification=error 若发生错误，则以邮件来通知用户。 –help 显示帮助。 cu(call up)功能说明：连接另一个系统主机。 语 法：cu [dehnotv][-a&lt;通信端口&gt;][-c&lt;电话号码&gt;][-E&lt;脱离字符&gt;][-I&lt;设置文件&gt;][-l&lt;外围设备代号&gt;][-s&lt;连线速率&gt;][-x&lt;排错模式&gt;][-z&lt;系统主机&gt;][–help][-nostop][–parity=none][&lt;系统主机&gt;/&lt;电话号码&gt;] 补充说明：本指令可连接另一台主机，并采用类似拨号终端机的接口工作，也可执行简易的文件传输作业。 参 数： -a&lt;通信端口&gt;或-p&lt;通信端口&gt;或–port&lt;通信端口&gt; 使用指定的通信端口进行连线。 -c&lt;电话号码&gt;或–phone&lt;电话号码&gt; 拨打该电话号码。 -d 进入排错模式。 -e或–parity=even 使用双同位检查。 -E&lt;脱离字符&gt;或–escape&lt;脱离字符&gt; 设置脱离字符。 -h或–halfduple 使用半双工模式。 -I&lt;配置文件&gt;或–config&lt;配置文件&gt; 指定要使用的配置文件。 -l&lt;外围设备代号&gt;或–line&lt;外围设备代号&gt; 指定某项外围设备，作为连接的设备。 -n或–prompt 拨号时等待用户输入电话号码。 -o或–parity=odd 使用单同位检查。 -s&lt;连线速率&gt;或–speed&lt;连线速率&gt;或–baud&lt;连线速率&gt;或-&lt;连线速率&gt; 设置连线的速率，单位以鲍率计算。 -t或–maper 把CR字符置换成LF+CR字符。 -v或–version 显示版本信息。 -x&lt;排错模式&gt;或–debug&lt;排错模式&gt; 使用排错模式。 -z&lt;系统主机&gt;或–system&lt;系统主机&gt; 连接该系统主机。 –help 在线帮助。 –nostop 关闭Xon/Xoff软件流量控制。 –parity=none 不使用同位检查。 dnsconf(dns configurator)功能说明：设置DNS服务器组态。 语 法：dnsconf [–deldomain&lt;域&gt;][–delsecondary&lt;域&gt;][–newdomain&lt;域&gt;][–set&lt;主机&gt;&lt;IP&gt;][–setcname&lt;CNAME&gt;&lt;主机&gt;][–setmx&lt;域&gt;&lt;主机&gt;][–setns&lt;域&gt;&lt;主机&gt;][–unset&lt;主机&gt;] 补充说明：dnsconf实际上为linuxconf的符号连接，提供图形截面的操作方式，供管理员管理DNS服务器。 参 数： –deldomain&lt;域&gt; 删除域。 –delsecondary&lt;域&gt; 删除次级域。 –newdomain&lt;域&gt; 新增域。 –set&lt;主机&gt;&lt;IP&gt; 新增主机记录。 –setcname&lt;CNAME&gt;&lt;主机&gt; 设置&lt;CNAME&gt;。 –setmx&lt;域&gt;&lt;主机&gt; 指定域的邮件主机。 –setns&lt;域&gt;&lt;主机&gt; 指定域的DNS服务器。 –unset&lt;主机&gt; 删除DNS中某台主机的记录。 efax功能说明：收发传真。 语 法：efax [-sw][-a&lt;AT指令&gt;][-c&lt;调制解调器属性&gt;][-d&lt;驱动程序&gt;][-f&lt;字体文件&gt;][-g&lt;指令&gt;][-h&lt;传真标题字符串&gt;][-i&lt;AT指令&gt;][-j&lt;AT指令&gt;][-k&lt;AT指令&gt;][-l&lt;识别码&gt;][-o&lt;选项&gt;][-q&lt;错误次数&gt;][-r&lt;文件名&gt;][-v&lt;信息类型&gt;][-x&lt;UUCP锁定文件&gt;][-t&lt;电话号码&gt;&lt;传真文件&gt;] 补充说明：支持Class 1与Class 2的调制解调器来收发传真。 参 数： -a&lt;AT指令&gt; 以指定的AT指令来接电话。 -c&lt;调制解调器属性&gt; 设置本机调制解调器的属性。 -d&lt;驱动程序&gt; 指定调制解调器驱动程序。 -f&lt;字体文件&gt; 使用指定的字体文件来建立传真标题。 -g&lt;指令&gt; 若接到的电话为数据，则执行指定的指令。 -h&lt;传真标题字符串&gt; 指定字符串为每页最前端的标题。 -i&lt;AT指令&gt; 在调制解调器进入传真模式前，传送AT指令到调制解调器。 -j&lt;AT指令&gt; 在调制解调器进入传真模式后，传送AT指令到调制解调器。 -k&lt;AT指令&gt; 在调制解调器离开传真模式前，传送AT指令到调制解调器。 -l&lt;识别码&gt; 设置本机调制解调器的识别码。 -o&lt;选项&gt; 使用非标准调制解调器时设置相关选项。 -q&lt;错误次数&gt; 接收传真时，当每页发生错误次数超过指定的数目时，要求对方重发。 -r&lt;文件名&gt; 在接收传真时，将每页分别保存成文件。 -v&lt;信息类型&gt; 选择要印出的信息类型。 -w 不要接听电话，等待OK或CONNECT的信号。 -x&lt;UUCP锁定文件&gt; 使用UUCP格式的锁定文件来锁定调制解调器。 -t&lt;电话号码&gt;&lt;传真文件&gt; 以&lt;电话号码&gt;中的号码来拨号，并将&lt;传真文件&gt;传真出去。 httpd(http daemon)功能说明：Apache HTTP服务器程序。 语 法：httpd [-hlLStvVX][-c&lt;httpd指令&gt;][-C&lt;httpd指令&gt;][-d&lt;服务器根目录&gt;][-D&lt;设定文件参数&gt;][-f&lt;设定文件&gt;] 补充说明：httpd为Apache HTTP服务器程序。直接执行程序可启动服务器的服务。 参 数： -c&lt;httpd指令&gt; 在读取配置文件前，先执行选项中的指令。 -C&lt;httpd指令&gt; 在读取配置文件后，再执行选项中的指令。 -d&lt;服务器根目录&gt; 指定服务器的根目录。 -D&lt;设定文件参数&gt; 指定要传入配置文件的参数。 -f&lt;设定文件&gt; 指定配置文件。 -h 显示帮助。 -l 显示服务器编译时所包含的模块。 -L 显示httpd指令的说明。 -S 显示配置文件中的设定。 -t 测试配置文件的语法是否正确。 -v 显示版本信息。 -V 显示版本信息以及建立环境。 -X 以单一程序的方式来启动服务器。 ifconfig功能说明：显示或设置网络设备。 语 法：ifconfig [网络设备][down up -allmulti -arp -promisc][add&lt;地址&gt;][del&lt;地址&gt;][&lt;hw&lt;网络设备类型&gt;&lt;硬件地址&gt;][io_addr&lt;I/O地址&gt;][irq&lt;IRQ地址&gt;][media&lt;网络媒介类型&gt;][mem_start&lt;内存地址&gt;][metric&lt;数目&gt;][mtu&lt;字节&gt;][netmask&lt;子网掩码&gt;][tunnel&lt;地址&gt;][-broadcast&lt;地址&gt;][-pointopoint&lt;地址&gt;][IP地址] 补充说明：ifconfig可设置网络设备的状态，或是显示目前的设置。 参 数： add&lt;地址&gt; 设置网络设备IPv6的IP地址。 del&lt;地址&gt; 删除网络设备IPv6的IP地址。 down 关闭指定的网络设备。 hw&lt;网络设备类型&gt;&lt;硬件地址&gt; 设置网络设备的类型与硬件地址。 io_addr&lt;I/O地址&gt; 设置网络设备的I/O地址。 irq&lt;IRQ地址&gt; 设置网络设备的IRQ。 media&lt;网络媒介类型&gt; 设置网络设备的媒介类型。 mem_start&lt;内存地址&gt; 设置网络设备在主内存所占用的起始地址。 metric&lt;数目&gt; 指定在计算数据包的转送次数时，所要加上的数目。 mtu&lt;字节&gt; 设置网络设备的MTU。 netmask&lt;子网掩码&gt; 设置网络设备的子网掩码。 tunnel&lt;地址&gt; 建立IPv4与IPv6之间的隧道通信地址。 up 启动指定的网络设备。 -broadcast&lt;地址&gt; 将要送往指定地址的数据包当成广播数据包来处理。 -pointopoint&lt;地址&gt; 与指定地址的网络设备建立直接连线，此模式具有保密功能。 -promisc 关闭或启动指定网络设备的promiscuous模式。 [IP地址] 指定网络设备的IP地址。 [网络设备] 指定网络设备的名称。 mesg功能说明：设置终端机的写入权限。 语 法：mesg [ny] 补充说明：将mesg设置y时，其他用户可利用write指令将信息直接显示在您的屏幕上。 参 数： n 不允许其他用户将信息直接显示在你的屏幕上。 y 允许其他用户将信息直接显示在你的屏幕上。 minicom功能说明：调制解调器通信程序。 语 法：minicom [-8lmMostz][-a&lt;on或0ff&gt;][-c&lt;on或off&gt;][-C&lt;取文件&gt;][-d&lt;编号&gt;][-p&lt;模拟终端机&gt;][-S&lt;script文件&gt;][配置文件] 补充说明：minicom是一个相当受欢迎的PPP拨号连线程序。 参 数： -8 不要修改任何8位编码的字符。 -a&lt;on或0ff&gt; 设置终端机属性。 -c&lt;on或off&gt; 设置彩色模式。 -C&lt;取文件&gt; 指定取文件，并在启动时开启取功能。 -d&lt;编号&gt; 启动或直接拨号。 -l 不会将所有的字符都转成ASCII码。 -m 以Alt或Meta键作为指令键。 -M 与-m参数类似。 -o 不要初始化调制解调器。 -p &lt;模拟终端机&gt; 使用模拟终端机。 -s 开启程序设置画面。 -S&lt;script文件&gt; 在启动时，执行指定的script文件。 -t 设置终端机的类型。 -z 在终端机上显示状态列。 [配置文件] 指定minicom配置文件。 nc功能说明：设置路由器。 语 法：nc [-hlnruz][-g&lt;网关…&gt;][-G&lt;指向器数目&gt;][-i&lt;延迟秒数&gt;][-o&lt;输出文件&gt;][-p&lt;通信端口&gt;][-s&lt;来源位址&gt;][-v…][-w&lt;超时秒数&gt;][主机名称][通信端口…] 补充说明：执行本指令可设置路由器的相关参数。 参 数： -g&lt;网关&gt; 设置路由器跃程通信网关，最丢哦可设置8个。 -G&lt;指向器数目&gt; 设置来源路由指向器，其数值为4的倍数。 -h 在线帮助。 -i&lt;延迟秒数&gt; 设置时间间隔，以便传送信息及扫描通信端口。 -l 使用监听模式，管控传入的资料。 -n 直接使用IP地址，而不通过域名服务器。 -o&lt;输出文件&gt; 指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存。 -p&lt;通信端口&gt; 设置本地主机使用的通信端口。 -r 乱数指定本地与远端主机的通信端口。 -s&lt;来源位址&gt; 设置本地主机送出数据包的IP地址。 -u 使用UDP传输协议。 -v 显示指令执行过程。 -w&lt;超时秒数&gt; 设置等待连线的时间。 -z 使用0输入/输出模式，只在扫描通信端口时使用。 netconf功能说明：设置各项网络功能。 语 法：netconf 补充说明：netconf是Red Hat Linux发行版专门用来调整Linux各项设置的程序。 netconfig功能说明：设置网络环境。 语 法：netconfig 补充说明：这是Slackware发行版内附程序，它具有互动式的问答界面，让用户轻易完成网络环境的设置。 netstat功能说明：显示网络状态。 语 法：netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][–ip] 补充说明：利用netstat指令可让你得知整个Linux系统的网络情况。 参 数： -a或–all 显示所有连线中的Socket。 -A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。 -c或–continuous 持续列出网络状态。 -C或–cache 显示路由器配置的快取信息。 -e或–extend 显示网络其他相关信息。 -F或–fib 显示FIB。 -g或–groups 显示多重广播功能群组组员名单。 -h或–help 在线帮助。 -i或–interfaces 显示网络界面信息表单。 -l或–listening 显示监控中的服务器的Socket。 -M或–masquerade 显示伪装的网络连线。 -n或–numeric 直接使用IP地址，而不通过域名服务器。 -N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。 -o或–timers 显示计时器。 -p或–programs 显示正在使用Socket的程序识别码和程序名称。 -r或–route 显示Routing Table。 -s或–statistice 显示网络工作信息统计表。 -t或–tcp 显示TCP传输协议的连线状况。 -u或–udp 显示UDP传输协议的连线状况。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -w或–raw 显示RAW传输协议的连线状况。 -x或–unix 此参数的效果和指定”-A unix”参数相同。 –ip或–inet 此参数的效果和指定”-A inet”参数相同。 ping功能说明：检测主机。 语 法：ping [-dfnqrRv][-c&lt;完成次数&gt;][-i&lt;间隔秒数&gt;][-I&lt;网络界面&gt;][-l&lt;前置载入&gt;][-p&lt;范本样式&gt;][-s&lt;数据包大小&gt;][-t&lt;存活数值&gt;][主机名称或IP地址] 补充说明：执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 参 数： -d 使用Socket的SO_DEBUG功能。 -c&lt;完成次数&gt; 设置完成要求回应的次数。 -f 极限检测。 -i&lt;间隔秒数&gt; 指定收发信息的间隔时间。 -I&lt;网络界面&gt; 使用指定的网络界面送出数据包。 -l&lt;前置载入&gt; 设置在送出要求信息之前，先行发出的数据包。 -n 只输出数值。 -p&lt;范本样式&gt; 设置填满数据包的范本样式。 -q 不显示指令执行过程，开头和结尾的相关信息除外。 -r 忽略普通的Routing Table，直接将数据包送到远端主机上。 -R 记录路由过程。 -s&lt;数据包大小&gt; 设置数据包的大小。 -t&lt;存活数值&gt; 设置存活数值TTL的大小。 -v 详细显示指令的执行过程。 pppstats(point to point protocol status)功能说明：显示PPP连线状态。 语 法：pppstats [-adrv][-c&lt;执行次数&gt;][-w&lt;间隔秒数&gt;][网络界面] 补充说明：利用pppstats指令可让你得知PPP连接网络的相关信息。 参 数： -a 显示绝对统计值。 -c&lt;执行次数&gt; 设置回报状况的次数。 -d 显示相对统计值。 -r 显示数据包压缩比率的统计值。 -v 显示VJTCP文件头的压缩效率统计值。 -w&lt;间隔秒数&gt; 设置显示统计信息的间隔时间。 samba功能说明：Samba服务器控制。 语 法：samba [start][stop][status][restart] 补充说明：samba为script文件，可启动，停止Samba服务器或回报目前的状态。 参 数： start 启动Samba服务器的服务。 stop 停止Samba服务器的服务。 status 显示Samba服务器目前的状态。 restart 重新启动Samba服务器。 setserial功能说明：设置或显示串口的相关信息。 语 法：setserial [-abgGqvVz][设备][串口参数] 补充说明：setserial可用来设置串口或显示目前的设置。 参 数： -a 显示详细信息。 -b 显示摘要信息。 -g 显示串口的相关信息。 -G 以指令列表的格式来显示信息。 -q 执行时显示较少的信息。 -v 执行时显示较多的信息。 -V 显示版本信息。 -z 设置前，先将所有的标记归零。 shapecfg(shaper configuration)功能说明：管制网络设备的流量。 语 法：shapecfg attach [流量管制器][网络设备] 或 shapecfg speed [流量管制器][带宽] 补充说明：自Linux-2.15开始，便支持流量管制的功能。 参 数： attach 将流量管制器与实际的网络设备结合。 speed 设置流量管制器的对外传输带宽。 smbd(samba daemon)功能说明：Samba服务器程序。 语 法：smbd [-aDhoP][-d&lt;排错层级&gt;][-i&lt;范围&gt;][-l&lt;记录文件&gt;][-O&lt;连接槽选项&gt;][-p&lt;连接端口编号&gt;][-s&lt;配置文件&gt;] 补充说明：smbd为Samba服务器程序，可分享文件与打印机等网络资源供Windows相关的用户端程序存取。 参 数： -a 所有的连线记录都会加到记录文件中。 -d&lt;排错层级&gt; 指定记录文件所记载事件的详细程度。 -D 使用此参数时，smbd会以服务程序的方式在后台执行。 -h 显示帮助。 -i&lt;范围&gt; 指定NetBIOS名称的范围。 -l&lt;记录文件&gt; 指定记录文件的名称。 -o 每次启动时，会覆盖原有的记录文件。 -O&lt;连接槽选项&gt; 设置连接槽选项。 -p&lt;连接端口编号&gt; 设置连接端口编号。 -P 仅用来测试smbd程序的正确性。 -s&lt;配置文件&gt; 指定smbd的设置文件。 statserial(status ofserial port)功能说明：显示串口状态。 语 法：statserial [-dnx][串口设备名称] 补充说明：statserial可显示各个接脚的状态，常用来判断串口是否正常。 参 数： -d 以10进制数字来表示串口的状态。 -n 仅显示一次串口的状态后即结束程序。 -x 与-n参数类似，但是以16进制来表示。 talk功能说明：与其他用户交谈。 语 法：talk [用户名称][终端机编号] 补充说明：通过talk指令，你可以和另一个用户线上交谈。 tcpdump功能说明：倾倒网络传输数据。 语 法：tcpdump [-adeflnNOpqStvx][-c&lt;数据包数目&gt;][-dd][-ddd][-F&lt;表达文件&gt;][-i&lt;网络界面&gt;][-r&lt;数据包文件&gt;][-s&lt;数据包大小&gt;][-tt][-T&lt;数据包类型&gt;][-vv][-w&lt;数据包文件&gt;][输出数据栏位] 补充说明：执行tcpdump指令可列出经过指定网络界面的数据包文件头，在Linux操作系统中，你必须是系统管理员。 参 数： -a 尝试将网络和广播地址转换成名称。 -c&lt;数据包数目&gt; 收到指定的数据包数目后，就停止进行倾倒操作。 -d 把编译过的数据包编码转换成可阅读的格式，并倾倒到标准输出。 -dd 把编译过的数据包编码转换成C语言的格式，并倾倒到标准输出。 -ddd 把编译过的数据包编码转换成十进制数字的格式，并倾倒到标准输出。 -e 在每列倾倒资料上显示连接层级的文件头。 -f 用数字显示网际网络地址。 -F&lt;表达文件&gt; 指定内含表达方式的文件。 -i&lt;网络界面&gt; 使用指定的网络截面送出数据包。 -l 使用标准输出列的缓冲区。 -n 不把主机的网络地址转换成名字。 -N 不列出域名。 -O 不将数据包编码最佳化。 -p 不让网络界面进入混杂模式。 -q 快速输出，仅列出少数的传输协议信息。 -r&lt;数据包文件&gt; 从指定的文件读取数据包数据。 -s&lt;数据包大小&gt; 设置每个数据包的大小。 -S 用绝对而非相对数值列出TCP关联数。 -t 在每列倾倒资料上不显示时间戳记。 -tt 在每列倾倒资料上显示未经格式化的时间戳记。 -T&lt;数据包类型&gt; 强制将表达方式所指定的数据包转译成设置的数据包类型。 -v 详细显示指令执行过程。 -vv 更详细显示指令执行过程。 -x 用十六进制字码列出数据包资料。 -w&lt;数据包文件&gt; 把数据包数据写入指定的文件。 testparm(test parameter)功能说明：测试Samba的设置是否正确无误。 语 法：testparm [-s][配置文件][&lt;主机名称&gt;&lt;IP地址&gt;] 补充说明：执行testparm指令可以简单测试Samba的配置文件，假如测试结果无误，Samba常驻服务就能正确载入该设置值，但并不保证其后的操作如预期般一切正常。 参 数： -s 不显示提示符号等待用户按下Enter键，就直接列出Samba服务定义信息。 traceroute功能说明：显示数据包到主机间的路径。 语 法：traceroute [-dFlnrvx][-f&lt;存活数值&gt;][-g&lt;网关&gt;…][-i&lt;网络界面&gt;][-m&lt;存活数值&gt;][-p&lt;通信端口&gt;][-s&lt;来源地址&gt;][-t&lt;服务类型&gt;][-w&lt;超时秒数&gt;][主机名称或IP地址][数据包大小] 补充说明：traceroute指令让你追踪网络数据包的路由途径，预设数据包大小是40Bytes，用户可另行设置。 参 数： -d 使用Socket层级的排错功能。 -f&lt;存活数值&gt; 设置第一个检测数据包的存活数值TTL的大小。 -F 设置勿离断位。 -g&lt;网关&gt; 设置来源路由网关，最多可设置8个。 -i&lt;网络界面&gt; 使用指定的网络界面送出数据包。 -I 使用ICMP回应取代UDP资料信息。 -m&lt;存活数值&gt; 设置检测数据包的最大存活数值TTL的大小。 -n 直接使用IP地址而非主机名称。 -p&lt;通信端口&gt; 设置UDP传输协议的通信端口。 -r 忽略普通的Routing Table，直接将数据包送到远端主机上。 -s&lt;来源地址&gt; 设置本地主机送出数据包的IP地址。 -t&lt;服务类型&gt; 设置检测数据包的TOS数值。 -v 详细显示指令的执行过程。 -w&lt;超时秒数&gt; 设置等待远端主机回报的时间。 -x 开启或关闭数据包的正确性检验。 tty(teletypewriter)功能说明：显示终端机连接标准输入设备的文件名称。 语 法：tty [-s][–help][–version] 补充说明：在Linux操作系统中，所有外围设备都有其名称与代号，这些名称代号以特殊文件的类型存放于/dev目录下。你可以执行tty指令查询目前使用的终端机的文件名称。 参 数： -s或–silent或–quiet 不显示任何信息，只回传状态代码。 –help 在线帮助。 –version 显示版本信息。 uuname功能说明：显示全部的UUCP远端主机。 语 法：uuname [-alv][-I&lt;配置文件&gt;][–help] 补充说明：uuname可显示UUCP远端主机。 参 数： -a或–aliases 显示别名。 -I&lt;配置文件&gt;或–config&lt;配置文件&gt; 指定程序的配置文件。 -l或–local 显示本机名称。 -v或–version 显示版本信息。 –help 显示帮助。 wall(write all)功能说明：传送信息。 语 法：wall [公告信息] 补充说明：通过wall指令可将信息发送给每位同意接收公众信息的终端机用户，若不给予其信息内容，则wall指令会从标准输入设备读取数据，然后再把所得到的数据传送给所有终端机用户。 write功能说明：传送信息。 语 法：write [用户名称][终端机编号] 补充说明：通过write指令可传递信息给另一位登入系统的用户，当输入完毕后，键入EOF表示信息结束，write指令就会将信息传给对方。如果接收信息的用户不只登入本地主机一次，你可以指定接收信息的终端机编号。 ytalk功能说明：与其他用户交谈。 语 法：ytalk [-isxY][-h&lt;主机名称IP地址&gt;][用户名称…] 补充说明：通过ytalk指令，你可以和其他用户线上交谈，如果想和其他主机的用户交谈，在用户名称后加上其主机名称或IP地址即可。 参 数： -h&lt;主机名称IP地址&gt; 指定交谈对象所在的远端主机。 -i 用提醒声响代替显示信息。 -s 在指令提示符号先开启ytalk交谈窗。 -x 关闭图形界面。 -Y 所有必须回应yes或no的问题，都必须用大写英文字母”Y”或”N”回答。 arpwatch(ARP watcher)功能说明：监听网络上ARP的记录。 语 法：arpwatch [-d][-f&lt;记录文件&gt;][-i&lt;接口&gt;][-r&lt;记录文件&gt;] 补充说明：ARP(Address Resolution Protocol)是用来解析IP与网络装置硬件地址的协议。arpwatch可监听区域网络中的ARP数据包并记录，同时将监听到的变化通过E-mail来报告。 参 数： -d 启动排错模式。 -f&lt;记录文件&gt; 设置存储ARP记录的文件，预设为/var/arpwatch/arp.dat。 -i&lt;接口&gt; 指定监听ARP的接口，预设的接口为eth0。 -r&lt;记录文件&gt; 从指定的文件中读取ARP记录，而不是从网络上监听。 apachectl(Apache control interface)功能说明：可用来控制Apache HTTP服务器的程序。 语 法：apachectl [configtest][fullstatus][graceful][help][restart][start][status][stop] 补充说明：apachectl是slackware内附Apache HTTP服务器的script文件，可供管理员控制服务器，但在其他Linux的Apache HTTP服务器不一定有这个文件。 参 数： configtest 检查设置文件中的语法是否正确。 fullstatus 显示服务器完整的状态信息。 graceful 重新启动Apache服务器，但不会中断原有的连接。 help 显示帮助信息。 restart 重新启动Apache服务器。 start 启动Apache服务器。 status 显示服务器摘要的状态信息。 stop 停止Apache服务器。 smbclient(samba client)功能说明：可存取SMB/CIFS服务器的用户端程序。 语 法：smbclient [网络资源][密码][-EhLN][-B&lt;IP地址&gt;][-d&lt;排错层级&gt;][-i&lt;范围&gt;][-I&lt;IP地址&gt;][-l&lt;记录文件&gt;][-M&lt;NetBIOS名称&gt;][-n&lt;NetBIOS名称&gt;][-O&lt;连接槽选项&gt;][-p&lt;TCP连接端口&gt;][-R&lt;名称解析顺序&gt;][-s&lt;目录&gt;][-t&lt;服务器字码&gt;][-T&lt;tar选项&gt;][-U&lt;用户名称&gt;][-W&lt;工作群组&gt;] 补充说明：SMB与CIFS为服务器通信协议，常用于Windows95/98/NT等系统。smbclient可让Linux系统存取Windows系统所分享的资源。 参 数： [网络资源] [网络资源]的格式为//服务器名称/资源分享名称。 [密码] 输入存取网络资源所需的密码。 -B&lt;IP地址&gt; 传送广播数据包时所用的IP地址。 -d&lt;排错层级&gt; 指定记录文件所记载事件的详细程度。 -E 将信息送到标准错误输出设备。 -h 显示帮助。 -i&lt;范围&gt; 设置NetBIOS名称范围。 -I&lt;IP地址&gt; 指定服务器的IP地址。 -l&lt;记录文件&gt; 指定记录文件的名称。 -L 显示服务器端所分享出来的所有资源。 -M&lt;NetBIOS名称&gt; 可利用WinPopup协议，将信息送给选项中所指定的主机。 -n&lt;NetBIOS名称&gt; 指定用户端所要使用的NetBIOS名称。 -N 不用询问密码。 -O&lt;连接槽选项&gt; 设置用户端TCP连接槽的选项。 -p&lt;TCP连接端口&gt; 指定服务器端TCP连接端口编号。 -R&lt;名称解析顺序&gt; 设置NetBIOS名称解析的顺序。 -s&lt;目录&gt; 指定smb.conf所在的目录。 -t&lt;服务器字码&gt; 设置用何种字符码来解析服务器端的文件名称。 -T&lt;tar选项&gt; 备份服务器端分享的全部文件，并打包成tar格式的文件。 -U&lt;用户名称&gt; 指定用户名称。 -W&lt;工作群组&gt; 指定工作群组名称。 pppsetup功能说明：设置PPP连线。 语 法：pppsetup 补充说明：这是Slackware发行版内附程序，它具有互动式的问答界面，让用户轻易完成PPP的连线设置。 linux电子邮件与新闻组命令archive功能说明：新闻组文件备存程序。 语 法：archive [-fmr][-a&lt;备存目录&gt;][-i&lt;索引文件&gt;][源文件] 补充说明：archive会读取新闻组的文件，然后加以备存。archive备存时，会保存[源文件]中文件的目录阶层。 参 数： -a&lt;备存目录&gt; 指定备存的目录。 -f 将阶层目录名称的阶层关系取消，以句点来取代斜线。 -i&lt;索引文件&gt; 制作文件索引。 -m 将文件复制到目的地，建立文件连接后，以连接取代原始的文件。 -r archive预设的标准错误输出为/var/errlog。-r参数可取消此功能。 [源文件] 在源文件中指定要备份的文件。 ctlinnd(control the internet news daemon)功能说明：设置INN新闻组服务器。 语 法：ctlinnd [-hs][-t&lt;超时秒数&gt;][控制命令&lt;设置数值&gt;…] 补充说明：通过ctlinnd指令，可以对新闻组服务器这项常驻服务下达控制命令，直接设置相关数值。 参 数： -h 在线帮助。 -s 不显示指令执行过程。 -t&lt;超时秒数&gt; 设置等待服务器回报的时间，单位以秒计算，超过此一时间则为超时。 elm功能说明：E-mail管理程序。 语 法：elm [-s&lt;邮件主题&gt;][收信人别名或地址]&lt;[要输入的文件] 或 elm [-hmvz][-f&lt;目录&gt;][-i&lt;文件名&gt;] 补充说明：elm为全屏文字模式的电子邮件管理程序。与mail相比，elm提供较易操作的界面，但仍不及pine来得友善。 参 数： -s&lt;邮件主题&gt; 指定新邮件的邮件主题。 -f&lt;目录&gt; 开启程序时，读取指定的目录。 -h 显示帮助。 -i&lt;文件名&gt; 将文件内容插入送出的邮件中。 -m 进入elm后，不显示指令说明。 -v 显示elm的版本信息。 -z 若收件信箱没有邮件，则不启动elm程序。 getlist功能说明：下载新闻组清单。 语 法：getlist [-h&lt;新闻组服务器&gt;][-p&lt;通信端口&gt;][群组类型&lt;范本样式&gt;&lt;群组名称&gt;] 补充说明：getlist指令会从新闻服务器下载新闻组清单，并把清单输出到标准输出设备。 参 数： -h&lt;新闻组服务器&gt; 指定欲连接的新闻组服务器。 -p&lt;通信端口&gt; 设置连接新闻组服务器的通讯端口。 inncheck(inn check)功能说明：检查inn相关配置文件的语法是否正确。 语 法：inncheck [-afv][-pedantic][-perm][-noperm][文件] 补充说明：inncheck会检查inn配置文件的语法是否正确，但不会实际修改这些配置文件。 参 数： -a 不管是否出现[文件]参数，仍然检查全部的配置文件。 -f 若发现配置文件有误，则列出可修正错误的指令。 -v 执行时显示详细的信息。 -pedantic 以较为严格的方式来检查。 -perm 检查文件权限是否有误。 –noperm 不检查文件的权限设置。 [文件] 若不指定此参数，则inncheck会检查所有预设的文件。 mail功能说明：E-mail管理程序。 语 法：mail [-iInNv][-b&lt;地址&gt;][-c&lt;地址&gt;][-f&lt;邮件文件&gt;][-s&lt;邮件主题&gt;][-u&lt;用户帐号&gt;][收信人地址] 补充说明：mail是一个文字模式的邮件管理程序，操作的界面不像elm或pine那么容易使用，但功能尚称完整。 参 数： -b&lt;地址&gt; 指定密件副本的收信人地址。 -c&lt;地址&gt; 指定副本的收信人地址。 -f&lt;邮件文件&gt; 读取指定邮件文件中的邮件。 -i 不显示终端发出的信息。 -I 使用互动模式。 -n 程序使用时，不使用mail.rc文件中的设置。 -N 阅读邮件时，不显示邮件的标题。 -s&lt;邮件主题&gt; 指定邮件的主题。 -u&lt;用户帐号&gt; 读取指定用户的邮件。 -v 执行时，显示详细的信息。 mailconf功能说明：Sendmail设置程序。 语 法：mailconf [–addvdom&lt;虚拟网络&gt;][–delvdom&lt;虚拟网络&gt;][–help] 补充说明：mailconf实际为linuxconf的符号连接，用来设置Sendmail相关的选项。执行mailconf时，不管目前的环境为指令列模式或是X Window，都会出现图形界面的设置画面，让您轻松地完成设置。所有指令列的参数都可以在图形界面中设置。建议不用加上任何参数，直接进入mailconf的图形界面来设置。 mailq(mail queue)功能说明：显示待寄邮件的清单。 语 法：mailq [-q] 补充说明：mailq可列出待寄邮件的清单，包括邮件ID，邮件大小，邮件保存时间，寄信人，收信人，以及邮件无法寄出的原因，提供管理员参考的信息。mailq实际上是执行sendmail -bp指令。 参 数： -v 显示较详细的信息。 messages功能说明：查看信箱中的邮件数。 语 法：messages [邮件目录或文件] 补充说明：messages实际上是一个script文件，可用来检查信箱中的邮件数。若不指定[邮件目录或文件]参数，则显示目前收件信箱中的邮件数。 metamail功能说明：负责处理非文字E-mail的程序。 语 法：metafile [-bBdehpPqrRTwxyz][-c&lt;邮件内容类型&gt;][-f&lt;寄信人地址&gt;][-m&lt;邮件管理程序名称&gt;][-s&lt;邮件主题&gt;][文件名] 补充说明：当E-mail管理程序遇到非ISO-8859-1标准文字的邮件内容，而不知道要如何处理的时候，即调用metamail。 参 数： -b 邮件只有内文，没有表头。 -B 在背景显示信息。 -c&lt;邮件内容类型&gt; 不要读取邮件表头的”Content-type”栏位，而去读取&lt;邮件内容类型&gt;中的设置，以决定要调用哪个程序来处理邮件信息。 -d metamail在调用其他程序显示信息前，不需向用户确认。 -e 删除内文前端的空白行。 -f&lt;寄信人地址&gt; 不要读取邮件表头中的寄信人地址，而去读取选项中的设置。 -h 使用metamail的邮件管理程序名称。 -m&lt;邮件管理程序名称&gt; 记录调用metamail的邮件管理程序名称。 -p 将显示内容送给more指令处理之后，再送到输出设备。 -P 与-p参数功能类似，但会在信息结束显示”Press RETURN to go on”字符串。 -q metamail在调用其他程序来显示信息前，不用在屏幕上显示相关的信息。 -r metamail预设无法让root用户执行。 -R 显示信息前先清除屏幕。 -s&lt;邮件主题&gt; 不要读取邮件表头中的邮件主题，而去读取选项中的设置。 -T 若要在一个新的终端机模拟视窗中重新启动metamail，则metamail必须以递归模式来调用程序本身，这时便需要使用-T参数。 -w 不要读取mailcap中的设置，而直接处理邮件，并将数据存入文件中。 -x 不要以终端机模式来执行，即不使用互动模式。 -y 抽出邮件内文中MIME格式的部分。 -z 程序结束时将输入的文件删除。 mutt功能说明：E-mail管理程序。 语 法：mutt [-hnpRvxz][-a&lt;文件&gt;][-b&lt;地址&gt;][-c&lt;地址&gt;][-f&lt;邮件文件&gt;][-F&lt;配置文件&gt;][-H&lt;邮件草稿&gt;][-i&lt;文件&gt;][-m&lt;类型&gt;][-s&lt;主题&gt;][邮件地址] 补充说明：mutt是一个文字模式的邮件管理程序，提供了全屏幕的操作界面。 参 数： -a&lt;文件&gt; 在邮件中加上附加文件。 -b&lt;地址&gt; 指定密件副本的收信人地址。 -c&lt;地址&gt; 指定副本的收信人地址。 -f&lt;邮件文件&gt; 指定要载入的邮件文件。 -F&lt;配置文件&gt; 指定mutt程序的设置文件，而不读取预设的.muttrc文件。 -h 显示帮助。 -H&lt;邮件草稿&gt; 将指定的邮件草稿送出。 -i&lt;文件&gt; 将指定文件插入邮件内文中。 -m&lt;类型&gt; 指定预设的邮件信箱类型。 -n 不要去读取程序培植文件(/etc/Muttrc)。 -p 在mutt中编辑完邮件后，而不想将邮件立即送出，可将该邮件暂缓寄出。 -R 以只读的方式开启邮件文件。 -s&lt;主题&gt; 指定邮件的主题。 -v 显示mutt的版本信息以及当初编译此文件时所给予的参数。 -x 模拟mailx的编辑方式。 -z 与-f参数一并使用时，若邮件文件中没有邮件即不启动mutt。 nntpget功能说明：下载新闻组文章。 语 法：nntpget [-ov][-d&lt;组类型清单&gt;][-f&lt;文件或目录&gt;][-n&lt;组名称清单&gt;][-t&lt;时间日期&gt;][-u&lt;文件或目录&gt;][新闻组服务器] 补充说明：执行nntpget指令将从指定的新闻组服务器下载文章。nntpget指令会从标准输入设备读取信息识别码，而下载的新闻文章则呈现到标准输出设备。 参 数： -d&lt;组类型清单&gt; 指定新闻组类型为distribution的清单。 -f&lt;文件或目录&gt; 下载其更改时间较指定文件或目录的更改时间更接近现在的新闻文章。 -n&lt;组名称清单&gt; 指定新闻组名称的清单。 -o 下载本地主机所没有的新闻文章。 -t&lt;时间日期&gt; 指定时间与日期，较该日期时间更新的新闻文章都会被下载。 -u&lt;文件或目录&gt; 此参数的效果和指定”-f”参数类似，但它在成功下载文章之后会去更改指定文件或木的更改时间，便于下次再作为参考。 -v 当指令执行时，把每篇文章的信息识别码都送到标准输出。本参数需配合参数”-o”使用。 pine功能说明：收发电子邮件，浏览新闻组。 语 法：pine [-ahikorz][-attach&lt;附件&gt;][-attach_and_delete&lt;附件&gt;][-attachlist&lt;附件清单&gt;][-c&lt;邮件编号&gt;][-conf][-create_lu&lt;地址薄&gt;&lt;排序法&gt;][-f&lt;收件箱&gt;][-F&lt;文件&gt;][-I&lt;暂存快捷键&gt;][-n&lt;邮件编号&gt;][-nr][-p&lt;环境文件&gt;][-pinerc&lt;输出文件&gt;][-P&lt;配置文件&gt;][-sort&lt;排序法&gt;&lt;/reverse&gt;][-url&lt;URL&gt;][-&lt;功能选项&gt;=&lt;设置值&gt;][用户名称或电子邮件地址…] 补充说明：pine是个以显示导向为住的邮件处理程序，包括一般电子邮件和新闻组的文章。pine也支持MIME格式，可让用户将MIME的对象保存成文件，甚至指定外挂程序直接开启对象。 参 数： -a 在UWIN中使用匿名模式。 -attach&lt;附件&gt; 把指定的文件附加在信件里，一并夹带寄出。 -attach_and_delete 此参数的效果和指定”-attach”参数类似，但在信件寄出后&lt;附件&gt;会将指定的文件删除。 -attachlist&lt;附件清单&gt; 设置一份文件清单，把该清单内的文件全部附加到信件中，夹带寄出。 -c&lt;邮件编号&gt; 直接开启指定编号的邮件。 -conf 产生pine.conf的范例文件，并输出到标准输出设备。 -create_lu&lt;地址薄&gt;&lt;排序法&gt; 将地址薄以指定的排序法排序，产生.addressbook.lu索引文件。 -f&lt;收件箱&gt; 开启指定的接收信件箱，而非预设的INBOX收件箱。 -F&lt;文件&gt; 直接浏览指定文件的内容。 -h 在线帮助。 -i 直接进入收件箱。 -I&lt;暂存快捷键&gt; 设置进入pine就要执行的命令，该命令以快捷键的按键指定。 -k 支持键盘上的F1,F2…等功能，而非使用一般快捷键。 -n&lt;邮件编号&gt; 设置现在的邮件编号。 -nr 使用UWIN的特殊模式。 -o 设置收件箱为只读。 -p&lt;环境文件&gt; 另外指定pine的环境文件。 -pinerc&lt;输出文件&gt; 读取.pinerc环境文件的内容并输出成文件保存起来，您必须指定输出文件的名称。 -r 展示模式。 -sort&lt;排序法&gt; 把收件箱内的信件依指定的方式排序。 -url&lt;URL&gt; 直接开启给予的URL。 -z 让pine可被Ctrl+z中断，暂存在后台作业里。 -&lt;功能选项&gt;=&lt;设置值&gt; 暂时指定各项功能的设置值。 slrn功能说明：新闻组阅读程序。 语 法：slrn [-Cdnp][-create][-help][-f&lt;新闻组文件&gt;][-h&lt;服务器&gt;][–version] 补充说明：slrn是一个很容易使用的新闻组阅读程序，提供全屏幕的图形操作界面，让用户能轻松地阅读与管理新闻组。 参 数： -C 使用彩色模式。 -d 下载组清单时，也一并下载每个组的说明。 -f&lt;新闻组文件&gt; 指定要存储组内容的文件，预设的文件名为.newsrc。 -h&lt;服务器&gt; 指定新闻组服务器。 -n 不要检查是否有新的新闻组。 -p 设置新闻组服务器的连接端口编号，预设为119。 -create 第一次连上某台新闻组服务器时，必须使用此参数，以便下载服务器中的组清单，并保存在指定的newsrc文件中。 -help 显示帮助。 –version 显示版本信息。 linux其他命令reconfig功能说明：转换配置文件。 语 法：reconfig [Xconfig] XF86Config 补充说明：reconfig指令能将XFree86 3.1及以前的版本的配置文件，转成新的格式。现今的配置文件XF86Config，存放在/etc或/etc/X11目录下，它的格式包含了更多的信息，因此在转换之后，您还须自行编辑设置文件，输入不足的部分。 startx(start X Window)功能说明：启动X Window。 语 法：startx [程序][–服务器设置] 补充说明：startx为启动X Window的script文件，实际上启动X Window的程序为xinit。 参 数： [程序] 此处所指定的[程序]为X Window所要执行程序的配置文件。 [–服务器设置] startx预设会去读取.xerverrc中的配置，决定启动X Window的方式。 xconfigurator功能说明：设置XFree86。 语 法：Xconfigurator [–card&lt;显卡型号&gt;][–expert][–help][–hsync”&lt;水平扫描频率&gt;”][-kickstart][–monitor&lt;显示器型号&gt;][–server&lt;显示服务器&gt;][–vsync”&lt;垂直扫描频率&gt;”] 补充说明：这是Red Hat公司遵循GPL规则所开发的程序，它具有互动式操作界面，您可以轻易地利用方向键和空格键等，设置显卡和显示器，并选择欲使用的画面分辨率与色彩深度，最后保存配置文件XF86Config。 参 数： –card&lt;显卡型号&gt; 设置显卡的型号，可用”–help”参数查询型号列表。 –expert 启用专家模式，增加用户得以自行选择的项目而非由程序完全自动检测。 –help 在线帮助。 –hsync”&lt;水平扫描频率&gt;” 设置水平扫描频率。 –kickstart 启用全自动模式。 –monitor&lt;显示器型号&gt; 设置显示器型号，可用”–help”参数查询型号列表。 –server&lt;显示服务器&gt; 设置显示服务器。 –vsync”&lt;垂直扫描频率&gt;” 设置垂直扫描频率。 XF86Setup功能说明：设置XFee86。 语 法：XF86Setup [-display&lt;主机名称或IP地址&gt;:&lt;显示器编号&gt;][-nodialog][-sync] 补充说明：这是Linux系统用来设置XFee86的程序，它会进入图形模式，通过互动操作界面，让用户轻松完成XFee86环境的设置。 参 数： -display&lt;主机名称或IP地址&gt;:&lt;显示器编号&gt; 指定显示XF86Setup设置画面的主机名称以及欲在该主机上的哪个显示器显示画面。 -nodialog 执行XF86Setup时，不显示交谈窗，直接以文字模式提出询问。 –sync 和X Server进行数据同步传输。 xlsatoms功能说明：列出X Server定义的成分。 语 法：xlsatoms [-display&lt;显示器编号&gt;][-format&lt;输出格式&gt;][-name&lt;成分名称&gt;][-range&lt;列表范围&gt;] 补充说明：执行xlsatoms指令会列出X Server内部所有定义的成分，每个成分都有其编号，您可利用参数设置列表范围，或直接指定欲查询的成分名称。 参 数： -display&lt;显示器编号&gt; 指定X Server连接的显示器编号，该编号由”0”开始计算， 依序递增。 -format&lt;输出格式&gt; 设置成分清单的列表格式，您可使用控制字符改变显示样式。 -name&lt;成分名称&gt; 列出指定的成分。 -range&lt;列表范围&gt; 设置成分清单的列表范围。 xlsclients功能说明：列出显示器中的客户端应用程序。 语 法：xlsclients [-al][-display&lt;显示器编号&gt;][-m&lt;最大指令长度&gt;] 补充说明：执行xlsclients指令可列出某个显示器中，正在执行的客户端应用程序信息。 参 数： -a 列出所有显示器的客户端应用程序信息。 -display&lt;显示器编号&gt; 指定X Server连接的显示器编号，该编号由”0”开始计算，依序递增。 -l 使用详细格式列表。 -m&lt;最大指令长度&gt; 设置显示指令信息的最大长度，单位以字符计算。 xlsfonts功能说明：列出X Server使用的字体。 语 法：xlsfonts [-1Clmou][-display&lt;主机名称或IP地址&gt;:&lt;显示器编号&gt;][-fn&lt;范本样式&gt;][-ll][-lll][-n&lt;显示栏位数&gt;][-w&lt;每列字符数&gt;] 补充说明：执行xlsfonts指令会显示目前X Server可使用的字体，也能使用范本样式仅列出的符合条件的字体。 参 数： -1 仅用单一栏位列出字体清单。 -C 使用多个栏位列出字体清单。 -display&lt;主机名称或IP地址&gt;:&lt;显示器编号&gt; 指定显示XF86Setup设置画面的主机名称以及欲放在该主机机上的哪个显示器画面。 -fn&lt;范本样式&gt; 只列出符合范本样式指定的条件的字体。 -l 除字体名称外，同时列出字体的属性。 -ll 此参数的效果和指定”l”参数类似，但显示更详细的信息。 -lll 此参数的效果和指定”ll”参数类似，但显示更详细的信息。 -m 配合参数”-l”使用时，一并列出字体大小的上下限。 -n&lt;显示栏位数&gt; 设置每列显示的栏位数。 -o 以OpenFont的形式列出字体清单。 -u 列出字体清单时不依照其名称排序。 -w&lt;每列字符数&gt; 设置每列的最大字符数。 yes1.命令说明 输出回应的字符串。yes回应“y”字符，后者指定字符。 2.路径 /usr/bin/yes 3.语法 :yes [–help] [–version] string 4.参数 –help 显示帮助字符。 –version 显示版本信息 5.相关命令 无 6.举例说明 1234567[root@bixuan/]#yes "OK" //回应“OK”字符串。命令执行结果如下：OKOKOK ...Ctrl+c //按Ctrl+c键中断]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次MySQL历史数据初始化的问题]]></title>
    <url>%2Fblog%2F%E8%AE%B0%E4%B8%80%E6%AC%A1MySQL%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言先说下背景吧，在某项目迭代过程中，由于系统设计问题，需要对b表新增加两个字段（证件类型type和证件号码number），这本来是一件很平常的事情。 但是对于b表的历史数据，需要对其进行初始化维护，也就是历史数据要赋值，这理论上也是一件简单的事情。 证件信息数据要从a表获取，但是a表中对于一个用户（userId）可以有多种类型证件，且证件可以被修改。 对于b表，只需要一种证件，它遵循一个规则，即证件类型有个优先级（d、b、e、c、a），取优先级最高的一张，如果证件被改动过（update_date），则从改动的证件里按照上面优先级找到一张。 看起来很懵逼，我们来分析下。 正文我们先来看下a，b表结构。 对于b表，我略去部分结构，b表主要简化如下，其中type和number是我们需求新增的字段，对于历史数据，这两个字段需要维护。 1234567CREATE TABLE `b` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增主键', `user_id` varchar(32) DEFAULT NULL COMMENT '用户ID', `type` varchar(10) DEFAULT '' COMMENT '证件类型', `number` varchar(50) DEFAULT NULL COMMENT '证件号', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='b表'; A表的简化如下： 123456789CREATE TABLE `a` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '自增主键', `user_id` varchar(32) DEFAULT NULL COMMENT '用户ID', `type` varchar(10) DEFAULT '' COMMENT '证件类型', `number` varchar(50) DEFAULT NULL COMMENT '证件号', `create_date` datetime DEFAULT NULL COMMENT '创建时间', `update_date` datetime DEFAULT NULL COMMENT '更新时间', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='a表'; 我们假设a、b表有如下数据： b表12345678910INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('1', '111111', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('2', '222222', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('3', '333333', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('4', '444444', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('5', '555555', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('6', '666666', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('7', '777777', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('8', '888888', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('9', '999999', '', NULL);INSERT INTO `b` (`id`, `user_id`, `type`, `number`) VALUES ('10', '111111', '', NULL); a表123456789101112131415161718INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('1', '111111', 'a', '123', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('10', '111111', 'c', '1234', '2019-09-10 14:02:46', '2019-09-11 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('2', '111111', 'd', '32122', '2019-09-10 14:02:46', '2019-09-11 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('12', '222222', 'd', '32111', '2019-09-10 14:02:46', '2019-09-12 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('3', '333333', 'b', '12131', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('13', '333333', 'd', '12133', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('4', '444444', 'b', '3434', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('14', '444444', 'e', '34341', '2019-09-10 14:02:46', '2019-09-13 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('5', '555555', 'c', '353452', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('15', '555555', 'a', '3534599', '2019-09-10 14:02:46', '2019-09-11 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('6', '666666', 'c', '3654688', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('16', '666666', 'a', '365461', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('7', '777777', 'd', '4645655', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('17', '777777', 'b', '4645653', '2019-09-10 14:02:46', '2019-09-11 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('8', '888888', 'd', '4444', '2019-09-10 14:02:46', '2019-09-11 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('18', '999999', 'c', '44447', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('9', '999999', 'd', '234243', '2019-09-10 14:02:46', '2019-09-10 14:02:46');INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('19', '999999', 'e', '234248', '2019-09-10 14:02:46', '2019-09-10 14:02:46'); 他们的表数据如下： 我们根据前言里提到的规则，b表只接受该用户一张证件，优先级为（d、b、e、c、a），但是如果证件被改动过的话，就在改动列表里继续按照优先级寻找。 所以对于上面的user_id = 444444的用户，我们应该放入e证件，因为它被改动过；对于 user_id = 111111的用户，可以看到c和d都被改动过，因此从c和d里选，而不考虑a类型，d的优先级高，故最后放入d证件。 虽然程序里很好解决，但是历史数据初始化需要使用SQL处理，我们来看下。 首先要更新b表数据，这些数据应该从a表（或者a表最后经过一系列查询形成的临时表）获取并赋值，它的SQL语句部分应大致如下： 123456UPDATE bINNER JOIN ( SELECT XXXXX from a) AS temp ON b.user_id = temp.user_idSET b.type = temp.type, b.number = temp.number; 这个SQL比较好理解，temp临时表部分表示我们在a里查询出一个user_id、type、number一组数据，在通过user_id相等对b表的type和number进行赋值。 接下来就是关于如何获取生成的临时表这部分了。 我们先寻找改动过的，在里面对于每个用户找到他的最高优先级的证件。 可以看到这是在MySQL中使用分组排序获取最高优先级的数据。 SQL如下： 12345678910111213141516171819202122232425262728293031SELECT *FROM ( SELECT user_id, type, number FROM a WHERE create_date &lt;&gt; update_date GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS tGROUP BY t.user_id 这儿我们看到，我们这里GROUP BY了user_id 和 type，因此t表返回的每个user_id的第一条即为需要的数据，这时候对t表根据user_id进行GROUP BY，进而获取到了每个的第一条数据。 我们在获取没有改动过的，这儿要注意要除去改动过的用户证件，因为它已经包含在上面的SQL里了。 123456789101112131415161718192021222324252627282930313233343536373839SELECT *FROM ( SELECT user_id, type, number FROM a WHERE create_date = update_date AND user_id NOT IN ( SELECT user_id FROM a WHERE create_date &lt;&gt; update_date ) GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS tGROUP BY t.user_id 然后我们使用UNION ALL 将两部分合并。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071(SELECT *FROM ( SELECT user_id, type, number FROM a WHERE create_date &lt;&gt; update_date GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS tGROUP BY t.user_id) UNION ALL(SELECT *FROM ( SELECT user_id, type, number FROM a WHERE create_date = update_date AND user_id NOT IN ( SELECT user_id FROM a WHERE create_date &lt;&gt; update_date ) GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS tGROUP BY t.user_id) 然后我们最后得到的SQL如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980UPDATE bINNER JOIN ( ( SELECT * FROM ( SELECT user_id, type, number FROM a WHERE create_date &lt;&gt; update_date GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS t GROUP BY t.user_id ) UNION ALL ( SELECT * FROM ( SELECT user_id, type, number FROM a WHERE create_date = update_date AND user_id NOT IN ( SELECT user_id FROM a WHERE create_date &lt;&gt; update_date ) GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS t GROUP BY t.user_id )) AS temp ON b.user_id = temp.user_idSET b.type = temp.type, b.number = temp.number; 我们也可以将上面数据放入临时表中，再通过临时表插入b表，如下： 创建临时表help_temp： 12345CREATE TEMPORARY TABLE help_temp ( user_id VARCHAR (32) NOT NULL, type VARCHAR (10) NOT NULL, number VARCHAR (100) NOT NULL); 插入临时表数据： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980INSERT INTO help_temp (user_id, type, number) SELECT *FROM ( ( SELECT * FROM ( SELECT user_id, type, number FROM a WHERE create_date &lt;&gt; update_date GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS t GROUP BY t.user_id ) UNION ALL ( SELECT * FROM ( SELECT user_id, type, number FROM a WHERE create_date = update_date AND user_id NOT IN ( SELECT user_id FROM a WHERE create_date &lt;&gt; update_date ) GROUP BY user_id, ( CASE type WHEN 'd' THEN 1 WHEN 'b' THEN 2 WHEN 'e' THEN 3 WHEN 'c' THEN 4 WHEN 'a' THEN 5 END ) ) AS t GROUP BY t.user_id ) ) AS temp; 将临时表数据放入到b表中： 1234UPDATE bINNER JOIN help_temp ON b.user_id = help_temp.user_idSET b.type = help_temp.type, b.number = help_temp.number; 删除临时表： 1DROP TEMPORARY TABLE help_temp; 总结我们上述使用了CASE…WHEN语句，它可以跟在ORDER BY 或者 GROUP BY语句后面，可以让数据按照指定格式排序； 我们通过GROUP BY user_id 和 type得到一次分组结构，这分组里面的第一条即是我们要求的数据，其实这样GROUP BY返回的数据也是通过对user_id分组，对type排序的数据集；我们拿到user_id这一层分组的第一条即可，故得到的数据再对user_id进行GROUP BY即可。 使用UNION ALL合并两部分数据，UNION ALL是合并全部数据，如果只使用UNION，合并时相同的数据只会保留一条。 要将一张表里的数据UPDATE到另一张表里，我们这里使用了INNER JOIN ON语句。 最后提到了临时表，当我们处理过程较复杂时，可以加入临时表进行辅助处理数据，处理完后连接断开后临时表自动删除，当然也可以主动去删除临时表。 引申我们上面所说的分组排序，相当于GROUP BY了两个字段，对于type来说，只能为不同的值，如果user_id,type相同，则两条数据会被分到同一组。 如下，a表我们新增一条数据。 1INSERT INTO `a` (`id`, `user_id`, `type`, `number`, `create_date`, `update_date`) VALUES ('20', '111111', 'a', '1234', '2019-09-10 14:02:46', '2019-09-10 14:02:46'); 可以看到此时a表数据如下： 对于user_id = 111111的用户，他有两个相同的type = a的证件，这时候我们GROUP BY user_id,type，得到的数据这两条肯定会只取一条。 我们如果想分组排序这样肯定是不对的，因为缺失了一条数据，怎么处理呢？ 一种非常简单的处理方式是再引入一个字段GROUP BY，比如证件号number，如下： 可以看到这时候数据就完全展示了。 会不会没有辅助字段可以引入呢？ 实际中，只要有主键id存在，我们理论上无论如何也是可以找到辅助GROUP BY的字段，如上面我们使用主键id，也是可以得到user_id和type的分组排序结果的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（九）- Java源码中的DualPivotQuicksort]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E4%B9%9D%EF%BC%89-Java%E6%BA%90%E7%A0%81%E4%B8%AD%E7%9A%84DualPivotQuicksort.html</url>
    <content type="text"><![CDATA[前言上篇文章中我们了解了双轴快排和三路快排的原理及实现，排序算法（七） - 双轴快速排序和排序算法（八） - 三路快速排序，了解了虽然快速排序有着较高的效率，但是在数据坏的情况下排序速度非常不乐观。 而后提到了Java源码中的DualPivotQuicksort，它是一种混合排序算法。 而双轴快排和三路快排正是Java源码中的DualPivotQuicksort的一部分，我们来看下它吧。 正文算法原理这个排序算法是Vladimir Yaroslavskiy、Jon Bentley、Josh Bloch在2011年完成的，最早见于JDK1.7版本。 它是一种混合排序算法，其内部主要包括插入排序、三路快排、双轴快排、计数排序、归并排序这几种排序算法，将几种排序算法的优点发挥出来。 在了解排序原理之前，先了解几个固定值，这几个固定值是根据大量实验确定的数据。 MAX_RUN_COUNT = 67 这个值，可以用来指使用归并排序的最大RUN数量，也可以指校验数据是否可以使用归并排序的一个标准。 比如对于int型数据int[] array，DualPivotQuicksort在开始时会从数组头部开始，寻找连续有序数据段，寻找到一段，就把末尾下标记录进一个run，如果该段逆序，就把它倒过来，比如[1,2,8,5,3,2,7,1,9]，它的有序段分隔为[1,2,8][5,3,2][7,1][9]，其中[5,3,2]和[7,1]在处理过程中会倒过来，这样如果当数据量大时，明显run数量会超过67，如果超了，就直接使用优化的快排处理了。 PS：上面的例子我们可以看到最后的run为 run[0] = 2 = index0 -index 2= [1,2,8] ; run[1] = 5 = index3 - index5 = [2,3,5] ; run[2] = 7 = index6 -index7= [1,7] ; run[3] = 8 = index8 = [9] ; 其实run[2]和run[3]是可以合并的，这样可以节省run数量。合并条件就是后一个run的第一个元素要大于等于前一个run的最后一个元素。JDK1.8并未对这块做处理，JDK9及以上版本这块有了优化： 123if (run[count] &gt; left &amp;&amp; a[run[count]] &gt;= a[run[count] - 1]) &#123; count--; &#125; 会处理这种情况。 QUICKSORT_THRESHOLD = 286 这个值表示快排阈值，如果数据量不超过286，就直接使用优化的快排处理。 INSERTION_SORT_THRESHOLD = 47 这个值表示插入排序阈值，对于长度小于等于这个数的数据，就使用插入排序（或变体）处理。 COUNTING_SORT_THRESHOLD_FOR_BYTE = 29 对于byte类型数据，如果数据长度小于等于这个值，直接使用插入排序处理。（超过会使用计数排序处理） COUNTING_SORT_THRESHOLD_FOR_SHORT_OR_CHAR = 3200 对于short或者char类型数据，如果长度超过3200阈值，就会使用计数排序处理。 NUM_SHORT_VALUES = 1 &lt;&lt; 16 = 65536 对于short类型数据，使用计数排序时，初始化桶的数量。（-32768 &lt;= short &lt;= 32767） NUM_CHAR_VALUES = 1 &lt;&lt; 16 = 65536 对于char类型数据，使用计数排序时，初始化桶的数量。（字符集范围Unicode 0-65535） NUM_BYTE_VALUES = 1 &lt;&lt; 8 = 256 对于byte类型数据，使用计数排序，初始化桶的数量。（-128 &lt;= byte &lt;= 127） 以上前五个都是大量实验确定的值。 DualPivotQuicksort的算法原理可以大致描述如下： int类型数据 对于待排序数组int[] array，如果数据长度小于 QUICKSORT_THRESHOLD = 286，直接使用优化的快速排序sort； 否则校验数据，构建一个 MAX_RUN_COUNT+1 = 68 的数组，用来存储run，也用来校验是否可以使用归并排序或者使用优化的快排sort； 对于待排序数组，依次寻找连续有序数组段，将末尾index记录进run，由于会出现相等的数据值： JDK1.8的处理方式是统计连续相等值的数量，超过 MAX_RUN_LENGTH=33 时会直接使用优化后的快速排序sort； JDK9及以上版本的处理方式为将这些相等值统计进了上一个run里。逆序后相邻两个run可以保持升序的也会被合并。 如上面例子：[1,2,8,5,3,2,7,1,9] 会分成[1,2,8][5,3,2][7,1][9]，倒序的逆序得到[1,2,8][2,3,5][1,7][9] 这四个run，JDK9及以上版本[1,7][9]这两个run会合并为一个[1,7,9]，得益于这个校验。 123if (run[count] &gt; left &amp;&amp; a[run[count]] &gt;= a[run[count] - 1]) &#123; count--;&#125; 如果统计过程中run数量超过MAX_RUN_COUNT（限制条件++count == MAX_RUN_COUNT），则说明数据不是高度结构化的（是杂乱无章的），就会使用优化后的快排进行处理sort；如果是高度结构化的（部分有序的），会使用归并排序进行处理。 再来说下优化后的快速排序sort： 如果输入数据长度小于 INSERTION_SORT_THRESHOLD = 47 ，则直接使用插入排序（或变种）进行处理。 具体使用普通的插入排序还是变种的插入排序，取决于leftmost这个boolean值，如果true的话就使用普通的插排；否则使用首部部分部分有序的插排（变种）。 如果长度大于等于47，就要开始真正的快排部分了，首先它会寻找5个点index（e1,e2,e3,e4,e5），这5个index可以大致把数据分成等长的7份（其中e3近似为数据长度中点）； 对这5个index上的值进行排序同时交换了它们的位置（用的是穷举法的插入排序）； 如果这5个点的值都不相等，用 array[e2] 和 array[e4] 作为基准轴，对数据进行双轴快排处理，这里面有一点优化，（我们知道双轴快排会把数据分成3份，左部分、中间部分、右部分）就是如果中间部分长度超过4/7总长度，会跳过等于pivot（基准值）的元素，因为它们是相等的； 如果5个点的值全部相等，就使用三路快排处理，其轴为 array[e3] 即中间值（5个值都相等，较大概率说明数据的重复度很高了）。 long类型数据同int类型数据处理 short类型数据 对于待排序数组short[] array，如果数据长度大于 COUNTING_SORT_THRESHOLD_FOR_SHORT_OR_CHAR = 3200 ，直接使用计数排序处理，计数排序的初始化桶数量为 NUM_SHORT_VALUES = 65536； 否则对于小于等于3200的数据，参考int类型处理。 char类型数据参考short类型处理方式 byte类型数据 对于待排序数组byte[] array，如果数据长度大于 COUNTING_SORT_THRESHOLD_FOR_BYTE = 29 ，直接使用计数排序处理，其中初始化的桶数量为 NUM_BYTE_VALUES = 256； 否则数据长度小于等于29，直接使用插入排序法处理。 float类型数据 对于待排序数组float[] array，先看看数据中有没有NaN这种数据，有的话直接与尾部数据交换，排序边界位置也应该缩小，因为NaN是无法参与比较的； 对于剩下可以比较大小的数据部分，直接参考int类型数据的处理方式； 排好后，由于可能有正0和负数0（-0.0 和 +0.0）这种数据，这种情况下负0应该在正0前面的，需要特殊处理下（程序中使用了 Float.floatToRawIntBits方法处理，这个可以返回浮点值的实际表示形式，可以确定正负）。 double类型数据参考float类型处理方式 以上就是DualPivotQuicksort的逻辑原理。 相关源代码我们可以在 java.util.DualPivotQuicksort找到这个类，它是一个not public且final，且构造函数为private的类。 下图是我对它源码的一些解读，有兴趣的可以看看，这儿使用的是JDK1.8_131版本，在高版本的JDK中，某些部分有些改动，但不影响整体阅读。 这个源码有3000多行，考虑到文字数量及可读性问题，我使用了图片，方便大家阅读部分逻辑。 其它上面我们看到Java里优化后的DualPivotQuicksort代码量是十分巨大的，这儿我们以int[]举例，来检测DualPivotQuicksort的效率问题。 我们创建几个测试数组，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class SortTest &#123; /** * 随机生成数组 * @param length * @return */ private static int[] randomArray(int length)&#123; int[] a = new int[length]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(length); &#125; return a; &#125; /** * 生成一个有序的倒序数组 * @param length * @return */ private static int[] reversalArray(int length)&#123; int[] a = new int[length]; for (int i = 0; i &lt; a.length; i++) &#123; a[i] = a.length - i - 1; &#125; return a; &#125; /** * 生成有个有序的正序数组 * @param length * @return */ private static int[] orderArray(int length)&#123; int[] a = new int[length]; for (int i = 0; i &lt; a.length; i++) &#123; a[i] = i; &#125; return a; &#125; /** * 生成前X部分有序后length-X部分无序的数组 * @param length * @return */ private static int[] orderAndRandomArray(int length,int x)&#123; if(x &gt;= length)&#123; throw new RuntimeException("x不能超过length"); &#125; int[] a = new int[length]; Random r = new Random(); for (int i = 0; i &lt; x; i++) &#123; a[i] = i; &#125; for(int i=a.length - x;i&lt;a.length;i++ )&#123; a[i] = r.nextInt(length); &#125; return a; &#125; /** * 生成前x部分无序后length-x部分有序的数组 * @param length * @param x * @return */ private static int[] randomAndOrderArray(int length,int x)&#123; if(x &gt;= length)&#123; throw new RuntimeException("x不能超过length"); &#125; int[] a = new int[length]; Random r = new Random(); for (int i = 0; i &lt; x; i++) &#123; a[i] = r.nextInt(length); &#125; for(int i=a.length - x;i&lt;a.length;i++ )&#123; a[i] = i; &#125; return a; &#125; private static void doSort(int[] a)&#123; System.out.println("Java-DualPivotQuicksort排序开始："); long start = System.currentTimeMillis(); JavaDualPivotQuicksort.sort(a,0,a.length-1,null,0,0); System.out.println("Java-DualPivotQuicksort耗时："+(System.currentTimeMillis()-start)+"ms"); System.out.println("Java-DualPivotQuicksort排序完成！"); System.out.println("数组是否有序："+isOrdered(a)); &#125; public static void main(String[] args) &#123; final int length = 100000000; //随机无序数组 int[] a1 = randomArray(length); doSort(a1); //有序数组 int[] a2 = orderArray(length); doSort(a2); //逆序数组 int[] a3 = reversalArray(length); doSort(a3); //前部分有序后部分无序数组 int[] a4 = orderAndRandomArray(length,length/3); doSort(a4); //前部分无序后部分有序数组 int[] a5 = randomAndOrderArray(length,length/3); doSort(a5); //前部分有序后部分无序数组 int[] a6 = orderAndRandomArray(length,length - length/3); doSort(a6); //前部分无序后部分有序数组 int[] a7 = randomAndOrderArray(length,length/2); doSort(a7); &#125;&#125; 最后可以得到类似的结果： PS：我们可以使用之前提到的所有排序算法进行测试，其效率都不如该算法稳定。对于O(n^2)复杂度的排序算法，很多出现内存或者栈溢出异常。 我们进一步分析下该算法。 可以看到该算法排序取决于几个要素：数据长度、数据类型、数据是否结构化（部分有序）。 其算法比较突出的地方有以下几点： 创建了 MAX_RUN_COUNT+1 个run，这个run一个重要的作用就是判断数据是否结构化，如果是的话用归并处理要快很多；如果不是，再使用快排，相比直接快排，只是浪费了一些校验时间，却规避了快排O(n^2)复杂度出现的情况，这也是run的数量要取合适的一个原因，算法作者通过大量实验确定了67这个数。 当对于short、char、byte类型数据时，由于其范围不是很大，计数排序的优势就体现出来了，因此使用了计数排序。 对数据进行快排时，首先选择了5个点，在分情况从中获取基准值，并决定使用双轴快排还是三路快排。 这儿有一点是比较有意思的，我们可以看下源码中的这个优化后的快排算法sort(int[] a, int left, int right, boolean leftmost)，它掺杂着双轴快排和三路快排，使用条件是看这5个点的值是否相等，也就是比如一个数组A，开始时使用了双轴快排，分成了3段A1、A2、A3，可能出现A1后面使用了三路快排（数据里重复元素较多，导致5个点的值全部相等），A2和A3继续使用双轴快排的情况。 复杂度情况该排序算法的时间复杂度如下： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n * log n) 时间复杂度（最差）：O(n * log n) 该排序算法的空间复杂度最差为O(n * log n)。 该排序算法为不稳定排序算法。 动图演示这个排序算法就不展示动图了。其排序流程图大致如下： 总结关于该算法的主要内容基本就介绍到这里了，有兴趣的同学可以看看源代码，了解下算法中每个排序（优化排序）具体的实现过程。 可以看到JDK源代码中对于一些常用方法、工具类，是有大量优化的。 源码地址上述所提到的所有Java代码均可见于我的Github 参考资料 JDK1.8 java.util.DualPivotQuicksort 源码 JDK9.0 java.util.DualPivotQuicksort 源码 JDK11.0 java.util.DualPivotQuicksort 源码]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>DualPivotQuicksort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（八） - 三路快速排序]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AB%EF%BC%89-%E4%B8%89%E8%B7%AF%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F.html</url>
    <content type="text"><![CDATA[前言上篇文章我们讲到了双轴快速排序，今天我们来看下快排的另一种 - 三路快速排序（ThirdWayQuickSort），也称三向切分的快速排序。 正文简介及原理三路快速排序（ThirdWayQuickSort）是如何出现的呢？ 我们知道，对于待排序数组，其数据是随机的，如果数组中有大量相同元素，无论普通快排还是双轴快排，由于快速排序的判定问题，会导致这些重复数移到一边从而大幅增加算法的运算时间。 为了优化这种数据情况，进而出现了三路快速排序。 三路快速排序也是采用一个轴值pivot，那具体是哪三路呢？ 三路指的就是每分割（Partition）一次，三路排序会把数据分为小于pivot、等于pivot和大于pivot的三部分。 三路排序的大致原理如下： 对于待排序数组，选取第一个元素为中轴pivot； 每次分割处理将小于pivot的数据放到左边，等于pivot的数据放在中间，大于pivot的数据放在右边； 对小于pivot和大于pivot的部分重复分割过程。 我们以一个数组为例，来看下它的原理,如下图。 其算法描述如下： 对于待排序数组a，最左索引为left，最右索引为right； 排序过程中，以a[left]最为pivot，用i、j、k将数组分割为4部分，其中i表示pivot元素的起始index，所有小于pivot的元素下标都应该小于i；k表示待扫描的元素索引从i+1开始，到j结束；j表示待扫描元素的最后索引； 开始时i=left，k=left+1,j=right,此时小于pivot的元素为0个，大于pivot的元素为0个，待扫描的元素为j-k+1个； 从k开始扫描，当该元素比pivot小时，交换a[i]和a[k]，同时i++，k++，这样就能把该元素放到pivot的左边了； 当该元素等于pivot时，略过即可，即k++； 当该元素大于pivot时，需要把它放到右边，先看下a[j]和pivot的大小。 (1) 如果a[j] &gt; pivot，显然它已经在右边了，我们只需j–即可； (2) 如果a[j] = pivot，这时候我们需要交换a[k]和a[j]的位置，同时k++,j–即可； (3) 如果a[j] &lt; pivot，这时候我们除了交换a[k]和a[j]位置后，继续交换a[i]和a[k]的位置，将这个小元素移到左边，同时i++,j–,k++; 最后数据会被分为大于pivot、小于pivot、等于pivot的三部分，我们对大于pivot和小于pivot的部分再进行上述逻辑，直到传入的数据长度为1，说明数据全部有序了。 动图演示 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class ThirdWayQuickSort &#123; public static void quickSort3Way(int[] a, int left, int right) &#123; //递归终止条件，少于等于一个元素的数组已有序 if (left &gt;= right) &#123; return; &#125; int i, j, k, pivot; //首元素作为中轴 pivot = a[left]; i = left; k = left + 1; j = right; outer: while (k &lt;= j) &#123; if (a[k] &lt; pivot) &#123; swap(a, i, k); i++; k++; &#125; else if (a[k] == pivot) &#123; k++; &#125; else &#123;// 遇到A[k]&gt;pivot的情况，j从右向左扫描 //A[j]&gt;pivot的情况,j继续向左扫描 while (a[j] &gt; pivot) &#123; j--; if (j &lt; k) &#123; break outer; &#125; &#125; //A[j]==pivot的情况 if (a[j] == pivot) &#123; swap(a, k, j); k++; j--; &#125; else &#123;//A[j]&lt;pivot的情况 swap(a, i, j); swap(a, j, k); i++; k++; j--; &#125; &#125; &#125; //A[i, j] 等于 pivot 且位置固定，不需要参与排序 // 对小于pivot的部分进行递归 quickSort3Way(a, left, i - 1); // 对大于pivot的部分进行递归 quickSort3Way(a, j + 1, right); &#125; private static void swap(int[] a, int i, int j) &#123; int temp = a[i]; a[i] = a[j]; a[j] = temp; &#125; public static void main(String[] args) &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100000); &#125; System.out.println("ThirdWayQuickSort排序开始："); long start = System.currentTimeMillis(); quickSort3Way(a,0,a.length-1); System.out.println("ThirdWayQuickSort耗时："+(System.currentTimeMillis()-start)+"ms"); System.out.println("ThirdWayQuickSort排序完成！"); System.out.println("数组是否有序："+isOrdered(a)); &#125;&#125; 其他注意事项三路快排的时间复杂度与快速排序相当： 时间复杂度（最好）：O(n * log n) 时间复杂度（平均）：O(n * log n) 时间复杂度（最差）：O(n^2) 三路快排的空间复杂度为O(n * log n) （递归消耗栈空间，是一种就地排序算法，如果从堆空间角度空间复杂度为O(1)） 三路快排也是一种不稳定排序算法。 相比普通快排和双轴快排，我们可以看到三路快排对于有重复数据的数组处理较佳，可以减少元素交换次数。 测试相关我们来实际测试下，相关代码如下： 我们准备1亿数据，但是数据集范围为[0,10000)，这时候相同元素产生的就会比较多，我们分别测试下三路快排、双轴快排和普通快排。 123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; int[] b = a.clone(); int[] c = a.clone(); System.out.println("ThirdWayQuickSort排序开始："); long start = System.currentTimeMillis(); quickSort3Way(a,0,a.length-1); System.out.println("ThirdWayQuickSort耗时："+(System.currentTimeMillis()-start)+"ms"); System.out.println("ThirdWayQuickSort排序完成！"); System.out.println("数组是否有序："+isOrdered(a)); System.out.println("DualPivotQuickSort排序开始："); long start1 = System.currentTimeMillis(); DualPivotQuickSort.dualPivotQuickSort(b,0,b.length-1); System.out.println("DualPivotQuickSort耗时："+(System.currentTimeMillis()-start1)+"ms"); System.out.println("DualPivotQuickSort排序完成！"); System.out.println("数组是否有序："+isOrdered(b)); System.out.println("QuickSort排序开始："); long start2 = System.currentTimeMillis(); QuickSort.quickSort(c,0,c.length-1,true); System.out.println("QuickSort耗时："+(System.currentTimeMillis()-start2)+"ms"); System.out.println("QuickSort排序完成！"); System.out.println("数组是否有序："+isOrdered(c));&#125; 多次测试结果大致如下： 我们可以看到，当数据集重复元素较多时，三路快排确实有着比较优的排序效率。 我们将数据集范围改为[0,100)，效果更加明显。 123for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100);&#125; 我们增大数据集范围，改为[0,100000000)，再来测试一下。 123for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100000000);&#125; 可以看到三路快排效率比不上双轴快排了，因为这种情况下，重复元素较少，三路快排基本退化为普通快排。 总结通过对三路快排（ThirdWayQuickSort）的介绍，我们大致了解了它的一些原理和排序过程，快速排序主要就有普通快排、三路快排和双轴快排三种。 在数据集元素重复较多的情况下，三路快排有着显著的优势，因此它和双轴快排一起，作为了Java对基本数据类型排序方法实现的一部分。 源码地址本篇文章提到的所有代码均可见于我的GitHub。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
        <tag>sort</tag>
        <tag>三路快排</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（七） - 双轴快速排序]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89-%E5%8F%8C%E8%BD%B4%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F.html</url>
    <content type="text"><![CDATA[前言上篇文章我们介绍了TimSort排序算法，排序算法（六）- TimSort，今天我们再来看一种排序算法，双轴快速排序（DualPivotQuicksort），这个排序算法也是非常有意思的，它也是目前Java对于基本数据类型数组排序使用的内置排序算法。 正文双轴快速排序（DualPivotQuicksort）简介及原理双轴快速排序（DualPivotQuicksort）是Vladimir Yaroslavskiy在2009年开发出来的一种排序算法，是快速排序的一种变体，与快排不同的是，它有两个基准值（快排有一个）。 相比快速排序，双轴快速排序有着更高的效率，我们来看下。 我们先来回顾下快速排序，快排的主要原理如下： 对于待排序数组，选择一个基数(pivot)，然后把比它小的那些数放在它的左边，把比它大的那些数放在它的右边，然后再对这个数左右两部分数递归的执行快排过程，直到子数组只剩一个数为止。 如下图所示。 而双轴快速排序会把待排序数组分为3份，有两个基准点，我们先来看下双轴快排的伪代码： 1234567891011121314151617181920212223242526272829// sort A[left..right]dual_pivot_quicksort(A,left,right) if right−left ≥ 1 p := min &#123;A[left],A[right]&#125; q := max&#123;A[left],A[right]&#125; ℓ := left +1; g := right −1; k := ℓ while k ≤ g if A[k] &lt; p Swap A[k] and A[ℓ]; ℓ := ℓ+1 else if A[k] ≥ q while A[g] &gt; q and k &lt; g g := g −1 end while Swap A[k] and A[g]; g := g −1 if A[k] &lt; p Swap A[k] and A[ℓ]; ℓ := ℓ+1 end if end if k := k +1 end while ℓ := ℓ−1; g := g +1 // p to final position A[left] := A[ℓ]; A[ℓ] := p // q to final position A[right] := A[g]; A[g] := q dual_pivot_quicksort(A, left , ℓ−1) dual_pivot_quicksort(A, ℓ+1,g −1) dual_pivot_quicksort(A,g +1,right) end if 它的原理图如下： 双轴快速排序的工作原理： 对于待排序数组，取最小索引上的值和最大索引上的值为基准，根据这两个基准，可以把数组分成小于最小基准的一部分、大于最小基准小于最大基准的一部分、大于最大基准的一部分，共三部分；再对这三部分进行双轴快速排序。 代码实现我们根据伪代码，可以写出双轴快排的具体代码，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class DualPivotQuickSort &#123; public static void dualPivotQuickSort(int[] a, int left, int right) &#123; //根据两个基准将数据分成三部分 if (right - left &gt;= 1) &#123; int pivot1 = Math.min(a[left], a[right]); int pivot2 = Math.max(a[left], a[right]); int start = left + 1, end = right - 1, tempIndex = start; while (tempIndex &lt;= end) &#123; if (a[tempIndex] &lt; pivot1) &#123; swap(a, tempIndex, start); start++; &#125; else if (a[tempIndex] &gt;= pivot2) &#123; while (a[end] &gt; pivot2 &amp;&amp; tempIndex &lt; end) &#123; end--; &#125; swap(a, tempIndex, end); end--; if (a[tempIndex] &lt; pivot1) &#123; swap(a, tempIndex, start); start++; &#125; &#125; tempIndex++; &#125; start--; end++; //分完堆后两个基准的位置还不对，需要将这两个基准移动到正确位置 a[left] = a[start]; a[start] = pivot1; a[right] = a[end]; a[end] = pivot2; //对分成的三部分继续进行双轴快排 dualPivotQuickSort(a, left, start - 1); dualPivotQuickSort(a, start + 1, end - 1); dualPivotQuickSort(a, end + 1, right); &#125; &#125; private static void swap(int[] a, int i, int j) &#123; int t = a[i]; a[i] = a[j]; a[j] = t; &#125; public static void main(String[] args) &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100000000); &#125; System.out.println("DualPivotQuicksort排序开始："); long start = System.currentTimeMillis(); dualPivotQuickSort(a, 0, a.length - 1); System.out.println("DualPivotQuicksort耗时：" + (System.currentTimeMillis() - start) + "ms"); System.out.println("DualPivotQuicksort排序完成！"); System.out.println("数组是否有序：" + isOrdered(a)); &#125;&#125; 我们继续使用1亿数据量对其进行测试，可得到如下结果： 可以看到双轴快排排序1亿数据耗时在9s左右，我们在排序算法（五）-双调排序中测过1亿数据情况下普通快速排序耗时大概在12s左右。 我们也可以测试大量数据，实际上，双轴快排效率是要优于普通快排的。 动图演示下面视频演示了在一定数据量下的排序过程。 其他注意事项我们来分析下双轴快排的复杂度情况。 我们知道，对于递归，其时间复杂度公式如下： T[n] = aT[n/b] + f(n) 我们对于普通快速排序，最好情况下，很容易得到其时间复杂度公式： T[n] = 2T[n/2] + n 其中 T[n/2]为平分后的子数组的时间复杂度，n 是划分两部分数组所需要的时间。这个公式表示快排每次正好可以把序列分成两个相等的子序列。其中T[0] = T[1] = 1。 迭代求公式有可以得到时间复杂度如下： 对于双轴快速排序，我们在最优情况下可以得到其时间复杂度公式： T[n] = 3T[n/3]+n 其中 T[n/3]为平分后的子数组的时间复杂度，n 是划分两部分数组所需要的时间。这个公式表示快排每次正好可以把序列分成三个相等的子序列。其中T[0] = T[1] = 1。 迭代求公式有可以得到时间复杂度如下： 可以看到双轴快速排序的时间复杂度也是O(n * log n)级别的。 双轴快速排序的时间复杂度： 时间复杂度（最好）：O(n * log n) 时间复杂度（平均）：O(n * log n) 时间复杂度（最差）：O(n^2) 对于空间复杂度，每次递归需要一定栈空间保存结果，其栈空间复杂度公式： S[n] = 3S[n/3] + n 其中 T[n/3]为平分后的子数组的空间复杂度，n 是保存此层递归结果所需要的空间。这个公式表示快排每次正好可以把序列分成三个相等的子序列。其中T[0] = T[1] = 1。 根据这个公式可以得到双轴快排的栈空间复杂度是O(n * log n)级别的，但是双轴快排却不消耗堆空间，其堆空间复杂度为O(1)。 双轴快速排序的空间复杂度： 就地排序算法的空间复杂度为O(1)，如果考虑到递归调用占用系统资源，它的空间复杂度为O(n * log n) 双轴快速排序也是一种不稳定排序算法。 双轴快速排序为什么比普通快排要快关于双轴快速排序有一篇论文 Why Is Dual-Pivot Quicksort Fast?，有兴趣的可以看一下。 论文中详细介绍了为什么相比普通快排，双轴快速排序要快。 论文中提到双轴快速排序的元素比较次数是要比普通快排要多的。 它们的比较次数比值大致如下： DualPivotQuickSort vs QuickSort =&gt; 1.7043nlnn vs 1.5697nlnn 一般排序算法中元素比较次数越多其耗费的时间越高，可是双轴快排却和普通快排呈现了两种不同的结果，这样理论与实验是相矛盾的。 论文作者提到在我们在排序时不仅要考虑元素比较次数，还应该考虑 CPU的速度，内存的速度，CPU和内存速度是否匹配 等的影响。 作者提出了“内存墙”问题： 据统计在过去的25年里面，CPU的速度平均每年增长46%, 而内存的带宽每年只增长37%，那么经过25年的这种不均衡发展，它们之间的差距已经蛮大了。假如这种不均衡持续持续发展，有一天CPU速度再增长也不会让程序变得更快，因为CPU始终在等待内存传输数据，这就是传说中内存墙(Memory Wall)。 同时给出了另一种比较排序算法优劣的方法：扫描元素个数算法。 在这种新的算法里面，我们把对于数组里面一个元素的访问: array[i] 称为一次扫描。但是对于同一个下标，并且对应的值也不变的话，即使访问多次我们也只算一次。而且我们不管这个访问到底是读还是写。 为什么只算一次呢？因为在CPU高速缓存下，再次访问数组同一下标下的元素要比访问一个新的下标元素的时间少很多。（缓存级别vs内存级别） 因为内存比较慢，统计CPU与内存之间的数据流量的大小也就把这个比较慢的内存的因素考虑进去了，因此也就比元素比较次数更能体现算法在当下计算机里面的性能指标。 在这种新算法下，作者计算的两种排序算法的扫描元素个数之比为： DualPivotQuickSort vs QuickSort =&gt; 1.4035nlnn vs 1.5697nlnn 也就是普通快排要比双轴快排多扫描了12%的元素，也相当于节约了大概12%的时间，在实际实验过程中来看，节约了10%左右的时间。 具体的计算过程可参考论文中的一些计算公式。 Java中的DualPivotQuicksort在看这个问题之前，我们先来构造一个完全倒序的数组，比如1亿数据量，来测试DualPivotQuicksort。 12345678910111213public static void main(String[] args) &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = 100000000 - i; &#125; System.out.println("DualPivotQuicksort排序开始："); long start = System.currentTimeMillis(); dualPivotQuickSort(a, 0, a.length - 1); System.out.println("DualPivotQuicksort耗时：" + (System.currentTimeMillis() - start) + "ms"); System.out.println("DualPivotQuicksort排序完成！"); System.out.println("数组是否有序：" + isOrdered(a)); &#125; 我们运行这个测试类，后面会等待很久然后出现栈溢出异常，原因很简单，这种情况下，双轴快排退化成冒泡排序（与普通快排类似），时间复杂度为O(n^2)。 这是我们无法忍受的。 作为一款优秀的排序算法，不仅要求其能适应最好情况和一般情况，更重要的是要其在最差情况下效率也要高效。更不用说如果作为一种语言的源代码的基础包里的部分了。 因此Java里的排序算法是有大量优化的。 我们在Java源代码中可以找到类 DualPivotQuicksort.java，它是Arrays.sort底层代码实现的一部分（另一部分是TimSort）,主要对于基本数据类型进行排序（不需要考虑稳定性）。 我们看到它是一种混合排序算法，而不单单是双轴快排。 其内部使用了插入排序、归并排序、双轴快速排序、单轴（普通）快速排序、计数排序等排序算法。 关于这个排序的分析我将在下篇文章介绍下。 总结关于双轴排序算法的内容就聊到这儿，本文介绍了双轴快排的实现原理及一些特点。 在下篇文章里我会介绍下Java源代码里的DualPivotQuicksort，这个类由Vladimir Yaroslavskiy、Jon Bentley、Josh Bloch编写，是一个高效的排序算法。 源码文中涉及到的程序代码详见我的 Github。 参考资料 Why Is Dual-Pivot Quicksort Fast?]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（六）- TimSort]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AD%EF%BC%89-TimSort.html</url>
    <content type="text"><![CDATA[前言我们在上篇文章 排序算法（五）-双调排序 介绍了双调排序，今天我们来看一下另一种排序算法 —— TimSort。 TimSort是Tim Peters发明的一种混合排序，最早是Python语言的内置排序算法。 关于Python内置的TimSort描述可以查看该 文档。 关于TimSort的理论基础，可以查看该篇论文 Optimistic Sorting and Information Theoretic Complexity，这篇论文论证了插入排序和归并排序合并后效率提高的可能性，即TimSort的理论基础。 Java自Java 7 后加入TimSort，其实现参考了Python版本的实现，我们可以在JDK源码的util包下找到它，java.util.TimSort，这个class不是public的，我们无法直接调用，只能够通过Java提供的sort方法等间接调用它。 为什么引入了TimSort呢？我们慢慢来看下吧。 正文TimSort的排序原理说到排序算法，我们就先来了解下排序算法的基本原理，这样有助于我们更快理解算法本身。 TimSort作为一种混合排序算法，其内部使用了插入排序（准确的说是二分插入排序）和归并排序。 根据我们之前说到的，归并排序是一种时间复杂度很平均（O(n * log n) 排序算法，其缺点一个是如果两个序列长度相差较大（一个长序列和一个短序列归并），排序效率无法体现；另一个是当数组长度很小时，归并的效率也不是很高且浪费空间。 对于插入排序，虽然时间复杂度为（O(n^2)），但是较小数据量下，其表现也不错，在一部分数据已经排序的情况下，其表现要更好，而使用插入排序的变种二分插入排序，虽不能减少移动次数，但减少了比较次数。 说到这里，我们再说TimSort基于的一个简单事实：数组中的数据都是部分有序（升序或降序）的。 什么意思呢？ 比如 数组[9,8,5,7,3,9,1,3,4,6,0,5,3]，可以看到里面部分数据[9,8,5],[1,3,4,6]等等有序。 这对插入排序是十分友好的。 Timsort排序算法可以概括成如下几步： 把待排数组划分成一个个run，当然run不能太短，长度最小阈值为minRun； run的划分规则：从数组最小下标low开始，寻找连续有序部分（连续逆序也算，寻找的时候会把这段顺序反过来），如果这段有序部分长度小于minRun，就用二分插入排序补充到minRun； 将run入栈，当栈顶的run的长度不满足下列约束条件中任意一个时， 12runLen[n-1] &gt; runLen[n] + runLen[n+1]runLen[n] &gt; runLen[n+1] 则利用归并排序将其中最短的2个run合并成一个新run； 最后会有一次强制合并合并所有栈内剩余所有run，最终栈空，生成有序数组。 TimSort算法的原理可以用下图大致表示： 这儿我们设minRun = 4. 如上图可以清晰的看到TimSort的排序过程。 TimSort源码分析下面我们来分析下TimSort的源码。 它的主要入口为sort方法，sort方法参数说明如下： T[] a : 表示待排序数组； int lo：排序的起始位置，如果排整个数组传0即可； int hi：排序的终止位置，排整个数组传数组长度n即可； Comparator&lt;? super T&gt; c：数据的比较规则； T[] work： 合并所需的临时存储空间设置，一般不需要我们设置，会有默认值，传null即可； int workBase ：合并所需的临时存储空间分片基数值，一般不需要我们设置，会有默认值，传0即可； int workLen：合并所需的临时存储空间默认长度，一般不需要我们设置，会有默认值，传0即可。 可以看到传入的排序位置lo和hi，如果长度小于2，直接返回，如果小于32，就做mini-TimSort，迷你TimSort就相当于上面图中说的部分有序的二分插入排序。 再来看下下面部分。 TimSort先计算出该数组的minRun，然后开始处理数据，拿到有序长度，判断是否小于minRun，小于的话就用插入排序补齐，然后入栈，看看是否需要进行归并排序，移动位置到下一个run，重复上述过程，最后在进行一次归并排序，得到有序数据。 二分插入排序法的源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041private static &lt;T&gt; void binarySort(T[] a, int lo, int hi, int start,Comparator&lt;? super T&gt; c) &#123; assert lo &lt;= start &amp;&amp; start &lt;= hi; if (start == lo) start++; for ( ; start &lt; hi; start++) &#123; T pivot = a[start]; // Set left (and right) to the index where a[start] (pivot) belongs int left = lo; int right = start; assert left &lt;= right; /* * Invariants: * pivot &gt;= all in [lo, left). * pivot &lt; all in [right, start). */ while (left &lt; right) &#123; int mid = (left + right) &gt;&gt;&gt; 1; if (c.compare(pivot, a[mid]) &lt; 0) right = mid; else left = mid + 1; &#125; assert left == right; /* * The invariants still hold: pivot &gt;= all in [lo, left) and * pivot &lt; all in [left, start), so pivot belongs at left. Note * that if there are elements equal to pivot, left points to the * first slot after them -- that's why this sort is stable. * Slide elements over to make room for pivot. */ int n = start - left; // The number of elements to move // Switch is just an optimization for arraycopy in default case switch (n) &#123; case 2: a[left + 2] = a[left + 1]; case 1: a[left + 1] = a[left]; break; default: System.arraycopy(a, left, a, left + 1, n); &#125; a[left] = pivot; &#125;&#125; 计算有序部分长度（如果逆序有序就把数据处理成正序）的源码如下： 123456789101112131415161718192021222324private static &lt;T&gt; int countRunAndMakeAscending(T[] a, int lo, int hi,Comparator&lt;? super T&gt; c) &#123; assert lo &lt; hi; int runHi = lo + 1; if (runHi == hi) return 1; // Find end of run, and reverse range if descending if (c.compare(a[runHi++], a[lo]) &lt; 0) &#123; // Descending while (runHi &lt; hi &amp;&amp; c.compare(a[runHi], a[runHi - 1]) &lt; 0) runHi++; reverseRange(a, lo, runHi); &#125; else &#123; // Ascending while (runHi &lt; hi &amp;&amp; c.compare(a[runHi], a[runHi - 1]) &gt;= 0) runHi++; &#125; return runHi - lo;&#125;private static void reverseRange(Object[] a, int lo, int hi) &#123; hi--; while (lo &lt; hi) &#123; Object t = a[lo]; a[lo++] = a[hi]; a[hi--] = t; &#125;&#125; 获取该数组的minRun的相关代码如下： 123456789private static int minRunLength(int n) &#123; assert n &gt;= 0; int r = 0; // Becomes 1 if any 1 bits are shifted off while (n &gt;= MIN_MERGE) &#123; r |= (n &amp; 1); n &gt;&gt;= 1; &#125; return n + r;&#125; 可以看到如果入参n小于MIN_MERGE（32）时，会直接返回n，因为太小了；如果n恰好是2的幂，则返回MIN_MERGE/2；否则返回一个int k, MIN_MERGE/2 &lt;= k &lt;= MIN_MERGE，使n/k接近但小于2的幂次值。 关于这样设计的基本原理，可以查看Tim在Python上关于它的描述文档 listsort.txt。 检测并合并不符合条件的栈元素的相关代码如下： 1234567891011121314private void mergeCollapse() &#123; while (stackSize &gt; 1) &#123; int n = stackSize - 2; if (n &gt; 0 &amp;&amp; runLen[n-1] &lt;= runLen[n] + runLen[n+1]) &#123; if (runLen[n - 1] &lt; runLen[n + 1]) n--; mergeAt(n); &#125; else if (runLen[n] &lt;= runLen[n + 1]) &#123; mergeAt(n); &#125; else &#123; break; // Invariant is established &#125; &#125;&#125; 我们可以看到它的合并条件，如上面提到的。 关于它的合并方法mergeAt，内容较多，在这儿就不展示了。 最后强制合并栈元素的代码如下： 12345678private void mergeForceCollapse() &#123; while (stackSize &gt; 1) &#123; int n = stackSize - 2; if (n &gt; 0 &amp;&amp; runLen[n - 1] &lt; runLen[n + 1]) n--; mergeAt(n); &#125;&#125; 该方法最终会将栈内元素合并为1个，即run[0],即有序数组。 初始化栈空间及归并缓存空间代码如下： 1234567891011121314151617181920212223242526private TimSort(T[] a, Comparator&lt;? super T&gt; c, T[] work, int workBase, int workLen) &#123; this.a = a; this.c = c; // Allocate temp storage (which may be increased later if necessary) int len = a.length; int tlen = (len &lt; 2 * INITIAL_TMP_STORAGE_LENGTH) ? len &gt;&gt;&gt; 1 : INITIAL_TMP_STORAGE_LENGTH; if (work == null || workLen &lt; tlen || workBase + tlen &gt; work.length) &#123; @SuppressWarnings(&#123;"unchecked", "UnnecessaryLocalVariable"&#125;) T[] newArray = (T[])java.lang.reflect.Array.newInstance (a.getClass().getComponentType(), tlen); tmp = newArray; tmpBase = 0; tmpLen = tlen; &#125; else &#123; tmp = work; tmpBase = workBase; tmpLen = workLen; &#125; int stackLen = (len &lt; 120 ? 5 : len &lt; 1542 ? 10 : len &lt; 119151 ? 24 : 49); runBase = new int[stackLen]; runLen = new int[stackLen]; &#125; Java版本TimSort曾经的Bug在JDK1.7时候，TimSort曾经有一个bug，会引发数组下标越界异常。 PS：这个Bug已经被fix了。 bug详情可参考如下链接 JDK-8011944 : Sort fails with ArrayIndexOutOfBoundsException。 关于这篇bug的发现验证解决可参考这篇pdf,OpenJDK’s java.utils.Collection.sort() is broken:The good, the bad and the worst case?。 我们这儿简单说下这个bug吧。 上面我们看到TimSort栈内元素合并的条件，它的目的在于尽量保证两个要进行归并排序的数组长度大致相同。 所以栈内所有run应该满足如下条件： 1232 &lt;= i &lt;= StackSize-1runLen[n-1] &gt; runLen[n] + runLen[n+1]runLen[n] &gt; runLen[n+1] 我们看mergeCollapse方法也能验证这一点。 大多数情况下我们检查栈顶的3个元素就能满足约束条件，但是一些特殊情况下就不行了，比如下面的这个栈：1120, 80, 25, 20, 30 根据源码，因为25 &lt; 20 + 30，25 &lt; 30，所以将25和20两个run进行合并，此时栈内的情况变为1120, 80, 45, 30 由于80 &gt; 45 + 30，45 &gt; 30，满足约束条件，此时归并就终止了。但是注意栈里的其他run，120 &lt; 80 + 45，这是不满足约束条件的，而由于我们只判断了栈顶的run，因此在这里就留下了“隐患”。 大多数情况下，这并不是什么问题，因为TimSort最终可以通过最后的强制归并将数据排序合并。 但是Bug发现者构造了一个非常精致的Array，成功的让Timsort算法抛出java.lang.ArrayIndexOutOfBoundsException，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class BreakTimSort &#123; private static final int MIN=16; ArrayDeque&lt;Integer&gt; chunks = new ArrayDeque&lt;Integer&gt;(); private static final int BOUND1 = 2*MIN+1; private static final int BOUND2 = BOUND1+MIN+2; private static final int BOUND3 = BOUND1+1+BOUND2; private static final int BOUND4 = BOUND2+1+BOUND3; private static final int BOUND5 = BOUND3+1+BOUND4; public int build(int size, int B) &#123; chunks.addFirst(B); if (size &lt; BOUND1) &#123; chunks.addFirst(size); return size; &#125; int asize = (size+2)/2; if (size &gt;= BOUND2 &amp;&amp; asize &lt; BOUND1) asize = BOUND1; else if (size &gt;= BOUND3 &amp;&amp; asize &lt; BOUND2) asize = BOUND2; else if (size &gt;= BOUND4 &amp;&amp; asize &lt; BOUND3) asize = BOUND3; else if (size &gt;= BOUND5 &amp;&amp; asize &lt; BOUND4) asize = BOUND4; if (size - asize &gt;= B) throw new AssertionError( " " +size+ " , " +asize+ " , " +B); return build (asize, size - asize); &#125; public void run() &#123; chunks.addFirst(MIN); int B = MIN+4; int A = B + MIN + 1; for (int i = 0; i&lt; 8; i++) &#123; int eps = build(A, B); B = B+A+1; A = B+eps + 1; &#125; chunks.addFirst(B); chunks.addFirst(A); int total = 0; for (Integer len: chunks) &#123; total += len; &#125; int pow = MIN; while (pow &lt; total) pow += pow; chunks.addLast(pow-total); System.err.println( " Total: " +total); Object[] array = new Object[pow]; int off = 0; int pos = 0; for (Integer len: chunks) &#123; for (int i = 0; i &lt; len; i++) &#123; array[pos++] = Integer.valueOf(i == 0 ? 0 : 1); &#125; off++; &#125; Arrays.sort(array); &#125; public static void main(String[] param) &#123; new BreakTimSort().run(); &#125;&#125; PS:这段代码我们现在测试是没有问题的，因为这个bug已经被Fix了，如果想复现这个bug，可以下载 JDK - 1.7.0_07 版本。 这个bug出现的原因是TimSort初始化时会申请栈空间，如下： 12345int stackLen = (len &lt; 120 ? 5 : len &lt; 1542 ? 10 : len &lt; 119151 ? 24 : 49); runBase = new int[stackLen]; runLen = new int[stackLen]; 在JDK - 1.7.0_07版本它是这样的： 12345int stackLen = (len &lt; 120 ? 5 : len &lt; 1542 ? 10 : len &lt; 119151 ? 19 : 40); runBase = new int[stackLen]; runLen = new int[stackLen]; 可以看到有几个数字不一样，是的，不要小看这几个“魔法”数字。 这几个数字怎么来呢？ 我们栈归并的条件上面有提到，它其实就是函数 F(n) &lt; F(n-1) + F(n-2) +1，我们设F(n) = F(n-1) + F(n-2) +1，这个函数是不是很熟悉，在 SmoothSort里我们有提到过，不过我们这里的起始是0，第一个元素是minRun。 我们设minRun为16（上面的minRunLength方法，可以看到minRun最小值为MIN_MERGE/2 = 16），如下程序： PS:minRun越小，数组长度固定的情况下，分的份数就越多，理论需要的栈数量也越多。栈数量要多，少的话可能出现数组下标越界问题。 12345678910111213141516171819private static void getNum()&#123; int[] k = new int[]&#123;5,10,19,40&#125;; for(int p=0;p&lt;k.length;p++)&#123; long sum =0; for(int i=0;i&lt;k[p];i++)&#123; sum += function(i); &#125; System.out.println(k[p] +"---&gt;"+sum); &#125;&#125;private static long function(long n)&#123; if(n ==0)&#123; return 0; &#125;else if(n==1)&#123; return 16; &#125; return function(n-1) + function(n-2)+1;&#125; 如下输出： 12345---&gt;11910---&gt;154119---&gt;11915040---&gt;2917196495 当为40的时候已经超出了int最大值。 上述数据是在理想情况下，即 F(n) = F(n-1) + F(n-2) +1，但是实际上会有不满足的情况，这时候需要的栈大小就应该大一些，因而就出现了我们上述所说的异常。 关于实际需要的栈大小，上面PDF中给出了，可以直接查看，如下： array size 64 128 160 65536 131072 67108864 1073741824 required stack size 3 4 5 21 23 41 49 runLen.length 5 10 10 19 (24) 40 40 40 可以看到JDK1.8中长度已经变为了5,10,24,49，相当于修复了这个bug。 上述PDF中还给出了一种出现无法合并的栈的情况的解决方法： 1234567891011121314private void mergeCollapse() &#123; while (stackSize &gt; 1) &#123; int n = stackSize - 2; if (n &gt; 0 &amp;&amp; runLen[n-1] &lt;=runLen[n] + runLen[n + 1] ||n-1 &gt; 0 &amp;&amp; runLen[n-2] &lt;=runLen[n] + runLen[n-1])&#123; if (runLen[n-1] &lt;runLen[n + 1]) &#123; n--; &#125; &#125; else if (n &lt; 0 || runLen[n] &gt; runLen[n + 1]) &#123; break; // Invariant is established &#125; mergeAt(n); &#125;&#125; 相当于增加了 n-1 &gt; 0 &amp;&amp; runLen[n-2] &lt;= runLen[n] + runLen[n-1] 这部分，把栈顶的第4个元素也加入了判断。 但是Java社区JDK1.8中并未采用这段代码，还是原来的3层栈元素判断，只是变更了栈的长度作为解决办法，原因不详。 以上就是TimSort曾经出现的bug。 测试TimSort我们将TimSort源码中的泛型去掉，排序数组改为int[]，测试下TimSort的性能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639public class TimSort &#123; private static final int MIN_MERGE = 32; private final int[] a; private static final int MIN_GALLOP = 7; private int minGallop = MIN_GALLOP; private static final int INITIAL_TMP_STORAGE_LENGTH = 256; private int[] tmp; private int tmpBase; private int tmpLen; private int stackSize = 0; private final int[] runBase; private final int[] runLen; private TimSort(int[] a, int[] work, int workBase, int workLen) &#123; this.a = a; // Allocate temp storage (which may be increased later if necessary) int len = a.length; int tlen = (len &lt; 2 * INITIAL_TMP_STORAGE_LENGTH) ? len &gt;&gt;&gt; 1 : INITIAL_TMP_STORAGE_LENGTH; if (work == null || workLen &lt; tlen || workBase + tlen &gt; work.length) &#123; tmp = new int[tlen]; tmpBase = 0; tmpLen = tlen; &#125; else &#123; tmp = work; tmpBase = workBase; tmpLen = workLen; &#125; int stackLen = (len &lt; 120 ? 5 : len &lt; 1542 ? 10 : len &lt; 119151 ? 24 : 49); runBase = new int[stackLen]; runLen = new int[stackLen]; &#125; static void sort(int[] a, int lo, int hi,int[] work, int workBase, int workLen) &#123; assert a != null &amp;&amp; lo &gt;= 0 &amp;&amp; lo &lt;= hi &amp;&amp; hi &lt;= a.length; int nRemaining = hi - lo; if (nRemaining &lt; 2) &#123; return; // Arrays of size 0 and 1 are always sorted &#125; // If array is small, do a "mini-TimSort" with no merges if (nRemaining &lt; MIN_MERGE) &#123; int initRunLen = countRunAndMakeAscending(a, lo, hi); binarySort(a, lo, hi, lo + initRunLen); return; &#125; TimSort ts = new TimSort(a,work, workBase, workLen); int minRun = minRunLength(nRemaining); do &#123; // Identify next run int runLen = countRunAndMakeAscending(a, lo, hi); // If run is short, extend to min(minRun, nRemaining) if (runLen &lt; minRun) &#123; int force = nRemaining &lt;= minRun ? nRemaining : minRun; binarySort(a, lo, lo + force, lo + runLen); runLen = force; &#125; // Push run onto pending-run stack, and maybe merge ts.pushRun(lo, runLen); ts.mergeCollapse(); // Advance to find next run lo += runLen; nRemaining -= runLen; &#125; while (nRemaining != 0); // Merge all remaining runs to complete sort assert lo == hi; ts.mergeForceCollapse(); assert ts.stackSize == 1; &#125; private static void binarySort(int[] a, int lo, int hi, int start) &#123; assert lo &lt;= start &amp;&amp; start &lt;= hi; if (start == lo) &#123; start++; &#125; for ( ; start &lt; hi; start++) &#123; int pivot = a[start]; // Set left (and right) to the index where a[start] (pivot) belongs int left = lo; int right = start; assert left &lt;= right; /* * Invariants: * pivot &gt;= all in [lo, left). * pivot &lt; all in [right, start). */ while (left &lt; right) &#123; int mid = (left + right) &gt;&gt;&gt; 1; if (pivot&lt;a[mid]) &#123; right = mid; &#125;else &#123; left = mid + 1; &#125; &#125; assert left == right; // The number of elements to move int n = start - left; // Switch is just an optimization for arraycopy in default case switch (n) &#123; case 2: a[left + 2] = a[left + 1]; case 1: a[left + 1] = a[left]; break; default: System.arraycopy(a, left, a, left + 1, n); &#125; a[left] = pivot; &#125; &#125; private static int countRunAndMakeAscending(int[] a, int lo, int hi) &#123; assert lo &lt; hi; int runHi = lo + 1; if (runHi == hi) &#123; return 1; &#125; // Find end of run, and reverse range if descending // Descending if (a[runHi++]&lt; a[lo]) &#123; while (runHi &lt; hi &amp;&amp; a[runHi]&lt; a[runHi - 1]) &#123; runHi++; &#125; reverseRange(a, lo, runHi); // Ascending &#125; else &#123; while (runHi &lt; hi &amp;&amp; a[runHi]&gt;= a[runHi - 1]) &#123; runHi++; &#125; &#125; return runHi - lo; &#125; private static void reverseRange(int[] a, int lo, int hi) &#123; hi--; while (lo &lt; hi) &#123; int t = a[lo]; a[lo++] = a[hi]; a[hi--] = t; &#125; &#125; private static int minRunLength(int n) &#123; assert n &gt;= 0; // Becomes 1 if any 1 bits are shifted off int r = 0; while (n &gt;= MIN_MERGE) &#123; r |= (n &amp; 1); n &gt;&gt;= 1; &#125; return n + r; &#125; private void pushRun(int runBase, int runLen) &#123; this.runBase[stackSize] = runBase; this.runLen[stackSize] = runLen; stackSize++; &#125; private void mergeCollapse() &#123; while (stackSize &gt; 1) &#123; int n = stackSize - 2; if (n &gt; 0 &amp;&amp; runLen[n-1] &lt;= runLen[n] + runLen[n+1]) &#123; if (runLen[n - 1] &lt; runLen[n + 1]) &#123; n--; &#125; mergeAt(n); &#125; else if (runLen[n] &lt;= runLen[n + 1]) &#123; mergeAt(n); &#125; else &#123; break; // Invariant is established &#125; &#125; &#125; private void mergeForceCollapse() &#123; while (stackSize &gt; 1) &#123; int n = stackSize - 2; if (n &gt; 0 &amp;&amp; runLen[n - 1] &lt; runLen[n + 1]) &#123; n--; &#125; mergeAt(n); &#125; &#125; private void mergeAt(int i) &#123; assert stackSize &gt;= 2; assert i &gt;= 0; assert i == stackSize - 2 || i == stackSize - 3; int base1 = runBase[i]; int len1 = runLen[i]; int base2 = runBase[i + 1]; int len2 = runLen[i + 1]; assert len1 &gt; 0 &amp;&amp; len2 &gt; 0; assert base1 + len1 == base2; runLen[i] = len1 + len2; if (i == stackSize - 3) &#123; runBase[i + 1] = runBase[i + 2]; runLen[i + 1] = runLen[i + 2]; &#125; stackSize--; int k = gallopRight(a[base2], a, base1, len1, 0); assert k &gt;= 0; base1 += k; len1 -= k; if (len1 == 0) &#123; return; &#125; len2 = gallopLeft(a[base1 + len1 - 1], a, base2, len2, len2 - 1); assert len2 &gt;= 0; if (len2 == 0) &#123; return; &#125; // Merge remaining runs, using tmp array with min(len1, len2) elements if (len1 &lt;= len2) &#123; mergeLo(base1, len1, base2, len2); &#125;else &#123; mergeHi(base1, len1, base2, len2); &#125; &#125; private static int gallopLeft(int key, int[] a, int base, int len, int hint) &#123; assert len &gt; 0 &amp;&amp; hint &gt;= 0 &amp;&amp; hint &lt; len; int lastOfs = 0; int ofs = 1; if (key&gt; a[base + hint]) &#123; // Gallop right until a[base+hint+lastOfs] &lt; key &lt;= a[base+hint+ofs] int maxOfs = len - hint; while (ofs &lt; maxOfs &amp;&amp; key&gt; a[base + hint + ofs]) &#123; lastOfs = ofs; ofs = (ofs &lt;&lt; 1) + 1; // int overflow if (ofs &lt;= 0) &#123; ofs = maxOfs; &#125; &#125; if (ofs &gt; maxOfs) &#123; ofs = maxOfs; &#125; // Make offsets relative to base lastOfs += hint; ofs += hint; &#125; else &#123; // key &lt;= a[base + hint] // Gallop left until a[base+hint-ofs] &lt; key &lt;= a[base+hint-lastOfs] final int maxOfs = hint + 1; while (ofs &lt; maxOfs &amp;&amp; key&lt;= a[base + hint - ofs]) &#123; lastOfs = ofs; ofs = (ofs &lt;&lt; 1) + 1; // int overflow if (ofs &lt;= 0) &#123; ofs = maxOfs; &#125; &#125; if (ofs &gt; maxOfs) &#123; ofs = maxOfs; &#125; // Make offsets relative to base int tmp = lastOfs; lastOfs = hint - ofs; ofs = hint - tmp; &#125; assert -1 &lt;= lastOfs &amp;&amp; lastOfs &lt; ofs &amp;&amp; ofs &lt;= len; lastOfs++; while (lastOfs &lt; ofs) &#123; int m = lastOfs + ((ofs - lastOfs) &gt;&gt;&gt; 1); if (key&gt; a[base + m]) &#123; // a[base + m] &lt; key lastOfs = m + 1; &#125;else &#123; // key &lt;= a[base + m] ofs = m; &#125; &#125; // so a[base + ofs - 1] &lt; key &lt;= a[base + ofs] assert lastOfs == ofs; return ofs; &#125; private static int gallopRight(int key, int[] a, int base, int len,int hint) &#123; assert len &gt; 0 &amp;&amp; hint &gt;= 0 &amp;&amp; hint &lt; len; int ofs = 1; int lastOfs = 0; if (key&lt; a[base + hint]) &#123; // Gallop left until a[b+hint - ofs] &lt;= key &lt; a[b+hint - lastOfs] int maxOfs = hint + 1; while (ofs &lt; maxOfs &amp;&amp; key&lt; a[base + hint - ofs]) &#123; lastOfs = ofs; ofs = (ofs &lt;&lt; 1) + 1; // int overflow if (ofs &lt;= 0) &#123; ofs = maxOfs; &#125; &#125; if (ofs &gt; maxOfs) &#123; ofs = maxOfs; &#125; // Make offsets relative to b int tmp = lastOfs; lastOfs = hint - ofs; ofs = hint - tmp; &#125; else &#123; // a[b + hint] &lt;= key // Gallop right until a[b+hint + lastOfs] &lt;= key &lt; a[b+hint + ofs] int maxOfs = len - hint; while (ofs &lt; maxOfs &amp;&amp; key&gt;= a[base + hint + ofs]) &#123; lastOfs = ofs; ofs = (ofs &lt;&lt; 1) + 1; // int overflow if (ofs &lt;= 0) &#123; ofs = maxOfs; &#125; &#125; if (ofs &gt; maxOfs) &#123; ofs = maxOfs; &#125; // Make offsets relative to b lastOfs += hint; ofs += hint; &#125; assert -1 &lt;= lastOfs &amp;&amp; lastOfs &lt; ofs &amp;&amp; ofs &lt;= len; lastOfs++; while (lastOfs &lt; ofs) &#123; int m = lastOfs + ((ofs - lastOfs) &gt;&gt;&gt; 1); if (key&lt; a[base + m]) &#123; // key &lt; a[b + m] ofs = m; &#125;else &#123; // a[b + m] &lt;= key lastOfs = m + 1; &#125; &#125; // so a[b + ofs - 1] &lt;= key &lt; a[b + ofs] assert lastOfs == ofs; return ofs; &#125; private void mergeLo(int base1, int len1, int base2, int len2) &#123; assert len1 &gt; 0 &amp;&amp; len2 &gt; 0 &amp;&amp; base1 + len1 == base2; // Copy first run into temp array // For performance int[] a = this.a; int[] tmp = ensureCapacity(len1); // Indexes into tmp array int cursor1 = tmpBase; // Indexes int a int cursor2 = base2; // Indexes int a int dest = base1; System.arraycopy(a, base1, tmp, cursor1, len1); // Move first element of second run and deal with degenerate cases a[dest++] = a[cursor2++]; if (--len2 == 0) &#123; System.arraycopy(tmp, cursor1, a, dest, len1); return; &#125; if (len1 == 1) &#123; System.arraycopy(a, cursor2, a, dest, len2); // Last elt of run 1 to end of merge a[dest + len2] = tmp[cursor1]; return; &#125; int minGallop = this.minGallop; outer: while (true) &#123; // Number of times in a row that first run won int count1 = 0; // Number of times in a row that second run won int count2 = 0; do &#123; assert len1 &gt; 1 &amp;&amp; len2 &gt; 0; if (a[cursor2]&lt; tmp[cursor1]) &#123; a[dest++] = a[cursor2++]; count2++; count1 = 0; if (--len2 == 0) &#123; break outer; &#125; &#125; else &#123; a[dest++] = tmp[cursor1++]; count1++; count2 = 0; if (--len1 == 1) &#123; break outer; &#125; &#125; &#125; while ((count1 | count2) &lt; minGallop); do &#123; assert len1 &gt; 1 &amp;&amp; len2 &gt; 0; count1 = gallopRight(a[cursor2], tmp, cursor1, len1, 0); if (count1 != 0) &#123; System.arraycopy(tmp, cursor1, a, dest, count1); dest += count1; cursor1 += count1; len1 -= count1; // len1 == 1 || len1 == 0 if (len1 &lt;= 1) &#123; break outer; &#125; &#125; a[dest++] = a[cursor2++]; if (--len2 == 0) &#123; break outer; &#125; count2 = gallopLeft(tmp[cursor1], a, cursor2, len2, 0); if (count2 != 0) &#123; System.arraycopy(a, cursor2, a, dest, count2); dest += count2; cursor2 += count2; len2 -= count2; if (len2 == 0) &#123; break outer; &#125; &#125; a[dest++] = tmp[cursor1++]; if (--len1 == 1) &#123; break outer; &#125; minGallop--; &#125; while (count1 &gt;= MIN_GALLOP | count2 &gt;= MIN_GALLOP); if (minGallop &lt; 0) &#123; minGallop = 0; &#125; // Penalize for leaving gallop mode minGallop += 2; &#125; // End of "outer" loop // Write back to field this.minGallop = minGallop &lt; 1 ? 1 : minGallop; if (len1 == 1) &#123; assert len2 &gt; 0; System.arraycopy(a, cursor2, a, dest, len2); // Last elt of run 1 to end of merge a[dest + len2] = tmp[cursor1]; &#125; else if (len1 == 0) &#123; throw new IllegalArgumentException( "Comparison method violates its general contract!"); &#125; else &#123; assert len2 == 0; assert len1 &gt; 1; System.arraycopy(tmp, cursor1, a, dest, len1); &#125; &#125; private void mergeHi(int base1, int len1, int base2, int len2) &#123; assert len1 &gt; 0 &amp;&amp; len2 &gt; 0 &amp;&amp; base1 + len1 == base2; // Copy second run into temp array // For performance int[] a = this.a; int[] tmp = ensureCapacity(len2); int tmpBase = this.tmpBase; System.arraycopy(a, base2, tmp, tmpBase, len2); // Indexes into a int cursor1 = base1 + len1 - 1; // Indexes into tmp array int cursor2 = tmpBase + len2 - 1; // Indexes into a int dest = base2 + len2 - 1; // Move last element of first run and deal with degenerate cases a[dest--] = a[cursor1--]; if (--len1 == 0) &#123; System.arraycopy(tmp, tmpBase, a, dest - (len2 - 1), len2); return; &#125; if (len2 == 1) &#123; dest -= len1; cursor1 -= len1; System.arraycopy(a, cursor1 + 1, a, dest + 1, len1); a[dest] = tmp[cursor2]; return; &#125; int minGallop = this.minGallop; outer: while (true) &#123; // Number of times in a row that first run won int count1 = 0; // Number of times in a row that second run won int count2 = 0; do &#123; assert len1 &gt; 0 &amp;&amp; len2 &gt; 1; if (tmp[cursor2]&lt; a[cursor1]) &#123; a[dest--] = a[cursor1--]; count1++; count2 = 0; if (--len1 == 0) &#123; break outer; &#125; &#125; else &#123; a[dest--] = tmp[cursor2--]; count2++; count1 = 0; if (--len2 == 1) &#123; break outer; &#125; &#125; &#125; while ((count1 | count2) &lt; minGallop); do &#123; assert len1 &gt; 0 &amp;&amp; len2 &gt; 1; count1 = len1 - gallopRight(tmp[cursor2], a, base1, len1, len1 - 1); if (count1 != 0) &#123; dest -= count1; cursor1 -= count1; len1 -= count1; System.arraycopy(a, cursor1 + 1, a, dest + 1, count1); if (len1 == 0) &#123; break outer; &#125; &#125; a[dest--] = tmp[cursor2--]; if (--len2 == 1) &#123; break outer; &#125; count2 = len2 - gallopLeft(a[cursor1], tmp, tmpBase, len2, len2 - 1); if (count2 != 0) &#123; dest -= count2; cursor2 -= count2; len2 -= count2; System.arraycopy(tmp, cursor2 + 1, a, dest + 1, count2); // len2 == 1 || len2 == 0 if (len2 &lt;= 1) &#123; break outer; &#125; &#125; a[dest--] = a[cursor1--]; if (--len1 == 0) &#123; break outer; &#125; minGallop--; &#125; while (count1 &gt;= MIN_GALLOP | count2 &gt;= MIN_GALLOP); if (minGallop &lt; 0) &#123; minGallop = 0; &#125; // Penalize for leaving gallop mode minGallop += 2; &#125; // End of "outer" loop // Write back to field this.minGallop = minGallop &lt; 1 ? 1 : minGallop; if (len2 == 1) &#123; assert len1 &gt; 0; dest -= len1; cursor1 -= len1; System.arraycopy(a, cursor1 + 1, a, dest + 1, len1); // Move first elt of run2 to front of merge a[dest] = tmp[cursor2]; &#125; else if (len2 == 0) &#123; throw new IllegalArgumentException( "Comparison method violates its general contract!"); &#125; else &#123; assert len1 == 0; assert len2 &gt; 0; System.arraycopy(tmp, tmpBase, a, dest - (len2 - 1), len2); &#125; &#125; private int[] ensureCapacity(int minCapacity) &#123; if (tmpLen &lt; minCapacity) &#123; // Compute smallest power of 2 &gt; minCapacity int newSize = minCapacity; newSize |= newSize &gt;&gt; 1; newSize |= newSize &gt;&gt; 2; newSize |= newSize &gt;&gt; 4; newSize |= newSize &gt;&gt; 8; newSize |= newSize &gt;&gt; 16; newSize++; // Not bloody likely! if (newSize &lt; 0) &#123; newSize = minCapacity; &#125;else &#123; newSize = Math.min(newSize, a.length &gt;&gt;&gt; 1); &#125; tmp = new int[newSize]; tmpLen = newSize; tmpBase = 0; &#125; return tmp; &#125; public static void main(String[] args) &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100000000); &#125; System.out.println("TimSort排序开始："); long start = System.currentTimeMillis(); sort(a,0,a.length,null,0,0); System.out.println("TimSort耗时："+(System.currentTimeMillis()-start)+"ms"); System.out.println("TimSort排序完成！"); System.out.println("数组是否有序："+isOrdered(a)); &#125;&#125; 有如下结果，排序1亿数据耗时在17s左右。 1234TimSort排序开始：TimSort耗时：17831msTimSort排序完成！数组是否有序：true TimSort的时间复杂度如下： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n * log n) 时间复杂度（最差）：O(n * log n) TimSort的空间复杂度：O(n) TimSort是一种稳定排序算法。 TimSort之所以能成为Java的内置排序算法之一，除了其优秀的性能，另一点就在于它的稳定性了。 我们可以对大量随机数据进行测试，虽然TimSort和快排（或者其变种）具有相同的时间复杂度【平均 O(n * log n)】，但实际数据显示快排还是要快一些的。 但是快排的缺点是它的不稳定性，比如100个人按名字进行排序，有两个叫张三的，张三(1)无序下是在张三(2)之前的，快排完后可能他们的顺序就变化了，这在某些情况下可能会有问题。 我们来看下TimSort排序动图： 图中可以清晰的看到TimSort的排序过程。 其他在util包下除了TimSort，我们还可以看到一个类 ComparableTimSort，它是针对未实现 Comparator 接口的数据的排序版本，如Object[]。 我们对于一个int[] a，调用数组的 Arrays.sort(a)，会使用TimSort吗？ 答案是否定的，对于一些基本数据类型数组，两个相同的数的前后顺序不会造成任何影响，所以Arrays.sort(a)里使用了另一种更快速的排序算法DualPivotQuicksort。 我们有时间再看一下DualPivotQuicksort这个排序。 总结本篇文章通过对TimSort的分析，了解了TimSort的工作运行原理，对Java内置的排序算法有了更深的了解。 后面我们将看下Java的sort接口实现，看看Java内部是如何进一步优化排序，提高效率的。 参考 JDK1.8 java.util.TimSort、java.util.ComparableTimSort源码 JDK1.7 java.util.TimSort源码 Python TimSort listsort.txt OpenJDK’s java.utils.Collection.sort() is broken:The good, the bad and the worst case? 源码地址上述文中涉及到的代码可见于我的 GitHub]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
        <tag>TimSort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（五）-双调排序]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89-%E5%8F%8C%E8%B0%83%E6%8E%92%E5%BA%8F.html</url>
    <content type="text"><![CDATA[前言续我们 排序算法（四） 所说的那样，今天我们来看下一种很适合多路归并的排序算法，双调排序（BitonicSort）。 正文双调排序（BitonicSort）原理及简介双调排序（BitonicSort）的原理是使用双调序列（Bitonic sequence）的性质来进行排序。 那什么是双调序列呢？ 如果一个序列A=[X0, X1, X2, ...... , Xn-1]，存在一个下标i（0≤i≤n-1），使得： X0 ≤ X1 ≤ ...... ≤ Xi, and Xi ≥ Xi+1 ≥ ...... ≥ Xn-1 那么我们称这个序列是双调的（Bitonic）。 如下面的数据格式： [5,9,10,17,6,3,2,1] 需要注意的几点： 一个序列如果是完全的非降序或非升序的，它也是Bitonic的。（i为0或者n-1时） Bitonic序列的子序列仍为Bitonic的。 将一个Bitonic序列进行循环移位操作后，也是Bitonic序列。 任意两个实数，都可以组成双调序列。 如下图： 因此可以知道，如果一个序列循环移位后可以表示成 up -&gt; down 形式，那么它就是Bitonic的。 如果一个无序序列（满足长度为2的幂），则可以从最小的Bitonic序列（数组中的每两个数为一个Bitonic序列）开始，将整个序列变为Bitonic的。 我们再来看下Batcher定理，这个定理用于双调排序的双调合并。 Batcher定理 将任意一个长为2n的双调序列A分为等长的两半X和Y，将X中的元素与Y中的元素一一按原序比较，即a[i]与a[i+n] (i &lt; n)比较，将较大者放入MAX序列，较小者放入MIN序列。则得到的MAX和MIN序列仍然是双调序列，并且MAX序列中的任意一个元素不小于MIN序列中的任意一个元素。 PS：这个定理的证明和 0-1序列 有关，有兴趣的可以查阅相关资料。 PS: 0-1序列，一个只有0，1元素的序列。 根据这个定理，我们继续下去，如果MAX和MIN也是长为2m的，那么它们可以继续拆分成 MAX(max),MAX(min)和MIN(max)和MIN(min)……一直继续下去，直到要拆分的序列长度为1，这时候整个序列A会完全有序排列。 这个定理非常有意思，我们来看下。 比如对于一个序列 A [3,5,7,9,10,17,19,22,17,15,12,10,9,8,5,1]，明显看出它是一个双调序列，其中X为[3,5,7,9,10,17,19,22]单调递增，Y为[17,15,12,10,9,8,5,1]单调递减，且X序列长度等于Y序列长度，我们试着按照上面定理将数据按照原序进行比较，得到MAX和MIN两个序列，它们分别为 MAX [17,15,12,10,10,17,19,22]，MIN [3,5,7,9,9,8,5,1]，对MAX和MIN按照Batcher定理继续拆分…… 我们将上述过程用图的形式描述出来，对于无序数组 [8,12,6,18,9,1,3,4]，其排序过程如下： 代码实现上述过程我们使用Java代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class BitonicSort &#123; /** * 升降序标志 * true 升序 * false 降序 */ private final static boolean ASCENDING = true, DESCENDING = false; public static void bitonicSort(int[] a) &#123; sort(a, 0, a.length, ASCENDING); &#125; private static void sort(int[] a, int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = n / 2; sort(a, lo, m, ASCENDING); sort(a, lo + m, m, DESCENDING); bitonicMerge(a, lo, n, dir); &#125; &#125; private static void bitonicMerge(int[] a, int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = n / 2; for (int i = lo; i &lt; lo + m; i++) &#123; compare(a, i, i + m, dir); &#125; bitonicMerge(a, lo, m, dir); bitonicMerge(a, lo + m, m, dir); &#125; &#125; private static void compare(int[] a, int i, int j, boolean dir) &#123; if (dir == (a[i] &gt; a[j])) &#123; exchange(a, i, j); &#125; &#125; private static void exchange(int[] a, int i, int j) &#123; int t = a[i]; a[i] = a[j]; a[j] = t; &#125; public static void main(String[] args) &#123; int[] a = new int[128]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("BitonicSort排序前："); System.out.println(Arrays.toString(a)); bitonicSort(a); System.out.println("BitonicSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 动图演示我们来看下排序动图，也很直观，可以看到非常明显的双调序列变化过程。 缺陷及优化上面的代码我们可以发现当数组长度为 2^n 时才会排序正确，长度不为2的幂的情况下排序是不正确的。 这一点是非常好理解的，对于我们上面双调序列和Batcher定理来说，只有当数组长度为2的幂的情况下数组才会被正确分割运算下去。 但是现实中数据长度大部分都不是2的幂次的，如何让我们的双调排序适应非2的幂次的长度的数据呢？ n!=2^k数据的双调排序方案改造数据长度有一些想法是改造数组，将数据填充到2的幂次长度，填充值为最大值，排序完成后，删除数组尾部填充长度的数据。 这种做法有一个明显缺陷，比如一个长度为1025的数据，我们要填充到2048长度才能解决问题，对于更大的数据，我们或许填充的更多，这显然不是一个好办法。 那如果只排序前 2^n 部分，对于剩余部分使用经典排序（插入排序、选择排序）处理呢？ 这种做法如果对于一个长度为2047的数组，使用双调排序排1024部分，剩下的1023用经典排序处理，好像也不是很完美。 如果开始判断数组长度，接近高次2的幂的数据就用填充法，接近低次2的幂的数据就用部分双调排序法呢？好像也不是特别理想。 n!=2^k数据的双调排序算法下面我们介绍一种对于任意n的双调排序网络算法，它的定理来源于Batcher定理的引申。 设想如果我们的序列A长度n!=2^k，我们可以找到一个最小的2的幂次长度p=2^q，使得p&gt;n，这时候p-n部分填充最大值Max，同时p/2 之前的序列单调递减，p/2 之后的队列单调递增，如下： [8,7,6,5,4,3,2,1,7] [8,7,6,5,4,3,2,1,7,Max,Max,Max,Max,Max,Max,Max] 我们使用Batcher定理可得到如下序列： X [7,7,6,5,4,3,2,1] Y[8,Max,Max,Max,Max,Max,Max,Max] -&gt;[8] 可以发现Y序列的每一个值都是不小于X序列的每一个值的，即 Xi &lt;= Yj。 我们可以用图表示这个过程，如下： 我们看下[8,12,6,18,9,1,3]这个数组的排序模拟过程。 可以看到，我们虚构出来的Max部分是始终不参与交换的。 在实际排序中，我们不用虚构Max部分，直接将数组长度/2 进行分割，保证左半部分单调递减，右半部分单调递增，再使用Batcher定理，就会得到需要的已排序序列。 我们可以写出适用于任意数据长度的双调排序算法代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class BitonicBetterSort &#123; /** * sorting direction */ private final static boolean ASCENDING = true; public static void bitonicSort(int[] a) &#123; sort(a,0, a.length, ASCENDING); &#125; private static void sort(int[] a, int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = n / 2; sort(a,lo, m, !dir); sort(a,lo + m, n - m, dir); bitonicMerge(a,lo, n, dir); &#125; &#125; private static void bitonicMerge(int[] a,int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = greatestPowerOfTwoLessThan(n); for (int i = lo; i &lt; lo + n - m; i++) &#123; compare(a,i, i + m, dir); &#125; bitonicMerge(a,lo, m, dir); bitonicMerge(a,lo + m, n - m, dir); &#125; &#125; private static void compare(int[] a,int i, int j, boolean dir) &#123; if (dir == (a[i] &gt; a[j])) &#123; exchange(a,i, j); &#125; &#125; private static void exchange(int[] a,int i, int j) &#123; int t = a[i]; a[i] = a[j]; a[j] = t; &#125; private static int greatestPowerOfTwoLessThan(int n) &#123; int k = 1; while (k &lt; n) &#123; k = k &lt;&lt; 1; &#125; return k &gt;&gt; 1; &#125; public static void main(String[] args) &#123; int[] a = new int[9]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("BitonicSort排序前："); System.out.println(Arrays.toString(a)); bitonicSort(a); System.out.println("BitonicSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 下面的动图给出了数组长度为25的数组排序演示： 算法复杂度为了从两个长度为n/2的排序序列中形成长度为n的排序序列，需要进行log(n)次比较。整个排序网络的比较方程T(n)为: T(n) = log(n) + T(n/2) 这个递归方程的解是： T(n) = log(n) + log(n)-1 + log(n)-2 +…+1 = log(n)·(log(n)+1) / 2 排序网络的每个阶段需要进行n/2个比较。 总的来说，复杂度为 [log(n)·(log(n)+1) / 2] · n/2 = [ n·log(n)·(log(n)+1) ]/4 = O(n·(log n)^2) 算法的空间复杂度为O(1)。 该算法是一种不稳定排序算法。 以上是在串行运行下的复杂度情况，可以看出它比一般的归并排序（O(n·(log n))）要慢。 并行条件下的双调排序既然说到双调排序十分适用于并行排序，我们就简单改造下代码，使其使用并行排序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122public class BitonicParallelSort &#123; /** * 可用CPU核数 */ private static final int NCPU = Runtime.getRuntime().availableProcessors(); /** * 分片数，取偶数，如果核数过少就使用默认值 */ private static final int MIN_PROCESSORS = NCPU &gt;= 16 ? (NCPU % 2 == 0 ? NCPU : NCPU + 1) : 16; /** * 工作线程池 */ private final static ExecutorService executorService = Executors.newCachedThreadPool(); /** * 分片大小 */ private static int PIECE = 10000; /** * sorting direction */ private final static boolean ASCENDING = true; public static void bitonicSort(int[] a) &#123; //初始化分片大小 PIECE = a.length/MIN_PROCESSORS; sort(a,0, a.length, ASCENDING); executorService.shutdown(); &#125; private static void sort(int[] a, int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = n / 2; if(n &gt; PIECE)&#123; Future future1 = parallelSort(a, lo, m, !dir); Future future2 = parallelSort(a, lo + m, n - m, dir); while (true)&#123; if(future1.isDone()&amp;&amp;future2.isDone())&#123; break; &#125; &#125; &#125;else&#123; sort(a, lo, m, !dir); sort(a, lo + m, n - m, dir); &#125; bitonicMerge(a, lo, n, dir); &#125; &#125; private static void bitonicMerge(int[] a, int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = greatestPowerOfTwoLessThan(n); for (int i = lo; i &lt; lo + n - m; i++) &#123; compare(a, i, i + m, dir); &#125; if(n &gt; PIECE)&#123; Future future1 = parallelMerge(a, lo, m, dir); Future future2 = parallelMerge(a, lo + m, n - m, dir); while (true)&#123; if(future1.isDone()&amp;&amp;future2.isDone())&#123; break; &#125; &#125; &#125;else&#123; bitonicMerge(a, lo, m, dir); bitonicMerge(a, lo + m, n - m, dir); &#125; &#125; &#125; private static void compare(int[] a, int i, int j, boolean dir) &#123; if (dir == (a[i] &gt; a[j])) &#123; exchange(a, i, j); &#125; &#125; private static void exchange(int[] a, int i, int j) &#123; int t = a[i]; a[i] = a[j]; a[j] = t; &#125; private static int greatestPowerOfTwoLessThan(int n) &#123; int k = 1; while (k &lt; n) &#123; k = k &lt;&lt; 1; &#125; return k &gt;&gt; 1; &#125; private static Future parallelSort(int[] a, int lo, int n, boolean dir) &#123; return executorService.submit(()-&gt;sort(a,lo,n,dir)); &#125; private static Future parallelMerge(int[] a, int lo, int n, boolean dir)&#123; return executorService.submit(()-&gt;bitonicMerge(a,lo,n,dir)); &#125; public static void main(String[] args) &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100000000); &#125; System.out.println("BitonicParallelSort排序开始："); long start = System.currentTimeMillis(); bitonicSort(a); System.out.println("BitonicParallelSort耗时："+(System.currentTimeMillis()-start)+"ms"); System.out.println("BitonicParallelSort排序完成！"); System.out.println("数组是否有序："+isOrdered(a)); &#125; public static boolean isOrdered(int[] a)&#123; for(int i=0;i&lt;a.length -1;i++)&#123; if(a[i]&lt;=a[i+1] != ASCENDING)&#123; System.out.println(a[i]+"....."+a[i+1]); return false; &#125; &#125; return true; &#125;&#125; 我这边使用了线程池来进行处理，我们定义了最小分片长度，这个长度需要初始化，一般取数组长度与每级分片数量之比； 可以看到我们上面生成双调序列（sort方法）和Batcher合并（bitonicMerge方法）均使用了多线程进行处理。 上面的方法按照理论上应该比正常的串行双调排序快1倍时间。 我们来验证下，我们准备1亿数据量，数据大小区间在[0 - 100000000]，对上述的 BitonicBetterSort（串行）和 BitonicParallelSort（并行）进行测试验证。 测试结果大致如下： 可以看到BitonicBetterSort（串行）耗时在55s左右，BitonicParallelSort（并行）耗时在30s左右，还是比较符合预期的。 我们看到，无论sort方法还是bitonicMerge方法，并行情况下（N &gt; PEICE），递归的每级都丢给了两个线程（一个负责升序排列的线程、一个负责降序排列的线程）。 原来的串行程序是执行完升序，再执行降序；现在是升序降序同时执行，所以快1倍时间由此得来。 我们可以看下双调并行排序动图演示过程。 并行条件下的双调排序调优以上就是双调排序（BitonicSort）的全部内容，通过这篇文章我们了解到了双调排序的原理和一些特点。 PS：本来这儿的标题是总结，但是写着写着又想到了一些东西，就把标题改了。 双调排序可以说是非常适合并行运行的一种排序算法了，它的无论开始先生成双调序列的方法，还是后面Batcher合并，都可以做到在同一数据组里处理而不相互干扰。 就如对于一个长度为64的序列，分为前32部分和后32部分，这两部分最后生成1个升序序列和一个降序序列（或者一个降序一个升序）即可，最后它们在进行Batcher合并，因此双调序列并行情况下也可以结合其他一些就地排序算法进行处理。 比如我们上述并行代码中，当数据长度不足以分片时，我们是继续按照双调排序进行的，其实这样效率会降低。 比如1亿量的数据，如果我们分了1000次片，每个片就有10w数据，对于这些数据，我们目的就是生成一个升序（或者降序）的序列，此时使用串行的双调排序效率不高，继续分片的话线程数量（系统开销）又大大增加，得到的时间提升甚微。 这种情况我们就可以使用一些O(n * log n)级别的就地排序算法，比如快速排序。 我下面写了一个当分片不足时采用并行快排的双调并行排序算法代码，大家可以看下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182public class BitonicParallelMixtureSort &#123; /** * 可用CPU核数 */ private static final int NCPU = Runtime.getRuntime().availableProcessors(); /** * 分片数，取偶数，如果核数过少就使用默认值 */ private static final int MIN_PROCESSORS = NCPU &gt;= 16 ? (NCPU % 2 == 0 ? NCPU : NCPU + 1) : 16; /** * 工作线程池 */ private final static ExecutorService executorService = Executors.newCachedThreadPool(); /** * 分片大小 */ private static int PIECE = 10000; /** * sorting direction */ private final static boolean ASCENDING = true; public static void bitonicSort(int[] a) &#123; //初始化分片大小 PIECE = a.length/MIN_PROCESSORS; sort(a,0, a.length, ASCENDING); executorService.shutdown(); &#125; private static void sort(int[] a, int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = n / 2; if(n &gt; PIECE)&#123; Future future1 = parallelSort(a, lo, m, !dir); Future future2 = parallelSort(a, lo + m, n - m, dir); while (true)&#123; if(future1.isDone()&amp;&amp;future2.isDone())&#123; break; &#125; &#125; &#125;else&#123; Future future1 = parallelQuickSort(a, lo, lo + m - 1, !dir); Future future2 = parallelQuickSort(a, lo + m, lo + n - 1, dir); while (true)&#123; if(future1.isDone()&amp;&amp;future2.isDone())&#123; break; &#125; &#125; &#125; bitonicMerge(a, lo, n, dir); &#125; &#125; private static void bitonicMerge(int[] a, int lo, int n, boolean dir) &#123; if (n &gt; 1) &#123; int m = greatestPowerOfTwoLessThan(n); for (int i = lo; i &lt; lo + n - m; i++) &#123; compare(a, i, i + m, dir); &#125; if(n &gt; PIECE)&#123; Future future1 = parallelMerge(a, lo, m, dir); Future future2 = parallelMerge(a, lo + m, n - m, dir); while (true)&#123; if(future1.isDone()&amp;&amp;future2.isDone())&#123; break; &#125; &#125; &#125;else&#123; bitonicMerge(a, lo, m, dir); bitonicMerge(a, lo + m, n - m, dir); &#125; &#125; &#125; private static void compare(int[] a, int i, int j, boolean dir) &#123; if (dir == (a[i] &gt; a[j])) &#123; exchange(a, i, j); &#125; &#125; private static void exchange(int[] a, int i, int j) &#123; int t = a[i]; a[i] = a[j]; a[j] = t; &#125; private static int greatestPowerOfTwoLessThan(int n) &#123; int k = 1; while (k &lt; n) &#123; k = k &lt;&lt; 1; &#125; return k &gt;&gt; 1; &#125; private static Future parallelSort(int[] a, int lo, int n, boolean dir) &#123; return executorService.submit(()-&gt;sort(a,lo,n,dir)); &#125; private static Future parallelMerge(int[] a, int lo, int n, boolean dir)&#123; return executorService.submit(()-&gt;bitonicMerge(a,lo,n,dir)); &#125; private static Future parallelQuickSort(int[] a,int low,int high,boolean dir)&#123; return executorService.submit(()-&gt;quickSort(a,low,high,dir)); &#125; /** * 快排 * @param a * @param low * @param high * @param dir true 表示从小到大 正序排序 */ private static void quickSort(int[] a,int low,int high,boolean dir)&#123; if(low&lt;high)&#123; //将表一分为二 int privotLoc=partition(a,low,high,dir); quickSort(a,low,privotLoc-1,dir); quickSort(a,privotLoc+1,high,dir); &#125; &#125; private static int partition(int[] a, int low, int high, boolean dir) &#123; int pivot = a[high]; while (low &lt; high) &#123; //升序 if (dir) &#123; while (a[low] &lt; pivot &amp;&amp; low &lt; high) &#123; low++; &#125; if (low &lt; high) &#123; a[high--] = a[low]; &#125; while (a[high] &gt; pivot &amp;&amp; low &lt; high) &#123; high--; &#125; if (low &lt; high) &#123; a[low++] = a[high]; &#125; &#125; else &#123; //降序 while (a[low] &gt; pivot &amp;&amp; low &lt; high) &#123; low++; &#125; if (low &lt; high) &#123; a[high--] = a[low]; &#125; while (a[high] &lt; pivot &amp;&amp; low &lt; high) &#123; high--; &#125; if (low &lt; high) &#123; a[low++] = a[high]; &#125; &#125; &#125; a[low] = pivot; return low; &#125; public static void main(String[] args) &#123; int[] a = new int[100000000]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(100000000); &#125; System.out.println("BitonicParallelMixtureSort排序开始："); long start = System.currentTimeMillis(); bitonicSort(a); System.out.println("BitonicParallelMixtureSort耗时："+(System.currentTimeMillis()-start)+"ms"); System.out.println("BitonicParallelMixtureSort排序完成！"); System.out.println("数组是否有序："+isOrdered(a)); &#125; public static boolean isOrdered(int[] a)&#123; for(int i=0;i&lt;a.length -1;i++)&#123; if(a[i]&lt;=a[i+1] != ASCENDING)&#123; System.out.println(a[i]+"....."+a[i+1]); return false; &#125; &#125; return true; &#125;&#125; 我们同样运行1亿数据量，测试该算法的耗时，如下： 可以看到处理1亿数据时间缩短到了15s，相比之前又提高了一倍。 上述代码主要在分片小于a.length/MIN_PROCESSORS的地方进行了优化，由于两部分不冲突，快速排序部分也使用了并行处理。 可以看到，我们不知不觉的写了一个属于自己的混合排序算法，emmmm….. 当然，这里不是结束，算法还是可以优化的，比如当处理到的数据过小时（比如处理长度小于16），快排的优势无法显著体现，我们这时候可以使用插入排序处理数据等，可以参考内省排序（IntroSort）的处理。 总结说到这里，基本上算是结束了，本文通过介绍双调排序，理解了它的运行原理的同时，又提供了一个并行版本，以及并行版本的一些优化，让我们了解到了排序算法非常有意思的一些地方。 本来以为到这儿就结束了，我手贱测试了下快排和内省排序1亿数据量的运行情况。 如下图： 内省排序不愧是C++标准模板库的排序算法，只耗时10s就完成了，比我们的并行排序要快，哈哈！！ 快速排序耗时12s左右，瞬间感觉到将快排应用到我们双调并行排序中是大材小用了。 同时也可以看到内省排序确实对快排做了优化，数据量越大越明显。 总的来说，快排大法好（内省也主要使用了快排）！ 源码本文中提到的所有源代码均可以在我的Github上看到。 Github-JavaZWT。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
        <tag>双调排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（四）]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言上篇文章排序算法（三）我们介绍了7种排序算法。 可以知道到目前我们已经介绍了27种排序算法，当然，不仅仅如此，还有若干排序算法，我们今天继续来看一下。 正文今天我们介绍4种排序算法，它们如下： Sort Name Time(Best) Time(Average) Time(Worst) Memory Stable IntroSort O(n * log n) O(n * log n) O(n * log n) O(log n) No SmoothSort O(n) O(n * log n) O(n * log n) O(1) No TreeSelectionSort O(n * log n) O(n * log n) O(n * log n) O(n) Yes AmericanFlagSort O(n * k/d) O(n * k/d) O(n * k/d) O(1) No 我们分别来看下。 内省排序（IntroSort）简介及原理内省排序（Introspective Sort）是一种比较排序算法，也是一个混合排序算法，是由David Musser在1997年设计的排序算法。 它内部使用了快速排序（QuickSort）、堆排序（HeapSort）和插入排序（InsertionSort）三种排序算法。 该排序算法的主要策略如下： 数据量大时采用快速排序（QuickSort），分段递归排序； 一旦分段后的数据量小于某个阈值，为避免快排的递归调用带来的额外负荷，就改用插入排序（InsertionSort）； 如果递归层次过深，就会采用堆排序（HeapSort）； 三点中值”获取好的数组分割。 关于“三点中值” 内省排序使用的分段递归，每一个数组段都会重复上述 1、2、3 策略部分，内省排序如何对数组进行分段的呢？这就涉及到它的“三点中值”算法了。 我们来看下它的代码： 12345678910111213141516171819private static int median3(int[] array, int first, int median, int end) &#123; if (array[first] &lt; array[median]) &#123; if (array[median] &lt; array[end]) &#123; return median; &#125; else if (array[first] &lt; array[end]) &#123; return end; &#125; else &#123; return first; &#125; &#125; else &#123; if (array[first] &lt; array[end]) &#123; return first; &#125; else if (array[median] &lt; array[end]) &#123; return end; &#125; else &#123; return median; &#125; &#125;&#125; 逻辑比较好理解： 如果数组首部元素比中间元素小： 中间元素小于尾部元素，就取中间的index； 首部元素小于尾部元素，就取尾部的index； 否则取首部的index。 如果数组首部元素比中间元素大或者等于： 首部元素小于尾部元素，就取首部的index； 中间元素小于尾部元素，就取尾部的index； 否则取中间的index。 我们知道快排要确立一个基准元素，对于普通快排，我们取a[low]，这儿内省排序中的快排，基准元素的index为 median3(array, begin, begin + (end - begin) / 2, end)，可以看到是寻找的begin，end和begin + (end - begin) / 2这三点的“三点中值”。 算法描述 检测待排序数组长度，如果大于阈值（默认16），就会采用分段递归排序； 递归最大深度为 2 * lg(array.length) ，对于每一段，都会使用快速排序，快排基准值为数组“三点中值”位置的元素值； 当递归深度过大时，就会采用堆排序； 随着不断分割，递归重复上述排序过程，当数组分割后的长度小于阈值时，就采用插入排序完成最后排序过程。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192public class IntroSort &#123; /** * 数据量的分界线，决定了使用quick sort/heap sort还是insertion sort */ private static final int THRESHOLD = 16; /** * 堆排序用到的辅助函数 * @param i * @return */ private static int parent(int i) &#123; return (i - 1) / 2; &#125; private static int left(int i) &#123; return 2 * i + 1; &#125; private static int right(int i) &#123; return (2 * i + 2); &#125; private static void swap(int[] array, int index1, int index2) &#123; int temp; temp = array[index1]; array[index1] = array[index2]; array[index2] = temp; &#125; private static void heapShiftDown(int[] heap, int i, int begin, int end) &#123; int l = left(i - begin) + begin; int r = right(i - begin) + begin; int largest = i; //找出左右字节点与父节点中的最大者 if (l &lt; end &amp;&amp; heap[l] &gt; heap[largest]) &#123; largest = l; &#125; if (r &lt; end &amp;&amp; heap[r] &gt; heap[largest]) &#123; largest = r; &#125; //若最大者不为父节点，则需交换数据，并持续向下滚动至满足最大堆特性 if (largest != i) &#123; swap(heap, largest, i); heapShiftDown(heap, largest, begin, end); &#125; &#125; /** * 自底向上的开始建堆，即从堆的倒数第二层开始 * @param heap * @param begin * @param end */ private static void buildHeap(int[] heap, int begin, int end) &#123; for (int i = (begin + end) / 2; i &gt;= begin; i--) &#123; heapShiftDown(heap, i, begin, end); &#125; &#125; /** * 堆排序 * @param heap * @param begin * @param end */ private static void heapSort(int[] heap, int begin, int end) &#123; buildHeap(heap, begin, end); for (int i = end; i &gt; begin; i--) &#123; swap(heap, begin, i); heapShiftDown(heap, begin, begin, i); &#125; &#125; /** * 插入排序 * @param array * @param len */ private static void insertionSort(int[] array, int len) &#123; int i, j, temp; for (i = 1; i &lt; len; i++) &#123; //store the original sorted array in temp temp = array[i]; //compare the new array with temp(maybe -1?) for (j = i; j &gt; 0 &amp;&amp; temp &lt; array[j - 1]; j--) &#123; //all larger elements are moved one pot to the right array[j] = array[j - 1]; &#125; array[j] = temp; &#125; &#125; /** * 三点中值计算 * @param array * @param first * @param median * @param end * @return */ private static int median3(int[] array, int first, int median, int end) &#123; if (array[first] &lt; array[median]) &#123; return helpMethod(array, first, median, end); &#125; else &#123; return helpMethod(array, median, first, end); &#125; &#125; private static int helpMethod(int[] array, int first, int median, int end) &#123; if (array[median] &lt; array[end]) &#123; return median; &#125; else if (array[first] &lt; array[end]) &#123; return end; &#125; else &#123; return first; &#125; &#125; /** * 对数组分割 * @param array * @param left * @param right * @param p * @return */ private static int partition(int[] array, int left, int right, int p) &#123; //选择最右侧的元素作为分割标准 int index = left; swap(array, p, right); int pivot = array[right]; //将所有小于标准的点移动到index的左侧 for (int i = left; i &lt; right; i++) &#123; if (array[i] &lt; pivot) &#123; swap(array, index++, i); &#125; &#125; //将标准与index指向的元素交换，返回index，即分割位置 swap(array, right, index); return index; &#125; /** * 递归的对数组进行分割排序 * @param array * @param begin * @param end * @param depthLimit */ private static void introSortLoop(int[] array, int begin, int end, int depthLimit) &#123; //子数组数据量大小，则交给后面的插入排序进行处理 while ((end - begin + 1) &gt; THRESHOLD) &#123; //递归深度过大，则由堆排序代替 if (depthLimit == 0) &#123; heapSort(array, begin, end); return; &#125; --depthLimit; //使用quick sort进行排序 int cut = partition(array, begin, end, median3(array, begin, begin + (end - begin) / 2, end)); introSortLoop(array, cut, end, depthLimit); //对左半段进行递归的sort end = cut; &#125; &#125; /** * 计算最大容忍的递归深度 * @param n * @return */ private static int lg(int n) &#123; int k; for (k = 0; n &gt; 1; n &gt;&gt;= 1) &#123; ++k; &#125; return k; &#125; /** * IntroSort排序 * @param array * @param len */ public static void introSort(int[] array, int len) &#123; if (len != 1) &#123; introSortLoop(array, 0, len - 1, lg(len) * 2); insertionSort(array, len); &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("IntroSort排序前："); System.out.println(Arrays.toString(a)); introSort(a,a.length); System.out.println("IntroSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 该排序算法吸取了快排、插入排序、堆排的优点，是一种效率较高的排序算法； 内省排序算法是C++标准模板库 std::sort采用的算法； 该排序算法的时间复杂度： 时间复杂度（最好）： O(n*log n) 时间复杂度（平均）： O(n*log n) 时间复杂度（最差）： O(n*log n) 该排序算法的空间复杂度为：O(log n) 该排序算法为不稳定排序算法 平滑排序（SmoothSort）简介及原理平滑排序（SmoothSort）是一种比较排序算法，是堆排序的一个变体，由 Edsger Dijkstra 在1981年发明并发表。 SmoothSort作为堆排序的变种，与其不同是其引入了斐波那契数列来确定堆排序中子节点的位置。 斐波那契数列（Fibonacci sequence）：又称黄金分割数列、兔子数列，函数表达式为 F[n]=F[n-1]+F[n-2] (n&gt;=3,F[1]=1,F[2]=1)。 可以知道斐波那契数列指的是这样一个数列 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233，377，610，987，1597，2584，4181，6765，10946，17711，28657，46368…….. 我们平滑排序中，使用的数列和其有细微不同，其函数表达式为 F[n]=F[n-1]+F[n-2]+1 (n&gt;=3,F[1]=1,F[2]=1)。 可以得到SmoothSort使用的数列是这样的 1, 1, 3, 5, 9, 15, 25, 41, 67, 109, 177, 287, 465, 753, 1219, 1973,3193, 5167, 8361, 13529, 21891, 35421, 57313…….. 我们来回顾下堆排序原理，堆排序是利用了完全二叉堆（完全二叉树），来在数组里隐式构建一个完全二叉堆。 之所以完全二叉树可以用数组来表示，是因为完全二叉树有一个性质：除了最底层，每一层都是满的，每个结点对应数组中的一个元素。 如下图： 分别是最大完全二叉树堆（特点是父节点的值大于两个小节点的值） 和 最小完全二叉树堆（特点是父节点的值小于两个小节点的值）以及它们分别对应的隐式数组结构。 开始时我们需要构建一个隐式完全二叉堆（从大到小排就构建最小堆、从小到大排就构建最大堆），因为对于一个待排序数组，它目前的顺序不一定符合完全二叉堆的性质。 比如对于数组从小到大排列，堆排序先将数组改造为最大堆，然后将堆顶和堆底元素交换，之后将底部上升，重新构建最大堆，保持最大堆性质。由于堆顶元素必然是堆中最大的元素，所以一次操作之后，堆中存在的最大元素被分离出堆，重复n-1次之后，数组排列完毕。 如下图： 关于构建最大堆（或者最小堆）和堆调整的相关代码，可以参考上面内省排序代码中的buildHeap和heapShiftDown相关方法。 好，回归到SmoothSort，与堆排序类似，Smooth也是首先将数组转为隐式数组堆结构，然后通过重复提取最大剩余元素（从小到大的排序），并在剩余元素上重新构建最大堆，来生成排序数组的。 我们可以看到，我们的堆排序，隐式堆在数组中表示后，父节点一定是在子节点之前，且初始元素是该隐式堆的根节点。 而SmoothSort有所不同的是，它的父节点总是在子节点之后，就相当于将树倒了过来。 比如一个数组[4,7,8,2,3,1,5,6]，使用堆排序时，初始构建的最大堆是这样的[8,7,5,6,3,1,4,2]，这样构建好后，将8与2换位置，数组变为[2,7,5,6,3,1,4][8]，剩余元素[2,7,5,6,3,1,4]在构建最大堆； 而使用SmoothSort后，初始构建的最大堆变为[3,1,6,2,4,7,5,8]，然后分离8，数组变为[3,1,6,2,4,7,5][8]，剩余部分[3,1,6,2,4,7,5]继续构建最大堆。 可以看到这样的话最大数已经在数组正确的位置，减少了不必要的交换（8已经在最后，堆排序的话还要和底部元素2进行交换）。 我们可以看到对于SmoothSort，其隐式数组结构是确定的，与数组本身无关。 PS：相比堆排序，我们需要构造完全二叉树，即每个父节点下面有两个子节点，但是SmoothSort不用这样，它可以先确定隐式二叉树结构，比如使用完全二叉树，就如上面[3,1,6,2,4,7,5,8]这个例子，也可以不使用这种树结构，比如某个父节点下有三个子节点。 Edsger Dijkstra（该算法作者）使用了一种分割方法，就是我们上面提到的斐波那契数列（Fibonacci sequence），这样的话，形成的树的兄弟节点的子节点永远不可能有相同的节点数（除为1时）。我们可以看看堆排序，使用的完全二叉树，除最后底部外，其他的节点其子节点都只有两个子节点，是固定的。 关于为什么没有使用完全二叉树，算法作者认为使用完全二叉树将会和堆排序有相同的算法效率，而且对于大量数据，使用完全二叉树会被分割成更多子树及子树的子树。 隐式树结构的实现可以很容易地通过一次性地计算斐波那契数字列表和父和子关系表(它们完全不依赖于要排序的值)来实现。但是为了使它成为一种就地（不额外占用空间）排序算法，Edsger Dijkstra以一种巧妙的方式维护了固定数量的整数变量，这样在计算的每个点都可以访问相关信息，而不需要任何表。虽然这样做不会影响算法复杂度，但是代码量大大提高且变得晦涩难懂。 算法描述 对于待排序数组，根据 斐波那契数列 构建堆结构； 每次找到堆结构根节点（最大值）分离出堆结构，剩余部分重新变为合适的堆结构； 依次进行，当取出堆结构最后一个元素时，排序完成。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130public class SmoothSort &#123; /** * 交换指定数组两个数的位置 * @param array * @param index1 * @param index2 */ private static void swap(int[] array,int index1,int index2) &#123; int temp = array[index1]; array[index1] = array[index2]; array[index2] = temp; &#125; /** * 斐波那契数列 */ private static int[] leonardo = new int[]&#123; 1, 1, 3, 5, 9, 15, 25, 41, 67, 109, 177, 287, 465, 753, 1219, 1973, 3193, 5167, 8361, 13529, 21891, 35421, 57313, 92735, 150049, 242785, 392835, 635621, 1028457, 1664079, 2692537, 4356617, 7049155, 11405773, 18454929, 29860703, 48315633, 78176337, 126491971, 204668309, 331160281, 535828591, 866988873, 1402817465 &#125;; /** * 堆调整函数 * @param array * @param currentHeap * @param levelIndex * @param levels */ private static void smoothSortFix(int[] array, int currentHeap, int levelIndex, int[] levels) &#123; int prevHeap; int maxChild; int childHeap1; int childHeap2; int currentLevel; while(levelIndex &gt; 0) &#123; prevHeap = currentHeap - leonardo[levels[levelIndex]]; if(array[currentHeap]&lt; array[prevHeap]) &#123; if(levels[levelIndex] &gt; 1) &#123; childHeap1 = currentHeap - 1 - leonardo[levels[levelIndex] - 2]; childHeap2 = currentHeap - 1; if(array[prevHeap]&lt; array[childHeap1]) &#123; break; &#125; if(array[prevHeap] &lt; array[childHeap2])&#123; break; &#125; &#125; swap(array,currentHeap,prevHeap); currentHeap = prevHeap; levelIndex -= 1; &#125; else &#123; break; &#125; &#125; currentLevel = levels[levelIndex]; while(currentLevel &gt; 1) &#123; maxChild = currentHeap; childHeap1 = currentHeap - 1 - leonardo[currentLevel - 2]; childHeap2 = currentHeap - 1; if(array[maxChild]&lt; array[childHeap1])&#123; maxChild = childHeap1; &#125; if(array[maxChild]&lt; array[childHeap2]) &#123; maxChild = childHeap2; &#125; if(maxChild == childHeap1) &#123; swap(array,currentHeap, childHeap1); currentHeap = childHeap1; currentLevel -= 1; &#125;else if(maxChild == childHeap2) &#123; swap(array,currentHeap, childHeap2); currentHeap = childHeap2; currentLevel -= 2; &#125; else &#123; break; &#125; &#125; &#125; /** * Smooth排序算法 * @param array * @param size */ public static void smoothSort(int[] array, int size) &#123; int[] levels = new int[32]; int toplevel = 0; int i; for(i = 1; i &lt; size; i++) &#123; if(toplevel &gt; 0 &amp;&amp; levels[toplevel - 1] - levels[toplevel] == 1) &#123; toplevel -= 1; levels[toplevel] += 1; &#125; else if(levels[toplevel] != 1) &#123; toplevel += 1; levels[toplevel] = 1; &#125; else &#123; toplevel += 1; levels[toplevel] = 0; &#125; smoothSortFix(array, i, toplevel, levels); &#125; for(i = size - 2; i &gt; 0; i--) &#123; if(levels[toplevel] &lt;= 1) &#123; toplevel -= 1; &#125; else &#123; levels[toplevel] -= 1; levels[toplevel + 1] = levels[toplevel] - 1; toplevel += 1; smoothSortFix(array, i - leonardo[levels[toplevel]], toplevel - 1, levels); smoothSortFix(array, i, toplevel, levels); &#125; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[10]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("SmoothSort排序前："); System.out.println(Arrays.toString(a)); smoothSort(a,a.length); System.out.println("SmoothSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 SmoothSort是排序算法中理论值比较好的，但由于SmoothSort所用的树构建是基于斐波那契数列，复杂度因子较大，所以该算法的实际效率并不是特别好； 可以看到SmoothSort在数据基本有序的情况下可以达到O(n)的时间复杂度，比堆排序要好； 该排序算法时间复杂度： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n * log n) 时间复杂度（最差）：O(n * log n) 该排序算法的空间复杂度：O(1)。 该排序算法为不稳定排序算法。 树形选择排序（TreeSelectionSort）简介及原理树形选择排序（TreeSelectionSort）又称锦标赛排序（TournamentSort），是一种选择排序算法。由于其排序比较过程类似于锦标赛，因此得名锦标赛排序。 对于一个待排序数组，如下[8,6,3,2,9,5]，我们可以两两比较找到一组最小值，拿到这组最小值后在两两比较下去……最终找到最小的值，如下： [8,6,3,2,9,5] [6,2,5] [2,5] [2] 拿到最小值后，我们把最小值的位置填充为无穷大，继续上述比较逻辑。 [8,6,3,Max,9,5] [6,3,5] [3,5] [3] 依次进行下去，当数组所有值都为无穷大时，我们可以得到有序数组。因为这个过程很像完全二叉树，因此也被叫做树形选择排序。 如下图： 算法描述 对于待排序数组，构建一个满二叉树，节点总数 = 叶子节点数*2-1，其中叶子节点数即为数组长度。可以用数组表示这个满二叉树 int [] tree = new int[totalSize + 1]; 填充二叉树叶子节点，需要比较每个节点的“冠军”放入父节点； 每次找到根节点（最小的元素）移走，并将最小元素的位置设置为正无穷； 重复上述比较过程； 动图演示 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class TreeSelectionSort &#123; public static void treeSelectionSort(int[] array) &#123; // 数组长度 int len = array.length; // 对一个满二叉树，节点总数 = 叶子节点数*2-1 int nodeSize = len * 2 - 1; // 这里将用数组表示二叉树的存储结构 int[] tree = new int[nodeSize + 1]; /* 填充叶子节点 */ for (int i = len - 1, j = 0; i &gt;= 0; i--, j++) &#123; tree[nodeSize - j] = array[i]; &#125; /* 填充其他节点 */ for (int i = nodeSize - len; i &gt; 0; i--) &#123; tree[i] = tree[i * 2] &lt; tree[i * 2 + 1] ? tree[i * 2] : tree[i * 2 + 1]; &#125; /* 将每次找出的最小元素移走 */ // 数组a的索引 int index = 0; // 最小值的索引 int minIndex = 0; while (index &lt; len) &#123; // 这是tree的根节点，也是最小元素 int min = tree[1]; // 将tree中最小的元素取到a[0]中 array[index++] = tree[1]; minIndex = nodeSize; /* 从最后的叶子节点开始，直到找到最小值的索引 */ while (tree[minIndex] != min) &#123; minIndex--; &#125; // 将这个最小元素置为最大 tree[minIndex] = Integer.MAX_VALUE; /* 如果这个节点还有父节点，那么就将它的兄弟节点升到父亲节点位置 */ // 根结点的索引是1 while (minIndex &gt; 1) &#123; // 这个节点是左节点 if (minIndex % 2 == 0) &#123; tree[minIndex / 2] = tree[minIndex] &lt; tree[minIndex + 1] ? tree[minIndex] : tree[minIndex + 1]; minIndex = minIndex / 2; &#125; else &#123;// 这个节点是右节点 tree[minIndex / 2] = tree[minIndex] &lt; tree[minIndex - 1] ? tree[minIndex] : tree[minIndex - 1]; minIndex = minIndex / 2; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[3]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("TreeSelectionSort排序前："); System.out.println(Arrays.toString(a)); treeSelectionSort(a); System.out.println("TreeSelectionSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 可以看到这种排序算法有辅助存储空间较多、和“最大值”进行多余比较等缺点。为了弥补该缺陷，J. willioms 在1964年提出了堆排序; 该排序算法的时间复杂度为：O(n * log n)； 该排序算法的空间复杂度为：O(n); 该排序算法为稳定排序算法。 美国旗帜排序（AmericanFlagSort）简介与原理美国旗帜排序（AmericanFlagSort）是基数排序（RadixSort）的一个变体。 和基数排序不同的是，AmericanFlagSort一般排序从数据高位切入，会用一个数组保存位数据信息，再用一个数组记录数据位置。 之所以称为美国旗帜排序，是因为排序时的算法很像美国星条旗（将数组划分成很多“条纹”）。 我们来看一下： 比如对于一个数组[8,100,7622,520,6542,7,8888,33,1234]，该排序算法会先找到它的最大值8888，并获取其位数，得到最大数为4位数，构建一个长度为10的数组，初始值为0，即[0,0,0,0,0,0,0,0,0,0]; 该数组0位表示0-999的数的个数，1位表示1000-1999 的数的个数……依次类推，因此我们可以得到如下数组[5,1,0,0,0,0,1,1,1,0]，同时需要一个位置数组记录位置，默认长度10，初始值为0，即[0,0,0,0,0,0,0,0,0,0]； 如何表示数据位置呢？我们使用offset[0] = 0;offset[i] = count[i - 1] + offset[i - 1];来记录数据位置，得到如下数组[0,5,6,6,6,6,6,7,8,9]; 然后结合原数组，将数据按照高位基数排序，得到如下数组[8,100,33,520,7,1234,6542,7622,8888]，继续根据第二高位进行排序，相当于重复上述过程，可以得到数组[8,33,7,100,520,1234,6542,7622,8888]; 继续第三高位排序得到[8,7,33,100,520,1234,6542,7622,8888]，继续最后一位排序可以得到[7,8,33,100,520,1234,6542,7622,8888]，为最终有序数组。 算法描述 对于未排序数组，找到它的基数N，比如对于十进制数，基数是10（数的位数为0-9，不可能有其他值）；对于二进制，基数是2 （只有0，1）；对于字符串，通常使用256或者128作为基数； 构建两个数组，一个用于存储数据位信息，另一个用来记录数据位置； 待排序数组根据上面两个数组进行高位排序，排序完成后高位上数字较小者都在前面，较大者在后面； 继续对第二高位、第三高位……重复3过程，直到排序完成。 动图演示 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class AmericanFlagSort &#123; /** * 10位数的基数是10 */ private static final int NUMBER_OF_BUCKETS = 10; public static void americanFlagSort(int[] unsorted) &#123; // Max number of digits int numberOfDigits = getMaxNumberOfDigits(unsorted); int max = 1; for (int i = 0; i &lt; numberOfDigits - 1; i++) &#123; max *= 10; &#125; sort(unsorted, 0, unsorted.length, max); &#125; private static void sort(int[] unsorted, int start, int length, int divisor) &#123; // First pass - find counts int[] count = new int[NUMBER_OF_BUCKETS]; int[] offset = new int[NUMBER_OF_BUCKETS]; int digit = 0; for (int i = start; i &lt; length; i++) &#123; int d = unsorted[i]; digit = getDigit(d, divisor); count[digit]++; &#125; offset[0] = start; for (int i = 1; i &lt; NUMBER_OF_BUCKETS; i++) &#123; offset[i] = count[i - 1] + offset[i - 1]; &#125; // Second pass - move into position for (int b = 0; b &lt; NUMBER_OF_BUCKETS; b++) &#123; while (count[b] &gt; 0) &#123; int origin = offset[b]; int from = origin; int num = unsorted[from]; unsorted[from] = -1; do &#123; digit = getDigit(num, divisor); int to = offset[digit]++; count[digit]--; int temp = unsorted[to]; unsorted[to] = num; num = temp; from = to; &#125; while (from != origin); &#125; &#125; if (divisor &gt; 1) &#123; // Sort the buckets for (int i = 0; i &lt; NUMBER_OF_BUCKETS; i++) &#123; int begin = (i &gt; 0) ? offset[i - 1] : start; int end = offset[i]; if (end - begin &gt; 1) &#123; sort(unsorted, begin, end, divisor / 10); &#125; &#125; &#125; &#125; /** * 获取最大值 位长度 * @param unsorted * @return */ private static int getMaxNumberOfDigits(int[] unsorted) &#123; int max = Integer.MIN_VALUE; int temp = 0; for (int i : unsorted) &#123; temp = (int) Math.log10(i) + 1; if (temp &gt; max) &#123; max = temp; &#125; &#125; return max; &#125; /** * 获取该位数字 * @param integer * @param divisor * @return */ private static int getDigit(int integer, int divisor) &#123; return (integer / divisor) % 10; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("AmericanFlagSort排序前："); System.out.println(Arrays.toString(a)); americanFlagSort(a); System.out.println("AmericanFlagSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 可以看到该排序和基数排序一样，只能用于正整数或者可以表示成正整数的数据排序； 该排序也是一种就地排序算法，除了需要新建两个基数N的数组外，排序过程都是在待排序数组内部进行的； 该排序效率最高的时候是处理二进制数据的时候，这时候可以使用移位操作来避免一些求幂运算； 因为该排序是比较随机的，每个桶里的数的分布完全取决于数据集，因此该排序对大数据集并不友好； 如果对于纯字符串排序，数据量较多的情况下，该排序方法理论上是优于快排的； 该排序方法的时间复杂度： 时间复杂度（最好）：O(n*k/d) 时间复杂度（平均）：O(n*k/d) 时间复杂度（最差）：O(n*k/d) 该排序算法的空间复杂度：O(1) 该排序算法为不稳定排序算法 总结今天的排序算法就介绍到这里，后面我会介绍两种比较有意思的排序，一个是双调排序，另一个是TimSort，TimSort是目前Java使用的排序，我们来详细了解一下它。对于双调排序，它是一种可以进行多路归并的并行排序算法，也是比较有特点的。 源码本文所有源码详见我的 GitHub]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（三）]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言上篇文章排序算法（二）我们介绍了10种排序算法，今天我们再来介绍一些其他的排序算法。 正文今天我们介绍7种排序算法，它们如下： Sort Name Time(Best) Time(Average) Time(Worst) Memory Stable BinaryTreeSort O(n) O(n * log n) O(n * log n) O(n) Yes FlashSort O(n) O(n+k) O(n^2) O(n+k) Yes PatienceSort O(n) O(n * log n) O(n * log n) O(n) No StoogeSort —— —— O(n^(log 3 / log 1.5)) O(n) Yes PancakeSort O(n) O(n^2) O(n^2) O(n) No InPlaceMergeSort —— —— O(n * (log n)^2) O(1) Yes StrandSort O(n) O(n^2) O(n^2) O(n) Yes 我们分别来看下。 二叉排序树排序（BinaryTreeSort）简介及原理二叉排序树排序（BinaryTreeSort），其原理是利用二叉树的特性，较小值依次比较放在树的左边，较大值依次比较放在树的右边，形成一个数据二叉树，然后将数据从左到右遍历出来即可。 算法描述 对于待排序数组array，构建一个二叉树tree，根节点可以取array[0]，此时根节点的左右叶子节点为null； 对数组里每个元素，放入二叉树tree，如果小于根节点数值，应该放到左边，左边如果没有节点就创建一个并放入，如果有则需要比较其节点值和放入值大小，进而确定要放置的位置；右边同理； 进而我们会得到一个二叉树，我们从左向右遍历二叉树，就可以得到有序的数组。 动图演示 代码实现相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class BinaryTreeSort &#123; public static int[] binaryTreeSort(int[] array)&#123; BinaryNode root = new BinaryNode(array[0], null, null); for(int i=1; i&lt;array.length; i++)&#123; root.addChild(array[i]); &#125; List&lt;Integer&gt; list = BinaryNode.getSortedList(root); int[] result = new int[list.size()]; for (int i=0;i&lt;list.size();i++) &#123; result[i] = list.get(i); &#125; return result; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("BinaryTreeSort排序前："); System.out.println(Arrays.toString(a)); int [] s = binaryTreeSort(a); System.out.println("BinaryTreeSort排序后："); System.out.println(Arrays.toString(s)); &#125;&#125;/** * 二叉树节点类 */class BinaryNode&#123; /** * 节点当前值 */ private int value; /** * 左节点 */ private BinaryNode lChild; /** * 右节点 */ private BinaryNode rChild; /** * 排序结果集 */ private static List&lt;Integer&gt; resultList = new ArrayList&lt;&gt;(); public BinaryNode(int value, BinaryNode l, BinaryNode r)&#123; this.value = value; this.lChild = l; this.rChild = r; &#125; public BinaryNode getLChild() &#123; return lChild; &#125; public BinaryNode getRChild() &#123; return rChild; &#125; public int getValue() &#123; return value; &#125; public static List&lt;Integer&gt; getSortedList(BinaryNode root) &#123; iterate(root); return resultList; &#125; /** * 添加一个节点 * **/ public void addChild(int n)&#123; if(n &lt; value)&#123; if(lChild!=null)&#123; lChild.addChild(n); &#125;else&#123; lChild = new BinaryNode(n, null, null); &#125; &#125;else&#123; if(rChild!=null)&#123; rChild.addChild(n); &#125;else&#123; rChild = new BinaryNode(n, null, null); &#125; &#125; &#125; /** * 迭代赋值 * @param root */ public static void iterate(BinaryNode root)&#123; if(root.lChild!=null)&#123; iterate(root.getLChild()); &#125; resultList.add(root.getValue()); if(root.rChild!=null)&#123; iterate(root.getRChild()); &#125; &#125;&#125; 其他注意事项 这儿是否可以使用平衡二叉树？ 可以，但平衡二叉树可以提高单个数据查找性能，我们这儿需要处理全部数据，同时平衡二叉树插入时涉及到树的旋转变化等，因此使用平衡二叉树的效率不一定比普通二叉树效率高。 某些情况下，使用平衡二叉树递归取值的栈深度会比普通二叉树要低。 该排序算法的时间复杂度： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n * log n) 时间复杂度（最差）：O(n * log n) 该排序算法空间复杂度：O(n) 该排序算法是稳定排序算法。 闪电排序（FlashSort）简介及原理闪电排序（FlashSort）是由Karl-Dietrich Neubert在1997年发展而来的一种排序算法，并且在欧洲理论计算机协会的新闻简报上发表了该算法。 该算法类似于桶排序，主要改进了对使用桶的预测。 我们知道，对于桶排序，我们首先要确定桶的数量，不能太多或者太少。 太多的话，桶中元素少，或者无用桶多，会浪费内存；太少的话，桶中元素多，每个桶中这些元素进行由大到小（或者由小到大）排序时就会比较耗时。（可参考HashMap理解桶排序） 因此桶的数量对排序性能的影响也不能忽视，FlashSort就是可以提前预知桶的大致数量及元素在桶中的位置，使其元素尽量利用每个桶，然后进行排序的。 FlashSort是如何预测的呢？ 我们知道，桶排序桶的数量bucketCount一般为 INT[(Amax - Amin)/Asize] +1，对于如下数组[1,152,1000,8763,3,88,1000001,666,9999,100]，根据公式可以算出需要的桶的数量为(1000001-1)/10 +1 = 1000001。 显然浪费了大量桶（内存），当然bucketCount也可以设置为数组长度，但是如果有一个长度为100000的数组，数组元素只有[0-9]之间的元素，我们显然设置的bucketCount也会不合适。 你一定会说可以取 INT[(Amax - Amin)/Asize] +1 或者数组长度中的最小值啊，哈哈，当然这么想也正确，但是即使这样，桶排序也不一定会完全把所有的桶充分利用，也会出现空桶的情况，因而造成内存浪费。 FlashSort就可以充分利用桶吗？ 是的，其数据的位置主要依赖一个公式，如下： K(Ai) = 1 + INT[ (m-1)(Ai-Amin)/(Amax-Amin) ] 这个公式可以算出数组的Ai项在桶中的位置。 比如如下数组[6,2,4,1,5,9]，m如果取数组长度的话 m=6（也表明桶的数量为m个），我们可以算出： K(6) = 1 + 5 * 5/8 = 4 K(2) = 1 + 5 * 1/8 = 1 K(4) = 1 + 5 * 3/8 = 2 K(1) = 1 + 5 * 0/8 = 1 K(5) = 1 + 5 * 4/8 = 3 K(9) = 1 + 5 * 8/8 = 6 可以看到极大的利用了桶，在数据量较多的情况下，与桶排序相比效果更明显。 可以看到上面我们m （桶的数量）取的数组长度，这显然不太合理，我们m一般取 0.1n (n代表数组长度)，当m&lt;1 时，m=1。这样保证桶尽量少的浪费的前提下，尽可能减少桶的使用数量。 如果数组过大，m过小，会导致每个桶内数据多，排序浪费较多时间；如果数据少，但桶较多，会导致内存浪费。所以对于具体数组，m也可以自己指定，以达到排序效率和空间的优化平衡。 算法描述 构建M个桶，M可以取0.1N （N数组长度）或者自定义； 根据FlashSort的K(Ai)计算公式，找到每个元素所在的桶，放入元素； 对于每个桶中的元素，如果元素个数大于等于2，需要进行内部排序； 从小到大（或者大到小）依次取出桶中元素，得到有序数组。 动图演示 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class FlashSort &#123; public static void flashSort(int[] array) &#123; //桶排序 partialFlashSort(array, array.length); //桶内元素使用插入排序 insertionSort(array); &#125; private static void partialFlashSort(int[] a, int n) &#123; //m值，取0.1n，也可以自由指定 int bucketSize = n / 10; if (bucketSize &lt; 1) &#123; bucketSize = 1; &#125; //构建bucket int[] buckets = new int[bucketSize]; int i, j, k; int min = a[0]; int maxIndex = 0; //找到最大最小值 for (i = 1; i &lt; n; i++) &#123; if (a[i] &lt; min) &#123; min = a[i]; &#125; if (a[i] &gt; a[maxIndex]) &#123; maxIndex = i; &#125; &#125; if (min == a[maxIndex]) &#123; return; &#125; //计算系数 double c1 = ((double) bucketSize - 1) / (a[maxIndex] - min); //计算元素位置 for (i = 0; i &lt; n; i++) &#123; k = (int) (c1 * (a[i] - min)); buckets[k]++; &#125; for (k = 1; k &lt; bucketSize; k++) &#123; buckets[k] += buckets[k - 1]; &#125; //元素入桶 int hold = a[maxIndex]; a[maxIndex] = a[0]; a[0] = hold; int nmove = 0; int flash; j = 0; k = bucketSize - 1; while (nmove &lt; n - 1) &#123; while (j &gt; (buckets[k] - 1)) &#123; j++; k = (int) (c1 * (a[j] - min)); &#125; flash = a[j]; while (j != buckets[k]) &#123; k = (int) (c1 * (flash - min)); hold = a[buckets[k] - 1]; a[buckets[k] - 1] = flash; flash = hold; buckets[k]--; nmove++; &#125; &#125; &#125; private static void insertionSort(int[] a) &#123; int i, j, hold; for (i = a.length - 3; i &gt;= 0; i--) &#123; if (a[i + 1] &lt; a[i]) &#123; hold = a[i]; j = i; while (a[j + 1] &lt; hold) &#123; a[j] = a[j + 1]; j++; &#125; a[j] = hold; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("FlashSort排序前："); System.out.println(Arrays.toString(a)); flashSort(a); System.out.println("FlashSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 可以看到FlashSort只是优化了桶排序的元素存放桶的位置，使其尽量均匀分布于每个桶，而且选择合适的m值（桶数量）也是至关重要的（无论FlashSort或者桶排序）； 该算法时间复杂度： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n+k) 时间复杂度（最差）：O(n^2) 该算法空间复杂度：O(n+k) 该算法为稳定排序算法。 耐心排序（PatienceSort）简介及原理耐心排序（PatienceSort）是将数组的元素分类成很多堆再串接回数组的一种排序算法。受到纸牌游戏的启发和命名。 我们在Windows系统上一定玩过纸牌游戏吧，我们知道，无序的纸牌要变为有序，我们需要将无序的纸牌通过操作进行有序分堆，最后在将这些堆纸牌合并生成有序的结果。 我们后面的动图也会模拟纸牌来看下耐心排序的执行过程。 算法描述 创建一个堆数组； 比较当前指向的元素和每个堆的第一个元素； 若当前元素比所有堆的第一个元素大，创建新的堆并加入到堆数组中； 如果当前元素比堆内的第一个元素小，就放入该堆头部作为新的第一个元素； 分类完后将每个堆有两种处理方式，可以通过合并后使用插入排序形成有序数列，也可以使用优先级队列依次取出元素完成排序。 PS：其实第4步，如果元素找到了多个符合条件的堆，可以放到任意一个堆里作为首元素。 动图演示 代码实现这里的代码有两个，都比较好理解，第一个是将生成的堆合并，然后使用插入排序处理；第二个是生成堆后，使用优先级队列处理堆中数据。 大家都可以看下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class PatienceSort &#123; public static void patienceSort(int[] theArray) &#123; List&lt;List&lt;Integer&gt;&gt; newList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; theArray.length; i++) &#123; List&lt;Integer&gt; bucketList = new ArrayList&lt;&gt;(); //先开始创建一个堆 if (i == 0) &#123; bucketList.add(theArray[i]); newList.add(bucketList); &#125; else &#123; boolean isOk = false; for (int j = 0; j &lt; newList.size(); j++) &#123; //如果当前元素比堆内的第一个元素小，就放入该堆头部作为新的第一个元素，然后执行下个元素判断 //这儿我们直接放到第一个符合的堆里了，其实放到其它符合的也是可以的，放到最后一个符合的堆里还可以解决子序列问题 if (theArray[i] &lt; (int) ((List) newList.get(j)).get(0)) &#123; (newList.get(j)).add(0, theArray[i]); isOk = true; break; &#125; &#125; //如果当前元素比所有堆内的第一个元素大，就创建个新堆，把元素作为第一个元素放进去 if (!isOk) &#123; bucketList.add(theArray[i]); newList.add(bucketList); &#125; &#125; &#125; ////生成的堆合并，而后使用插入排序 int q = 0; for (int m = 0; m &lt; newList.size(); m++) &#123; for (int n = 0; n &lt; (newList.get(m)).size(); n++) &#123; theArray[q] = (int) ((List) newList.get(m)).get(n); q++; &#125; &#125; //插入排序 int tmp; int j; for (int i = 1; i &lt; theArray.length; i++) &#123; tmp = theArray[i]; for (j = i - 1; j &gt;= 0 &amp;&amp; theArray[j] &gt; tmp; j--) &#123; theArray[j + 1] = theArray[j]; &#125; theArray[j + 1] = tmp; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[200]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("PatienceSort排序前："); System.out.println(Arrays.toString(a)); patienceSort(a); System.out.println("PatienceSort排序后："); System.out.println(Arrays.toString(a)); &#125; public static &lt;E extends Comparable&lt;? super E&gt;&gt; void patienceSort(E[] n) &#123; List&lt;Pile&lt;E&gt;&gt; piles = new ArrayList&lt;Pile&lt;E&gt;&gt;(); //生成堆 for (E x : n) &#123; Pile&lt;E&gt; newPile = new Pile&lt;E&gt;(); newPile.push(x); int i = Collections.binarySearch(piles, newPile); if (i &lt; 0)&#123; i = ~i; &#125; if (i != piles.size()) &#123; piles.get(i).push(x); &#125;else &#123; piles.add(newPile); &#125; &#125; //使用优先级队列处理数据 PriorityQueue&lt;Pile&lt;E&gt;&gt; heap = new PriorityQueue&lt;Pile&lt;E&gt;&gt;(piles); for (int c = 0; c &lt; n.length; c++) &#123; Pile&lt;E&gt; smallPile = heap.poll(); n[c] = smallPile.pop(); if (!smallPile.isEmpty()) &#123; heap.offer(smallPile); &#125; &#125; assert(heap.isEmpty()); &#125; private static class Pile&lt;E extends Comparable&lt;? super E&gt;&gt; extends Stack&lt;E&gt; implements Comparable&lt;Pile&lt;E&gt;&gt; &#123; @Override public int compareTo(Pile&lt;E&gt; y) &#123; return peek().compareTo(y.peek()); &#125; &#125;&#125; 其他注意事项 该算法有一个变体，可以有效地计算给定数据中的最长增长子列和最长递减子列。比如有个数组[7,8,2,3,5,8,6,4,3,1,5,9]，我们可以看到最长增长子列为[2,3,5,8]，最长递减子列为[8,6,4,3,1] 我们使用耐心排序算法来看下： 算法描述的PS部分：如果元素找到了多个符合条件的堆，可以放到任意一个堆里作为首元素，我们在这儿放到最后一个符合的堆中。 根据耐心排序，我们可以把[7,8,2,3,5,8,6,4,3,1,5,9]数组按照上述算法分为如下堆数组： [3,7],[2,8],[5],[1,3,4,6,8],[5],[9]。显而易见最长递减子列为[1,3,4,6,8]的反向序列。 我们对于上述所讲的算法，也可以反过来，即比较当前指向的元素和每个堆的第一个元素，如果当前元素大于该堆第一个元素，在进行放入操作，这样可以得到如下堆数组：[8,7],[8,5,3,2],[6],[4],[3],[9,5,1]。显而易见[8,5,3,2]是最长递增子列的反向序列。 该排序算法时间复杂度： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n * log n) 时间复杂度（最差）：O(n * log n) 该排序算法空间复杂度：O(n) 该排序算法为不稳定排序算法 臭皮匠排序（StoogeSort）简介及原理臭皮匠排序（StoogeSort）是一种低效的递归排序算法，是由Howard、Fine等教授提出的所谓“漂亮的”排序算法，代码很漂亮但是很耗时。 臭皮匠排序翻译的由来：因为这个排序和3有很大关系（3个臭皮匠）。 我们来看下算法描述： 算法描述 如果最后一个值小于第一个值，则交换这两个数； 如果当前集合元素数量大于等于3： 使用臭皮匠排序前2/3的元素； 使用臭皮匠排序后2/3的元素； 再次使用臭皮匠排序前2/3的元素。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142public class StoogeSort &#123; public static void stoogeSort(int[] array) &#123; stoogeSort(array, 0, array.length - 1); &#125; private static void stoogeSort(int[] array, int low, int high) &#123; //如果第一个数大于最后一个数，交换位置 if (array[low] &gt; array[high]) &#123; swap(array, low, high); &#125; if (low + 1 &gt;= high)&#123; return; &#125; int third = (high - low + 1) / 3; //排序前2/3数组元素 stoogeSort(array, low, high - third); //排序后2/3数组元素 stoogeSort(array, low + third, high); //排序前2/3数组元素 stoogeSort(array, low, high - third); &#125; private static void swap(int[] a, int b, int c) &#123; if (b == c)&#123; return; &#125; int temp = a[b]; a[b] = a[c]; a[c] = temp; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("StoogeSort排序前："); System.out.println(Arrays.toString(a)); stoogeSort(a); System.out.println("StoogeSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 该算法是一种低效的递归排序算法，甚至慢于冒泡排序，相比经典排序，臭皮匠排序性能十分差； 该算法的时间复杂度： 时间复杂度（最差）：O(n^(log 3 / log 1.5)) 该算法的空间复杂度：O(n) 该排序算法为稳定排序算法。 煎饼排序（PancakeSort）简介及原理煎饼排序（PancakeSort），我们对于排序数组，可以看成一叠大大小小的煎饼，假设我们有一把锅铲，可以每次从任意位置铲起上方全部煎饼并翻面，最终我们可以实现按煎饼大小进行排序的煎饼堆。 比如对于一个数组 [3,1,3,6,8,2,7,1]，我们把它看成煎饼堆，则我们使用“锅铲”翻转的过程如下： [3,1,3,6,8,2,7,1] 初始化 [8,6,3,1,3,2,7,1] 先把“大煎饼”8翻转到上面 [1,7,2,3,1,3,6,8] 再把“大煎饼”8翻转到下面 [7,1,2,3,1,3,6,8] 再把“第二大煎饼”7翻转到上面 [6,3,1,3,2,1,7,8] 再把7翻转到下面 [1,2,3,1,3,6,7,8] 再把6翻转到下面 [3,2,1,1,3,6,7,8] 再把3翻转到上面 [1,1,2,3,3,6,7,8] 再把3翻转到下面，完成排序 算法描述 对于待排序数组，找到最大值最小值及索引； 将最大值翻转到顶部，在翻转到数组底部； 此时再从未排序数组里找到最大值及索引，重复1、2过程。 动图演示 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class PancakeSort &#123; /** * 翻转 * @param n * @param heap */ private static void flip(int n, int[] heap) &#123; for (int i = 0; i &lt; (n + 1) / 2; ++i) &#123; int tmp = heap[i]; heap[i] = heap[n - i]; heap[n - i] = tmp; &#125; &#125; /** * 获取最小最大值数组 * @param n * @param heap * @return */ private static int[] minmax(int n, int[] heap) &#123; int xm, xM; xm = xM = heap[0]; int posm = 0, posM = 0; for (int i = 1; i &lt; n; ++i) &#123; if (heap[i] &lt; xm) &#123; xm = heap[i]; posm = i; &#125; else if (heap[i] &gt; xM) &#123; xM = heap[i]; posM = i; &#125; &#125; return new int[]&#123;posm, posM&#125;; &#125; private static void sort(int n, int dir, int[] heap) &#123; if (n == 0) &#123; return; &#125; int[] mM = minmax(n, heap); int bestXPos = mM[dir]; int altXPos = mM[1 - dir]; boolean flipped = false; if (bestXPos == n - 1) &#123; --n; &#125; else if (bestXPos == 0) &#123; flip(n - 1, heap); --n; &#125; else if (altXPos == n - 1) &#123; dir = 1 - dir; --n; flipped = true; &#125; else &#123; flip(bestXPos, heap); &#125; sort(n, dir, heap); if (flipped) &#123; flip(n, heap); &#125; &#125; public static void pancakeSort(int[] array) &#123; sort(array.length, 1, array); &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("PancakeSort排序前："); System.out.println(Arrays.toString(a)); pancakeSort(a); System.out.println("PancakeSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 煎饼排序的一个问题变种为焦煎饼排序，即对于我们的煎饼，每张煎饼都有一面是烤焦的，最终除了按照大小排序以外还要让所有焦面向下； 焦煎饼问题： 比如对于上面的[3,1,3,6,8,2,7,1]数组，它们的右面都是“焦”的，我们按照上面的步骤走，会发现当执行到第4步得到[7,1,2,3,1,3,6,8]，其中[7,1][3,1,3,6,8]是焦面向下，[2]是焦面向上的，我们执行第5步把7翻转到底下时，7的焦面就向上了，就不符合要求了，因此这时候我们需要在单独翻转一次“煎饼”7，然后把[7,1,2,3,1,3,6]整个翻转一下。 关于这部分的代码我就略过了，有兴趣的同学可以自己写一下。关于如何表示焦煎饼的正反两面也可以思考下。 煎饼排序的时间复杂度： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n^2) 时间复杂度（最差）：O(n^2) 该排序算法的空间复杂度：O(n) 该排序算法是一种不稳定排序算法。 原地归并排序（InPlaceMergeSort）简介及原理原地归并排序（InPlaceMergeSort）是归并排序（MergeSort）的一个变种。 我们知道，当使用归并排序时，我们需要借助辅助数组。原地归并排序可以使我们不使用辅助数组即可完成目标数组的排序。 算法描述 将长度为n的数组分为两个n/2的子序列； 对两个子序列分别进行归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class InPlaceMergeSort &#123; public static void inPlaceMergeSort(int[] array) &#123; inPlaceMergeSort(array, 0, array.length - 1); &#125; private static void inPlaceMergeSort(int[] array, int first, int last) &#123; int mid, lt, rt; int tmp; if (first &gt;= last) &#123; return; &#125; mid = (first + last) / 2; inPlaceMergeSort(array, first, mid); inPlaceMergeSort(array, mid + 1, last); lt = first; rt = mid + 1; // One extra check: can we SKIP the merge? if (array[mid] &lt;= array[rt]) &#123; return; &#125; while (lt &lt;= mid &amp;&amp; rt &lt;= last) &#123; // Select from left: no change, just advance lt if (array[lt] &lt;= array[rt]) &#123; lt++; // Select from right: rotate [lt..rt] and correct &#125; else &#123; // Will move to [lt] tmp = array[rt]; System.arraycopy(array, lt, array, lt + 1, rt - lt); array[lt] = tmp; // EVERYTHING has moved up by one lt++; mid++; rt++; &#125; &#125; // Whatever remains in [rt..last] is in place &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("InPlaceMergeSort排序前："); System.out.println(Arrays.toString(a)); inPlaceMergeSort(a); System.out.println("InPlaceMergeSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 该排序的时间复杂度与归并排序相当： 时间复杂度（最差）：O(n * (log n)^2) 由于未使用辅助数组，因此该排序的空间复杂度为：O(1) 该排序为稳定排序算法。 Strand排序（StrandSort）简介及原理Strand排序（StrandSort）又称串排序、线排序，其原理主要和子有序数组有关。 这儿的子有序数组是这样定义的：比如一个数组[5,3,7,8,2,1,9]，则它的子有序数组为[5,7,8,9],此时原数组变为[3,2,1],其中子有序数组为[3]，依次类推……得到最终子有序数组为[5,7,8,9],[3],[2],[1]。 得到子有序数组后将它们合并为新的有序数组。 比如数组[7,9,6,3,2,5,8,1]，我们使用Strand排序： [7,9,6,3,2,5,8,1] 得到子有序数组和另一个子数组 [7,9][6,3,2,5,8,1]; [6,3,2,5,8,1] 得到子有序数组和另一个子数组 [6,8][3,2,5,1] ，将[7,9]和[6,8]有序合并得到[6,7,8,9]; [3,2,5,1] 继续得到 [3,5][2,1]， [3,5]与[6,7,8,9]合并得到[3,5,6,7,8,9]; [2,1]得到[2][1]，继续合并得到[1,2,3,5,6,7,8,9]； [1,2,3,5,6,7,8,9]即为要求的有序数组。 算法描述 对于待排序数组，取首元素为基础数； 向后寻找子有序数组，原数组变为一个子有序数组和一个新的子数组； 对于新的子数组，继续1、2步逻辑，得到新的子子有序数组和另一个新的子子数组； 将子有序数组和子子有序数组合并成有序数组，新的子子数组重复上述逻辑。 动图演示 代码实现这儿我们借助LinkedList实现StrandSort 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class StrandSort &#123; private static &lt;E extends Comparable&lt;? super E&gt;&gt; LinkedList&lt;E&gt; strandSort(LinkedList&lt;E&gt; list) &#123; if (list.size() &lt;= 1) &#123; return list; &#125; LinkedList&lt;E&gt; result = new LinkedList&lt;&gt;(); while (list.size() &gt; 0) &#123; LinkedList&lt;E&gt; sorted = new LinkedList&lt;&gt;(); //same as remove() or remove(0) sorted.add(list.removeFirst()); for (Iterator&lt;E&gt; it = list.iterator(); it.hasNext(); ) &#123; E elem = it.next(); if (sorted.peekLast().compareTo(elem) &lt;= 0) &#123; //same as add(elem) or add(0, elem) sorted.addLast(elem); it.remove(); &#125; &#125; result = merge(sorted, result); &#125; return result; &#125; private static &lt;E extends Comparable&lt;? super E&gt;&gt; LinkedList&lt;E&gt; merge(LinkedList&lt;E&gt; left, LinkedList&lt;E&gt; right) &#123; LinkedList&lt;E&gt; result = new LinkedList&lt;&gt;(); while (!left.isEmpty() &amp;&amp; !right.isEmpty()) &#123; //change the direction of this comparison to change the direction of the sort if (left.peek().compareTo(right.peek()) &lt;= 0) &#123; result.add(left.remove()); &#125; else &#123; result.add(right.remove()); &#125; &#125; result.addAll(left); result.addAll(right); return result; &#125; public static void strandSort(int[] array)&#123; LinkedList&lt;Integer&gt; linkedList = new LinkedList&lt;&gt;(); Arrays.stream(array).forEach(e -&gt; linkedList.add(e)); List&lt;Integer&gt; list = strandSort(linkedList); assert list.size() == array.length; for(int i=0;i&lt;array.length;i++)&#123; array[i] = list.get(i); &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("StrandSort排序前："); System.out.println(Arrays.toString(a)); strandSort(a); System.out.println("StrandSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 该排序算法的时间复杂度如下： 时间复杂度（最好）： O(n) 时间复杂度（平均）： O(n^2) 时间复杂度（最差）： O(n^2) 该排序算法的空间复杂度为： O(n) 该排序算法是稳定排序算法。 总结今天我们了解了7种排序算法，这7种排序算法还是比较好理解的，我们可以看到，各个排序都有各自的一些优点（某些个例除外），比如有的时间复杂度要低些，但空间复杂度要高些；有的空间复杂度低，但是时间复杂度要高些；还有一些数据量小的情况下，可能经典排序（冒泡、插入、选择）效率要高，因此可以选择一个合适的阈值，当要排序的数组部分数据量小时，我们可以使用它们代替。 比如归并排序，如果数组较小，就没必要继续归并，而采用插入排序可能会提高效率等。 所以后面我们会介绍一些经典的混合排序，还有一些基于某些原理的排序等。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法（二）]]></title>
    <url>%2Fblog%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言之前一篇文章介绍了10种常用的排序算法，Java排序算法,今天我们来介绍一些其它的排序算法。 数据排序问题到今天，虽然是一个“已经被解决了”的问题，但是仍有许多排序算法出现，它们有些基于一些原理，有些是基础排序算法的优化。 我们了解下这些排序算法，对我们是十分有帮助的。 正文今天我们主要介绍十种排序算法，它们如下： Sort Name Time(Best) Time(Average) Time(Worst) Memory Stable BeadSort O(n) O(n^2) O(n^2) O(n^2) YES SleepSort unpredictable unpredictable ∞ unpredictable NO BogoSort O(n) O(n * n!) O(n * n! -&gt; ∞ ) O(1) NO PermutationSort O(n!) O(n!) O(n!) O(1) NO GnomeSort O(n) O(n^2) O(n^2) O(1) YES PigeonholeSort O(n+k) O(n+k) O(n+k) O(n*k) YES CombSort —— O(n^2) O(n^2) O(1) NO CocktailSort O(n) O(n^2) O(n^2) O(1) YES BinaryInsertionSort O(n) O(n^2) O(n^2) O(1) YES CycleSort O(n) O(n^2) O(n^2) O(1) NO 我们分别来看一下。 珠排序（BeadSort）简介及原理珠排序(BeadSort)，也叫重力排序，是一种自然排序算法，由Joshua J. Arulanandham, Cristian S. Calude 和 Michael J. Dinneen 在2002年发展而来，并且在欧洲理论计算机协会的新闻简报上发表了该算法。 该算法的原理可认为基于重力原理，我们对算盘一定比较熟悉，设想算盘上的每个珠子代表1，开始时算珠排列无序，当我们竖起算盘时，算珠就会整齐的排列起来。 算法描述 我们需要找到待排序数组中的最大值，以此值为x轴最大值，以待排序数组长度为y轴，我们可以构建一个二维数组（算盘）； y轴上数据表示数组元素的值，因此我们以y轴为基准，对于一个数组元素N，就在该位置放N个珠子（表现在二维数组上即为1）; 上面即是算盘的初始化，初始化完成后，我们以x轴为基准，这时候y轴珠子的高度不同，切换基准后，由于重力原因，珠子会下坠，直到稳定; 最后我们以y轴为基准获取上面的值，从下到上可形成数组的倒序，从上到下可形成数组的正序，这样数据便排序完成。 动图演示 代码实现根据上面描述，我们很容易实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class BeadSort &#123; public static void beadSort(int[] array) &#123; int length = array.length; //获取待排序数组的最大值 int max = array[0]; int min = array[0]; for (int i = 1; i &lt; length; i++) &#123; if (array[i] &gt; max) &#123; max = array[i]; &#125; if(array[i] &lt; min)&#123; min = array[i]; &#125; &#125; //初始化算盘 //构建一个二维数组，算盘最小长度应该为 max,高为 length int[][] bead = new int[max][length]; for (int y=0;y&lt;length;y++)&#123; for(int x=0;x&lt;array[y];x++)&#123; bead[x][y] = 1; &#125; &#125; //珠子下落 for(int x=0;x&lt;max;x++)&#123; int count = 0; for(int y=0;y&lt;length;y++)&#123; if(bead[x][y]==0)&#123; count ++; &#125; &#125; //有值，需要移动 if(count!=0)&#123; int[] temp = new int[max]; for(int k=0;k&lt;count;k++)&#123; temp[k] = 1; &#125; bead[x] = temp; &#125; &#125; //完成后取出数值 for(int y=0;y&lt;length;y++)&#123; int len = 0; for(int x=0;x&lt;max;x++)&#123; if(bead[x][y]!=0)&#123; len ++; &#125; &#125; array[y] = len; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("BeadSort排序前："); System.out.println(Arrays.toString(a)); beadSort(a); System.out.println("BeadSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 可以看到我们构建了一个 max length 的二维数组，实际我们想下，也可以构建一个 (max-min+1) length 的二维数组，最后排好后都加上min，理论上可以节约空间。 这个排序算法有一个缺点很明显：那就是它只适用于正整数的排序，原因很明显，“算盘”上我们没法表示负数。 如果你非要用这种排序方法排含有负数的数组，可以对数组分类，分成正数数组和负数数组，负数数组数据先转为正数排序，排好后在转成负数数组和另一个排好的正数数组合并。 关于珠排序的时间复杂度： O(1)：即所有珠子都同时移动，但这种算法只是概念上的，无法在计算机中实现。 O(√n)：在真实的物理世界中用引力实现，所需时间正比于珠子最大高度的平方根，而最大高度正比于n。 O(n)：一次移动一列珠子，可以用模拟和数字的硬件实现。 O(S)，S是所有输入数据的和：一次移动一个珠子，能在软件中实现。 空间复杂度：可以看到影响空间的max值和length值，都和数据大小n成正比关系，因此该算法即使在最好的情况下，也是有O(n^2)空间复杂度的。 稳定性：我们看动图里3的位置变化，可以知道该排序算法是稳定的。 实用性：该排序方法并不实用，也不建议使用。 睡排序（SleepSort）简介及原理顾名思义，就是睡一会儿，按谁先醒的顺序输出。这要借助计算机中的线程休眠（sleep）机制。 算法描述 对于待排序数组，开 数组长度 个线程，并使它们同时等待； 同时执行线程的sleep方法，sleep数组元素值的时间，哪一个线程先苏醒，就把该值输出； 所有线程sleep完成后，就得到了排序好的数据。 动图演示 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SleepSort &#123; public static List&lt;Integer&gt; sleepSort(int[] nums) &#123; //创建num个线程池，进行睡排序 ExecutorService executorService = Executors.newFixedThreadPool(nums.length); final CountDownLatch doneSignal = new CountDownLatch(nums.length); final List&lt;Integer&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;()); List&lt;Future&gt; futureList = new ArrayList&lt;&gt;(); for (final int num:nums) &#123; Future future = executorService.submit(()-&gt;&#123; doneSignal.countDown(); try &#123; doneSignal.await(); //线程时间为毫秒，可靠性较弱，我们 * 1000 改为秒，可靠性会提高，但是仍有可能排序错误！！！ Thread.sleep(num * 1000); list.add(num); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); futureList.add(future); &#125; //循环检测线程是否完成 while (true)&#123; boolean finish = true; for(Future future:futureList)&#123; if(!future.isDone())&#123; finish = false; &#125; &#125; if(finish)&#123; executorService.shutdown(); break; &#125; &#125; return list; &#125; public static void main(String[] args) &#123; int[] a = new int[10]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10); &#125; System.out.println("SleepSort排序前："); System.out.println(Arrays.toString(a)); List&lt;Integer&gt; list = sleepSort(a); System.out.println("SleepSort排序后："); System.out.println(Arrays.toString(list.toArray())); &#125;&#125; 其他注意事项 强烈不推荐在任何地方使用此方法，如果应用于项目，被发现后后果自付，与本作者无关。 该排序方法有许多明显缺点： 该排序方法只能应用于较小的正整数排序； 该排序方法是不可靠的，上述代码我们即使对num * 1000增加了可靠度，而后输出的排序结果也有可能不正确； 该排序方法是不稳定的； 该排序方法的时间复杂度取决于数据值大小，一般远远大于任何排序算法； 由于开了N个线程，我们一般认为该排序方法空间复杂度大于其它排序算法。 Bogo排序（BogoSort）简介及原理Bogo排序又称猴子排序，其原理基于猴子无限定理。 猴子无限定理：无限只猴子，在无限的时间内，随机敲击键盘，总有一只可以敲出莎士比亚全集。 可以看到，本排序方法主要思想就是基于运气！！！ 算法描述 检查数组是否已排序，如果已排序，输出结果； 如果不是有序数组，随机打乱数组里数据位置，返回第一步。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445public class BogoSort &#123; public static void bogoSort(int[] array) &#123; int size = array.length; int i, j; boolean tag; while (true) &#123; tag = true; //检测是否有序 for (i = 1; i &lt; size; i++) &#123; if (array[i] &lt; array[i - 1]) &#123; tag = false; break; &#125; &#125; //如果有序，则排序完成 if (tag) &#123; break; &#125; //顺序不对，则随机打乱 Random random = new Random(); for (i = 0; i &lt; size; i++) &#123; j = random.nextInt(size); //随机交换两值 swap(array, i, j); &#125; &#125; &#125; private static void swap(int[] a, int p, int q) &#123; int temp = a[p]; a[p] = a[q]; a[q] = temp; &#125; public static void main(String[] args) &#123; int[] a = new int[10]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("BogoSort排序前："); System.out.println(Arrays.toString(a)); bogoSort(a); System.out.println("BogoSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 当数据量逐渐变大时，该排序方法时间复杂度会激增，因此该排序方法非常不实用。 可以看到该排序方法是不稳定的。 该排序方法的时间复杂度如下： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n * n!) 时间复杂度（最差）：O(n * n! -&gt; ∞ ) 该排序方法的空间复杂度为 O(1)。 全排序（PermutationSort）简介及原理全排序（PermutationSort）又称全排列排序，顾名思义，一个长度有限的数组，其内部数据的排列组合也是有限的，我们找到全部排列组合，里面总至少有一个组合满足数组有序这个条件。 全排列：从n个不同元素中任取m（m≤n）个元素，按照一定的顺序排列起来，叫做从n个不同元素中取出m个元素的一个排列，当m=n时所有的排列情况叫全排列。 PS：对于一个长度为n的数组，其全排列组合有 n! 种。 算法描述 对于一个长度为n的数组，列出其所有排列组合情况； 校验所有排列组合是否有序，如果有序，输出该组合结果。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class PermutationSort &#123; public static int[] permutationSort(int[] a) &#123; List&lt;int[]&gt; list = new ArrayList&lt;&gt;(); permute(a, a.length, list); for (int[] x : list) &#123; if (isSorted(x)) &#123; return x; &#125; &#125; return a; &#125; //获取数组的全排列 private static void permute(int[] a, int n, List&lt;int[]&gt; list) &#123; if (n == 1) &#123; int[] b = new int[a.length]; System.arraycopy(a, 0, b, 0, a.length); list.add(b); return; &#125; for (int i = 0; i &lt; n; i++) &#123; swap(a, i, n - 1); permute(a, n - 1, list); swap(a, i, n - 1); &#125; &#125; //判断数组是否有序 private static boolean isSorted(int[] a) &#123; for (int i = 1; i &lt; a.length; i++) &#123; if (a[i - 1] &gt; a[i]) &#123; return false; &#125; &#125; return true; &#125; //交换数组两数数值 private static void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; public static void main(String[] args) &#123; int[] a = new int[10]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("PermutationSort排序前："); System.out.println(Arrays.toString(a)); int [] s = permutationSort(a); System.out.println("PermutationSort排序后："); System.out.println(Arrays.toString(s)); &#125;&#125; 其他注意事项 首先看到我们上面代码，列出了所有排列情况，再一一判断每种情况是不是有序，实际上List&lt;int[]&gt; list这个是多余的，我们完全可以在生成每一种组合后，直接判断它是否有序，这样可以节约一定空间和时间，这儿不再过多介绍。 可以看到这种排序方法也是不实用的，当数据逐渐变大时，排序耗时我们是无法容忍的。 可以看到这种排序方法是不稳定的。 该方法的时间复杂度为 O(n!)。 该排序方法的空间复杂度为 O(1)。 侏儒排序（GnomeSort）简介及原理侏儒排序（GnomeSort或StupidSort）最初由伊朗计算机工程师Hamid Sarbazi-Azad博士于2000年提出并被称为“愚蠢排序”，然后由Dick Grune描述并命名为“GnomeSort”。它在概念上很简单，不需要嵌套循环。 关于GnomeSort名字的由来：Dick Grune描述了一个花园侏儒的故事，侏儒对花园花盆进行分类，他看着旁边的花盆和前一个花盆; 如果他们按照正确的顺序，他会向前迈出一步；否则他会将它们交换掉，并向后退一步；如果无法继续后退，他会继续前进; 如果他前进后前面没有了花盆，他就完成了花盆排序。 算法描述 对于给定的数组a，从0下标开始，跳过下标0； 对于大于0的下标，如3，判断数组值a[3]和a[2]大小，如果a[3]&gt;=a[2]，继续下一个值比较； 如果a[3] &lt; a[2],就会将a[3]和a[2]交换，同时我们的位置会移动到下标2上，比较a[2]（实际上是原来的a[3]）与a[1]的大小，相当于重复第二步过程； 可以看到边界条件就是位置为0和位置为数组长度-1的地方，一个是无法继续向“后退”（说明这个值是当前已排序部分的最小值），一个是无法继续“前进”（说明排序完成）。 动图演示 代码实现123456789101112131415161718192021222324252627public class GnomeSort &#123; public static void gnomeSort(int[] ar) &#123; int n = ar.length; int i = 0; while (i &lt; n) &#123; if (i == 0 || ar[i - 1] &lt;= ar[i]) &#123; i++; &#125; else &#123; int tmp = ar[i]; ar[i] = ar[i - 1]; ar[--i] = tmp; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("GnomeSort排序前："); System.out.println(Arrays.toString(a)); gnomeSort(a); System.out.println("GnomeSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 可以看到侏儒排序十分“简单”（代码层面上），它只有一层循环，但是循环次数 &gt;=n次（n为数组长度），当顺序错误（ar[i - 1] &gt; ar[i]）时，除了交互两数外，i值还会减1，这就是所说的“后退”； 我们也可以引入变量，用来记录a[i]，在与a[i - 1]及之前的数进行对比，找到位置后进行存储，这样处理的话侏儒排序类似于插入排序的变种； 可以看到该排序方法为稳定排序； 该排序方法的时间复杂度如下： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n^2) 时间复杂度（最差）：O(n^2) 该排序方法的空间复杂度为 O(1)。 鸽巢排序（PigeonholeSort）简介及原理鸽巢排序（PigeonholeSort）也被称作基数分类，原理类似桶排序,同样需要一个很大的鸽巢（桶排序里管这个叫桶），鸽巢其实就是数组，数组的索引位置就表示值，该索引位置的值表示出现次数，如果全部为1次或0次那就是桶排序。 鸽巢排序的原理：我们可以构建数组元素最大值个“鸽巢”，对于待排序数组，将它的元素值一个个对应到“鸽巢”索引值上，如果“鸽巢”该索引位置值为0，表示数组里没有此值，如果为n（&gt;=1），表示数组里该元素（索引值）有n个该值。 算法描述 给定一个待排序数组，创建一个备用数组（鸽巢），并初始化元素为0，备用数组的索引即是待排序数组的值； 把待排序数组的值，放到“鸽巢”里（即用作备用数组的索引）； 把“鸽巢”里的值再依次送回待排序数组。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445public class PigeonholeSort &#123; private static int[] pigeonhole(int[] array, int maxNumber)&#123; int[] pigeonhole = new int[maxNumber + 1]; /* * pigeonhole[10] = 4; 的含意是 * 在待排数组中有4个10出现,同理其它 */ for (int item:array) &#123; pigeonhole[item]++; &#125; return pigeonhole; &#125; public static void pigeonholeSort(int[] array)&#123; int max = array[0]; for (int e:array) &#123; if(e&gt;max)&#123; max = e; &#125; &#125; //构建鸽巢 int[] sorted = pigeonhole(array,max); int index = 0; for(int i=0;i&lt;sorted.length;i++)&#123; //如果有不为空的说明放入了元素，下标即为元素值 if(sorted[i]!=0)&#123; for(int k=0;k&lt;sorted[i];k++)&#123; array[index] = i; index++; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("PigeonholeSort排序前："); System.out.println(Arrays.toString(a)); pigeonholeSort(a); System.out.println("PigeonholeSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 感觉是不是很像计数排序？是的，鸽巢排序的一个比较有名的变形就是计数排序，对于解决指定问题有奇效； 我们可以看到，如果我们数组元素差值较大，比如[1,100,100000,888888]这个数组，我们使用鸽巢排序，会浪费很大空间，所以它只有在差值(或者可被映射在差值)很小的范围内的数值排序的情况下适用（鸽巢也可以是对象数组等，对象的比较需要有具体实现等）; 我们一般很少使用鸽巢排序, 因为它很少可以在灵活性, 简便性, 尤是速度上超过其他主流排序算法； 该排序算法为稳定排序算法； 该排序算法时间复杂度如下： 时间复杂度（最好）： O(n+k) 时间复杂度（平均）： O(n+k) 时间复杂度（最差）： O(n+k) 该排序算法空间复杂度 ： O(n*k) 梳排序（CombSort）简介及原理梳排序（CombSort）是一种由Wlodzimierz Dobosiewicz于1980年所发明的不稳定排序算法，并由Stephen Lacey和Richard Box于1991年四月号的Byte杂志中推广。 梳排序改良自冒泡排序和快速排序，其要旨在于消除“乌龟”（亦即在数组尾部的小数值），这些数值是造成冒泡排序缓慢的主因。相对地，“兔子”（亦即在数组前端的大数值）不影响冒泡排序的性能。 我们知道，在冒泡排序中，只比较数组中相邻的二项，即比较的二项的间距（Gap）是1，梳排序提出此间距其实可大于1。 梳排序中，开始时的间距设置为数组长度，并在循环中以固定比率递减，即递减率。在一次循环中，梳排序如同冒泡排序一样把数组从首到尾扫描一次，比较及交换两项，不同的是两项的间距不固定于1。如果间距递减至1，梳排序假定输入数组大致排序好，并以冒泡排序作最后检查及修正。 递减率的设置影响着梳排序的效率，原作者以随机数作实验，得到最有效递减率为1.3。 如果此比率太小，则导致循环中有过多的比较，如果比率太大，则未能有效消除数组中的“乌龟”。有时候我们也会取递减率倒数与间距相乘（因为编程语言乘法较快）来进行计算，这个倒数通常取0.8. 算法描述 对于待排序数组，我们开始以数组长度为间距delta，比较两值，如果a[i] &gt; a[i + delta]，则交换他们的位置； 根据递减率shrink，先判断间距delta是否 &gt;1，是的话下次间距变为delta = delta/shrink ，继续第一步比较； 最后再用冒泡排序排序一遍得到排序好的数组。 动图演示 代码实现12345678910111213141516171819202122232425262728293031323334public class CombSort &#123; public static void combSort(int[] data) &#123; int n = data.length; final double shrink = 1.3; int i, delta = n, noswap = 0; while (noswap == 0) &#123; for (noswap = 1, i = 0; i + delta &lt; n; i++) &#123; if (data[i] &gt; data[i + delta]) &#123; data[i] ^= data[i + delta]; data[i + delta] ^= data[i]; data[i] ^= data[i + delta]; noswap = 0; &#125; &#125; if (delta &gt; 1) &#123; delta /= shrink; noswap = 0; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("CombSort排序前："); System.out.println(Arrays.toString(a)); combSort(a); System.out.println("CombSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 上述代码我们使用了除法计算循环间距，当然也可以使用乘法，乘数可以取0.8； 梳排序的效率在开始时最佳，接近结束时，即进入冒泡排序时最差。如果间距delta变得太小时(例如小于10)，我们可以改用插入排序等其他排序算法，提升整体性能； 梳排序是一种不稳定排序算法； 该排序算法时间复杂度如下： 时间复杂度（平均）： O(n^2) 时间复杂度（最差）： O(n^2) 该排序算法空间复杂度 ： O(1) 鸡尾酒排序（CocktailSort）简介及原理鸡尾酒排序（CocktailSort）是冒泡排序（BubbleSort）的一种变形，也称双向冒泡排序。 之所以称为双向冒泡排序，是因为该排序算法与冒泡排序的不同处在于排序时是以双向在序列中进行排序。如何双向呢？ 对于冒泡排序，我们每次是由左到右（或者由右到左）依次比较序列里的每个元素；而对于鸡尾酒排序，我们是先由左到右然后在由右到左去比较序列中的元素。 算法描述 对于待排序数组，对于每次循环，我们都会由左到右（升序）进行冒泡排序，然后在由右到左（降序）进行冒泡排序； 这样对于外层循环，我们只需循环数组一半长度即可； 先对数组从左到右排序（升序）将最大的数字放在最右端; 再对数组从右到左排序（降序）将最小的数字放在最左端; 以此类推（先找最大，再找最小，然后找第二大，再找第二小），不断缩小未排序数字的范围，直到最后一个数字结束。 动图演示 代码实现1234567891011121314151617181920212223242526272829303132333435public class CocktailSort &#123; public static void cocktailSort(int[] src) &#123; for (int i = 0; i &lt; src.length / 2; i++) &#123; //将最小值排到队首 for (int j = i; j &lt; src.length - i - 1; j++) &#123; if (src[j] &gt; src[j + 1]) &#123; int temp = src[j]; src[j] = src[j + 1]; src[j + 1] = temp; &#125; &#125; //将最大值排到队尾 for (int j = src.length - 1 - (i + 1); j &gt; i; j--) &#123; if (src[j] &lt; src[j - 1]) &#123; int temp = src[j]; src[j] = src[j - 1]; src[j - 1] = temp; &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = r.nextInt(10000); &#125; System.out.println("CocktailSort排序前："); System.out.println(Arrays.toString(a)); cocktailSort(a); System.out.println("CocktailSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 我们可以看到，鸡尾酒排序相对于普通冒泡排序减少了数据比较次数，因此鸡尾酒排序是冒泡排序的一种优化，其理论性能会高于普通冒泡排序； 鸡尾酒排序的时间复杂度： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n^2) 时间复杂度（最差）：O(n^2) 空间复杂度：O(1) 该排序算法为稳定排序算法。 折半插入排序（BinaryInsertionSort）简介及原理折半插入排序（BinaryInsertionSort）又称二分插入排序，是普通插入排序（InsertionSort）的一种优化。 听名字也很好理解，对于普通插入排序，我们是和已排队列一个一个比较找到该值要插入的位置；而对于折半插入排序，是在已排数组中通过二分查找查找到数据插入位置，在将数据统一后移来实现。 算法描述 二分法查找插入位置，创建两个指针 low = 0，high = i-1； mid = (low+high)/2 ,对于data[i]，如果data[i] &lt; data[mid]，说明还要向小查找，此时将high = mid-1；如果data[i] &gt; data[mid]。说明还要向大查找，此时将low = mid+1； 需要判断low和high，如果low &gt; high，就无需继续查找了，要插入的位置即为low，否则继续进行2步骤。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839public class BinaryInsertionSort &#123; public static int[] binaryInsertionSort(int[] data) &#123; for (int i = 1, len = data.length; i &lt; len; i++) &#123; // 要插入的元素 int temp = data[i]; int low = 0; int high = i - 1; // 折半比较，直到找到low大于high时（找到比他大的值的位置low） while(low &lt;= high) &#123; int mid = (low+high)/2; if (data[mid] &gt; temp) &#123; high = mid - 1; &#125; else &#123; low = mid + 1; &#125; &#125; // 移动 比他大的值，全部后移 for (int j = i; j &gt; low; j--) &#123; data[j] = data[j-1]; &#125; // 插入 data[low] = temp; &#125; return data; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("BinaryInsertionSort排序前："); System.out.println(Arrays.toString(a)); int[] s = binaryInsertionSort(a); System.out.println("BinaryInsertionSort排序后："); System.out.println(Arrays.toString(s)); &#125;&#125; 其他注意事项 和普通插入排序比起来，因为使用了二分查找，所以理论上比较次数平均会少一些； 折半查找只是减少了比较次数，但是元素的移动次数不变； 该排序时间复杂度： 时间复杂度和排序完成度没有关系，和数组大小有关系，因此该排序时间复杂度与普通插入排序相当。 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n^2) 时间复杂度（最差）：O(n^2) 空间复杂度：O(1) 该排序算法为稳定排序算法。 圈排序（CycleSort）简介及原理圈排序的理论基础是，如果每个坏组的元素返回到它们的正确位置，那么整个序列将被排序。 什么是坏组？ 给定一个对象序列，例如一个整数数组;如果这些元素没有按顺序排列，那是因为其中一些元素在它们之间交换了位置。如下： 对于数列 [4,1,2,3,5,0]不是按顺序排列的，因为4,5,0交换了位置(1个坏组)，[4、2、3、7、5、0、1]顺序不对，因为4、5、0交换了位置，2、3、7、1交换了位置(两个坏组)。 在离散数学中，每一组，无论好坏，都被称为一个周期或一个轨道。 我们按照此理论进行的排序便可称为圈排序。 算法描述 对于待排序数组，比如[5、3、4、8、6、1、2]，可以看到它的有序数组为[1、2、3、4、5、6、8]； 可以看到5、3、4、8、6、1、2顺序不对，不在自己位置上，5应该在6的位置，6应该在1的位置，1应该在5的位置上，因此5、6、1构成了一个坏组；同理3应该在4位置，4应该在8位置，8应该在2位置，2应该在3位置，因此3、4、8、2构成一个坏组； 我们通过循坏将上述两个坏组的位置调整，单项循坏，位置变化 5-&gt;6-&gt;1-&gt;5，3-&gt;4-&gt;8-&gt;2-&gt;3，便可以得到有序数组[1、2、3、4、5、6、8]。 动图演示 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class CycleSort &#123; public static void cycleSort(int[] array) &#123; for (int cs = 0, seeker, pos; cs &lt; array.length - 1; cs++) &#123; //假设array[cs]中的元素不合适 seeker = array[cs]; pos = cs; //找到seeker的正确位置(pos) for (int i = cs + 1; i &lt; array.length; i++) &#123; if (array[i] &lt; seeker) &#123; pos++; &#125; &#125; //如果seeker已经在正确的位置，继续 if (pos == cs) &#123; continue; &#125; //复制后移动索引pos(如果有的话) while (seeker == array[pos]) &#123; pos++; &#125; //seeker放到了它正确的位置（索引pos处），同时原来pos处的元素成为了新的seeker，需要找到另一个位置 seeker = set(array, seeker, pos); //在进入下一个循环之前完成当前循环。在当前周期结束时，pos==cs，因为一个周期总是在它开始的地方结束。 while (pos != cs) &#123; //代码同上 pos = cs; for (int i = cs + 1; i &lt; array.length; i++) &#123; if (array[i] &lt; seeker) &#123; pos++; &#125; &#125; while (seeker == array[pos]) &#123; pos++; &#125; seeker = set(array, seeker, pos); &#125; &#125; &#125; private static int set(int[] array, int data, int ndx) &#123; try &#123; return array[ndx]; &#125; finally &#123; array[ndx] = data; &#125; &#125; public static void main(String[] args) &#123; int[] a = new int[100]; Random r = new Random(); for (int i = 0; i &lt; a.length; i++) &#123; a[i] = r.nextInt(10000); &#125; System.out.println("CycleSort排序前："); System.out.println(Arrays.toString(a)); cycleSort(a); System.out.println("CycleSort排序后："); System.out.println(Arrays.toString(a)); &#125;&#125; 其他注意事项 可以看到圈排序期间，每个元素最多移动一次； 该算法的时间复杂度： 时间复杂度（最好）：O(n) 时间复杂度（平均）：O(n^2) 时间复杂度（最差）：O(n^2) 该算法空间复杂度：O(1) 该算法是不稳定排序算法。 总结通过上述10种排序算法，我们了解了许多“奇奇怪怪”的排序算法，以及它们的一些原理，排序算法也远远不止这些，后面我们还会介绍更多的排序算法，及其一些基本原理。 源码上述文章中包含本章中所有的源代码，有兴趣的同学也可以通过我的 Github 查看源码。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka详解]]></title>
    <url>%2Fblog%2FEureka%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[前言上一篇文章Eureka简介及简单使用 我们通过一些简单示例，对Eureka有了一些简单了解。 我们这片文章将深入了解Eureka，详细介绍Eureka的基础架构、节点间的通信机制以及一些进阶配置等。 正文基础架构Eureka服务治理基础架构的三个核心要素： 服务注册中心：Eureka提供的服务端，提供服务注册与发现功能。也就是上篇文章里的eureka-server。 服务提供者：提供服务的应用，可以是SpringBoot应用，也可以是其他技术平台且遵循Eureka通信机制的应用。它将自己提供的服务注册到Eureka，以供其他应用发现。也就是上篇文章里的sakura-service。 服务消费者：消费者应用从服务注册中心获取服务列表，从而使消费者可以知道去何处调用其所需要的服务。也就是上篇文章里的sakura-consumer。 注：很多时候，客户端既是服务提供者也是服务调用者。 服务治理机制要理解服务治理机制，我们先看一下这张图。 根据上图，我们来详细了解下Eureka从服务注册到服务调用，及各个元素所涉及的一些重要通信行为。 服务提供者服务提供者主要有服务注册、服务续约、服务下线等行为，我们分别来看下。 服务注册 服务提供者在启动的时候会通过发送REST请求的发送将自己注册到Eureka Server上，同时带上自身服务的一些元数据信息。Eureka Server接受到这个REST请求之后，将元数据信息存储在一个双层结构Map中，其中第一层key是服务名，第二层key是具体服务的实例名。 服务注册时，需要 eureka.client.register-with-eureka参数为true(默认为true)，如果设置为false该服务实例将不会进行服务注册操作。 服务续约 服务注册完成后，服务提供者会维护一个心跳，用来持续告诉Eureka Server：“我还活着”，以防止Eureka Server的“剔除任务”将该服务实例从服务列表剔除，我们称该操作为服务续约。 服务续约有两个重要参数可以进行配置(Eureka Server的配置文件里)，如下： 1234# 定义服务续约任务的调用间隔时间，默认30seureka.instance.lease-renewal-interval-in-seconds=30# 定义服务失效的时间，默认90seureka.instance.lease-expiration-duration-in-seconds=90 服务下线 在系统运行过程中存在临时关闭或者重启某个实例的情况，在该实例服务关闭期间，我们不希望客户端继续调用服务关闭的实例。所以在客户端程序中，当服务实例进行正常的关闭操作时，它会触发一个服务下线的REST请求给Eureka Server，告诉注册中心“我要下线了”，注册中心收到请求后会将该服务状态置为下线（DOWN），并把该下线事件传播出去。 服务消费者服务消费者主要有获取服务、服务调用等行为，我们分别来看下。 获取服务 当我们启动服务消费者时，它会发送一个REST请求给服务注册中心，来获取上面注册的服务清单。为了性能考虑，Eureka Server会维护一份只读的服务清单来返回给客户端，同时该缓存清单会每个30s更新一次。 如果想要修改缓存清单更新时间，可以通过下面参数进行修改，该参数默认30s。 1eureka.client.registry-fetch-interval-seconds=30 获取服务列表是服务消费者的基础，可以通过修改下面参数为false使服务消费者不生效，该值默认为true。 1eureka.client.fetch-registry=true 服务调用 服务消费者在获取到服务清单后，通过服务名可以获取到提供该服务的实例名和该实例的元数据信息。有了这些服务实例的详细信息，服务消费者可以根据自己的需要选择要调用的实例。如采用轮询等方式，进而实现客户端负载均衡。 对于访问实例的选择，Eureka中有Region和Zone的概念，一个Region中可以包含多个Zone，每个服务客户端需要被注册到一个Zone中，所以每个客户端对应一个Region和一个Zone。在进行服务调用的时候，优先访问同处于一个Zone的服务提供方，若访问不到，就访问其他的Zone。 服务注册中心服务注册中心主要有服务同步、失效剔除、自我保护等行为，我们分别来看下。 服务同步 如上图，两个服务提供者分别注册到了两个不同的服务注册中心上，即它们的信息分别被两个服务注册中心维护。由于服务注册中心之间相互注册为服务，当服务提供者发送注册请求到一个注册中心时，注册中心会将该请求转发给集群中相连的其他注册中心，从而实现注册中心之间的服务同步。 服务同步后，两个服务提供者的服务信息可以在这两台注册中心中任意一台上获取到。 失效剔除 有些时候，我们服务实例不一定会正常下线，可能由于内存溢出、网络故障等原因导致该实例无法正常工作，而注册中心无法收到下线请求。为剔除这些无法提供服务的实例，Eureka Server在启动时会创建一个定时任务，每个一段时间（默认60s）将当前清单中超时（默认90s）没有续约的服务实例剔除出去。 自我保护 我们知道，服务注册到Eureka Server之后，会维护一个心跳连接，告诉Eureka Server自己还活着。Eureka Server在运行期间，会统计心跳的比例在15min之内是否低于85%，如果出现低于的情况（如单机调试、生产环境网络问题等），Eureka Server会将当前的实例信息保护起来，让这些实例不会过期，尽可能保护这些注册信息。但是保护期内如果实例出现问题，服务调用方拿到已经不存在的服务实例，就会出现调用失败的情况，所以服务调用方需要有容错机制，如请求重试、断路器等。 Eureka Server进入自我保护后，注册中心信息面板上会出现类型下面的红色警告信息： 1EMERGENCY! EUREKA MAY BE INCORRECTLY CLAIMING INSTANCES ARE UP WHEN THEY&apos;RE NOT. RENEWALS ARE LESSER THAN THRESHOLD AND HENCE THE INSTANCES ARE NOT BEING EXPIRED JUST TO BE SAFE. 我们本地调试很容易触发Eureka的自我保护机制，这可能导致我们的服务实例不是那么准确。我们可以通过设置下面参数来关闭自我保护机制，该值默认为true，表示开启自我保护机制。 1eureka.server.enable-self-preservation=false 源码分析我们通过@EnableDiscoveryClient这个注解，可以找到com.netflix.discovery.DiscoveryClient这个类，过程略。 我们来看下这个类，这个类是Netflix开源包中的内容，位于eureka-client包下。 这个类主要有以下功能： 向Eureka Server注册服务实例； 向Eureka Server服务租约； 当服务关闭时，向Eureka Server取消租约； 查询Eureka Server中的服务实例列表。 为保证与Eureka Server交互，我们需要配置一个Eureka Server的URL列表，即之前所说的eureka.client.service-url参数。 关于这个类，我们主要看一下它的initScheduledTasks方法。我们可以看到这个方法在DiscoveryClient的构造器里被调用。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071private void initScheduledTasks() &#123; if (clientConfig.shouldFetchRegistry()) &#123; // registry cache refresh timer int registryFetchIntervalSeconds = clientConfig.getRegistryFetchIntervalSeconds(); int expBackOffBound = clientConfig.getCacheRefreshExecutorExponentialBackOffBound(); scheduler.schedule( new TimedSupervisorTask( "cacheRefresh", scheduler, cacheRefreshExecutor, registryFetchIntervalSeconds, TimeUnit.SECONDS, expBackOffBound, new CacheRefreshThread() ), registryFetchIntervalSeconds, TimeUnit.SECONDS); &#125; if (clientConfig.shouldRegisterWithEureka()) &#123; int renewalIntervalInSecs = instanceInfo.getLeaseInfo().getRenewalIntervalInSecs(); int expBackOffBound = clientConfig.getHeartbeatExecutorExponentialBackOffBound(); logger.info("Starting heartbeat executor: " + "renew interval is: &#123;&#125;", renewalIntervalInSecs); // Heartbeat timer scheduler.schedule( new TimedSupervisorTask( "heartbeat", scheduler, heartbeatExecutor, renewalIntervalInSecs, TimeUnit.SECONDS, expBackOffBound, new HeartbeatThread() ), renewalIntervalInSecs, TimeUnit.SECONDS); // InstanceInfo replicator instanceInfoReplicator = new InstanceInfoReplicator( this, instanceInfo, clientConfig.getInstanceInfoReplicationIntervalSeconds(), 2); // burstSize statusChangeListener = new ApplicationInfoManager.StatusChangeListener() &#123; @Override public String getId() &#123; return "statusChangeListener"; &#125; @Override public void notify(StatusChangeEvent statusChangeEvent) &#123; if (InstanceStatus.DOWN == statusChangeEvent.getStatus() || InstanceStatus.DOWN == statusChangeEvent.getPreviousStatus()) &#123; // log at warn level if DOWN was involved logger.warn("Saw local status change event &#123;&#125;", statusChangeEvent); &#125; else &#123; logger.info("Saw local status change event &#123;&#125;", statusChangeEvent); &#125; instanceInfoReplicator.onDemandUpdate(); &#125; &#125;; if (clientConfig.shouldOnDemandUpdateStatusChange()) &#123; applicationInfoManager.registerStatusChangeListener(statusChangeListener); &#125; instanceInfoReplicator.start(clientConfig.getInitialInstanceInfoReplicationIntervalSeconds()); &#125; else &#123; logger.info("Not registering with Eureka server per configuration"); &#125;&#125; 可以看到这个方法里面主要有几个定时任务线程，分别被两个if包围，分别是if (clientConfig.shouldFetchRegistry()) 和if (clientConfig.shouldRegisterWithEureka())，这两个参数也是我们上面说的可以配置的，代表着是否开启服务消费eureka.client.fetch-registry=true和是否开启服务注册eureka.client.register-with-eureka=true。 对于if (clientConfig.shouldRegisterWithEureka())逻辑里我们可以看到创建了InstanceInfoReplicator的一个实例并启动，该类继承Runable接口，我们找到它的run方法，如下： 12345678910111213141516public void run() &#123; try &#123; discoveryClient.refreshInstanceInfo(); Long dirtyTimestamp = instanceInfo.isDirtyWithTime(); if (dirtyTimestamp != null) &#123; discoveryClient.register(); instanceInfo.unsetIsDirty(dirtyTimestamp); &#125; &#125; catch (Throwable t) &#123; logger.warn("There was a problem with the instance info replicator", t); &#125; finally &#123; Future next = scheduler.schedule(this, replicationIntervalSeconds, TimeUnit.SECONDS); scheduledPeriodicRef.set(next); &#125;&#125; 可以看到里面调用了discoveryClient.register()方法，这个方法便是服务注册的主要方法，我们看下： 1234567891011121314boolean register() throws Throwable &#123; logger.info(PREFIX + "&#123;&#125;: registering service...", appPathIdentifier); EurekaHttpResponse&lt;Void&gt; httpResponse; try &#123; httpResponse = eurekaTransport.registrationClient.register(instanceInfo); &#125; catch (Exception e) &#123; logger.warn(PREFIX + "&#123;&#125; - registration failed &#123;&#125;", appPathIdentifier, e.getMessage(), e); throw e; &#125; if (logger.isInfoEnabled()) &#123; logger.info(PREFIX + "&#123;&#125; - registration status: &#123;&#125;", appPathIdentifier, httpResponse.getStatusCode()); &#125; return httpResponse.getStatusCode() == Status.NO_CONTENT.getStatusCode();&#125; 可以看到注册操作通过REST进行，发送一个com.netflix.appinfo.InstanceInfo对象，该对象就是注册时客户端给服务端的服务元数据。 继续跟进registrationClient.register方法，找到RestTemplateEurekaHttpClient实现的register方法，代码如下： 12345678910111213public EurekaHttpResponse&lt;Void&gt; register(InstanceInfo info) &#123; String urlPath = serviceUrl + "apps/" + info.getAppName(); HttpHeaders headers = new HttpHeaders(); headers.add(HttpHeaders.ACCEPT_ENCODING, "gzip"); headers.add(HttpHeaders.CONTENT_TYPE, MediaType.APPLICATION_JSON_VALUE); ResponseEntity&lt;Void&gt; response = restTemplate.exchange(urlPath, HttpMethod.POST, new HttpEntity&lt;&gt;(info, headers), Void.class); return anEurekaHttpResponse(response.getStatusCodeValue()) .headers(headersOf(response)).build();&#125; 可以看到注册是以gzip和POST请求发送的。 在com.netflix.eureka.resources.ApplicationsResource类，可以看到服务注册中心是如何处理服务注册请求的，如下： 12345678910111213@Path("/&#123;version&#125;/apps")@Produces(&#123;"application/xml", "application/json"&#125;)public class ApplicationsResource &#123; ... @Path("&#123;appId&#125;") public ApplicationResource getApplicationResource( @PathParam("version") String version, @PathParam("appId") String appId) &#123; CurrentRequestVersion.set(Version.toEnum(version)); return new ApplicationResource(appId, serverConfig, registry); &#125; ...&#125; 123456789@POST@Consumes(&#123;"application/json", "application/xml"&#125;)public Response addInstance(InstanceInfo info, @HeaderParam(PeerEurekaNode.HEADER_REPLICATION) String isReplication) &#123; ...//部分代码略 registry.register(info, "true".equals(isReplication)); return Response.status(204).build(); // 204 to be backwards compatible ...&#125; 经过一些校验然后进行注册。 12345678910111213@Overridepublic void register(final InstanceInfo info, final boolean isReplication) &#123; handleRegistration(info, resolveInstanceLeaseDuration(info), isReplication); super.register(info, isReplication);&#125;private void handleRegistration(InstanceInfo info, int leaseDuration, boolean isReplication) &#123; log("register " + info.getAppName() + ", vip " + info.getVIPAddress() + ", leaseDuration " + leaseDuration + ", isReplication " + isReplication); publishEvent(new EurekaInstanceRegisteredEvent(this, info, leaseDuration, isReplication));&#125; 最终通过publishEvent方法将注册事件传播出去，提供给其他Eureka Server。这一块的过多代码我们不在分析。 再来看下com.netflix.discovery.DiscoveryClient这个类剩下的两个Timer，它们分别调用了new HeartbeatThread()和new CacheRefreshThread()，明显知道两个分别是心跳检测和刷新线程，里面也可以看到它们也可以通过参数进行配置一些信息。 我们先来看下HeartbeatThread类，代码如下： 12345678private class HeartbeatThread implements Runnable &#123; public void run() &#123; if (renew()) &#123; lastSuccessfulHeartbeatTimestamp = System.currentTimeMillis(); &#125; &#125;&#125; 可以看到renew方法，这是服务续约的主要方法，我们看下代码： 123456789101112131415161718192021boolean renew() &#123; EurekaHttpResponse&lt;InstanceInfo&gt; httpResponse; try &#123; httpResponse = eurekaTransport.registrationClient.sendHeartBeat(instanceInfo.getAppName(), instanceInfo.getId(), instanceInfo, null); logger.debug(PREFIX + "&#123;&#125; - Heartbeat status: &#123;&#125;", appPathIdentifier, httpResponse.getStatusCode()); if (httpResponse.getStatusCode() == Status.NOT_FOUND.getStatusCode()) &#123; REREGISTER_COUNTER.increment(); logger.info(PREFIX + "&#123;&#125; - Re-registering apps/&#123;&#125;", appPathIdentifier, instanceInfo.getAppName()); long timestamp = instanceInfo.setIsDirtyWithTime(); boolean success = register(); if (success) &#123; instanceInfo.unsetIsDirty(timestamp); &#125; return success; &#125; return httpResponse.getStatusCode() == Status.OK.getStatusCode(); &#125; catch (Throwable e) &#123; logger.error(PREFIX + "&#123;&#125; - was unable to send heartbeat!", appPathIdentifier, e); return false; &#125; &#125; 这段代码不再过多介绍。 我们再来看下CacheRefreshThread类，这个类可以获取服务列表并缓存。 12345class CacheRefreshThread implements Runnable &#123; public void run() &#123; refreshRegistry(); &#125;&#125; refreshRegistry方法和fetchRegistry方法代码较多，我只展示了关键部分。 12345678910@VisibleForTestingvoid refreshRegistry() &#123; ... boolean success = fetchRegistry(remoteRegionsModified); if (success) &#123; registrySize = localRegionApps.get().size(); lastSuccessfulRegistryFetchTimestamp = System.currentTimeMillis(); &#125; ...&#125; 123456789101112131415private boolean fetchRegistry(boolean forceFullRegistryFetch) &#123; ... if (clientConfig.shouldDisableDelta() || (!Strings.isNullOrEmpty(clientConfig.getRegistryRefreshSingleVipAddress())) || forceFullRegistryFetch || (applications == null) || (applications.getRegisteredApplications().size() == 0) || (applications.getVersion() == -1)) //Client application does not have latest library supporting delta &#123; getAndStoreFullRegistry(); &#125; else &#123; getAndUpdateDelta(applications); &#125; ... &#125; 关于fetchRegistry方法，可以看到它会根据是否第一次进行服务获取而发起了不同的REST请求和响应，如果是第一次，会获取全部服务信息getAndStoreFullRegistry方法，否则获取更新部分的信息getAndUpdateDelta。 关于Eureka中的Region、Zone和serviceUrls，我们来看下，通过配置的eureka.client.service-url属性，我们可以很方便的找到org.springframework.cloud.netflix.eureka.EurekaClientConfigBean类中的getEurekaServerServiceUrls方法，它的代码如下： 1234567891011121314151617181920public List&lt;String&gt; getEurekaServerServiceUrls(String myZone) &#123; String serviceUrls = this.serviceUrl.get(myZone); if (serviceUrls == null || serviceUrls.isEmpty()) &#123; serviceUrls = this.serviceUrl.get(DEFAULT_ZONE); &#125; if (!StringUtils.isEmpty(serviceUrls)) &#123; final String[] serviceUrlsSplit = StringUtils .commaDelimitedListToStringArray(serviceUrls); List&lt;String&gt; eurekaServiceUrls = new ArrayList&lt;&gt;(serviceUrlsSplit.length); for (String eurekaServiceUrl : serviceUrlsSplit) &#123; if (!endsWithSlash(eurekaServiceUrl)) &#123; eurekaServiceUrl += "/"; &#125; eurekaServiceUrls.add(eurekaServiceUrl.trim()); &#125; return eurekaServiceUrls; &#125; return new ArrayList&lt;&gt;();&#125; 这个方法在com.netflix.discovery.endpoint.EndpointUtils的getServiceUrlsFromConfig方法会被调用，用于获取Region和Zone。 123456789101112131415161718192021222324252627282930313233public static List&lt;String&gt; getServiceUrlsFromConfig(EurekaClientConfig clientConfig, String instanceZone, boolean preferSameZone) &#123; List&lt;String&gt; orderedUrls = new ArrayList&lt;String&gt;(); String region = getRegion(clientConfig); String[] availZones = clientConfig.getAvailabilityZones(clientConfig.getRegion()); if (availZones == null || availZones.length == 0) &#123; availZones = new String[1]; availZones[0] = DEFAULT_ZONE; &#125; logger.debug("The availability zone for the given region &#123;&#125; are &#123;&#125;", region, availZones); int myZoneOffset = getZoneOffset(instanceZone, preferSameZone, availZones); List&lt;String&gt; serviceUrls = clientConfig.getEurekaServerServiceUrls(availZones[myZoneOffset]); if (serviceUrls != null) &#123; orderedUrls.addAll(serviceUrls); &#125; int currentOffset = myZoneOffset == (availZones.length - 1) ? 0 : (myZoneOffset + 1); while (currentOffset != myZoneOffset) &#123; serviceUrls = clientConfig.getEurekaServerServiceUrls(availZones[currentOffset]); if (serviceUrls != null) &#123; orderedUrls.addAll(serviceUrls); &#125; if (currentOffset == (availZones.length - 1)) &#123; currentOffset = 0; &#125; else &#123; currentOffset++; &#125; &#125; if (orderedUrls.size() &lt; 1) &#123; throw new IllegalArgumentException("DiscoveryClient: invalid serviceUrl specified!"); &#125; return orderedUrls; &#125; 从上面代码可以看出，当我们没有为Region配置Zone时，将默认采用defaultZone，若要为应用指定Zone，可以通过eureka.client.availability-zones来进行指定，Zone可以设置多个，通过逗号分隔来进行设置，可以判断Region和Zone是一对多的关系。 设置Zone可以在负载均衡时实现区域亲和特性，例如Ribbon的负载均衡策略会优先访问客户端处于同一个Zone的服务端实例，该Zone域没有服务端可用实例后才会访问其他Zone。我们结合实际部署物理结构，可以有效地设计出针对区域性故障的容错集群。 配置信息我们上面讲了许多源码，可能很多同学也没有细看，大家更注重的是Eureka的快速上手和使用，这儿，我把Eureka配置的一些参数总结下，来供大家参考，用于解决一些实际问题。 Eureka client 配置项 参数Key 说明 参数Value类型 参数Value默认值 注意 eureka.client.register-with-eureka 是否向注册中心注册当前实例 boolean true 对于高可用的Eureka Server集群，该值需为true，以形成一组相互注册的Server集群 eureka.client.fetch-registry 是否需要检索服务信息（从注册中心获取服务信息） boolean true 同上 eureka.client.enabled 是否启用Eureka Client boolean true eureka.client.service-url 服务注册地址(注册中心地址)（一般使用默认配置eureka.client.service-url.defaultZone） Map&lt;String,String&gt; 该值由于是Map，有一个默认key值defaultZone（常用），也可以自己指定Zone，如下格式eureka.client.service-url.testZone=http://172.30.13.173:8002/eureka/,http://172.30.13.173:8003/eureka/ 参数是服务注册中心地址，多个地址以逗号隔开，其中testZone需要通过eureka.client.availability-zones参数指定 eureka.client.availability-zones 可用的Zone列表（一般无需配置） Map&lt;String,String&gt; 该值由于是Map，有一个默认key值us-east-1（代表Region），也可以自己指定Region，如下格式eureka.client.availability-zones.testRegion=testZone,testZone1 参数表示可用的Zone，以逗号分开，其中testRegion需要通过eureka.client.region指定 eureka.client.region Eureka Client Region域 String us-east-1 eureka.client.registry-fetch-interval-seconds 从注册中心同步服务信息的间隔时间（单位：秒） int 30 eureka.client.prefer-same-zone-eureka 是否偏好使用处于相同Zone的Eureka服务端 boolean true eureka.client.filter-only-up-instances 获取实例时是否过滤，仅保留UP状态的实例 boolean true eureka.client.use-dns-for-fetching-service-urls 使用DNS来获取Eureka服务端的serviceUrl boolean false eureka.client.heartbeat-executor-thread-pool-size 心跳连接池的初始化线程数 int 2 eureka.client.heartbeat-executor-exponential-back-off-bound 心跳超时重试延迟时间的最大乘数值 int 10 eureka.client.cache-refresh-executor-thread-pool-size 缓存刷新线程池的初始化线程数 int 2 eureka.client.cache-refresh-executor-exponential-back-off-bound 缓存刷新超时重试延迟时间的最大乘数值 int 10 eureka.client.healthcheck.enabled 是否进行健康状态检测 boolean true eureka.client.instance-info-replication-interval-seconds 更新实例信息的变化到Eureka服务端的间隔时间，单位为秒 int 30 eureka.client.initial-instance-info-replication-interval-seconds 初始化实例信息的变化到Eureka服务端的间隔时间，单位为秒 int 40 eureka.client.eureka-service-url-poll-interval-seconds 轮询Eureka服务地址更改的时间间隔，单位秒 int 300 当我们与Spring Cloud Config配合，动态刷新Eureka的serviceURL地址时需要关注该参数 eureka.client.eureka-server-read-timeout-seconds 读取Eureka Server信息超时时间，单位秒 int 8 eureka.client.eureka-server-connect-timeout-seconds 连接Eureka Server的超时时间，单位秒 int 5 eureka.client.eureka-server-total-connections 从Eureka客户端到Eureka服务端的连接总数 int 200 eureka.client.eureka-server-total-connections-per-host 从Eureka客户端到每个Eureka服务端的连接总数 int 50 eureka.client.eureka-connection-idle-timeout-seconds Eureka服务端空闲连接关闭时间，单位秒 int 30 eureka.client.eureka-server-port 获取要用于构造服务url的端口，以便在eureka服务器列表来自DNS时联系eureka服务器 String 如果返回服务url eurekaServerServiceUrls(字符串)，则不需要此信息。当useDnsForFetchingServiceUrls设置为true时，将使用DNS机制，eureka客户机希望DNS以某种方式配置，以便它能够动态获取更改的eureka服务器。更改在运行时生效 eureka.client.eureka-server-d-n-s-name 获取要查询的DNS名称，以获得eureka服务器列表 String 如果通过实现serviceUrls返回服务url，则不需要此信息。当useDnsForFetchingServiceUrls设置为true时，将使用DNS机制，eureka客户机希望DNS以某种方式配置，以便它能够动态获取更改的eureka服务器。更改在运行时生效 eureka.client.eureka-server-u-r-l-context 获取要用于构造服务URL的URL上下文，以便在eureka服务器列表来自DNS时通知eureka服务器 String 如果从eurekaServerServiceUrls返回服务url，则不需要此信息。当useDnsForFetchingServiceUrls设置为true时，将使用DNS机制，eureka客户机希望DNS以某种方式配置，以便它能够动态获取更改的eureka服务器。更改在运行时生效 eureka.client.proxy-host 获取eureka服务的代理主机 String eureka.client.proxy-port 获取eureka服务的代理端口 String eureka.client.proxy-user-name 获取eureka服务的代理用户名 String eureka.client.proxy-password 获取eureka服务的代理密码 String eureka.client.g-zip-content eureka注册表的内容是否要进行压缩 boolean true eureka.client.allow-redirects 服务器是否可以将客户机请求重定向到备份服务器/集群 boolean false 如果设置为false，服务器将直接处理请求，如果设置为true，它可能向客户机发送HTTP重定向，并提供一个新的服务器位置。 eureka.client.should-enforce-registration-at-init 客户端是否应在初始化期间强制注册 boolean false eureka.client.should-unregister-on-shutdown 客户端关闭时是否应显式地从远程服务器注销自己 boolean true eureka.client.registry-refresh-single-vip-address 客户端是否只对单个VIP的注册地址信息感兴趣 String eureka.client.client-data-accept 客户端数据接收的EurekaAccept名称 String full full支持LegacyJacksonJson、JacksonJson、XStreamJson、XStreamXml、JacksonXml;compact支持JacksonJsonMini、JacksonXmlMini eureka.client.fetch-remote-regions-registry 将获取eureka注册表信息的区域的逗号分隔列表 String 配置此参数后，必须为每个availability zones定义regions用于返回availabilityZones参数。否则，将导致客户端启动失败。 eureka.client.decoder-name 临时解码器 String 这是一个临时配置，一旦最新的编解码器稳定下来，就可以删除它(因为只有一个编解码器) eureka.client.encoder-name 临时编码器 String 这是一个临时配置，一旦最新的编解码器稳定下来，就可以删除它(因为只有一个编解码器) eureka.client.order 该参数可使“CompositeDiscoveryClient”对服务提供者上可用的客户端排序 int 0 eureka.client.disable-delta eureka客户端是否应该禁用对delta的抓取，从而每次都是请求获取全部服务信息 boolean false 注意，设置为true后不会增量更新服务注册信息，而是全量更新，delta获取（增量更新）可以极大地减少流量，因为eureka服务器的更改速度通常比获取速度低得多。更改在运行时的下一个服务注册信息获取周期中有效，该周期由registryFetchIntervalSeconds指定 eureka.client.log-delta-diff 是否根据服务注册信息记录eureka服务端和eureka客户端之间注册信息的差异 boolean false Eureka客户端尝试仅检索来自Eureka服务器的增量更改，以最小化网络流量。在接收到增量之后，eureka客户端将协调来自服务器的信息，以验证它没有遗漏某些信息。当客户端与服务器通信出现网络问题时，可能会发生协调失败。如果对账失败，eureka客户端将获得完整的服务注册信息。在获得完整的注册表信息时，eureka客户端可以记录客户端和服务器之间的差异，而这个设置控制着这一点。更改在运行时的下一个注册信息获取周期中有效，该周期由registryFetchIntervalSecondsr指定 eureka.client.on-demand-update-status-change 是否将客户端状态同步到远程Eureka服务器 boolean true 如果设置为true，则通过ApplicationInfoManager将本地状态变化注册/更新到远程eureka服务器 eureka.client.backup-registry-impl 获取服务注册信息实现类的名称，该类需要实现BackupRegistry接口，在Eureka客户端启动时只能触发一次，来拿到服务注册信息 String 对于需要额外弹性来处理注册信息的应用程序来说，这个参数可能会用到，否则服务注册信息我们是无法显式操作的 eureka.client.property-resolver property参数转换，可以实现自定义property文件类 PropertyResolver 需要实现PropertyResolver接口的类 eureka.client.dollar-replacement 在序列化/反序列化eureka服务器中的信息时，获取美元符号$的替换字符串 String _- eureka.client.escape-char-replacement 在序列化/反序列化eureka服务器中的信息时，获取下划线符号_的替换字符串 String __ Eureka instance 配置项 参数Key 说明 参数Value类型 参数Value默认值 注意 eureka.instance.hostname 主机名 String 不配置的话将根据操作系统的主机名来获取 eureka.instance.appname 服务名 String unknown 默认取spring.application.name的配置值，两者都没有配置为默认值unknown eureka.instance.instance-id 实例ID String 一般如下配置eureka.instance.instance-id=${server.address}:${server.port} eureka.instance.prefer-ip-address 是否使用IP定义主机名标识 boolean false eureka.instance.lease-renewal-interval-in-seconds Eureka客户端向服务端发送心跳的时间间隔，单位秒 int 30 eureka.instance.lease-expiration-duration-in-seconds Eureka服务端在收到最后一次心跳之后等待的时间上限，单位秒 int 90 超过改时间后服务端会将该实例从服务清单上剔除，从而禁止服务调用请求被发送到该实例上 eureka.instance.non-secure-port 非安全通信端口号 int 80 eureka.instance.non-secure-port-enabled 是否开启非安全通信端口号 boolean true eureka.instance.secure-port 安全通信端口号 int 443 eureka.instance.secure-port-enabled 是否开启安全通信端口号 boolean false eureka.instance.namespace 获取用于查找属性的名称空间 String eureka SpringCloud下该属性不生效，会被忽略 eureka.instance.virtual-host-name 获取为此实例定义的虚拟主机名 String unknown 这通常是其他实例使用虚拟主机名查找此实例的方式。如果你需要查找此实例，这个主机名也是完全合格的 eureka.instance.secure-virtual-host-name 获取此实例的安全的虚拟主机名 String unknown eureka.instance.environment 实例所属环境 Environment 设置后会读取指定环境下的spring.application.name属性 eureka.instance.health-check-url-path 实例健康检测的相对路径 String /actuator/health eureka.instance.health-check-url 实例健康状态检测的绝对地址URL String 如果页面就是在当前服务实例下，只需提供healthCheckUrlPath参数即可，否则可能代理的其他的服务器信息，需要提供绝对地址，如果提供了绝对地址，优先使用绝对地址 eureka.instance.secure-health-check-url 实例健康状态检查安全访问地址 String eureka.instance.home-page-url-path Eureka实例展示主页的相对路径 String / eureka.instance.home-page-url Eureka实例展示主页的绝对路径 String 参照health-check-url eureka.instance.status-page-url-path 实例状态信息监控相对地址 String /actuator/info eureka.instance.status-page-url 实例状态信息监控绝对地址 String 参照health-check-url eureka.instance.instance-enabled-onit 是否使该实例在向eureka注册后是否可以立即进行通信 boolean false 有时，应用程序可能需要进行一些预处理，然后才能处理请求；如不需预处理，可设置为true eureka.instance.registry.default-open-for-traffic-count 该实例，注册服务中心，默认打开的通信数量 int 1 eureka.instance.registry.expected-number-of-clients-sending-renews 单位时间期望的续约连接数量 int 1 此数需要大于0，以确保注册中心可以根据注册数量调整失效清除策略，如果设为0，即使注册成功也不会重置速率阈值 eureka.instance.data-center-info 返回部署此实例的数据中心。如果实例部署在AWS中，则此信息用于获取一些特定于AWS的实例信息 DataCenterInfo DataCenterInfo.Name.MyOwn eureka.instance.initial-status 实例初始化状态 InstanceStatus UP eureka.instance.ip-address 配置实例的IPAdress String 此信息仅用于测试，因为与其他实例的通信主要使用getHostName中提供的信息进行 eureka.instance.app-group-name 获取要在eureka中注册的应用程序组的名称 String eureka.instance.a-s-g-name 用于AWS平台自动扩展的与此实例关联的组名 String eureka.instance.default-address-resolution-order 默认的地址解析顺序 String[] [] eureka.instance.metadata-map 该服务实例的子定义元数据，可以被服务中心接受到 Map&lt;String,String&gt; Eureka Server配置项 参数Key 说明 参数Value类型 参数Value默认值 注意 eureka.server.enable-self-preservation Eureka Server是否启用自我保护机制 boolean true eureka.server.response-cache-update-interval-ms eureka server刷新readCacheMap的时间，单位毫秒 long 30000 client读取的是readCacheMap，这个时间决定了多久会把readWriteCacheMap的缓存更新到readCacheMap上，默认30s eureka.server.response-cache-auto-expiration-in-seconds eureka server缓存readWriteCacheMap失效时间，单位秒 long 180 这个只有在这个时间过去后缓存才会失效，失效前不会更新，过期后从registry重新读取注册服务信息，registry是一个ConcurrentHashMap。 eureka.server.eviction-interval-timer-in-ms 每次主动失效检测间隔，单位毫秒 long 60000 eureka.server.renewal-percent-threshold 阈值因子，一段时间内心跳比例是否低于此值，低于此值开启自我保护 double 0.85 eureka.server.renewal-threshold-update-interval-ms 阈值更新的时间间隔 int 900000 结合阈值因子参数我们可以看到，默认情况下，当15min内心跳比例低于85%，Eureka会进入自我保护模式 eureka.server.rate-limiter-enabled 是否启用请求频率限制 boolean false eureka.server.rate-limiter-burst-size 请求频率大小限制 int 10 eureka.server.rate-limiter-full-fetch-average-rate 请求频率的平均值 int 100 eureka.server.rate-limiter-registry-fetch-average-rate 注册服务、拉去服务列表数据的请求频率的平均值 int 500 eureka.server.rate-limiter-throttle-standard-clients 是否对标准的client进行频率请求限制。如果是false，则只对非标准client进行限制 boolean false eureka.server.rate-limiter-privileged-clients 设置信任的client list Set eureka.server.peer-eureka-nodes-update-interval-ms eureka节点间间隔多长时间更新一次数据,单位毫秒 int 600000 eureka.server.peer-eureka-status-refresh-time-interval-ms eureka服务状态的相互更新的时间间隔，单位毫秒 int 30000 eureka.server.peer-node-connect-timeout-ms eureka对等节点间连接超时时间，单位毫秒 int 200 eureka.server.peer-node-connection-idle-timeout-seconds eureka对等节点连接后的空闲时间，单位秒 int 30 eureka.server.peer-node-read-timeout-ms 节点间的读数据连接超时时间，单位毫秒 int 200 eureka.server.peer-node-total-connections eureka server 节点间连接的总共最大连接数量 int 1000 eureka.server.peer-node-total-connections-per-host eureka server 节点间连接的单机最大连接数量 int 500 eureka.server.enable-replicated-request-compression 发送复制数据是否在request中总是压缩 boolean false eureka.server.batch-replication 指示群集节点之间的复制是否应批处理以提高网络效率 boolean false eureka.server.max-elements-in-peer-replication-pool 允许备份到备份池的最大复制事件数量 int 10000 这个备份池负责除状态更新的其他事件。可以根据内存大小，超时和复制流量，来设置此值得大小 eureka.server.max-elements-in-status-replication-pool 允许备份到状态备份池的最大复制事件数量 int 10000 eureka.server.max-idle-thread-age-in-minutes-for-peer-replication 多个服务中心相互同步信息线程的最大空闲时间，单位min long 15 eureka.server.max-idle-thread-in-minutes-age-for-status-replication 状态同步线程的最大空闲时间，单位min long 10 eureka.server.max-threads-for-peer-replication 服务注册中心各个instance相互复制数据的最大线程数量 int 20 eureka.server.max-threads-for-status-replication 服务注册中心各个instance相互复制状态数据的最大线程数量 int 1 eureka.server.max-time-for-replication instance之间复制数据的通信时长，单位毫秒 int 300000 eureka.server.min-available-instances-for-peer-replication 正常的对等服务instance最小数量 int -1 -1表示服务中心为单节点。 eureka.server.min-threads-for-peer-replication instance之间相互复制开启的最小线程数量 int 5 eureka.server.min-threads-for-status-replication instance之间用于状态复制，开启的最小线程数量 int 1 eureka.server.number-of-replication-retries instance之间复制数据时可以重试的次数 int 5 eureka.server.registry-sync-retries 在服务节点启动时，eureka尝试获取注册信息的次数 int 0 eureka.server.registry-sync-retry-wait-ms 在服务节点启动时，eureka多次尝试获取注册信息的间隔时间 int 30000 eureka.server.wait-time-in-ms-when-sync-empty 当eureka server启动的时候，不能从对等节点获取instance注册信息的情况，应等待多长时间 int 300000 eureka.server.disable-delta-for-remote-regions 过期数据，是否也提供给远程region boolean false eureka.server.disable-transparent-fallback-to-other-region 回退到远程区域中的应用程序的旧行为 (如果已配置) 如果本地区域中没有该应用程序的实例, 则将被禁用 boolean false eureka.server.g-zip-content-from-remote-region 指示在服务器支持的情况下, 是否必须为远程区域压缩从eureka服务器获取的内容 boolean true eureka.server.remote-region-connect-timeout-ms 连接eureka remote note的连接超时时间，单位毫秒 int 1000 eureka.server.remote-region-app-whitelist remote region 应用白名单 Map&lt;String,String&gt; eureka.server.remote-region-connection-idle-timeout-seconds 连接eureka remote note的连接空闲时间,单位秒 int 30 eureka.server.remote-region-fetch-thread-pool-size 执行remote region 获取注册信息的请求线程池大小 int 20 eureka.server.remote-region-read-timeout-ms remote region 从对等eureka加点读取数据的超时时间，单位毫秒 int 1000 eureka.server.remote-region-registry-fetch-interval 从remote region 获取注册信息的时间间隔，单位秒 int 30 eureka.server.remote-region-total-connections remote region 连接eureka节点的总连接数量 int 1000 eureka.server.remote-region-total-connections-per-host remote region 连接eureka节点的单机连接数量 int 50 eureka.server.remote-region-trust-store remote region抓取注册信息的存储文件，而这个可靠的存储文件需要全限定名来指定 String “” eureka.server.remote-region-trust-store-password remote region 储存的文件的密码 String “changeit” eureka.server.remote-region-urls 远程region URL地址 String remote region url.多个逗号隔开 eureka.server.remote-region-urls-with-name 远程region URL地址名称 Map&lt;String,String&gt; remote region url.多个逗号隔开 eureka.server.a-s-g-cache-expiry-timeout-ms 缓存ASG信息的过期时间，单位毫秒 int 60000 eureka.server.a-s-g-query-timeout-ms 查询ASG信息的超时时间，单位毫秒 int 300 eureka.server.a-s-g-update-interval-ms 服务更新ASG信息的频率，单位毫秒 int 30000 eureka.server.a-w-s-access-id AWS访问ID String eureka.server.a-w-s-secret-key AWS安全密钥 String eureka.server.binding-strategy AWS绑定策略 AwsBindingStrategy eip eureka.server.list-auto-scaling-groups-role-name 用于从第三方AWS 帐户描述自动扩展分组的角色的名称 String eureka.server.prime-aws-replica-connections 是否应该建立连接引导 boolean true eureka.server.e-i-p-bind-rebind-retries 服务端尝试绑定候选EIP的次数 int 3 eureka.server.e-i-p-binding-retry-interval-ms 服务端绑定EIP的时间间隔.如果绑定就检查;如果绑定失效就重新绑定。当且仅当已经绑定的情况，单位毫秒 int 300000 eureka.server.e-i-p-binding-retry-interval-ms-when-unbound 服务端绑定EIP的时间间隔.当且仅当服务为绑定的情况，单位毫秒 int 60000 eureka.server.route53-bind-rebind-retries 服务端尝试绑定route53的次数 int 3 eureka.server.route53-binding-retry-interval-ms 服务端间隔多长时间尝试绑定route53，单位毫秒 int 300000 eureka.server.route53-domain-t-t-l route53 domain生存时间 int 30 eureka.server.delta-retention-timer-interval-in-ms 间隔多长时间，清除过期的delta数据 int 30000 eureka.server.disable-delta 过期数据，是否也提供给client boolean false eureka.server.log-identity-headers eureka服务端是否记录client的身份header boolean true eureka.server.retention-time-in-m-s-in-delta-queue 缓存增量数据的时间，以便在检索的时候不丢失信息 int 180000 eureka.server.sync-when-timestamp-differs 当时间戳不一致的时候，是否进行同步 boolean true eureka.server.use-read-only-response-cache 是否采用只读缓存策略，只读策略对于缓存的数据不会过期 boolean true eureka.server.json-codec-name json的转换的实现类名 String eureka.server.property-resolver 属性解析器 PropertyResolver eureka.server.xml-codec-name eureka server xml的编解码实现名称 String 总结这篇文章通过对Eureka的一些分析，简单了解了Eureka的一些工作原理，对我们使用Eureka还是蛮有帮助的。 参考资料 SpringCloud 微服务实战 Eureka源码]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Eureka</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring循环依赖问题]]></title>
    <url>%2Fblog%2Fpring%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言最近对Spring的循环依赖比较感兴趣，特地研究了一下并分享给大家。 要说循环依赖，先理解循环引用，如果一个Class如TestA，需要引用TestB，而同时TestB又引用TestA，则可以称这两个类循环引用。 如下： 12345678910111213141516171819202122232425262728public class TestA &#123; private TestB testB; public TestA() &#123; &#125; public TestA(TestB testB) &#123; this.testB = testB; &#125; public void setTestB(TestB testB) &#123; this.testB = testB; &#125; public TestB getTestB() &#123; return testB; &#125;&#125;public class TestB &#123; private TestA testA; public TestB() &#123; &#125; public TestB(TestA testA) &#123; this.testA = testA; &#125; public void setTestA(TestA testA) &#123; this.testA = testA; &#125; public TestA getTestA() &#123; return testA; &#125;&#125; 如果这两个类被Spring管理，成为Spring的Bean，就会有循环依赖问题。 对于循环依赖问题，可能会有问题导致Bean创建不成功，我们来看一下。 正文我们知道，对于Spring的Bean，有singleton（单例）和prototype（多例）两种模式。 对于SpringBean的创建，我们有构造器注入和Setter注入两种方式。 它们都会对循环依赖问题造成影响，我们分别讨论。 Bean都为singleton模式，都通过构造器注入代码如下： 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; @Autowired public TestA(TestB testB) &#123; this.testB = testB; &#125; public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; @Autowired public TestB(TestA testA) &#123; this.testA = testA; &#125; public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 我们使用下面的测试类观测Bean创建情况，后面的都可以使用这个测试类。 12345678910111213141516171819@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; @Test public void test()&#123; try&#123; TestA a = SpringUtils.getBean(TestA.class); System.out.println(a); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; try&#123; TestB b = SpringUtils.getBean(TestB.class); System.out.println(b); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 对于上面的TestA和TestB我们运行后发现出现异常，Bean创建不成功，部分异常如下： 123...Caused by: org.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name &apos;testA&apos;: Requested bean is currently in creation: Is there an unresolvable circular reference?... Bean都为prototype模式，都通过构造器注入我们再看看把Bean改为prototype模式后如何。 代码如下： 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; @Autowired public TestA(TestB testB) &#123; this.testB = testB; &#125; public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; @Autowired public TestB(TestA testA) &#123; this.testA = testA; &#125; public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 经过测试可以发现也是不成功的，抛出BeanCurrentlyInCreationException异常。 其实上面两个例子是比较好理解的，通过构造器注入，也就是Bean TestA在创建的时候就需要TestB，TestB在创建的时候就需要TestA，显然，Spring无法解决这种情况，我们也无法解决这种情况。 Bean都为singleton模式，都通过Setter注入我们再来看下这种情况，代码如下： 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; public TestA(TestB testB) &#123; this.testB = testB; &#125; @Autowired public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; public TestB(TestA testA) &#123; this.testA = testA; &#125; @Autowired public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 我们测试一下可以发现运行成功了，成功输出两个Bean对象信息。 12com.zwt.demo.circularreference.TestA@2cc04358com.zwt.demo.circularreference.TestB@68b58644 其实这种情况也是比较好理解的： 当Bean TestA创建的时候需要TestB，TestB会创建一个默认的使用无参构造器的Bean对象，此时TestB里的TestA为空，我们称为TestB（空）； TestA使用TestB（空）完成依赖注入，生成Bean TestA； TestA构造完成，其返回给TestB（空），这时候TestB里的TestA不在为空，TestB构造完成。 其过程犹如如下代码： 123456789101112public static void main(String[] args) &#123; //1.生成1个TestB对象 TestB testB = new TestB(); //2.将TestB注入到TestA TestA testA = new TestA(); testA.setTestB(testB); //3.将TestA注入到TestB testB.setTestA(testA); //4.最后两者完成循环依赖 System.out.println(testA.getTestB()); System.out.println(testB.getTestA());&#125; 其实可以看到最关键的还是TestA、TestB的无参构造函数，其实这个无参构造函数指的是无TestA或者TestB参与的构造函数，这样初始化实例时不涉及循环依赖类，而在创建成功后通过参数注入。 我们把上面TestA和TestB的无参构造函数删去，只保留有参构造，测试运行可以看到出现了BeanCurrentlyInCreationException异常。 Bean都为prototype模式，都通过Setter注入这种情况貌似也可以？？我们来看下代码： 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; public TestA(TestB testB) &#123; this.testB = testB; &#125; @Autowired public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; public TestB(TestA testA) &#123; this.testA = testA; &#125; @Autowired public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 我们测试后发现它也抛出BeanCurrentlyInCreationException异常，证明是不可以的。 这种情况如何理解呢？我先上段代码： 12345678public static void main(String[] args) &#123; TestB testB1 = new TestB(); TestA testA1 = new TestA(); testA1.setTestB(testB); testB1.setTestA(testA); System.out.println(testA1.getTestB()); System.out.println(testB1.getTestA());&#125; 我们明显看到上述代码是错的，明显不存在testB和testA对象，testA1和testB1如何set？ 是的，Spring就是这样，对于prototype（多例）模式，它不会保存已经创建的TestA或者TestB对象的引用。 也就是在注入时，我们可以认为完成了若干个（多例）TestA，但是都是“半成品”（没有TestB的注入），这时候TestB（多例）创建，需要TestA，即使拿到TestA（空），也无法将创建好的TestB反作用于TestA（空），使其成为TestA。 我们来看下Spring相关源码。 我们在AbstractBeanFactory的doGetBean方法看起，如下图： 可以看到拿Bean前会先检查单例Bean，如果拿到了sharedInstance并且参数为空，并判断这个Singleton是不是正在被创建中，如果是的话那么就存在循环引用，拿到的是个Bean“半成品”。 如果拿不到，它就是个多例，判断下是不是正在创建中，是的话就直接抛出异常了。 再来看下调用的DefaultSingletonBeanRegistry类的getSingleton方法。 可以看到这种Bean都会保存在一个叫earlySingletonObjects的HashMap里，如果没有就尝试去一个叫singletonFactories的HashMap里去获取。 然后可以看到这个HashMap的值是通过addSingletonFactory方法获得的。 继续跟踪可以看到放入条件：单例、允许循环依赖、当前Bean正在被创建，如下图： 最后getEarlyBeanReference返回了一个比较特殊的Object。 我们也可以通过debug观察代码运行情况，这儿不再过多叙述。 Bean一个为prototype模式，一个为singleton模式，都通过构造器注入这个根据我们上面等判断应该是不可以的。我们看到代码： 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; @Autowired public TestA(TestB testB) &#123; this.testB = testB; &#125; public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; @Autowired public TestB(TestA testA) &#123; this.testA = testA; &#125; public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 我们可以测试同样抛出异常，实际上如果循环依赖的Bean都是通过构造器注入的，那么无论如何都是创建不成功的，与Bean创建顺序和Bean类型没有任何关系。 Bean一个为prototype模式，一个为singleton模式，都通过Setter注入代码如下： 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; public TestA(TestB testB) &#123; this.testB = testB; &#125; @Autowired public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; public TestB(TestA testA) &#123; this.testA = testA; &#125; @Autowired public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 输出日志： 12com.zwt.demo.circularreference.TestA@327ed9f5com.zwt.demo.circularreference.TestB@67594471 我们可以看到这种情况下运行成功，TestA或者TestB有一个为Singleton的，通过参数注入，就能成功创建Bean。 Bean一个为prototype模式，通过构造器注入，一个为singleton模式，通过Setter注入如下代码： 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; public TestA(TestB testB) &#123; this.testB = testB; &#125; @Autowired public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; @Autowired public TestB(TestA testA) &#123; this.testA = testA; &#125; public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 我们可以看到运行成功，单例的TestA会先创建，名为TestA（空），然后创建TestB，使用了TestA（空），TestB创建成功后，TestA（空）里的testB被属性赋值。 Bean一个为prototype模式，通过Setter注入，一个为singleton模式，通过构造器注入这种情况根据上面经验，应该是不可以的。 12345678910111213141516171819202122232425262728@Scope(value = BeanDefinition.SCOPE_SINGLETON)@Componentpublic class TestA &#123; private TestB testB; public TestA() &#123; &#125; @Autowired public TestA(TestB testB) &#123; this.testB = testB; &#125; public void setTestB(TestB testB) &#123; this.testB = testB; &#125;&#125;@Scope(value = BeanDefinition.SCOPE_PROTOTYPE)@Componentpublic class TestB &#123; private TestA testA; public TestB() &#123; &#125; public TestB(TestA testA) &#123; this.testA = testA; &#125; @Autowired public void setTestA(TestA testA) &#123; this.testA = testA; &#125;&#125; 我们测试一下，确实也是不可以的，抛出BeanCurrentlyInCreationException异常。 这种情况可以认为TestA（单例）创建需要TestB，但是TestB是多例的，Spring中无法形成TestB的唯一引用作用于TestA（形成了TestB就是单例的了），也就无法创建TestA Bean。 总结上面说了很多情况，我们简单总结下。 对于TestA和TestB两个类，如果存在循环依赖： TestA TestB Spring创建结果 Singleton模式Constructor注入 Singleton模式Constructor注入 失败 Prototype模式Constructor注入 Prototype模式Constructor注入 失败 Singleton模式Setter注入 Singleton模式Setter注入 成功 Prototype模式Setter注入 Prototype模式Setter注入 失败 Singleton模式Constructor注入 Prototype模式Constructor注入 失败 Prototype模式Constructor注入 Singleton模式Constructor注入 同上（失败） Singleton模式Setter注入 Prototype模式Setter注入 成功 Prototype模式Setter注入 Singleton模式Setter注入 同上（成功） Prototype模式Constructor注入 Singleton模式Setter注入 成功 Singleton模式Setter注入 Prototype模式Constructor注入 同上（成功） Singleton模式Constructor注入 Prototype模式Setter注入 失败 Prototype模式Setter注入 Singleton模式Constructor注入 同上（失败） 以上列举了所有情况，根据表格我们可以看出创建成功的，一定至少有一个Bean为Singleton模式且Setter注入。 这个结论也与我们刚才分析的一部分Spring源码是一致的，首先构造一个未完全初始化的Bean，这个Bean要求为单例的，而后通过实例化另一个循环依赖Bean，成功后通过Setter完成原来单例Bean的初始化。 以上就是Spring循环依赖的全部内容。]]></content>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（4）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%EF%BC%884%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言接 搭建自己的Hexo博客（3）这篇文章，最近又对博客进行了一些简单优化，特地分享下。 正文音乐问题原来是直接使用的网易云音乐外链，考虑到音乐版权问题，可能以后外链失效影响到音乐播放。 故我将要使用的音乐文件迁移到了云服务器上，同时我们使用一款Hexo的音乐播放插件hexo-tag-aplayer。 我们直接在package.json文件里添加hexo-tag-aplayer并使用npm install命令进行安装。 在 themes/next/layout/_custom/sidebar.swig文件里，我们原来使用的网易云外链，注释掉。 123456&lt;!--网易云音乐外链，已废弃--&gt;&lt;!--&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=509313150&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=34723470&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=640866&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=35345243&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=512733081&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;--&gt; 使用如下Aplayer相关加载代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;!--基于hexo-tag-aplayer的音乐播放器--&gt;&lt;div id="aplayer-rWUYQYrx" class="aplayer aplayer-tag-marker" style="margin: 30px 0px 30px 0px;"&gt;&lt;/div&gt;&lt;script&gt; var options = &#123; "narrow": false,//样式 "autoplay": false,//是否自动播放 "showlrc": 0,//是否显示歌词，需要有歌词文件 "mutex": true,//true的话播放时会暂停其它播放器 "theme": "#e6d0b2",//主题 "preload": "none", //预加载模式 "listmaxheight": "500px",//列表最大高度 "music": [ &#123; "title": "Sakura Tears",//显示的音乐名 "author": "Nigel Silin",//作家 "url": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/Nigel%20Silin%20-%20Sakura%20Tears.mp3",//音乐文件路径 "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music1.jpeg"//音乐图片 &#125;, &#123; "title": "东京不太热", "author": "封茗囧菌", "url": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E5%B0%81%E8%8C%97%E5%9B%A7%E8%8F%8C%20-%20%E4%B8%9C%E4%BA%AC%E4%B8%8D%E5%A4%AA%E7%83%AD.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music2.jpeg" &#125;, &#123; "title": "春风吹", "author": "锦零", "url": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E9%94%A6%E9%9B%B6%20-%20%E6%98%A5%E9%A3%8E%E5%90%B9%EF%BC%88Cover%20%E6%96%B9%E5%A4%A7%E5%90%8C%EF%BC%89.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music3.jpeg" &#125;, &#123; "title": "Secret", "author": "茶太", "url": "https://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E8%8C%B6%E5%A4%AA%20-%20Secret.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music4.jpeg" &#125;, &#123; "title": "secret base ~君がくれたもの~", "author": "茅野愛衣,戸松遥,早見沙織", "url": "https://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E8%8C%85%E9%87%8E%E6%84%9B%E8%A1%A3%2C%E6%88%B8%E6%9D%BE%E9%81%A5%2C%E6%97%A9%E8%A6%8B%E6%B2%99%E7%B9%94%20-%20secret%20base%20%7E%E5%90%9B%E3%81%8B%E3%82%99%E3%81%8F%E3%82%8C%E3%81%9F%E3%82%82%E3%81%AE%7E%20%2810%20years%20after%20Ver.%29.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music5.jpeg" &#125;, &#123; "title": "ファンファーレ", "author": "sumika", "url": "https://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/sumika%20-%20%E3%83%95%E3%82%A1%E3%83%B3%E3%83%95%E3%82%A1%E3%83%BC%E3%83%AC.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music6.jpeg" &#125; ]&#125;; options.element = document.getElementById("aplayer-rWUYQYrx"); var ap = new APlayer(options); window.aplayers || (window.aplayers = []); window.aplayers.push(ap);&lt;/script&gt; 这样我们部署后会看到音乐栏如下图所示，也是比较美观的。 我们使用hexo-tag-aplayer还有一个好处，当我们想向一些文章里添加音乐时，只需在文章markdown文章中添加如下代码即可。 1&#123;% aplayer "Caffeine" "Jeff Williams" "caffeine.mp3" "picture.jpg" "lrc:caffeine.txt" %&#125; hexo-tag-aplayer是基于APlayer而构建的，有兴趣的同学可以看下APlayer，一款非常漂亮的H5音乐播放器。 相册问题上篇文章简单制作了一个相册，但是不是很满意，最近又对相册做了些优化。大致想法如下： 相册页应该支持Tab切换，可以按照不同类别进行区分 原来瀑布流下拉加载有些问题，需要修复下 照片处理时手动操作步骤应该尽量少且简单 根据以上问题，优化后的相册如下动图，可以看到更美观实用一些了。 我简单把自己改动的代码分享下。 photo 文件夹下的index.md文件如下：1&lt;div id = "ImageGrid" class="photo-record"&gt;&lt;div class="tab"&gt;&lt;div class="tab-item active"&gt;动漫&lt;/div&gt;&lt;div class="tab-item"&gt;风景&lt;/div&gt;&lt;div class="tab-item"&gt;TODO&lt;/div&gt;&lt;/div&gt;&lt;div class="photoContent"&gt;&lt;ul class="img-box-ul1 mainCont selected"&gt;&lt;/ul&gt;&lt;ul class="img-box-ul2 mainCont"&gt;&lt;/ul&gt;&lt;ul class="img-box-ul3 mainCont"&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style="height:100px"&gt;&lt;/div&gt; 样式如下，放在custom.styl里。 1234567891011121314151617181920212223242526.photo-record&#123; width: 100%;&#125;.photo-record .tab&#123; height:30px; line-height:30px;&#125;.photo-record .tab .tab-item&#123; display:inline-block; width:60px; text-align:center; font-size:15px;&#125;.photo-record .tab .tab-item.active&#123; color: #ff9a22; border-bottom: 3px solid #ffc472;&#125;.photoContent .mainCont &#123; display: none; width:100%; overflow: auto; text-align: center;&#125;.photoContent .mainCont.selected &#123; display: block;&#125; photos.js的改动较大，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980var photo = function() &#123; this.page = 1, this.offset = 10, this.flag = false, this.init = function(index) &#123; if (this.flag) &#123; return; &#125; this.flag = true; var that = this; $.getJSON("../js/src/photos.json", function(data) &#123; for (var i = 0; i &lt; data.length; i++) &#123; if (i + 1 === index) &#123; that.render(that.page, data[i], index); that.scroll(data[i], index); &#125; &#125; &#125;); &#125;, this.render = function(page, data, index) &#123; var begin = (page - 1) * this.offset; var end = page * this.offset; if (begin &gt;= data.length) return; var imgNameWithPattern, imgName, imageSize, imageX, imageY, li = ""; for (var i = begin; i &lt; end &amp;&amp; i &lt; data.length; i++) &#123; imgNameWithPattern = data[i].split(' ')[1]; imgName = imgNameWithPattern.split('.')[0] imageSize = data[i].split(' ')[0]; imageX = imageSize.split('.')[0]; imageY = imageSize.split('.')[1]; li += '&lt;div class="card" style="width:330px"&gt;' + '&lt;div class="img-box" style="height:' + 330 * imageY / imageX + 'px"&gt;' + '&lt;a data-fancybox="gallery" class="fancybox fancybox.image" href="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + index + "/" + imgNameWithPattern + '?raw=true" data-caption="' + imgName + '"&gt;' + '&lt;img src="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + index + "/" + imgNameWithPattern + '?raw=true"/&gt;' + '&lt;/a&gt;' + '&lt;/div&gt;' + '&lt;/div&gt;' &#125; var box = $(".img-box-ul" + index); box.append(li); box.lazyload(); this.minigrid(index); &#125;, this.minigrid = function(index) &#123; var grid = new Minigrid(&#123; container: ".img-box-ul" + index, item: '.card', gutter: 12 &#125;); grid.mount(); // $(window).resize(function() &#123; // grid.mount(); // &#125;); &#125;, this.scroll = function(data, index) &#123; var that = this; $(window).scroll(function() &#123; var windowPageYOffset = window.pageYOffset; var windowPageYOffsetAddHeight = windowPageYOffset + window.innerHeight; var sensitivity = 0; var grid = $("#ImageGrid"); var offsetTop = grid.offset().top + grid.height(); if (offsetTop &gt;= windowPageYOffset &amp;&amp; offsetTop &lt; windowPageYOffsetAddHeight + sensitivity) &#123; that.render(++that.page, data, index); &#125; &#125;) &#125;&#125;;(function() &#123; $(".photo-record .tab .tab-item").each(function (i,e) &#123; var tempPhoto = new photo(); $(e).click(function()&#123; $(this).addClass("active").siblings().removeClass("active"); $(".photo-record .photoContent .mainCont").eq(i).addClass("selected").siblings().removeClass("selected"); tempPhoto.init(i + 1); &#125;); &#125;); $(".photo-record .tab .tab-item.active").click();&#125;)(); 从上面的photos.js我们可以看到photos.json应该为下面这种格式。 123456789101112131415161718192021222324252627282930313233[ [ "3200.2000 photo-1-10.jpg", "1920.1080 photo-1-1.jpg", "1920.1080 photo-1-12.jpg", "1920.1200 photo-1-11.jpg", "1920.1200 photo-1-13.jpg", "1920.1080 photo-1-14.jpg", "1920.1200 photo-1-16.jpg", "1920.1080 photo-1-15.jpg", "1920.1186 photo-1-18.jpg", "1920.1080 photo-1-19.jpg", "1920.1080 photo-1-17.jpg", "1024.610 photo-1-2.jpg", "3160.2560 photo-1-20.jpg", "2560.1440 photo-1-21.jpg", "1920.1080 photo-1-22.jpg", "3139.2203 photo-1-23.jpg", "1024.1024 photo-1-24.jpeg", "2200.1400 photo-1-3.jpg", "1200.750 photo-1-4.jpg", "3500.1898 photo-1-5.jpg", "1080.1920 photo-1-6.jpg", "1600.1120 photo-1-7.jpg", "1728.1080 photo-1-8.jpg", "1920.1080 photo-1-9.jpg" ], [ "3968.2976 photo-2-1.jpg", "2976.3968 photo-2-2.jpg" ], []] 这就需要我们使用photosTools.js来进行生成了，原来的这个文件是不满足要求的，我们改造如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748"use strict";const fs = require("fs");const sizeOf = require('image-size');const path = "source/images_backup/photos";const output = "themes/next/source/js/src/photos.json";var dimensions;fs.readdir(path, function(err, dirents) &#123; if (err) &#123; return; &#125; let arr = []; for (let i = 0; i &lt; dirents.length; i++) &#123; console.log(dirents); if(dirents[i]===".DS_Store")&#123; continue; &#125; fs.readdir(path + "/" + dirents[i], function(err, files) &#123; console.log(files); let tempArr = []; for (let j = 0; j &lt; files.length; j++) &#123; fs.stat(path + "/" + dirents[i] + "/" + files[j], function(err, stats) &#123; if (!files[j].endsWith(".jpg") || files[j].endsWith(".jpeg")) &#123; &#125; if (err) &#123; return; &#125; if (stats.isFile()) &#123; if (files[j].endsWith(".jpg") || files[j].endsWith(".jpeg") || files[j].endsWith(".mp4") || files[j].endsWith(".png") || files[j].endsWith(".gif")) &#123; dimensions = sizeOf(path + "/" + dirents[i] + "/" + files[j]); console.log(dimensions.width, dimensions.height); tempArr.push(dimensions.width + '.' + dimensions.height + ' ' + files[j]); &#125; &#125; &#125;) &#125; arr.push(tempArr); &#125;); if (i === dirents.length - 1) &#123; setTimeout(function() &#123; fs.writeFile(output, JSON.stringify(arr, null, "\t"), function() &#123;&#125;); &#125;, 10000); return; &#125; &#125;&#125;); 同时原来的photos图片文件夹里，应该使用1，2，3…等文件夹，并将图片放入到这些文件夹中，至于1，2，3…和图片类型的对应关系，就需要自己配置就行了。 以上就是新的图片标签页面的一些设计。 可以看到我们创建了3个Tab页切换（如需更多，可以继续进行配置），每个Tab页对应一个photo对象，该对象会加载并渲染图片信息，每次加载10个，使用向下滚动我们可以加载更多，并使用fancbox美化图片样式。 由于需要知道图片宽高来美化图片样式，因此我们使用了NodeJs的fs和imagesize模块，用来读取文件并生成photos.json文件。 PS：弄完相册后，突然想到了视频相关的一些展示，这个在后面我会考虑美观等特性，考虑是否为博客添加视频功能，及添加的位置等一些要素。 屏蔽相关我们为博客添加一些屏蔽功能，先上代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138/** * 屏蔽f12 审查元素 */document.onkeydown = function()&#123; if(window.event)&#123; var e = window.event; if(e.keyCode == 123) &#123; console.log("禁用F12键"); e.keyCode=0; e.returnValue=false; &#125; // if(e.keyCode == 13) &#123; // console.log("禁用Enter键"); // e.keyCode = 0; // e.returnValue=false; // &#125; // if(e.keyCode == 8) &#123; // console.log("禁用Backspace键"); // e.returnValue=false; // &#125; var ctrlKey = e.ctrlKey || e.metaKey; var shiftKey = e.shiftKey; if(ctrlKey &amp;&amp; shiftKey &amp;&amp; e.keyCode == 73)&#123; console.log("禁用Ctrl+Shift+I键"); e.keyCode=0; e.returnValue=false; &#125; &#125;&#125;/** * 屏蔽右键菜单 * @param event * @returns &#123;boolean&#125; */document.oncontextmenu = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if (!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽粘贴 * @param event * @returns &#123;boolean&#125; */document.onpaste = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if (!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽复制 * @param event * @returns &#123;boolean&#125; */document.oncopy = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if(!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽剪切 * @param event * @returns &#123;boolean&#125; */document.oncut = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if(!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽选中 * @param event * @returns &#123;boolean&#125; */document.onselectstart = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if (!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125; catch (e) &#123; return false; &#125;&#125;;/** * 尝试阻止开发者模式 * @type &#123;HTMLElement&#125; */var fuck = document.createElement('div');Object.defineProperty(fuck, 'id', &#123; get:function()&#123; console.log("Hello World!"); location.href="about:blank"; history.replaceState(); &#125;&#125;);console.log(fuck); 我们将上述代码引入到博客代码中，就会欣然发现： 在博客页面上： 鼠标右键审查元素失效 F12失效，Chrome浏览器下Ctrl+Shift+I启动开发者模式快捷键失效 复制、粘贴、剪贴、鼠标选中元素失效 开发者模式和博客页面是无法共存的 当然这并不能阻止大家获取到网站运行的代码，因为js、html或者图片等资源，一旦被加载到客户端，客户端就已经拿到代码了。 然而这仍然是有趣的一件事情。 我们这里主要就是监听一些键盘事件来对某些按键行为进行阻止，对于屏蔽了开发者模式，主要和Object.defineProperty这个属性相关，但这个属性一些低版本浏览器是不支持的。 总结今天到这里就差不多了，我们对博客进行了一些优化操作，及简单介绍了如何进行这些操作，接下来我仍将去探寻博客一些可以优化的地方。 我的博客地址：https://www.sakuratears.top]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于.DS_Store文件的一些问题]]></title>
    <url>%2Fblog%2F%E5%85%B3%E4%BA%8E-DS-Store%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言之前在Mac电脑上拷贝文件到Windows电脑时，文件夹里会额外多出文件，就是我们今天要说的.DS_Store文件，当时并没有在意。 昨天在写一个文件流读取功能时，解析一直出问题，后面发现是文件夹里多了.DS_Store文件，导致文件遍历解析出了些问题，过程我就不再详述。 因此也对这个.DS_Store文件产生了一些兴趣，特地分享记录下。 正文简介.DS_Store，全称 Desktop Services Store，是Mac OS中保存文件夹自定义属性的隐藏文件，目的在于存储文件夹的自定义属性，例如文件图标位置、视图设置，或背景色等，相当于Windows下的 desktop.ini。.DS_Store 默认放在每个文件夹的下面。 解析当我们尝试用文本打开时可以发现它是乱码的，我们接下来尝试解读一下它。 我们使用Notepad++，在其 插件-插件管理 添加Hex-Editor插件，安装好此插件后Notepad++可以以十六进制形式打开指定文件。 我们使用Notepad++借助此插件可以打开.DS_Store，可以看到它的十六进制格式。 当然我们也可以使用VSCode，安装hexdump for vscode插件，来显示十六进制格式。 两种方式插件的安装过程略，有兴趣的可以查看相关文章等。 这儿我们使用VSCode结合插件来打开该文件，如下图，我们尝试解析一下该十六进制数据。 关于该文件二进制的结构说明我们可以参考这篇文章 解析.DS_Store文件格式。 .DS_Store文件转为二进制树后，根据上面这篇文章，它的结构大致如下： 文件头部（Header） 根块 偏移部分（Offsets） 内容表（Toc） 空闲表（FreeList） 文件头部： 通常文件头部用来进行校验，判断这个文件是不是.DS_Store文件（详见代码readHeader方法）。 偏移部分： 偏移部分记录了有关文件中树(叶)块的偏移量信息，这些块存储的都是目录的实际信息，如文件名。获得偏移量需要遍历这个树（详见代码readOffsets方法）。 内容表： 在偏移部分结束后，内容表部分就会呈现出来。他通常存在只有一个名为DSDB的表,并且值为1。这个特殊的表通常引用了我们将要遍历的第一个块的ID（详见代码readTOC方法）。 空闲表： 最后一部分是空闲表，也就是在树中还有哪些地方是没有使用的或者是空闲的模块（详见代码readFreelist方法）。 然后我们使用Java来解析下这个二进制结构数据。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484public class DataBlock &#123; private byte[] data; private int pos; private boolean debug; /** * Returns an byte array of length from data at the given offset or pos. * If offset==0 (no offset is given) , pos will be increased by length. * Throws Exception if offset+length &gt; this.data.length * @Params: [length, offset] * @Return: byte[] */ public byte[] offsetRead(int length,int offset)&#123; int offsetPosition; if(offset==0)&#123; offsetPosition = this.pos; &#125;else&#123; offsetPosition = offset; &#125; if(this.data.length &lt; offsetPosition +length)&#123; throw new RuntimeException("Offset+Length &gt; this.data.length"); &#125; if(offset==0)&#123; this.pos+=length; &#125; byte[] value =new byte[length]; System.arraycopy(this.data,offsetPosition,value,0,length); if(debug)&#123; System.out.println(String.format("Reading: %s-%s =&gt; %s",offsetPosition, offsetPosition+length, value)); &#125; return value; &#125; /** * Increases pos by length without reading data! * @Params: [length] * @Return: void */ public void skip(int length)&#123; this.pos+=length; &#125; /** * Extracts a file name from the current position. * @Params: [] * @Return: java.lang.String */ public String readFileName()&#123; //The length of the file name in bytes. int length = ByteBuffer.wrap(offsetRead(4,0)).getInt(); //The file name in UTF-16, which is two bytes per character. String fileName = new String(offsetRead(2 * length,0), StandardCharsets.UTF_16BE); //A structure ID that I haven't found any use of. int structureId = ByteBuffer.wrap(offsetRead(4,0)).getInt(); //Now read the structure type as a string of four characters and decode it to ascii. String structureType = new String(offsetRead(4,0), StandardCharsets.US_ASCII); if(debug)&#123; System.out.println("Structure type "+ structureType); &#125; //If we don't find a match, skip stays &lt; 0 and we will do some magic to find the right skip due to somehow broken .DS_Store files.. int skip = -1; //Source: http://search.cpan.org/~wiml/Mac-Finder-DSStore/DSStoreFormat.pod while (skip &lt; 0)&#123; if(structureType.equals("bool"))&#123; skip = 1; &#125;else if(structureType.equals("type") || structureType.equals("long") || structureType.equals("shor") || structureType.equals("fwsw") || structureType.equals("fwvh") || structureType.equals("icvt") || structureType.equals("lsvt") || structureType.equals("vSrn") || structureType.equals("vstl"))&#123; skip = 4; &#125;else if(structureType.equals("comp") || structureType.equals("dutc") || structureType.equals("icgo") || structureType.equals("icsp") || structureType.equals("logS") || structureType.equals("lg1S") || structureType.equals("lssp") || structureType.equals("modD") || structureType.equals("moDD") || structureType.equals("phyS") || structureType.equals("ph1S"))&#123; skip = 8; &#125;else if(structureType.equals("blob"))&#123; skip = ByteBuffer.wrap(offsetRead(4,0)).getInt(); &#125;else if(structureType.equals("ustr") || structureType.equals("cmmt") || structureType.equals("extn") || structureType.equals("GRP0"))&#123; skip = 2 * ByteBuffer.wrap(offsetRead(4,0)).getInt(); &#125;else if(structureType.equals("BKGD"))&#123; skip = 12; &#125;else if(structureType.equals("ICVO") || structureType.equals("LSVO") || structureType.equals("dscl"))&#123; skip = 1; &#125;else if(structureType.equals("Iloc") || structureType.equals("fwi0"))&#123; skip = 16; &#125;else if(structureType.equals("dilc"))&#123; skip = 32; &#125;else if(structureType.equals("lsvo"))&#123; skip = 76; &#125;else if(structureType.equals("icvo"))&#123; &#125;else if(structureType.equals("info"))&#123; &#125;else &#123; &#125; if(skip &lt;= 0)&#123; //We somehow didn't find a matching type. Maybe this file name's length value is broken. Try to fix it! //This is a bit voodoo and probably not the nicest way. Beware, there by dragons! if(debug)&#123; System.out.println("Re-reading!"); &#125; // Rewind 8 bytes, so that we can re-read structure_id and structure_type skip(-1 * 2 * 0x4); fileName = new String(offsetRead(0x2,0), StandardCharsets.UTF_16BE); //re-read structure_id and structure_type structureId = ByteBuffer.wrap(offsetRead(4,0)).getInt(); structureType = new String(offsetRead(4,0), StandardCharsets.US_ASCII); //Look-ahead and check if we have structure_type==Iloc followed by blob. //If so, we're interested in blob, not Iloc. Otherwise continue! String futureStructureType = new String(offsetRead(4,this.pos), StandardCharsets.US_ASCII); if(debug)&#123; System.out.println(String.format("Re-read structure_id %s / structure_type %s",structureId, structureType)); &#125; if ((!structureType.equals("blob")) &amp;&amp; (!futureStructureType.equals("blob")))&#123; structureType = ""; if(debug)&#123; System.out.println("Forcing another round!"); &#125; &#125; &#125; &#125; // Skip bytes until the next (file name) block skip(skip); if(debug)&#123; System.out.println(String.format("Filename %s",fileName)); &#125; return fileName; &#125; public DataBlock() &#123; &#125; public DataBlock(byte[] data, int pos, boolean debug) &#123; this.data = data; this.pos = pos; this.debug = debug; &#125; public byte[] getData() &#123; return data; &#125; public void setData(byte[] data) &#123; this.data = data; &#125; public int getPos() &#123; return pos; &#125; public void setPos(int pos) &#123; this.pos = pos; &#125; public boolean isDebug() &#123; return debug; &#125; public void setDebug(boolean debug) &#123; this.debug = debug; &#125;&#125;public class DS_Store extends DataBlock&#123; private byte[] data; private boolean debug; private DataBlock root; private List&lt;Integer&gt; offsets; private Map&lt;String,Integer&gt; toc; private Map&lt;Integer,List&lt;Integer&gt;&gt; freeList; /** * Constructor of DS_Store * @Params: [data, debug] * @Return: */ public DS_Store(byte[] data, boolean debug) &#123; super(data,0,debug); this.data = data; this.debug = debug; this.data = data; this.debug = debug; this.root = readHeader(); this.offsets = readOffsets(); this.toc = readTOC(); this.freeList = readFreelist(); &#125; /** * Checks if this.data is actually a .DS_Store file by checking the magic bytes. * It returns the file's root block. * @Params: [] * @Return: com.zwt.framework.utils.util.dsstore.DataBlock */ private DataBlock readHeader()&#123; // We read at least 32+4 bytes for the header! if (this.data.length &lt; 36)&#123; throw new RuntimeException("Length of data is too short!"); &#125; // Check the magic bytes for .DS_Store int magic1 = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); int magic2 = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); if(magic1 != 0x1 &amp;&amp; magic2 != 0x42756431)&#123; throw new RuntimeException("Magic byte 1 does not match!"); &#125; // After the magic bytes, the offset follows two times with block's size in between. // Both offsets have to match and are the starting point of the root block int offset = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); int size = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); int offset2 = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Offset 1: %s",offset)); System.out.println(String.format("Size: %s",size)); System.out.println(String.format("Offset 2: %s",offset2)); &#125; if(offset!=offset2)&#123; throw new RuntimeException("Offsets do not match!"); &#125; //Skip 16 bytes of unknown data... skip(4*4); return new DataBlock(this.offsetRead(size, offset+4),0, this.debug); &#125; /** * Reads the offsets which follow the header * @Params: [] * @Return: java.util.List&lt;java.lang.Integer&gt; */ private List&lt;Integer&gt; readOffsets()&#123; int startPos = this.root.getPos(); // First get the number of offsets in this file. int count = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Offset count: %s",count)); &#125; // Always appears to be zero! this.root.skip(4); // Iterate over the offsets and get the offset addresses. List&lt;Integer&gt; offsets = new ArrayList&lt;&gt;(); for(int i=0;i&lt;count;i++)&#123; // Address of the offset. int address = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Offset %s is %s",i, address)); &#125; if (address == 0)&#123; // We're only interested in non-zero values continue; &#125; offsets.add(address); &#125; // Calculate the end of the address space (filled with zeroes) instead of dumbly reading zero values... int sectionEnd = startPos + (count / 256 + 1) * 256 * 4 - count*4; // Skip to the end of the section this.root.skip(sectionEnd); if(debug)&#123; System.out.println(String.format("Skipped %s to %s",(this.root.getPos() + sectionEnd),this.root.getPos())); System.out.println(String.format("Offsets: %s",offsets)); &#125; return offsets; &#125; /** * Reads the table of contents (TOCs) from the file. * @Params: [] * @Return: java.util.Map&lt;java.lang.String,java.lang.Integer&gt; */ private Map&lt;String,Integer&gt; readTOC()&#123; if(debug)&#123; System.out.println(String.format("POS %s",this.root.getPos())); &#125; // First get the number of ToC entries. int count = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Toc count: %s",count)); &#125; Map&lt;String,Integer&gt; toc = new HashMap&lt;&gt;(); // Iterate over all ToCs for(int i=0;i&lt;count;i++)&#123; // Get the length of a ToC's name int tocLen = this.root.offsetRead(1,0)[0]; // Read the ToC's name String tocName = new String(this.root.offsetRead(tocLen,0), StandardCharsets.UTF_8); // Read the address (block id) in the data section int blockId = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); // Add all values to the dictionary toc.put(tocName,blockId); &#125; if(debug)&#123; System.out.println(String.format("Toc %s",toc)); &#125; return toc; &#125; /** * Read the free list from the header. * The free list has n=0..31 buckets with the index 2^n * @Params: [] * @Return: java.util.Map&lt;java.lang.Integer,java.util.List&lt;java.lang.Integer&gt;&gt; */ private Map&lt;Integer,List&lt;Integer&gt;&gt; readFreelist()&#123; Map&lt;Integer,List&lt;Integer&gt;&gt; freelist = new HashMap&lt;&gt;(); for(int i=0;i&lt;32;i++)&#123; freelist.put(1&lt;&lt;i,new ArrayList&lt;&gt;()); // Read the amount of blocks in the specific free list. int blkcount = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); for(int j=0;j&lt;blkcount;j++)&#123; // Read blkcount block offsets. int freeOffset = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); freelist.get(1&lt;&lt;i).add(freeOffset); &#125; &#125; if(debug)&#123; System.out.println(String.format("Freelist: %s",freelist)); &#125; return freelist; &#125; /** * Create a DataBlock from a given block ID (e.g. from the ToC) * @Params: [blockId] * @Return: com.zwt.framework.utils.util.dsstore.DataBlock */ public DataBlock blockById(int blockId)&#123; // First check if the block_id is within the offsets range if(this.offsets.size() &lt; blockId)&#123; throw new RuntimeException("BlockID out of range!"); &#125; // Get the address of the block int addr = this.offsets.get(blockId); // Do some necessary bit operations to extract the offset and the size of the block. // The address without the last 5 bits is the offset in the file int offset = addr &gt;&gt; 0x5 &lt;&lt; 0x5; //The address' last five bits are the block's size. int size = 1 &lt;&lt; (addr &amp; 0x1f); if(debug)&#123; System.out.println(String.format("New block: addr %s offset %s size %s",addr, offset + 0x4, size)); &#125; // Return the new block return new DataBlock(this.offsetRead(size, offset + 0x4),0, this.debug); &#125; /** * Traverses a block identified by the given block_id and extracts the file names. * @Params: [blockId] * @Return: java.util.List&lt;java.lang.String&gt; */ public List&lt;String&gt; traverse(int blockId)&#123; // Get the responsible block by it's ID DataBlock node = this.blockById(blockId); // Extract the pointer to the next block int nextPointer = ByteBuffer.wrap(node.offsetRead(4,0)).getInt(); // Get the number of next blocks or records int count = ByteBuffer.wrap(node.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Next Ptr %s with %s ",nextPointer,count)); &#125; List&lt;String&gt; filenames =new ArrayList&lt;&gt;(); // If a next_pointer exists (&gt;0), iterate through the next blocks recursively // If not, we extract all file names from the current block if(nextPointer &gt; 0)&#123; for(int i=0;i&lt;count;i++)&#123; // Get the block_id for the next block int nextId = ByteBuffer.wrap(node.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Child: %s",nextId)); &#125; // Traverse it recursively List&lt;String&gt; files = this.traverse(nextId); filenames.addAll(files); // Also get the filename for the current block. String filename = node.readFileName(); if(debug)&#123; System.out.println(String.format("Filename: %s", filename)); &#125; filenames.add(filename); &#125; // Now that we traversed all childs of the next_pointer, traverse the pointer itself. // TODO: Check if that is really necessary as the last child should be the current node... (or so?) List&lt;String&gt; files = this.traverse(nextPointer); filenames.addAll(files); &#125;else&#123; // We're probably in a leaf node, so extract the file names. for(int i=0;i&lt;count;i++)&#123; String f = node.readFileName(); filenames.add(f); &#125; &#125; return filenames; &#125; /** * Traverse from the root block and extract all file names. * @Params: [] * @Return: java.util.List&lt;java.lang.String&gt; */ public List&lt;String&gt; traverseRoot()&#123; // Get the root block from the ToC 'DSDB' DataBlock root = this.blockById(this.toc.get("DSDB")); // Read the following root block's ID, so that we can traverse it. int rootId = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Root-ID %s", rootId)); &#125; // Read other values that we might be useful, but we're not interested in... (at least right now) int internalBlockCount = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); int recordCount = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); int blockCount = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); int unknown = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); // traverse from the extracted root block id. return this.traverse(rootId); &#125; public DS_Store() &#123; &#125; public byte[] getData() &#123; return data; &#125; public void setData(byte[] data) &#123; this.data = data; &#125; public boolean isDebug() &#123; return debug; &#125; public void setDebug(boolean debug) &#123; this.debug = debug; &#125;&#125;public class DS_StoreParser &#123; /** * Return bytes by reading .DS_Store File * Throw Exception if file not exist * Throw Exception if Reading Error * @Params: [fileName] * @Return: byte[] */ public static byte[] readFile(String fileName)&#123; File file = new File(fileName); if((!file.exists())||(!file.isFile()))&#123; throw new RuntimeException(".DS_Store File not exist ！"); &#125; try (FileInputStream fis = new FileInputStream(file); ByteArrayOutputStream bos = new ByteArrayOutputStream())&#123; byte[] b = new byte[1024]; int len; while((len = fis.read(b)) != -1) &#123; bos.write(b, 0, len); &#125; return bos.toByteArray(); &#125;catch (IOException e)&#123; throw new RuntimeException("Reading .DS_Store File Error!"+e); &#125; &#125; public static void main(String[] args) &#123; byte[] data = readFile("/Users/zhangwentong/Desktop/DS_Store/bak.DS_Store"); DS_Store store = new DS_Store(data,true); List&lt;String&gt; files = store.traverseRoot(); System.out.println("Count: "+ files.size()); for(int i=0;i&lt;files.size();i++)&#123; System.out.println(files.get(i)); &#125; &#125;&#125; 上述代码过程较复杂，有兴趣的可以参考最后面的参考资料部分，先对.DS_Store文件结构有些了解，在看代码就比较容易了。 我们运行上述代码后会看到如下输出： 可以看到这里面包含着我们的一些文件目录信息等，这些文件信息时是当前目录下的所有文件信息。这就可能造成一些安全问题。 有什么安全问题呢？我们来看下。 信息安全问题当这一文件上传到了web服务器时，往往会带来一定的危害。 它带来的危害是它包含的文件名。MacOs在几乎所有文件夹都创建了一个.DS_Store文件。 信息泄漏(敏感文件)问题： 我们可以在https://en.internetwache.org/scanning-the-alexa-top-1m-for-ds-store-files-12-03-2018/ 这里查看。这篇文章涉及的Internetwache.org网站项目，对Alexa Top 1000的网站的根目录进行扫描，证明在有的网站中的确存在这一文件，导致信息泄漏。通过解析这一文件，他们发现了数据库备份，配置文件，以及一些缓存文件，甚至是密钥。 产生这种情况的原因就是我们在Git合作中，将.DS_Store进行了上传，而后对项目进行了部署。 PS：需要说明的一个事实是，存储在.DS_Store文件中的文件名仅代表本地MacOS系统上的目录内容。这就意味着解析出来的文件列表中有些文件可能不存在于我们的服务器或者要查找的机器上。 解决方案尽管这样，我们在使用MacOS系统传输文件或者上传文件时，还是要尽量避免该文件的产生。 这个文件除了会记录目录信息，我们在使用程序遍历文件读取时也可能遇到它引起的问题（如我开头所说），我们在版本协作时，也有可能产生由此文件产生的不必要的冲突问题等。 我们处理.DS_Store文件大概有以下一些方法。 通常，在传输文件时，我们可以直接删除文件目录下的.DS_Store文件，如在MacOS上传输文件给Windows系统，这些.DS_Store文件就成了垃圾文件，对我们是毫无用处的。 对于提交给版本控制系统的文件，我们可以将.DS_Store文件加入到.gitignore文件中以达到忽略提交的目的。 如果我们想查询服务器（Linux）或者MacOS上有多少.DS_Store文件，可以使用下面的命令。 1find . -name '*.DS_Store' 如果要删除它们，可以使用下面的指令。 1find . -name '*.DS_Store' -type f -delete 上面的两个命令可以查询和删除当前目录下的.DS_Store文件。 PS：在MacOS上删除后会影响到如文件图标位置等问题，需要注意。 我们也可以让.DS_Store文件不在MacOS外接设备上（U盘等）继续生成，如下指令： 1defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool TRUE 如果要开启外接设备继续生成.DS_Store文件，指令如下： 1defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool FALSE 如果想直接禁止.DS_Store在MacOS上生成，可以使用Asepsis。 Asepsis的工作原理是拦截所有.DS_Store文件的创建或写入，并将它们重定向到 /usr/local/.dscage。 这样 Finder 如常工作，且不会有这种无用文件污染文件系统。 不幸的是，在 OS X 10.11 El Capitan 发布之后，Apple 启用了 System Integrity Protection (SIP)，它会阻止 Asepsis 的安装和正常运行。Asepsis 的作者已经放弃了对它的后续支持，因为他不希望用户为了使用这个工具而禁用系统关键安全服务。 然而网上也有相关可以继续使用Asepsis的方法，如这篇文章禁止.DS_store生成，有兴趣的可以看一看。 总结.DS_Store文件一般情况下是无用文件，也不会造成多大问题，但我们也应该对其存在提高警惕，尤其是在服务器上出现时，我是因为遇到了它造成的一个bug才决定研究下它，同时用程序解析一下锻炼自己，网上关于该文件的解析文章不算多，下面我把一些参考资料分享给大家，希望大家对其有更深的了解。 参考资料 DS_Store Format DS_Store_File_Format fdb.php 解析.DS_Store文件格式 Scanning the Alexa Top 1M for .DS_Store files mac-dumpster-diving-identifying-deleted-file-references-in-the-trash-ds_store-files-part-1 mac-dumpster-diving-identifying-deleted-file-references-in-the-trash-ds_store-files-part-2 源码地址上述代码地址： GitHub .DS_Store Parser]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>.DS_Store</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis功能文档（转）]]></title>
    <url>%2Fblog%2FRedis%E5%8A%9F%E8%83%BD%E6%96%87%E6%A1%A3%EF%BC%88%E8%BD%AC%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言本文转载自 Redis命令参考-功能文档 功能文档Redis 集群规范引言这个文档是正在开发中的 Redis 集群功能的规范（specification）文档， 文档分为两个部分： 第一部分介绍目前已经在 unstable 分支中实现了的那些功能。 第二部分介绍目前仍未实现的那些功能。 文档各个部分的内容可能会随着集群功能的设计修改而发生改变， 其中， 未实现功能发生修改的几率比已实现功能发生修改的几率要高。 这个规范包含了编写客户端库（client library）所需的全部知识， 不过请注意， 这里列出的一部分细节可能会在未来发生变化。 什么是 Redis 集群？Redis 集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis 实现， 集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset）。 Redis 集群中不存在中心（central）节点或者代理（proxy）节点， 集群的其中一个主要设计目标是达到线性可扩展性（linear scalability）。 Redis 集群为了保证一致性（consistency）而牺牲了一部分容错性： 系统会在保证对网络断线（net split）和节点失效（node failure）具有有限（limited）抵抗力的前提下， 尽可能地保持数据的一致性。 Note 集群将节点失效视为网络断线的其中一种特殊情况。 集群的容错功能是通过使用主节点（master）和从节点（slave）两种角色（role）的节点（node）来实现的： 主节点和从节点使用完全相同的服务器实现， 它们的功能（functionally）也完全一样， 但从节点通常仅用于替换失效的主节点。 不过， 如果不需要保证“先写入，后读取”操作的一致性（read-after-write consistency）， 那么可以使用从节点来执行只读查询。 Redis 集群实现的功能子集Redis 集群实现了单机 Redis 中， 所有处理单个数据库键的命令。 针对多个数据库键的复杂计算操作， 比如集合的并集操作、合集操作没有被实现， 那些理论上需要使用多个节点的多个数据库键才能完成的命令也没有被实现。 在将来， 用户也许可以通过 MIGRATE COPY 命令， 在集群的计算节点（computation node）中执行针对多个数据库键的只读操作， 但集群本身不会去实现那些需要将多个数据库键在多个节点中移来移去的复杂多键命令。 Redis 集群不像单机 Redis 那样支持多数据库功能， 集群只使用默认的 0 号数据库， 并且不能使用 SELECT index 命令。 Redis 集群协议中的客户端和服务器Redis 集群中的节点有以下责任： 持有键值对数据。 记录集群的状态，包括键到正确节点的映射（mapping keys to right nodes）。 自动发现其他节点，识别工作不正常的节点，并在有需要时，在从节点中选举出新的主节点。 为了执行以上列出的任务， 集群中的每个节点都与其他节点建立起了“集群连接（cluster bus）”， 该连接是一个 TCP 连接， 使用二进制协议进行通讯。 节点之间使用 Gossip 协议 来进行以下工作： 传播（propagate）关于集群的信息，以此来发现新的节点。 向其他节点发送 PING 数据包，以此来检查目标节点是否正常运作。 在特定事件发生时，发送集群信息。 除此之外， 集群连接还用于在集群中发布或订阅信息。 因为集群节点不能代理（proxy）命令请求， 所以客户端应该在节点返回 -MOVED 或者 -ASK 转向（redirection）错误时， 自行将命令请求转发至其他节点。 因为客户端可以自由地向集群中的任何一个节点发送命令请求， 并可以在有需要时， 根据转向错误所提供的信息， 将命令转发至正确的节点， 所以在理论上来说， 客户端是无须保存集群状态信息的。 不过， 如果客户端可以将键和节点之间的映射信息保存起来， 可以有效地减少可能出现的转向次数， 籍此提升命令执行的效率。 键分布模型Redis 集群的键空间被分割为 16384 个槽（slot）， 集群的最大节点数量也是 16384 个。 Note 推荐的最大节点数量为 1000 个左右。 每个主节点都负责处理 16384 个哈希槽的其中一部分。 当我们说一个集群处于“稳定”（stable）状态时， 指的是集群没有在执行重配置（reconfiguration）操作， 每个哈希槽都只由一个节点进行处理。 Note 重配置指的是将某个/某些槽从一个节点移动到另一个节点。 Note 一个主节点可以有任意多个从节点， 这些从节点用于在主节点发生网络断线或者节点失效时， 对主节点进行替换。 以下是负责将键映射到槽的算法：1HASH_SLOT = CRC16(key) mod 16384 以下是该算法所使用的参数： 算法的名称: XMODEM (又称 ZMODEM 或者 CRC-16/ACORN) 结果的长度: 16 位 多项数（poly）: 1021 (也即是 x16 + x12 + x5 + 1) 初始化值: 0000 反射输入字节（Reflect Input byte）: False 发射输出 CRC （Reflect Output CRC）: False 用于 CRC 输出值的异或常量（Xor constant to output CRC）: 0000 该算法对于输入 “123456789” 的输出: 31C3 附录 A 中给出了集群所使用的 CRC16 算法的实现。 CRC16 算法所产生的 16 位输出中的 14 位会被用到。 在我们的测试中， CRC16 算法可以很好地将各种不同类型的键平稳地分布到 16384 个槽里面。 集群节点属性每个节点在集群中都有一个独一无二的 ID ， 该 ID 是一个十六进制表示的 160 位随机数， 在节点第一次启动时由 /dev/urandom 生成。 节点会将它的 ID 保存到配置文件， 只要这个配置文件不被删除， 节点就会一直沿用这个 ID 。 节点 ID 用于标识集群中的每个节点。 一个节点可以改变它的 IP 和端口号， 而不改变节点 ID 。 集群可以自动识别出 IP/端口号的变化， 并将这一信息通过 Gossip 协议广播给其他节点知道。 以下是每个节点都有的关联信息， 并且节点会将这些信息发送给其他节点： 节点所使用的 IP 地址和 TCP 端口号。 节点的标志（flags）。 节点负责处理的哈希槽。 节点最近一次使用集群连接发送 PING 数据包（packet）的时间。 节点最近一次在回复中接收到 PONG 数据包的时间。 集群将该节点标记为下线的时间。 该节点的从节点数量。 如果该节点是从节点的话，那么它会记录主节点的节点 ID 。 如果这是一个主节点的话，那么主节点 ID 这一栏的值为 0000000 。 以上信息的其中一部分可以通过向集群中的任意节点（主节点或者从节点都可以）发送 CLUSTER NODES 命令来获得。 以下是一个向集群中的主节点发送 CLUSTER NODES 命令的例子， 该集群由三个节点组成：1234$ redis-cli cluster nodesd1861060fe6a534d42d8a19aeb36600e18785e04 :0 myself - 0 1318428930 connected 0-13643886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 connected 1365-2729d289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 connected 2730-4095 在上面列出的三行信息中， 从左到右的各个域分别是： 节点 ID ， IP 地址和端口号， 标志（flag）， 最后发送 PING 的时间， 最后接收 PONG 的时间， 连接状态， 节点负责处理的槽。 节点握手（已实现）节点总是应答（accept）来自集群连接端口的连接请求， 并对接收到的 PING 数据包进行回复， 即使这个 PING 数据包来自不可信的节点。 然而， 除了 PING 之外， 节点会拒绝其他所有并非来自集群节点的数据包。 要让一个节点承认另一个节点同属于一个集群， 只有以下两种方法： 一个节点可以通过向另一个节点发送 MEET 信息， 来强制让接收信息的节点承认发送信息的节点为集群中的一份子。 一个节点仅在管理员显式地向它发送 CLUSTER MEET ip port 命令时， 才会向另一个节点发送 MEET 信息。 另外， 如果一个可信节点向另一个节点传播第三者节点的信息， 那么接收信息的那个节点也会将第三者节点识别为集群中的一份子。 也即是说， 如果 A 认识 B ， B 认识 C ， 并且 B 向 A 传播关于 C 的信息， 那么 A 也会将 C 识别为集群中的一份子， 并尝试连接 C 。 这意味着如果我们将一个/一些新节点添加到一个集群中， 那么这个/这些新节点最终会和集群中已有的其他所有节点连接起来。 这说明只要管理员使用 CLUSTER MEET 命令显式地指定了可信关系， 集群就可以自动发现其他节点。 这种节点识别机制通过防止不同的 Redis 集群因为 IP 地址变更或者其他网络事件的发生而产生意料之外的联合（mix）， 从而使得集群更具健壮性。 当节点的网络连接断开时， 它会主动连接其他已知的节点。 MOVED 转向一个 Redis 客户端可以向集群中的任意节点（包括从节点）发送命令请求。 节点会对命令请求进行分析， 如果该命令是集群可以执行的命令， 那么节点会查找这个命令所要处理的键所在的槽。 如果要查找的哈希槽正好就由接收到命令的节点负责处理， 那么节点就直接执行这个命令。 另一方面， 如果所查找的槽不是由该节点处理的话， 节点将查看自身内部所保存的哈希槽到节点 ID 的映射记录， 并向客户端回复一个 MOVED 错误。 以下是一个 MOVED 错误的例子：123GET x-MOVED 3999 127.0.0.1:6381 错误信息包含键 x 所属的哈希槽 3999 ， 以及负责处理这个槽的节点的 IP 和端口号 127.0.0.1:6381 。 客户端需要根据这个 IP 和端口号， 向所属的节点重新发送一次 GET key 命令请求。 注意， 即使客户端在重新发送 GET key 命令之前， 等待了非常久的时间， 以至于集群又再次更改了配置， 使得节点 127.0.0.1:6381 已经不再处理槽 3999 ， 那么当客户端向节点 127.0.0.1:6381 发送 GET key 命令的时候， 节点将再次向客户端返回 MOVED 错误， 指示现在负责处理槽 3999 的节点。 虽然我们用 ID 来标识集群中的节点， 但是为了让客户端的转向操作尽可能地简单， 节点在 MOVED 错误中直接返回目标节点的 IP 和端口号， 而不是目标节点的 ID 。 虽然不是必须的， 但一个客户端应该记录（memorize）下“槽 3999 由节点 127.0.0.1:6381 负责处理“这一信息， 这样当再次有命令需要对槽 3999 执行时， 客户端就可以加快寻找正确节点的速度。 注意， 当集群处于稳定状态时， 所有客户端最终都会保存有一个哈希槽至节点的映射记录（map of hash slots to nodes）， 使得集群非常高效： 客户端可以直接向正确的节点发送命令请求， 无须转向、代理或者其他任何可能发生单点故障（single point failure）的实体（entiy）。 除了 MOVED 转向错误之外， 一个客户端还应该可以处理稍后介绍的 ASK 转向错误。 集群在线重配置（live reconfiguration）Redis 集群支持在集群运行的过程中添加或者移除节点。 实际上， 节点的添加操作和节点的删除操作可以抽象成同一个操作， 那就是， 将哈希槽从一个节点移动到另一个节点： 添加一个新节点到集群， 等于将其他已存在节点的槽移动到一个空白的新节点里面。 从集群中移除一个节点， 等于将被移除节点的所有槽移动到集群的其他节点上面去。 因此， 实现 Redis 集群在线重配置的核心就是将槽从一个节点移动到另一个节点的能力。 因为一个哈希槽实际上就是一些键的集合， 所以 Redis 集群在重哈希（rehash）时真正要做的， 就是将一些键从一个节点移动到另一个节点。 要理解 Redis 集群如何将槽从一个节点移动到另一个节点， 我们需要对 CLUSTER 命令的各个子命令进行介绍， 这些命理负责管理集群节点的槽转换表（slots translation table）。 以下是 CLUSTER 命令可用的子命令：12345CLUSTER ADDSLOTS slot1 [slot2] ... [slotN]CLUSTER DELSLOTS slot1 [slot2] ... [slotN]CLUSTER SETSLOT slot NODE nodeCLUSTER SETSLOT slot MIGRATING nodeCLUSTER SETSLOT slot IMPORTING node 最开头的两条命令 ADDSLOTS 和 DELSLOTS 分别用于向节点指派（assign）或者移除节点， 当槽被指派或者移除之后， 节点会将这一信息通过 Gossip 协议传播到整个集群。 ADDSLOTS 命令通常在新创建集群时， 作为一种快速地将各个槽指派给各个节点的手段来使用。 CLUSTER SETSLOT slot NODE node 子命令可以将指定的槽 slot 指派给节点 node 。 至于 CLUSTER SETSLOT slot MIGRATING node 命令和 CLUSTER SETSLOT slot IMPORTING node 命令， 前者用于将给定节点 node 中的槽 slot 迁移出节点， 而后者用于将给定槽 slot 导入到节点 node ： 当一个槽被设置为 MIGRATING 状态时， 原来持有这个槽的节点仍然会继续接受关于这个槽的命令请求， 但只有命令所处理的键仍然存在于节点时， 节点才会处理这个命令请求。 如果命令所使用的键不存在与该节点， 那么节点将向客户端返回一个 -ASK 转向（redirection）错误， 告知客户端， 要将命令请求发送到槽的迁移目标节点。 当一个槽被设置为 IMPORTING 状态时， 节点仅在接收到 ASKING 命令之后， 才会接受关于这个槽的命令请求。 如果客户端没有向节点发送 ASKING 命令， 那么节点会使用 -MOVED 转向错误将命令请求转向至真正负责处理这个槽的节点。 上面关于 MIGRATING 和 IMPORTING 的说明有些难懂， 让我们用一个实际的实例来说明一下。 假设现在， 我们有 A 和 B 两个节点， 并且我们想将槽 8 从节点 A 移动到节点 B ， 于是我们： 向节点 B 发送命令 CLUSTER SETSLOT 8 IMPORTING A 向节点 A 发送命令 CLUSTER SETSLOT 8 MIGRATING B 每当客户端向其他节点发送关于哈希槽 8 的命令请求时， 这些节点都会向客户端返回指向节点 A 的转向信息： 如果命令要处理的键已经存在于槽 8 里面， 那么这个命令将由节点 A 处理。 如果命令要处理的键未存在于槽 8 里面（比如说，要向槽添加一个新的键）， 那么这个命令由节点 B 处理。 这种机制将使得节点 A 不再创建关于槽 8 的任何新键。 与此同时， 一个特殊的客户端 redis-trib 以及 Redis 集群配置程序（configuration utility）会将节点 A 中槽 8 里面的键移动到节点 B 。 键的移动操作由以下两个命令执行：1CLUSTER GETKEYSINSLOT slot count 上面的命令会让节点返回 count 个 slot 槽中的键， 对于命令所返回的每个键， redis-trib 都会向节点 A 发送一条 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令， 该命令会将所指定的键原子地（atomic）从节点 A 移动到节点 B （在移动键期间，两个节点都会处于阻塞状态，以免出现竞争条件）。 以下为 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令的运作原理：1MIGRATE target_host target_port key target_database id timeout 执行 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令的节点会连接到 target 节点， 并将序列化后的 key 数据发送给 target ， 一旦 target 返回 OK ， 节点就将自己的 key 从数据库中删除。 从一个外部客户端的视角来看， 在某个时间点上， 键 key 要么存在于节点 A ， 要么存在于节点 B ， 但不会同时存在于节点 A 和节点 B 。 因为 Redis 集群只使用 0 号数据库， 所以当 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令被用于执行集群操作时， target_database 的值总是 0 。 target_database 参数的存在是为了让 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令成为一个通用命令， 从而可以作用于集群以外的其他功能。 我们对 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令做了优化， 使得它即使在传输包含多个元素的列表键这样的复杂数据时， 也可以保持高效。 不过， 尽管 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 非常高效， 对一个键非常多、并且键的数据量非常大的集群来说， 集群重配置还是会占用大量的时间， 可能会导致集群没办法适应那些对于响应时间有严格要求的应用程序。 ASK 转向在之前介绍 MOVED 转向的时候， 我们说除了 MOVED 转向之外， 还有另一种 ASK 转向。 当节点需要让一个客户端长期地（permanently）将针对某个槽的命令请求发送至另一个节点时， 节点向客户端返回 MOVED 转向。 另一方面， 当节点需要让客户端仅仅在下一个命令请求中转向至另一个节点时， 节点向客户端返回 ASK 转向。 比如说， 在我们上一节列举的槽 8 的例子中， 因为槽 8 所包含的各个键分散在节点 A 和节点 B 中， 所以当客户端在节点 A 中没找到某个键时， 它应该转向到节点 B 中去寻找， 但是这种转向应该仅仅影响一次命令查询， 而不是让客户端每次都直接去查找节点 B ： 在节点 A 所持有的属于槽 8 的键没有全部被迁移到节点 B 之前， 客户端应该先访问节点 A ， 然后再访问节点 B 。 因为这种转向只针对 16384 个槽中的其中一个槽， 所以转向对集群造成的性能损耗属于可接受的范围。 因为上述原因， 如果我们要在查找节点 A 之后， 继续查找节点 B ， 那么客户端在向节点 B 发送命令请求之前， 应该先发送一个 ASKING 命令， 否则这个针对带有 IMPORTING 状态的槽的命令请求将被节点 B 拒绝执行。 接收到客户端 ASKING 命令的节点将为客户端设置一个一次性的标志（flag）， 使得客户端可以执行一次针对 IMPORTING 状态的槽的命令请求。 从客户端的角度来看， ASK 转向的完整语义（semantics）如下： 如果客户端接收到 ASK 转向， 那么将命令请求的发送对象调整为转向所指定的节点。 先发送一个 ASKING 命令，然后再发送真正的命令请求。 不必更新客户端所记录的槽 8 至节点的映射： 槽 8 应该仍然映射到节点 A ， 而不是节点 B 。 一旦节点 A 针对槽 8 的迁移工作完成， 节点 A 在再次收到针对槽 8 的命令请求时， 就会向客户端返回 MOVED 转向， 将关于槽 8 的命令请求长期地转向到节点 B 。 注意， 即使客户端出现 Bug ， 过早地将槽 8 映射到了节点 B 上面， 但只要这个客户端不发送 ASKING 命令， 客户端发送命令请求的时候就会遇上 MOVED 错误， 并将它转向回节点 A 。 容错节点失效检测以下是节点失效检查的实现方法： 当一个节点向另一个节点发送 PING 命令， 但是目标节点未能在给定的时限内返回 PING 命令的回复时， 那么发送命令的节点会将目标节点标记为 PFAIL （possible failure，可能已失效）。 等待 PING 命令回复的时限称为“节点超时时限（node timeout）”， 是一个节点选项（node-wise setting）。 每次当节点对其他节点发送 PING 命令的时候， 它都会随机地广播三个它所知道的节点的信息， 这些信息里面的其中一项就是说明节点是否已经被标记为 PFAIL 或者 FAIL 。 当节点接收到其他节点发来的信息时， 它会记下那些被其他节点标记为失效的节点。 这称为失效报告（failure report）。 如果节点已经将某个节点标记为 PFAIL ， 并且根据节点所收到的失效报告显式， 集群中的大部分其他主节点也认为那个节点进入了失效状态， 那么节点会将那个失效节点的状态标记为 FAIL 。 一旦某个节点被标记为 FAIL ， 关于这个节点已失效的信息就会被广播到整个集群， 所有接收到这条信息的节点都会将失效节点标记为 FAIL 。 简单来说， 一个节点要将另一个节点标记为失效， 必须先询问其他节点的意见， 并且得到大部分主节点的同意才行。 因为过期的失效报告会被移除， 所以主节点要将某个节点标记为 FAIL 的话， 必须以最近接收到的失效报告作为根据。 在以下两种情况中， 节点的 FAIL 状态会被移除： 如果被标记为 FAIL 的是从节点， 那么当这个节点重新上线时， FAIL 标记就会被移除。 保持（retaning）从节点的 FAIL 状态是没有意义的， 因为它不处理任何槽， 一个从节点是否处于 FAIL 状态， 决定了这个从节点在有需要时能否被提升为主节点。 如果一个主节点被打上 FAIL 标记之后， 经过了节点超时时限的四倍时间， 再加上十秒钟之后， 针对这个主节点的槽的故障转移操作仍未完成， 并且这个主节点已经重新上线的话， 那么移除对这个节点的 FAIL 标记。 在第二种情况中， 如果故障转移未能顺利完成， 并且主节点重新上线， 那么集群就继续使用原来的主节点， 从而免去管理员介入的必要。 集群状态检测（已部分实现）每当集群发生配置变化时（可能是哈希槽更新，也可能是某个节点进入失效状态）， 集群中的每个节点都会对它所知道的节点进行扫描（scan）。 一旦配置处理完毕， 集群会进入以下两种状态的其中一种： FAIL ： 集群不能正常工作。 当集群中有某个节点进入失效状态时， 集群不能处理任何命令请求， 对于每个命令请求， 集群节点都返回错误回复。 OK ： 集群可以正常工作， 负责处理全部 16384 个槽的节点中， 没有一个节点被标记为 FAIL 状态。 这说明即使集群中只有一部分哈希槽不能正常使用， 整个集群也会停止处理任何命令。 不过节点从出现问题到被标记为 FAIL 状态的这段时间里， 集群仍然会正常运作， 所以集群在某些时候， 仍然有可能只能处理针对 16384 个槽的其中一个子集的命令请求。 以下是集群进入 FAIL 状态的两种情况： 至少有一个哈希槽不可用，因为负责处理这个槽的节点进入了 FAIL 状态。 集群中的大部分主节点都进入下线状态。当大部分主节点都进入 PFAIL 状态时，集群也会进入 FAIL 状态。 第二个检查是必须的， 因为要将一个节点从 PFAIL 状态改变为 FAIL 状态， 必须要有大部分主节点进行投票表决， 但是， 当集群中的大部分主节点都进入失效状态时， 单凭一个两个节点是没有办法将一个节点标记为 FAIL 状态的。 因此， 有了第二个检查条件， 只要集群中的大部分主节点进入了下线状态， 那么集群就可以在不请求这些主节点的意见下， 将某个节点判断为 FAIL 状态， 从而让整个集群停止处理命令请求。 从节点选举一旦某个主节点进入 FAIL 状态， 如果这个主节点有一个或多个从节点存在， 那么其中一个从节点会被升级为新的主节点， 而其他从节点则会开始对这个新的主节点进行复制。 新的主节点由已下线主节点属下的所有从节点中自行选举产生， 以下是选举的条件： 这个节点是已下线主节点的从节点。 已下线主节点负责处理的槽数量非空。 从节点的数据被认为是可靠的， 也即是， 主从节点之间的复制连接（replication link）的断线时长不能超过节点超时时限（node timeout）乘以 REDIS_CLUSTER_SLAVE_VALIDITY_MULT 常量得出的积。 如果一个从节点满足了以上的所有条件， 那么这个从节点将向集群中的其他主节点发送授权请求， 询问它们， 是否允许自己（从节点）升级为新的主节点。 如果发送授权请求的从节点满足以下属性， 那么主节点将向从节点返回 FAILOVER_AUTH_GRANTED 授权， 同意从节点的升级要求： 发送授权请求的是一个从节点， 并且它所属的主节点处于 FAIL 状态。 在已下线主节点的所有从节点中， 这个从节点的节点 ID 在排序中是最小的。 这个从节点处于正常的运行状态： 它没有被标记为 FAIL 状态， 也没有被标记为 PFAIL 状态。 一旦某个从节点在给定的时限内得到大部分主节点的授权， 它就会开始执行以下故障转移操作： 通过 PONG 数据包（packet）告知其他节点， 这个节点现在是主节点了。 通过 PONG 数据包告知其他节点， 这个节点是一个已升级的从节点（promoted slave）。 接管（claiming）所有由已下线主节点负责处理的哈希槽。 显式地向所有节点广播一个 PONG 数据包， 加速其他节点识别这个节点的进度， 而不是等待定时的 PING / PONG 数据包。 所有其他节点都会根据新的主节点对配置进行相应的更新，特别地： 所有被新的主节点接管的槽会被更新。 已下线主节点的所有从节点会察觉到 PROMOTED 标志， 并开始对新的主节点进行复制。 如果已下线的主节点重新回到上线状态， 那么它会察觉到 PROMOTED 标志， 并将自身调整为现任主节点的从节点。 在集群的生命周期中， 如果一个带有 PROMOTED 标识的主节点因为某些原因转变成了从节点， 那么该节点将丢失它所带有的 PROMOTED 标识。 发布/订阅（已实现，但仍然需要改善）在一个 Redis 集群中， 客户端可以订阅任意一个节点， 也可以向任意一个节点发送信息， 节点会对客户端所发送的信息进行转发。 在目前的实现中， 节点会将接收到的信息广播至集群中的其他所有节点， 在将来的实现中， 可能会使用 bloom filter 或者其他算法来优化这一操作。 附录 A： CRC16 算法的 ANSI 实现参考12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/* * Copyright 2001-2010 Georges Menie (www.menie.org) * Copyright 2010 Salvatore Sanfilippo (adapted to Redis coding style) * All rights reserved. * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions are met: * * * Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * Neither the name of the University of California, Berkeley nor the * names of its contributors may be used to endorse or promote products * derived from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE * DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. *//* CRC16 implementation acording to CCITT standards. * * Note by @antirez: this is actually the XMODEM CRC 16 algorithm, using the * following parameters: * * Name : "XMODEM", also known as "ZMODEM", "CRC-16/ACORN" * Width : 16 bit * Poly : 1021 (That is actually x^16 + x^12 + x^5 + 1) * Initialization : 0000 * Reflect Input byte : False * Reflect Output CRC : False * Xor constant to output CRC : 0000 * Output for "123456789" : 31C3 */static const uint16_t crc16tab[256]= &#123; 0x0000,0x1021,0x2042,0x3063,0x4084,0x50a5,0x60c6,0x70e7, 0x8108,0x9129,0xa14a,0xb16b,0xc18c,0xd1ad,0xe1ce,0xf1ef, 0x1231,0x0210,0x3273,0x2252,0x52b5,0x4294,0x72f7,0x62d6, 0x9339,0x8318,0xb37b,0xa35a,0xd3bd,0xc39c,0xf3ff,0xe3de, 0x2462,0x3443,0x0420,0x1401,0x64e6,0x74c7,0x44a4,0x5485, 0xa56a,0xb54b,0x8528,0x9509,0xe5ee,0xf5cf,0xc5ac,0xd58d, 0x3653,0x2672,0x1611,0x0630,0x76d7,0x66f6,0x5695,0x46b4, 0xb75b,0xa77a,0x9719,0x8738,0xf7df,0xe7fe,0xd79d,0xc7bc, 0x48c4,0x58e5,0x6886,0x78a7,0x0840,0x1861,0x2802,0x3823, 0xc9cc,0xd9ed,0xe98e,0xf9af,0x8948,0x9969,0xa90a,0xb92b, 0x5af5,0x4ad4,0x7ab7,0x6a96,0x1a71,0x0a50,0x3a33,0x2a12, 0xdbfd,0xcbdc,0xfbbf,0xeb9e,0x9b79,0x8b58,0xbb3b,0xab1a, 0x6ca6,0x7c87,0x4ce4,0x5cc5,0x2c22,0x3c03,0x0c60,0x1c41, 0xedae,0xfd8f,0xcdec,0xddcd,0xad2a,0xbd0b,0x8d68,0x9d49, 0x7e97,0x6eb6,0x5ed5,0x4ef4,0x3e13,0x2e32,0x1e51,0x0e70, 0xff9f,0xefbe,0xdfdd,0xcffc,0xbf1b,0xaf3a,0x9f59,0x8f78, 0x9188,0x81a9,0xb1ca,0xa1eb,0xd10c,0xc12d,0xf14e,0xe16f, 0x1080,0x00a1,0x30c2,0x20e3,0x5004,0x4025,0x7046,0x6067, 0x83b9,0x9398,0xa3fb,0xb3da,0xc33d,0xd31c,0xe37f,0xf35e, 0x02b1,0x1290,0x22f3,0x32d2,0x4235,0x5214,0x6277,0x7256, 0xb5ea,0xa5cb,0x95a8,0x8589,0xf56e,0xe54f,0xd52c,0xc50d, 0x34e2,0x24c3,0x14a0,0x0481,0x7466,0x6447,0x5424,0x4405, 0xa7db,0xb7fa,0x8799,0x97b8,0xe75f,0xf77e,0xc71d,0xd73c, 0x26d3,0x36f2,0x0691,0x16b0,0x6657,0x7676,0x4615,0x5634, 0xd94c,0xc96d,0xf90e,0xe92f,0x99c8,0x89e9,0xb98a,0xa9ab, 0x5844,0x4865,0x7806,0x6827,0x18c0,0x08e1,0x3882,0x28a3, 0xcb7d,0xdb5c,0xeb3f,0xfb1e,0x8bf9,0x9bd8,0xabbb,0xbb9a, 0x4a75,0x5a54,0x6a37,0x7a16,0x0af1,0x1ad0,0x2ab3,0x3a92, 0xfd2e,0xed0f,0xdd6c,0xcd4d,0xbdaa,0xad8b,0x9de8,0x8dc9, 0x7c26,0x6c07,0x5c64,0x4c45,0x3ca2,0x2c83,0x1ce0,0x0cc1, 0xef1f,0xff3e,0xcf5d,0xdf7c,0xaf9b,0xbfba,0x8fd9,0x9ff8, 0x6e17,0x7e36,0x4e55,0x5e74,0x2e93,0x3eb2,0x0ed1,0x1ef0&#125;;uint16_t crc16(const char *buf, int len) &#123; int counter; uint16_t crc = 0; for (counter = 0; counter &lt; len; counter++) crc = (crc&lt;&lt;8) ^ crc16tab[((crc&gt;&gt;8) ^ *buf++)&amp;0x00FF]; return crc;&#125; 持久化（persistence）这篇文章提供了 Redis 持久化的技术性描述， 推荐所有 Redis 用户阅读。 要更广泛地了解 Redis 持久化， 以及这种持久化所保证的耐久性（durability）， 请参考文章 Redis persistence demystified （中文）。 Redis 持久化Redis 提供了多种不同级别的持久化方式： RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 你甚至可以关闭持久化功能，让数据只在服务器运行时存在。 了解 RDB 持久化和 AOF 持久化之间的异同是非常重要的， 以下几个小节将详细地介绍这这两种持久化功能， 并对它们的相同和不同之处进行说明。 RDB 的优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次 RDB 文件，并且在每个月的每一天，也备份一个 RDB 文件。 这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心，或者亚马逊 S3 中。 RDB 可以最大化 Redis 的性能：父进程在保存 RDB 文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB 的缺点 如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 虽然 Redis 允许你设置不同的保存点（save point）来控制保存 RDB 文件的频率， 但是， 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOF 的优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF 的缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH source destination timeout 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 RDB 和 AOF ，我应该用哪一个？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 Note 因为以上提到的种种原因， 未来可能会将 AOF 和 RDB 整合成单个持久化模型。 （这是一个长期计划。） 接下来的几个小节将介绍 RDB 和 AOF 的更多细节。 RDB 快照在默认情况下， Redis 将数据库快照保存在名字为 dump.rdb 的二进制文件中。 你可以对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。 你也可以通过调用 SAVE 或者 BGSAVE ， 手动让 Redis 进行数据集保存操作。 比如说， 以下设置会让 Redis 在满足“ 60 秒内有至少有 1000 个键被改动”这一条件时， 自动保存一次数据集：1save 60 1000 这种持久化方式被称为快照（snapshot）。 快照的运作方式当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作： Redis 调用 fork() ，同时拥有父进程和子进程。 子进程将数据集写入到一个临时 RDB 文件中。 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益。 只进行追加操作的文件（append-only file，AOF）快照功能并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。 尽管对于某些程序来说， 数据的耐久性并不是最重要的考虑因素， 但是对于那些追求完全耐久能力（full durability）的程序来说， 快照功能就不太适用了。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。 你可以通过修改配置文件来打开 AOF 功能：1appendonly yes 从现在开始， 每当 Redis 执行一个改变数据集的命令时（比如 SET key value [EX seconds] [PX milliseconds] [NX|XX]）， 这个命令就会被追加到 AOF 文件的末尾。 这样的话， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。 AOF 重写因为 AOF 的运作方式是不断地将命令追加到文件的末尾， 所以随着写入命令的不断增加， AOF 文件的体积也会变得越来越大。 举个例子， 如果你对一个计数器调用了 100 次 INCR key ， 那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录（entry）。 然而在实际上， 只使用一条 SET key value [EX seconds] [PX milliseconds] [NX|XX] 命令已经足以保存计数器的当前值了， 其余 99 条记录实际上都是多余的。 为了处理这种情况， Redis 支持一种有趣的特性： 可以在不打断服务客户端的情况下， 对 AOF 文件进行重建（rebuild）。 执行 BGREWRITEAOF 命令， Redis 将生成一个新的 AOF 文件， 这个文件包含重建当前数据集所需的最少命令。 Redis 2.2 需要自己手动执行 BGREWRITEAOF 命令； Redis 2.4 则可以自动触发 AOF 重写， 具体信息请查看 2.4 的示例配置文件。 AOF 的耐久性如何？你可以配置 Redis 多久才将数据 fsync 到磁盘一次。 有三个选项： 每次有新命令追加到 AOF 文件时就执行一次 fsync ：非常慢，也非常安全。 每秒 fsync 一次：足够快（和使用 RDB 持久化差不多），并且在故障时只会丢失 1 秒钟的数据。 从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。 推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。 总是 fsync 的策略在实际使用中非常慢， 即使在 Redis 2.0 对相关的程序进行了改进之后仍是如此 —— 频繁调用 fsync 注定了这种策略不可能快得起来。 如果 AOF 文件出错了，怎么办？服务器可能在程序正在对 AOF 文件进行写入时停机， 如果停机造成了 AOF 文件出错（corrupt）， 那么 Redis 在重启时会拒绝载入这个 AOF 文件， 从而确保数据的一致性不会被破坏。 当发生这种情况时， 可以用以下方法来修复出错的 AOF 文件： 为现有的 AOF 文件创建一个备份。 使用 Redis 附带的 redis-check-aof 程序，对原来的 AOF 文件进行修复。 1$ redis-check-aof --fix （可选）使用 diff -u 对比修复后的 AOF 文件和原始 AOF 文件的备份，查看两个文件之间的不同之处。 重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复。 AOF 的运作方式AOF 重写和 RDB 创建快照一样，都巧妙地利用了写时复制机制。 以下是 AOF 重写的执行步骤： Redis 执行 fork() ，现在同时拥有父进程和子进程。 子进程开始将新 AOF 文件的内容写入到临时文件。 对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾： 这样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。 搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 怎么从 RDB 持久化切换到 AOF 持久化在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换到 AOF ： 为最新的 dump.rdb 文件创建一个备份。 将备份放到一个安全的地方。 执行以下两条命令： 123redis-cli&gt; CONFIG SET appendonly yesredis-cli&gt; CONFIG SET save "" 确保命令执行之后，数据库的键的数量没有改变。 确保写命令会被正确地追加到 AOF 文件的末尾。 步骤 3 执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。 步骤 3 执行的第二条命令用于关闭 RDB 功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用 RDB 和 AOF 这两种持久化功能。 Note 别忘了在 redis.conf 中打开 AOF 功能！ 否则的话， 服务器重启之后， 之前通过 CONFIG SET 设置的配置就会被遗忘， 程序会按原来的配置来启动服务器。 RDB 和 AOF 之间的相互作用在版本号大于等于 2.4 的 Redis 中， BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE 。 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户， BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。 备份 Redis 数据在阅读这个小节前， 先将下面这句话铭记于心： 一定要备份你的数据库！ 磁盘故障， 节点失效， 诸如此类的问题都可能让你的数据消失不见， 不进行备份是非常危险的。 Redis 对于数据备份是非常友好的， 因为你可以在服务器运行的时候对 RDB 文件进行复制： RDB 文件一旦被创建， 就不会进行任何修改。 当服务器要创建一个新的 RDB 文件时， 它先将文件的内容保存在一个临时文件里面， 当临时文件写入完毕时， 程序才使用 rename(2) 原子地用临时文件替换原来的 RDB 文件。 这也就是说， 无论何时， 复制 RDB 文件都是绝对安全的。 以下是我们的建议： 创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹。 确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照。 至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外。 容灾备份Redis 的容灾备份基本上就是对数据进行备份， 并将这些备份传送到多个不同的外部数据中心。 容灾备份可以在 Redis 运行并产生快照的主数据中心发生严重的问题时， 仍然让数据处于安全状态。 因为很多 Redis 用户都是创业者， 他们没有大把大把的钱可以浪费， 所以下面介绍的都是一些实用又便宜的容灾备份方法： Amazon S3 ，以及其他类似 S3 的服务，是一个构建灾难备份系统的好地方。 最简单的方法就是将你的每小时或者每日 RDB 备份加密并传送到 S3 。 对数据的加密可以通过 gpg -c 命令来完成（对称加密模式）。 记得把你的密码放到几个不同的、安全的地方去（比如你可以把密码复制给你组织里最重要的人物）。 同时使用多个储存服务来保存数据文件，可以提升数据的安全性。 传送快照可以使用 SCP 来完成（SSH 的组件）。 以下是简单并且安全的传送方法： 买一个离你的数据中心非常远的 VPS ， 装上 SSH ， 创建一个无口令的 SSH 客户端 key ， 并将这个 key 添加到 VPS 的 authorized_keys 文件中， 这样就可以向这个 VPS 传送快照备份文件了。 为了达到最好的数据安全性，至少要从两个不同的提供商那里各购买一个 VPS 来进行数据容灾备份。 需要注意的是， 这类容灾系统如果没有小心地进行处理的话， 是很容易失效的。 最低限度下， 你应该在文件传送完毕之后， 检查所传送备份文件的体积和原始快照文件的体积是否相同。 如果你使用的是 VPS ， 那么还可以通过比对文件的 SHA1 校验和来确认文件是否传送完整。 另外， 你还需要一个独立的警报系统， 让它在负责传送备份文件的传送器（transfer）失灵时通知你。 发布与订阅（pub/sub）SUBSCRIBE channel [channel …] 、 UNSUBSCRIBE [channel [channel …]] 和 PUBLISH channel message 三个命令实现了发布与订阅信息泛型（Publish/Subscribe messaging paradigm）， 在这个实现中， 发送者（发送信息的客户端）不是将信息直接发送给特定的接收者（接收信息的客户端）， 而是将信息发送给频道（channel）， 然后由频道将信息转发给所有对这个频道感兴趣的订阅者。 发送者无须知道任何关于订阅者的信息， 而订阅者也无须知道是那个客户端给它发送信息， 它只要关注自己感兴趣的频道即可。 对发布者和订阅者进行解构（decoupling）， 可以极大地提高系统的扩展性（scalability）， 并得到一个更动态的网络拓扑（network topology）。 比如说， 要订阅频道 foo 和 bar ， 客户端可以使用频道名字作为参数来调用 SUBSCRIBE channel [channel …] 命令：1redis&gt; SUBSCRIBE foo bar 当有客户端发送信息到这些频道时， Redis 会将传入的信息推送到所有订阅这些频道的客户端里面。 正在订阅频道的客户端不应该发送除 SUBSCRIBE channel [channel …] 和 UNSUBSCRIBE [channel [channel …]] 之外的其他命令。 其中， SUBSCRIBE channel [channel …] 可以用于订阅更多频道， 而 UNSUBSCRIBE [channel [channel …]] 则可以用于退订已订阅的一个或多个频道。 SUBSCRIBE channel [channel …] 和 UNSUBSCRIBE [channel [channel …]] 的执行结果会以信息的形式返回， 客户端可以通过分析所接收信息的第一个元素， 从而判断所收到的内容是一条真正的信息， 还是 SUBSCRIBE channel [channel …] 或 UNSUBSCRIBE [channel [channel …]] 命令的操作结果。 信息的格式频道转发的每条信息都是一条带有三个元素的多条批量回复（multi-bulk reply）。 信息的第一个元素标识了信息的类型： subscribe ： 表示当前客户端成功地订阅了信息第二个元素所指示的频道。 而信息的第三个元素则记录了目前客户端已订阅频道的总数。 unsubscribe ： 表示当前客户端成功地退订了信息第二个元素所指示的频道。 信息的第三个元素记录了客户端目前仍在订阅的频道数量。 当客户端订阅的频道数量降为 0 时， 客户端不再订阅任何频道， 它可以像往常一样， 执行任何 Redis 命令。 message ： 表示这条信息是由某个客户端执行 PUBLISH channel message 命令所发送的， 真正的信息。 信息的第二个元素是信息来源的频道， 而第三个元素则是信息的内容。 举个例子， 如果客户端执行以下命令：1redis&gt; SUBSCRIBE first second 那么它将收到以下回复：12345671) "subscribe"2) "first"3) (integer) 14) "subscribe"5) "second"6) (integer) 2 如果在这时， 另一个客户端执行以下 PUBLISH channel message 命令：1redis&gt; PUBLISH second Hello 那么之前订阅了 second 频道的客户端将收到以下信息：1231) "message"2) "second"3) "hello" 当订阅者决定退订所有频道时， 它可以执行一个无参数的 UNSUBSCRIBE [channel [channel …]] 命令：1redis&gt; UNSUBSCRIBE 这个命令将接到以下回复：12345671) "unsubscribe"2) "second"3) (integer) 11) "unsubscribe"2) "first"3) (integer) 0 订阅模式Redis 的发布与订阅实现支持模式匹配（pattern matching）： 客户端可以订阅一个带 * 号的模式， 如果某个/某些频道的名字和这个模式匹配， 那么当有信息发送给这个/这些频道的时候， 客户端也会收到这个/这些频道的信息。 比如说，执行命令1redis&gt; PSUBSCRIBE news.* 的客户端将收到来自 news.art.figurative 、 news.music.jazz 等频道的信息。 客户端订阅的模式里面可以包含多个 glob 风格的通配符， 比如 * 、 ? 和 […] ， 等等。 执行命令1redis&gt; PUNSUBSCRIBE news.* 将退订 news.* 模式， 其他已订阅的模式不会被影响。 通过订阅模式接收到的信息， 和通过订阅频道接收到的信息， 这两者的格式不太一样： 通过订阅模式而接收到的信息的类型为 pmessage ： 这代表有某个客户端通过 PUBLISH channel message 向某个频道发送了信息， 而这个频道刚好匹配了当前客户端所订阅的某个模式。 信息的第二个元素记录了被匹配的模式， 第三个元素记录了被匹配的频道的名字， 最后一个元素则记录了信息的实际内容。 客户端处理 PSUBSCRIBE pattern [pattern …] 和 PUNSUBSCRIBE [pattern [pattern …]] 返回值的方式， 和客户端处理 SUBSCRIBE channel [channel …] 和 UNSUBSCRIBE [channel [channel …]] 的方式类似： 通过对信息的第一个元素进行分析， 客户端可以判断接收到的信息是一个真正的信息， 还是 PSUBSCRIBE pattern [pattern …] 或 PUNSUBSCRIBE [pattern [pattern …]] 命令的返回值。 通过频道和模式接收同一条信息如果客户端订阅的多个模式匹配了同一个频道， 或者客户端同时订阅了某个频道、以及匹配这个频道的某个模式， 那么它可能会多次接收到同一条信息。 举个例子， 如果客户端执行了以下命令：12SUBSCRIBE fooPSUBSCRIBE f* 那么当有信息发送到频道 foo 时， 客户端将收到两条信息： 一条来自频道 foo ，信息类型为 message ； 另一条来自模式 f* ，信息类型为 pmessage 。 订阅总数在执行 SUBSCRIBE channel [channel …] 、 UNSUBSCRIBE [channel [channel …]] 、 PSUBSCRIBE pattern [pattern …] 和 PUNSUBSCRIBE [pattern [pattern …]] 命令时， 返回结果的最后一个元素是客户端目前仍在订阅的频道和模式总数。 当客户端退订所有频道和模式， 也即是这个总数值下降为 0 的时候， 客户端将退出订阅与发布状态。 编程示例Pieter Noordhuis 提供了一个使用 EventMachine 和 Redis 编写的 高性能多用户网页聊天软件 ， 这个软件很好地展示了发布与订阅功能的用法。 客户端库实现提示因为所有接收到的信息都会包含一个信息来源： 当信息来自频道时，来源是某个频道； 当信息来自模式时，来源是某个模式。 因此， 客户端可以用一个哈希表， 将特定来源和处理该来源的回调函数关联起来。 当有新信息到达时， 程序就可以根据信息的来源， 在 O(1) 复杂度内， 将信息交给正确的回调函数来处理。 SentinelRedis 的 Sentinel 系统用于管理多个 Redis 服务器（instance）， 该系统执行以下三个任务： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis Sentinel 是一个分布式系统， 你可以在一个架构中运行多个 Sentinel 进程（progress）， 这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移， 以及选择哪个从服务器作为新的主服务器。 虽然 Redis Sentinel 释出为一个单独的可执行文件 redis-sentinel ， 但实际上它只是一个运行在特殊模式下的 Redis 服务器， 你可以在启动一个普通 Redis 服务器时通过给定 –sentinel 选项来启动 Redis Sentinel 。 Warning Redis Sentinel 目前仍在开发中， 这个文档的内容可能随着 Sentinel 实现的修改而变更。 Redis Sentinel 兼容 Redis 2.4.16 或以上版本， 推荐使用 Redis 2.8.0 或以上的版本。 获取 Sentinel目前 Sentinel 系统是 Redis 的 unstable 分支的一部分， 你必须到 Redis 项目的 Github 页面 克隆一份 unstable 分值， 然后通过编译来获得 Sentinel 系统。 Sentinel 程序可以在编译后的 src 文档中发现， 它是一个命名为 redis-sentinel 的程序。 你也可以通过下一节介绍的方法， 让 redis-server 程序运行在 Sentinel 模式之下。 另外， 一个新版本的 Sentinel 已经包含在了 Redis 2.8.0 版本的释出文件中。 启动 Sentinel对于 redis-sentinel 程序， 你可以用以下命令来启动 Sentinel 系统：1redis-sentinel /path/to/sentinel.conf 对于 redis-server 程序， 你可以用以下命令来启动一个运行在 Sentinel 模式下的 Redis 服务器：1redis-server /path/to/sentinel.conf --sentinel 两种方法都可以启动一个 Sentinel 实例。 启动 Sentinel 实例必须指定相应的配置文件， 系统会使用配置文件来保存 Sentinel 的当前状态， 并在 Sentinel 重启时通过载入配置文件来进行状态还原。 如果启动 Sentinel 时没有指定相应的配置文件， 或者指定的配置文件不可写（not writable）， 那么 Sentinel 会拒绝启动。 配置 SentinelRedis 源码中包含了一个名为 sentinel.conf 的文件， 这个文件是一个带有详细注释的 Sentinel 配置文件示例。 运行一个 Sentinel 所需的最少配置如下所示：123456789sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1sentinel monitor resque 192.168.1.3 6380 4sentinel down-after-milliseconds resque 10000sentinel failover-timeout resque 180000sentinel parallel-syncs resque 5 第一行配置指示 Sentinel 去监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 127.0.0.1 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个 Sentinel 同意 （只要同意 Sentinel 的数量不达标，自动故障迁移就不会执行）。 不过要注意， 无论你设置要多少个 Sentinel 同意才能判断一个服务器失效， 一个 Sentinel 都需要获得系统中多数（majority） Sentinel 的支持， 才能发起一次自动故障迁移， 并预留一个给定的配置纪元 （configuration Epoch ，一个配置纪元就是一个新主服务器配置的版本号）。 换句话说， 在只有少数（minority） Sentinel 进程正常运作的情况下， Sentinel 是不能执行自动故障迁移的。 其他选项的基本格式如下： sentinel &lt;选项的名字&gt; &lt;主服务器的名字&gt; &lt;选项的值&gt; 各个选项的功能如下： down-after-milliseconds 选项指定了 Sentinel 认为服务器已经断线所需的毫秒数。 如果服务器在给定的毫秒数之内， 没有返回 Sentinel 发送的 PING 命令的回复， 或者返回一个错误， 那么 Sentinel 将这个服务器标记为主观下线（subjectively down，简称 SDOWN ）。 不过只有一个 Sentinel 将服务器标记为主观下线并不一定会引起服务器的自动故障迁移： 只有在足够数量的 Sentinel 都将一个服务器标记为主观下线之后， 服务器才会被标记为客观下线（objectively down， 简称 ODOWN ）， 这时自动故障迁移才会执行。 将服务器标记为客观下线所需的 Sentinel 数量由对主服务器的配置决定。 parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。 如果从服务器被设置为允许使用过期数据集（参见对 redis.conf 文件中对 slave-serve-stale-data 选项的说明）， 那么你可能不希望所有从服务器都在同一时间向新的主服务器发送同步请求， 因为尽管复制过程的绝大部分步骤都不会阻塞从服务器， 但从服务器在载入主服务器发来的 RDB 文件时， 仍然会造成从服务器在一段时间内不能处理命令请求： 如果全部从服务器一起对新的主服务器进行同步， 那么就可能会造成所有从服务器在短时间内全部不可用的情况出现。 你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。 本文档剩余的内容将对 Sentinel 系统的其他选项进行介绍， 示例配置文件 sentinel.conf 也对相关的选项进行了完整的注释。 主观下线和客观下线前面说过， Redis 的 Sentinel 中关于下线（down）有两个不同的概念： 主观下线（Subjectively Down， 简称 SDOWN）指的是单个 Sentinel 实例对服务器做出的下线判断。 客观下线（Objectively Down， 简称 ODOWN）指的是多个 Sentinel 实例在对同一个服务器做出 SDOWN 判断， 并且通过 SENTINEL is-master-down-by-addr 命令互相交流之后， 得出的服务器下线判断。 （一个 Sentinel 可以通过向另一个 Sentinel 发送 SENTINEL is-master-down-by-addr 命令来询问对方是否认为给定的服务器已下线。） 如果一个服务器没有在 master-down-after-milliseconds 选项所指定的时间内， 对向它发送 PING 命令的 Sentinel 返回一个有效回复（valid reply）， 那么 Sentinel 就会将这个服务器标记为主观下线。 服务器对 PING 命令的有效回复可以是以下三种回复的其中一种： 返回 +PONG 。 返回 -LOADING 错误。 返回 -MASTERDOWN 错误。 如果服务器返回除以上三种回复之外的其他回复， 又或者在指定时间内没有回复 PING 命令， 那么 Sentinel 认为服务器返回的回复无效（non-valid）。 注意， 一个服务器必须在 master-down-after-milliseconds 毫秒内， 一直返回无效回复才会被 Sentinel 标记为主观下线。 举个例子， 如果 master-down-after-milliseconds 选项的值为 30000 毫秒（30 秒）， 那么只要服务器能在每 29 秒之内返回至少一次有效回复， 这个服务器就仍然会被认为是处于正常状态的。 从主观下线状态切换到客观下线状态并没有使用严格的法定人数算法（strong quorum algorithm）， 而是使用了流言协议： 如果 Sentinel 在给定的时间范围内， 从其他 Sentinel 那里接收到了足够数量的主服务器下线报告， 那么 Sentinel 就会将主服务器的状态从主观下线改变为客观下线。 如果之后其他 Sentinel 不再报告主服务器已下线， 那么客观下线状态就会被移除。 客观下线条件只适用于主服务器： 对于任何其他类型的 Redis 实例， Sentinel 在将它们判断为下线前不需要进行协商， 所以从服务器或者其他 Sentinel 永远不会达到客观下线条件。 只要一个 Sentinel 发现某个主服务器进入了客观下线状态， 这个 Sentinel 就可能会被其他 Sentinel 推选出， 并对失效的主服务器执行自动故障迁移操作。 每个 Sentinel 都需要定期执行的任务 每个 Sentinel 以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 一个有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。 如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有 Sentinel 要以每秒一次的频率确认主服务器的确进入了主观下线状态。 如果一个主服务器被标记为主观下线， 并且有足够数量的 Sentinel （至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断， 那么这个主服务器被标记为客观下线。 在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO [section] 命令。 当一个主服务器被 Sentinel 标记为客观下线时， Sentinel 向下线主服务器的所有从服务器发送 INFO [section] 命令的频率会从 10 秒一次改为每秒一次。 当没有足够数量的 Sentinel 同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向 Sentinel 的 PING 命令返回有效回复时， 主服务器的主管下线状态就会被移除。 自动发现 Sentinel 和从服务器一个 Sentinel 可以与其他多个 Sentinel 进行连接， 各个 Sentinel 之间可以互相检查对方的可用性， 并进行信息交换。 你无须为运行的每个 Sentinel 分别设置其他 Sentinel 的地址， 因为 Sentinel 可以通过发布与订阅功能来自动发现正在监视相同主服务器的其他 Sentinel ， 这一功能是通过向频道 sentinel:hello 发送信息来实现的。 与此类似， 你也不必手动列出主服务器属下的所有从服务器， 因为 Sentinel 可以通过询问主服务器来获得所有从服务器的信息。 每个 Sentinel 会以每两秒一次的频率， 通过发布与订阅功能， 向被它监视的所有主服务器和从服务器的 sentinel:hello 频道发送一条信息， 信息中包含了 Sentinel 的 IP 地址、端口号和运行 ID （runid）。 每个 Sentinel 都订阅了被它监视的所有主服务器和从服务器的 sentinel:hello 频道， 查找之前未出现过的 sentinel （looking for unknown sentinels）。 当一个 Sentinel 发现一个新的 Sentinel 时， 它会将新的 Sentinel 添加到一个列表中， 这个列表保存了 Sentinel 已知的， 监视同一个主服务器的所有其他 Sentinel 。 Sentinel 发送的信息中还包括完整的主服务器当前配置（configuration）。 如果一个 Sentinel 包含的主服务器配置比另一个 Sentinel 发送的配置要旧， 那么这个 Sentinel 会立即升级到新配置上。 在将一个新 Sentinel 添加到监视主服务器的列表上面之前， Sentinel 会先检查列表中是否已经包含了和要添加的 Sentinel 拥有相同运行 ID 或者相同地址（包括 IP 地址和端口号）的 Sentinel ， 如果是的话， Sentinel 会先移除列表中已有的那些拥有相同运行 ID 或者相同地址的 Sentinel ， 然后再添加新 Sentinel 。 Sentinel API在默认情况下， Sentinel 使用 TCP 端口 26379 （普通 Redis 服务器使用的是 6379 ）。 Sentinel 接受 Redis 协议格式的命令请求， 所以你可以使用 redis-cli 或者任何其他 Redis 客户端来与 Sentinel 进行通讯。 有两种方式可以和 Sentinel 进行通讯： 第一种方法是通过直接发送命令来查询被监视 Redis 服务器的当前状态， 以及 Sentinel 所知道的关于其他 Sentinel 的信息， 诸如此类。 另一种方法是使用发布与订阅功能， 通过接收 Sentinel 发送的通知： 当执行故障转移操作， 或者某个被监视的服务器被判断为主观下线或者客观下线时， Sentinel 就会发送相应的信息。 Sentinel 命令以下列出的是 Sentinel 接受的命令： PING ：返回 PONG 。 SENTINEL masters ：列出所有被监视的主服务器，以及这些主服务器的当前状态。 SENTINEL slaves \ ：列出给定主服务器的所有从服务器，以及这些从服务器的当前状态。 SENTINEL get-master-addr-by-name \ ： 返回给定名字的主服务器的 IP 地址和端口号。 如果这个主服务器正在执行故障转移操作， 或者针对这个主服务器的故障转移操作已经完成， 那么这个命令返回新的主服务器的 IP 地址和端口号。 SENTINEL reset \&lt;pattern> ： 重置所有名字和给定模式 pattern 相匹配的主服务器。 pattern 参数是一个 Glob 风格的模式。 重置操作清除主服务器目前的所有状态， 包括正在执行中的故障转移， 并移除目前已经发现和关联的， 主服务器的所有从服务器和 Sentinel 。 SENTINEL failover \ ： 当主服务器失效时， 在不询问其他 Sentinel 意见的情况下， 强制开始一次自动故障迁移 （不过发起故障转移的 Sentinel 会向其他 Sentinel 发送一个新的配置，其他 Sentinel 会根据这个配置进行相应的更新）。 发布与订阅信息客户端可以将 Sentinel 看作是一个只提供了订阅功能的 Redis 服务器： 你不可以使用 PUBLISH channel message 命令向这个服务器发送信息， 但你可以用 SUBSCRIBE channel [channel …] 命令或者 PSUBSCRIBE pattern [pattern …] 命令， 通过订阅给定的频道来获取相应的事件提醒。 一个频道能够接收和这个频道的名字相同的事件。 比如说， 名为 +sdown 的频道就可以接收所有实例进入主观下线（SDOWN）状态的事件。 通过执行 PSUBSCRIBE * 命令可以接收所有事件信息。 以下列出的是客户端可以通过订阅来获得的频道和信息的格式： 第一个英文单词是频道/事件的名字， 其余的是数据的格式。 注意， 当格式中包含 instance details 字样时， 表示频道所返回的信息中包含了以下用于识别目标实例的内容：1&lt;instance-type&gt; &lt;name&gt; &lt;ip&gt; &lt;port&gt; @ &lt;master-name&gt; &lt;master-ip&gt; &lt;master-port&gt; @ 字符之后的内容用于指定主服务器， 这些内容是可选的， 它们仅在 @ 字符之前的内容指定的实例不是主服务器时使用。 +reset-master \ ：主服务器已被重置。 +slave \ ：一个新的从服务器已经被 Sentinel 识别并关联。 +failover-state-reconf-slaves \ ：故障转移状态切换到了 reconf-slaves 状态。 +failover-detected \ ：另一个 Sentinel 开始了一次故障转移操作，或者一个从服务器转换成了主服务器。 +slave-reconf-sent \ ：领头（leader）的 Sentinel 向实例发送了 SLAVEOF host port 命令，为实例设置新的主服务器。 +slave-reconf-inprog \ ：实例正在将自己设置为指定主服务器的从服务器，但相应的同步过程仍未完成。 +slave-reconf-done \ ：从服务器已经成功完成对新主服务器的同步。 -dup-sentinel \ ：对给定主服务器进行监视的一个或多个 Sentinel 已经因为重复出现而被移除 —— 当 Sentinel 实例重启的时候，就会出现这种情况。 +sentinel \ ：一个监视给定主服务器的新 Sentinel 已经被识别并添加。 +sdown \ ：给定的实例现在处于主观下线状态。 -sdown \ ：给定的实例已经不再处于主观下线状态。 +odown \ ：给定的实例现在处于客观下线状态。 -odown \ ：给定的实例已经不再处于客观下线状态。 +new-epoch \ ：当前的纪元（epoch）已经被更新。 +try-failover \ ：一个新的故障迁移操作正在执行中，等待被大多数 Sentinel 选中（waiting to be elected by the majority）。 +elected-leader \ ：赢得指定纪元的选举，可以进行故障迁移操作了。 +failover-state-select-slave \ ：故障转移操作现在处于 select-slave 状态 —— Sentinel 正在寻找可以升级为主服务器的从服务器。 no-good-slave \ ：Sentinel 操作未能找到适合进行升级的从服务器。Sentinel 会在一段时间之后再次尝试寻找合适的从服务器来进行升级，又或者直接放弃执行故障转移操作。 selected-slave \ ：Sentinel 顺利找到适合进行升级的从服务器。 failover-state-send-slaveof-noone \ ：Sentinel 正在将指定的从服务器升级为主服务器，等待升级功能完成。 failover-end-for-timeout \ ：故障转移因为超时而中止，不过最终所有从服务器都会开始复制新的主服务器（slaves will eventually be configured to replicate with the new master anyway）。 failover-end \ ：故障转移操作顺利完成。所有从服务器都开始复制新的主服务器了。 +switch-master \ \&lt;oldip> \&lt;oldport> \&lt;newip> \&lt;newport> ：配置变更，主服务器的 IP 和地址已经改变。 这是绝大多数外部用户都关心的信息。 +tilt ：进入 tilt 模式。 -tilt ：退出 tilt 模式。 故障转移一次故障转移操作由以下步骤组成： 发现主服务器已经进入客观下线状态。 对我们的当前纪元进行自增（详情请参考 Raft leader election ）， 并尝试在这个纪元中当选。 如果当选失败， 那么在设定的故障迁移超时时间的两倍之后， 重新尝试当选。 如果当选成功， 那么执行以下步骤。 选出一个从服务器，并将它升级为主服务器。 向被选中的从服务器发送 SLAVEOF NO ONE 命令，让它转变为主服务器。 通过发布与订阅功能， 将更新后的配置传播给所有其他 Sentinel ， 其他 Sentinel 对它们自己的配置进行更新。 向已下线主服务器的从服务器发送 SLAVEOF host port 命令， 让它们去复制新的主服务器。 当所有从服务器都已经开始复制新的主服务器时， 领头 Sentinel 终止这次故障迁移操作。 Note 每当一个 Redis 实例被重新配置（reconfigured） —— 无论是被设置成主服务器、从服务器、又或者被设置成其他主服务器的从服务器 —— Sentinel 都会向被重新配置的实例发送一个 CONFIG REWRITE 命令， 从而确保这些配置会持久化在硬盘里。 Sentinel 使用以下规则来选择新的主服务器： 在失效主服务器属下的从服务器当中， 那些被标记为主观下线、已断线、或者最后一次回复 PING 命令的时间大于五秒钟的从服务器都会被淘汰。 在失效主服务器属下的从服务器当中， 那些与失效主服务器连接断开的时长超过 down-after 选项指定的时长十倍的从服务器都会被淘汰。 在经历了以上两轮淘汰之后剩下来的从服务器中， 我们选出复制偏移量（replication offset）最大的那个从服务器作为新的主服务器； 如果复制偏移量不可用， 或者从服务器的复制偏移量相同， 那么带有最小运行 ID 的那个从服务器成为新的主服务器。 Sentinel 自动故障迁移的一致性特质Sentinel 自动故障迁移使用 Raft 算法来选举领头（leader） Sentinel ， 从而确保在一个给定的纪元（epoch）里， 只有一个领头产生。 这表示在同一个纪元中， 不会有两个 Sentinel 同时被选中为领头， 并且各个 Sentinel 在同一个纪元中只会对一个领头进行投票。 更高的配置纪元总是优于较低的纪元， 因此每个 Sentinel 都会主动使用更新的纪元来代替自己的配置。 简单来说， 我们可以将 Sentinel 配置看作是一个带有版本号的状态。 一个状态会以最后写入者胜出（last-write-wins）的方式（也即是，最新的配置总是胜出）传播至所有其他 Sentinel 。 举个例子， 当出现网络分割（network partitions）时， 一个 Sentinel 可能会包含了较旧的配置， 而当这个 Sentinel 接到其他 Sentinel 发来的版本更新的配置时， Sentinel 就会对自己的配置进行更新。 如果要在网络分割出现的情况下仍然保持一致性， 那么应该使用 min-slaves-to-write 选项， 让主服务器在连接的从实例少于给定数量时停止执行写操作， 与此同时， 应该在每个运行 Redis 主服务器或从服务器的机器上运行 Redis Sentinel 进程。 Sentinel 状态的持久化Sentinel 的状态会被持久化在 Sentinel 配置文件里面。 每当 Sentinel 接收到一个新的配置， 或者当领头 Sentinel 为主服务器创建一个新的配置时， 这个配置会与配置纪元一起被保存到磁盘里面。 这意味着停止和重启 Sentinel 进程都是安全的。 Sentinel 在非故障迁移的情况下对实例进行重新配置即使没有自动故障迁移操作在进行， Sentinel 总会尝试将当前的配置设置到被监视的实例上面。 特别是： 根据当前的配置， 如果一个从服务器被宣告为主服务器， 那么它会代替原有的主服务器， 成为新的主服务器， 并且成为原有主服务器的所有从服务器的复制对象。 那些连接了错误主服务器的从服务器会被重新配置， 使得这些从服务器会去复制正确的主服务器。不过， 在以上这些条件满足之后， Sentinel 在对实例进行重新配置之前仍然会等待一段足够长的时间， 确保可以接收到其他 Sentinel 发来的配置更新， 从而避免自身因为保存了过期的配置而对实例进行了不必要的重新配置。 TILT 模式Redis Sentinel 严重依赖计算机的时间功能： 比如说， 为了判断一个实例是否可用， Sentinel 会记录这个实例最后一次相应 PING 命令的时间， 并将这个时间和当前时间进行对比， 从而知道这个实例有多长时间没有和 Sentinel 进行任何成功通讯。 不过， 一旦计算机的时间功能出现故障， 或者计算机非常忙碌， 又或者进程因为某些原因而被阻塞时， Sentinel 可能也会跟着出现故障。 TILT 模式是一种特殊的保护模式： 当 Sentinel 发现系统有些不对劲时， Sentinel 就会进入 TILT 模式。 因为 Sentinel 的时间中断器默认每秒执行 10 次， 所以我们预期时间中断器的两次执行之间的间隔为 100 毫秒左右。 Sentinel 的做法是， 记录上一次时间中断器执行时的时间， 并将它和这一次时间中断器执行的时间进行对比： 如果两次调用时间之间的差距为负值， 或者非常大（超过 2 秒钟）， 那么 Sentinel 进入 TILT 模式。 如果 Sentinel 已经进入 TILT 模式， 那么 Sentinel 延迟退出 TILT 模式的时间。 当 Sentinel 进入 TILT 模式时， 它仍然会继续监视所有目标， 但是： 它不再执行任何操作，比如故障转移。 当有实例向这个 Sentinel 发送 SENTINEL is-master-down-by-addr 命令时， Sentinel 返回负值： 因为这个 Sentinel 所进行的下线判断已经不再准确。 如果 TILT 可以正常维持 30 秒钟， 那么 Sentinel 退出 TILT 模式。 处理 -BUSY 状态Warning 该功能尚未实现 当 Lua 脚本的运行时间超过指定时限时， Redis 就会返回 -BUSY 错误。 当出现这种情况时， Sentinel 在尝试执行故障转移操作之前， 会先向服务器发送一个 SCRIPT KILL 命令， 如果服务器正在执行的是一个只读脚本的话， 那么这个脚本就会被杀死， 服务器就会回到正常状态。 Sentinel 的客户端实现关于 Sentinel 客户端的实现信息可以参考 Sentinel 客户端指引手册 。 集群教程本文档是 Redis 集群的入门教程， 从用户的角度介绍了设置、测试和操作集群的方法。 本教程不包含晦涩难懂的分布式概念， 也没有像 Redis 集群规范 那样包含 Redis 集群的实现细节， 如果你打算深入地学习 Redis 集群的部署方法， 那么推荐你在阅读完这个教程之后， 再去看一看集群规范。 Redis 集群目前仍处于 Alpha 测试版本， 如果在使用过程中发现任何问题， 请到 Redis 的邮件列表 发贴， 或者到 Redis 的 Github 页面 报告错误。 集群简介Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施（installation）。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处： 将数据自动切分（split）到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。 Redis 集群数据共享。Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说： 如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。 因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。 Redis 集群中的主从复制为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000 号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis 集群的一致性保证（guarantee）Redis 集群不保证数据的强一致性（strong consistency）： 在特定条件下， Redis 集群可能会丢失已经被执行过的写命令。 使用异步复制（asynchronous replication）是 Redis 集群可能会丢失写命令的其中一个原因。 考虑以下这个写命令的例子： 客户端向主节点 B 发送一条写命令。 主节点 B 执行写命令，并向客户端返回命令回复。 主节点 B 将刚刚执行的写命令复制给它的从节点 B1 、 B2 和 B3 。 如你所见， 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 Note 如果真的有必要的话， Redis 集群可能会在将来提供同步地（synchronou）执行写命令的方法。 Redis 集群另外一种可能会丢失命令的情况是， 集群出现网络分裂（network partition）， 并且一个客户端与至少包括一个主节点在内的少数（minority）实例被孤立。 举个例子， 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， 而 A1 、B1 、C1 分别为三个主节点的从节点， 另外还有一个客户端 Z1 。 假设集群中发生网络分裂， 那么集群可能会分裂为两方， 大多数（majority）的一方包含节点 A 、C 、A1 、B1 和 C1 ， 而少数（minority）的一方则包含节点 B 和客户端 Z1 。 在网络分裂期间， 主节点 B 仍然会接受 Z1 发送的写命令： 如果网络分裂出现的时间很短， 那么集群会继续正常运行； 但是， 如果网络分裂出现的时间足够长， 使得大多数一方将从节点 B1 设置为新的主节点， 并使用 B1 来代替原来的主节点 B ， 那么 Z1 发送给主节点 B 的写命令将丢失。 注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项： 对于大多数一方来说， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么集群会将这个主节点视为下线， 并使用从节点来代替这个主节点继续工作。 对于少数一方， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么它将停止处理写命令， 并向客户端报告错误。 创建并使用 Redis 集群Redis 集群由多个运行在集群模式（cluster mode）下的 Redis 实例组成， 实例的集群模式需要通过配置来开启， 开启集群模式的实例将可以使用集群特有的功能和命令。 以下是一个包含了最少选项的集群配置文件示例： 12345port 7000cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes 文件中的 cluster-enabled 选项用于开实例的集群模式， 而 cluster-conf-file 选项则设定了保存节点配置文件的路径， 默认值为 nodes.conf 。 节点配置文件无须人为修改， 它由 Redis 集群在启动时创建， 并在有需要时自动进行更新。 要让集群正常运作至少需要三个主节点， 不过在刚开始试用集群功能时， 强烈建议使用六个节点： 其中三个为主节点， 而其余三个则是各个主节点的从节点。 首先， 让我们进入一个新目录， 并创建六个以端口号为名字的子目录， 稍后我们在将每个目录中运行一个 Redis 实例： 123mkdir cluster-testcd cluster-testmkdir 7000 7001 7002 7003 7004 7005 在文件夹 7000 至 7005 中， 各创建一个 redis.conf 文件， 文件的内容可以使用上面的示例配置文件， 但记得将配置中的端口号从 7000 改为与文件夹名字相同的号码。 现在， 从 Redis Github 页面 的 unstable 分支中取出最新的 Redis 源码， 编译出可执行文件 redis-server ， 并将文件复制到 cluster-test 文件夹， 然后使用类似以下命令， 在每个标签页中打开一个实例： 12cd 7000../redis-server ./redis.conf 实例打印的日志显示， 因为 nodes.conf 文件不存在， 所以每个节点都为它自身指定了一个新的 ID ： 1[82462] 26 Nov 11:56:55.329 * No cluster configuration found, I'm 97a3a64667477371c4479320d683e4c8db5858b1 实例会一直使用同一个 ID ， 从而在集群中保持一个独一无二（unique）的名字。 每个节点都使用 ID 而不是 IP 或者端口号来记录其他节点， 因为 IP 地址和端口号都可能会改变， 而这个独一无二的标识符（identifier）则会在节点的整个生命周期中一直保持不变。 我们将这个标识符称为节点 ID。 创建集群现在我们已经有了六个正在运行中的 Redis 实例， 接下来我们需要使用这些实例来创建集群， 并为每个节点编写配置文件。 通过使用 Redis 集群命令行工具 redis-trib ， 编写节点配置文件的工作可以非常容易地完成： redis-trib 位于 Redis 源码的 src 文件夹中， 它是一个 Ruby 程序， 这个程序通过向实例发送特殊命令来完成创建新集群， 检查集群， 或者对集群进行重新分片（reshared）等工作。 我们需要执行以下命令来创建集群： 12./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 命令的意义如下： 给定 redis-trib.rb 程序的命令是 create ， 这表示我们希望创建一个新的集群。 选项 –replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。 之后跟着的其他参数则是实例的地址列表， 我们希望程序使用这些地址所指示的实例来创建新集群。 简单来说， 以上命令的意思就是让 redis-trib 程序创建一个包含三个主节点和三个从节点的集群。 接着， redis-trib 会打印出一份预想中的配置给你看， 如果你觉得没问题的话， 就可以输入 yes ， redis-trib 就会将这份配置应用到集群当中： 12345678910111213141516171819202122232425&gt;&gt;&gt; Creating clusterConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7005: OK&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002127.0.0.1:7000 replica #1 is 127.0.0.1:7003127.0.0.1:7001 replica #1 is 127.0.0.1:7004127.0.0.1:7002 replica #1 is 127.0.0.1:7005M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5460 (5461 slots) masterM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5461-10921 (5461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:10922-16383 (5462 slots) masterS: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003S: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004S: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005Can I set the above configuration? (type 'yes' to accept): yes 输入 yes 并按下回车确认之后， 集群就会将配置应用到各个节点， 并连接起（join）各个节点 —— 也即是， 让各个节点开始互相通讯： 1234567891011121314151617&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5460 (5461 slots) masterM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5461-10921 (5461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:10922-16383 (5462 slots) masterM: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003slots: (0 slots) masterM: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004slots: (0 slots) masterM: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005slots: (0 slots) master[OK] All nodes agree about slots configuration. 如果一切正常的话， redis-trib 将输出以下信息： 123&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这表示集群中的 16384 个槽都有至少一个主节点在处理， 集群运作正常。 集群的客户端Redis 集群现阶段的一个问题是客户端实现很少。 以下是一些我知道的实现： redis-rb-cluster 是我（@antirez）编写的 Ruby 实现， 用于作为其他实现的参考。 该实现是对 redis-rb 的一个简单包装， 高效地实现了与集群进行通讯所需的最少语义（semantic）。 redis-py-cluster 看上去是 redis-rb-cluster 的一个 Python 版本， 这个项目有一段时间没有更新了（最后一次提交是在六个月之前）， 不过可以将这个项目用作学习集群的起点。 流行的 Predis 曾经对早期的 Redis 集群有过一定的支持， 但我不确定它对集群的支持是否完整， 也不清楚它是否和最新版本的 Redis 集群兼容 （因为新版的 Redis 集群将槽的数量从 4k 改为 16k 了）。 Redis unstable 分支中的 redis-cli 程序实现了非常基本的集群支持， 可以使用命令 redis-cli -c 来启动。 测试 Redis 集群比较简单的办法就是使用 redis-rb-cluster 或者 redis-cli ， 接下来我们将使用 redis-cli 为例来进行演示： 12345678910111213141516$ redis-cli -c -p 7000redis 127.0.0.1:7000&gt; set foo bar-&gt; Redirected to slot [12182] located at 127.0.0.1:7002OKredis 127.0.0.1:7002&gt; set hello world-&gt; Redirected to slot [866] located at 127.0.0.1:7000OKredis 127.0.0.1:7000&gt; get foo-&gt; Redirected to slot [12182] located at 127.0.0.1:7002"bar"redis 127.0.0.1:7000&gt; get hello-&gt; Redirected to slot [866] located at 127.0.0.1:7000"world" redis-cli 对集群的支持是非常基本的， 所以它总是依靠 Redis 集群节点来将它转向（redirect）至正确的节点。 一个真正的（serious）集群客户端应该做得比这更好： 它应该用缓存记录起哈希槽与节点地址之间的映射（map）， 从而直接将命令发送到正确的节点上面。 这种映射只会在集群的配置出现某些修改时变化， 比如说， 在一次故障转移（failover）之后， 或者系统管理员通过添加节点或移除节点来修改了集群的布局（layout）之后， 诸如此类。 使用 redis-rb-cluster 编写一个示例应用在展示如何使用集群进行故障转移、重新分片等操作之前， 我们需要创建一个示例应用， 了解一些与 Redis 集群客户端进行交互的基本方法。 在运行示例应用的过程中， 我们会尝试让节点进入失效状态， 又或者开始一次重新分片， 以此来观察 Redis 集群在真实世界运行时的表现， 并且为了让这个示例尽可能地有用， 我们会让这个应用向集群进行写操作。 本节将通过两个示例应用来展示 redis-rb-cluster 的基本用法， 以下是本节的第一个示例应用， 它是一个名为 example.rb 的文件， 包含在redis-rb-cluster 项目里面： 123456789101112131415161718192021222324252627282930require './cluster'startup_nodes = [ &#123;:host =&gt; "127.0.0.1", :port =&gt; 7000&#125;, &#123;:host =&gt; "127.0.0.1", :port =&gt; 7001&#125;]rc = RedisCluster.new(startup_nodes,32,:timeout =&gt; 0.1)last = falsewhile not last begin last = rc.get("__last__") last = 0 if !last rescue =&gt; e puts "error #&#123;e.to_s&#125;" sleep 1 endend((last.to_i+1)..1000000000).each&#123;|x| begin rc.set("foo#&#123;x&#125;",x) puts rc.get("foo#&#123;x&#125;") rc.set("__last__",x) rescue =&gt; e puts "error #&#123;e.to_s&#125;" end sleep 0.1&#125; 这个应用所做的工作非常简单： 它不断地以 foo 为键， number 为值， 使用 SET key value [EX seconds] [PX milliseconds] [NX|XX] 命令向数据库设置键值对。 如果我们执行这个应用的话， 应用将按顺序执行以下命令： SET foo0 0 SET foo1 1 SET foo2 2 诸如此类。。。 代码中的每个集群操作都使用一个 begin 和 rescue 代码块（block）包裹着， 因为我们希望在代码出错时， 将错误打印到终端上面， 而不希望应用因为异常（exception）而退出。 代码的第七行是代码中第一个有趣的地方， 它创建了一个 Redis 集群对象， 其中创建对象所使用的参数及其意义如下： 第一个参数是记录了启动节点的 startup_nodes 列表， 列表中包含了两个集群节点的地址。 第二个参数指定了对于集群中的各个不同的节点， Redis 集群对象可以获得（take）的最大连接数 （maximum number of connections this object is allowed to take）。 第三个参数 timeout 指定了一个命令在执行多久之后， 才会被看作是执行失败。 记住， 启动列表中并不需要包含所有集群节点的地址， 但这些地址中至少要有一个是有效的（reachable）： 一旦 redis-rb-cluster 成功连接上集群中的某个节点时， 集群节点列表就会被自动更新， 任何真正的（serious）的集群客户端都应该这样做。 现在， 程序创建的 Redis 集群对象实例被保存到 rc 变量里面， 我们可以将这个对象当作普通 Redis 对象实例来使用。 在十一至十九行， 我们先尝试阅读计数器中的值， 如果计数器不存在的话， 我们才将计数器初始化为 0 ： 通过将计数值保存到 Redis 的计数器里面， 我们可以在示例重启之后， 仍然继续之前的执行过程， 而不必每次重启之后都从 foo0 开始重新设置键值对。 为了让程序在集群下线的情况下， 仍然不断地尝试读取计数器的值， 我们将读取操作包含在了一个 while 循环里面， 一般的应用程序并不需要如此小心。 二十一至三十行是程序的主循环， 这个循环负责设置键值对， 并在设置出错时打印错误信息。 程序在主循环的末尾添加了一个 sleep 调用， 让写操作的执行速度变慢， 帮助执行示例的人更容易看清程序的输出。 执行 example.rb 程序将产生以下输出： 1234567891011ruby ./example.rb123456789... 这个程序并不是十分有趣， 稍后我们就会看到一个更有趣的集群应用示例， 不过在此之前， 让我们先使用这个示例来演示集群的重新分片操作。 对集群进行重新分片现在， 让我们来试试对集群进行重新分片操作。 在执行重新分片的过程中， 请让你的 example.rb 程序处于运行状态， 这样你就会看到， 重新分片并不会对正在运行的集群程序产生任何影响， 你也可以考虑将 example.rb 中的 sleep 调用删掉， 从而让重新分片操作在近乎真实的写负载下执行。 重新分片操作基本上就是将某些节点上的哈希槽移动到另外一些节点上面， 和创建集群一样， 重新分片也可以使用 redis-trib 程序来执行。 执行以下命令可以开始一次重新分片操作： 1$ ./redis-trib.rb reshard 127.0.0.1:7000 你只需要指定集群中其中一个节点的地址， redis-trib 就会自动找到集群中的其他节点。 目前 redis-trib 只能在管理员的协助下完成重新分片的工作， 要让 redis-trib 自动将哈希槽从一个节点移动到另一个节点， 目前来说还做不到 （不过实现这个功能并不难）。 执行 redis-trib 的第一步就是设定你打算移动的哈希槽的数量： 12345678910111213141516171819202122232425$ ./redis-trib.rb reshard 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7004: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5460 (5461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:10922-16383 (5462 slots) masterS: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005slots: (0 slots) slaveM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5461-10921 (5461 slots) masterS: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003slots: (0 slots) slaveS: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004slots: (0 slots) slave[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 1000 我们将打算移动的槽数量设置为 1000 个， 如果 example.rb 程序一直运行着的话， 现在 1000 个槽里面应该有不少键了。 除了移动的哈希槽数量之外， redis-trib 还需要知道重新分片的目标（target node）， 也即是， 负责接收这 1000 个哈希槽的节点。 指定目标需要使用节点的 ID ， 而不是 IP 地址和端口。 比如说， 我们打算使用集群的第一个主节点来作为目标， 它的 IP 地址和端口是 127.0.0.1:7000 ， 而节点 ID 则是 9991306f0e50640a5684f1958fd754b38fa034c9 ， 那么我们应该向 redis-trib 提供节点的 ID ： 123$ ./redis-trib.rb reshard 127.0.0.1:7000...What is the receiving node ID? 9991306f0e50640a5684f1958fd754b38fa034c9 Note redis-trib 会打印出集群中所有节点的 ID ， 并且我们也可以通过执行以下命令来获得节点的运行 ID ： 12$ ./redis-cli -p 7000 cluster nodes | grep myself9991306f0e50640a5684f1958fd754b38fa034c9 :0 myself,master - 0 0 0 connected 0-5460 接着， redis-trib 会向你询问重新分片的源节点（source node）， 也即是， 要从哪个节点中取出 1000 个哈希槽， 并将这些槽移动到目标节点上面。 如果我们不打算从特定的节点上取出指定数量的哈希槽， 那么可以向 redis-trib 输入 all ， 这样的话， 集群中的所有主节点都会成为源节点， redis-trib 将从各个源节点中各取出一部分哈希槽， 凑够 1000 个， 然后移动到目标节点上面： 123456$ ./redis-trib.rb reshard 127.0.0.1:7000...Please enter all the source node IDs.Type 'all' to use all the nodes as source nodes for the hash slots.Type 'done' once you entered all the source nodes IDs.Source node #1:all 输入 all 并按下回车之后， redis-trib 将打印出哈希槽的移动计划， 如果你觉得没问题的话， 就可以输入 yes 并再次按下回车： 123456789$ ./redis-trib.rb reshard 127.0.0.1:7000...Moving slot 11421 from 393c6df5eb4b4cec323f0e4ca961c8b256e3460aMoving slot 11422 from 393c6df5eb4b4cec323f0e4ca961c8b256e3460aMoving slot 5461 from e68e52cee0550f558b03b342f2f0354d2b8a083bMoving slot 5469 from e68e52cee0550f558b03b342f2f0354d2b8a083b...Moving slot 5959 from e68e52cee0550f558b03b342f2f0354d2b8a083bDo you want to proceed with the proposed reshard plan (yes/no)? yes 输入 yes 并使用按下回车之后， redis-trib 就会正式开始执行重新分片操作， 将指定的哈希槽从源节点一个个地移动到目标节点上面： 12345678$ ./redis-trib.rb reshard 127.0.0.1:7000...Moving slot 5934 from 127.0.0.1:7001 to 127.0.0.1:7000:Moving slot 5935 from 127.0.0.1:7001 to 127.0.0.1:7000:Moving slot 5936 from 127.0.0.1:7001 to 127.0.0.1:7000:Moving slot 5937 from 127.0.0.1:7001 to 127.0.0.1:7000:...Moving slot 5959 from 127.0.0.1:7001 to 127.0.0.1:7000: 在重新分片的过程中， example.rb 应该可以继续正常运行， 不会出现任何问题。 在重新分片操作执行完毕之后， 可以使用以下命令来检查集群是否正常： 123456789101112131415161718192021222324$ ./redis-trib.rb check 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7004: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5959,10922-11422 (6461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:11423-16383 (4961 slots) masterS: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005slots: (0 slots) slaveM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5960-10921 (4962 slots) masterS: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003slots: (0 slots) slaveS: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004slots: (0 slots) slave[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 根据检查结果显示， 集群运作正常。 需要注意的就是， 在三个主节点中， 节点 127.0.0.1:7000 包含了 6461 个哈希槽， 而节点 127.0.0.1:7001 和节点 127.0.0.1:7002 都只包含了 4961 个哈希槽， 因为后两者都将自己的 500 个哈希槽移动到了节点 127.0.0.1:7000 。 一个更有趣的示例应用我们在前面使用的示例程序 example.rb 并不是十分有趣， 因为它只是不断地对集群进行写入， 但并不检查写入结果是否正确。 比如说， 集群可能会错误地将 example.rb 发送的所有 SET key value [EX seconds] [PX milliseconds] [NX|XX] 命令都改成了 SET foo 42 ， 但因为 example.rb 并不检查写入后的值， 所以它不会意识到集群实际上写入的值是错误的。 因为这个原因， redis-rb-cluster 项目包含了一个名为 consistency-test.rb 的示例应用， 这个应用比起 example.rb 有趣得多： 它创建了多个计数器（默认为 1000 个）， 并通过发送 INCR key 命令来增加这些计数器的值。 在增加计数器值的同时， consistency-test.rb 还执行以下操作： 每次使用 INCR key 命令更新一个计数器时， 应用会记录下计数器执行 INCR key 命令之后应该有的值。 举个例子， 如果计数器的起始值为 0 ， 而这次是程序第 50 次向它发送 INCR key 命令， 那么计数器的值应该是 50 。 在每次发送 INCR key 命令之前， 程序会随机从集群中读取一个计数器的值， 并将它与自己记录的值进行对比， 看两个值是否相同。 换句话说， 这个程序是一个一致性检查器（consistency checker）： 如果集群在执行 INCR key 命令的过程中， 丢失了某条 INCR key 命令， 又或者多执行了某条客户端没有确认到的 INCR key 命令， 那么检查器将察觉到这一点 —— 在前一种情况中， consistency-test.rb 记录的计数器值将比集群记录的计数器值要大； 而在后一种情况中， consistency-test.rb 记录的计数器值将比集群记录的计数器值要小。 运行 consistency-test 程序将产生类似以下的输出： 12345678$ ruby consistency-test.rb925 R (0 err) | 925 W (0 err) |5030 R (0 err) | 5030 W (0 err) |9261 R (0 err) | 9261 W (0 err) |13517 R (0 err) | 13517 W (0 err) |17780 R (0 err) | 17780 W (0 err) |22025 R (0 err) | 22025 W (0 err) |25818 R (0 err) | 25818 W (0 err) | 每行输出都打印了程序执行的读取次数和写入次数， 以及执行操作的过程中因为集群不可用而产生的错误数。 如果程序察觉了不一致的情况出现， 它将在输出行的末尾显式不一致的详细情况。 比如说， 如果我们在 consistency-test.rb 运行的过程中， 手动修改某个计数器的值： 12$ redis 127.0.0.1:7000&gt; set key_217 0OK 那么 consistency-test.rb 将向我们报告不一致情况： 123456(in the other tab I see...)94774 R (0 err) | 94774 W (0 err) |98821 R (0 err) | 98821 W (0 err) |102886 R (0 err) | 102886 W (0 err) | 114 lost |107046 R (0 err) | 107046 W (0 err) | 114 lost | 在我们修改计数器值的时候， 计数器的正确值是 114 （执行了 114 次 INCR key 命令）， 因为我们将计数器的值设成了 0 ， 所以 consistency-test.rb 会向我们报告说丢失了 114 个 INCR key 命令。 因为这个示例程序具有一致性检查功能， 所以我们用它来测试 Redis 集群的故障转移操作。 故障转移测试Note 在执行本节操作的过程中， 请一直运行 consistency-test 程序。 要触发一次故障转移， 最简单的办法就是令集群中的某个主节点进入下线状态。 首先用以下命令列出集群中的所有主节点： 1234$ redis-cli -p 7000 cluster nodes | grep master3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385482984082 0 connected 5960-109212938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 master - 0 1385482983582 0 connected 11423-1638397a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422 通过命令输出， 我们知道端口号为 7000 、 7001 和 7002 的节点都是主节点， 然后我们可以通过向端口号为 7002 的主节点发送 DEBUG SEGFAULT 命令， 让这个主节点崩溃： 12$ redis-cli -p 7002 debug segfaultError: Server closed the connection 现在， 切换到运行着 consistency-test 的标签页， 可以看到， consistency-test 在 7002 下线之后的一段时间里将产生大量的错误警告信息： 1234567891018849 R (0 err) | 18849 W (0 err) |23151 R (0 err) | 23151 W (0 err) |27302 R (0 err) | 27302 W (0 err) |... many error warnings here ...29659 R (578 err) | 29660 W (577 err) |33749 R (578 err) | 33750 W (577 err) |37918 R (578 err) | 37919 W (577 err) |42077 R (578 err) | 42078 W (577 err) | 从 consistency-test 的这段输出可以看到， 集群在执行故障转移期间， 总共丢失了 578 个读命令和 577 个写命令， 但是并没有产生任何数据不一致。 这听上去可能有点奇怪， 因为在教程的开头我们提到过， Redis 使用的是异步复制， 在执行故障转移期间， 集群可能会丢失写命令。 但是在实际上， 丢失命令的情况并不常见， 因为 Redis 几乎是同时执行将命令回复发送给客户端， 以及将命令复制给从节点这两个操作， 所以实际上造成命令丢失的时间窗口是非常小的。 不过， 尽管出现的几率不高， 但丢失命令的情况还是有可能会出现的， 所以我们对 Redis 集群不能提供强一致性的这一描述仍然是正确的。 现在， 让我们使用 cluster nodes 命令， 查看集群在执行故障转移操作之后， 主从节点的布局情况： 1234567$ redis-cli -p 7000 cluster nodes3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385503418521 0 connecteda211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385503419023 0 connected97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-114223c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385503419023 3 connected 11423-163833e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385503417005 0 connected 5960-109212938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385503418016 3 connected 我重启了之前下线的 127.0.0.1:7002 节点， 该节点已经从原来的主节点变成了从节点， 而现在集群中的三个主节点分别是 127.0.0.1:7000 、 127.0.0.1:7001 和 127.0.0.1:7005 ， 其中 127.0.0.1:7005 就是因为 127.0.0.1:7002 下线而变成主节点的。 cluster nodes 命令的输出有点儿复杂， 它的每一行都是由以下信息组成的： 节点 ID ：例如 3fc783611028b1707fd65345e763befb36454d73 。 ip:port ：节点的 IP 地址和端口号， 例如 127.0.0.1:7000 ， 其中 :0 表示的是客户端当前连接的 IP 地址和端口号。 flags ：节点的角色（例如 master 、 slave 、 myself ）以及状态（例如 fail ，等等）。 如果节点是一个从节点的话， 那么跟在 flags 之后的将是主节点的节点 ID ： 例如 127.0.0.1:7002 的主节点的节点 ID 就是 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 。 集群最近一次向节点发送 PING 命令之后， 过去了多长时间还没接到回复。 节点最近一次返回 PONG 回复的时间。 节点的配置纪元（configuration epoch）：详细信息请参考 Redis 集群规范 。 本节点的网络连接情况：例如 connected 。 节点目前包含的槽：例如 127.0.0.1:7001 目前包含号码为 5960 至 10921 的哈希槽。 添加新节点到集群根据新添加节点的种类， 我们需要用两种方法来将新节点添加到集群里面： 如果要添加的新节点是一个主节点， 那么我们需要创建一个空节点（empty node）， 然后将某些哈希桶移动到这个空节点里面。 另一方面， 如果要添加的新节点是一个从节点， 那么我们需要将这个新节点设置为集群中某个节点的复制品（replica）。 本节将对以上两种情况进行介绍， 首先介绍主节点的添加方法， 然后再介绍从节点的添加方法。 无论添加的是那种节点， 第一步要做的总是添加一个空节点。 我们可以继续使用之前启动 127.0.0.1:7000 、 127.0.0.1:7001 等节点的方法， 创建一个端口号为 7006 的新节点， 使用的配置文件也和之前一样， 只是记得要将配置中的端口号改为 7000 。 以下是启动端口号为 7006 的新节点的详细步骤： 在终端里创建一个新的标签页。 进入 cluster-test 文件夹。 创建并进入 7006 文件夹。 将 redis.conf 文件复制到 7006 文件夹里面，然后将配置中的端口号选项改为 7006 。 使用命令 ../../redis-server redis.conf 启动节点。 如果一切正常， 那么节点应该会正确地启动。 接下来， 执行以下命令， 将这个新节点添加到集群里面： 1./redis-trib.rb add-node 127.0.0.1:7006 127.0.0.1:7000 命令中的 add-node 表示我们要让 redis-trib 将一个节点添加到集群里面， add-node 之后跟着的是新节点的 IP 地址和端口号， 再之后跟着的是集群中任意一个已存在节点的 IP 地址和端口号， 这里我们使用的是 127.0.0.1:7000 。 通过 cluster nodes 命令， 我们可以确认新节点 127.0.0.1:7006 已经被添加到集群里面了： 12345678redis 127.0.0.1:7006&gt; cluster nodes3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385543178575 0 connected 5960-109213fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385543179583 0 connectedf093c80dde814da99c5cf72a7dd01590792b783b :0 myself,master - 0 0 0 connected2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543178072 3 connecteda211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385543178575 0 connected97a3a64667477371c4479320d683e4c8db5858b1 127.0.0.1:7000 master - 0 1385543179080 0 connected 0-5959 10922-114223c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385543177568 3 connected 11423-16383 新节点现在已经连接上了集群， 成为集群的一份子， 并且可以对客户端的命令请求进行转向了， 但是和其他主节点相比， 新节点还有两点区别： 新节点没有包含任何数据， 因为它没有包含任何哈希桶。 尽管新节点没有包含任何哈希桶， 但它仍然是一个主节点， 所以在集群需要将某个从节点升级为新的主节点时， 这个新节点不会被选中。 接下来， 只要使用 redis-trib 程序， 将集群中的某些哈希桶移动到新节点里面， 新节点就会成为真正的主节点了。 因为使用 redis-trib 移动哈希桶的方法在前面已经介绍过， 所以这里就不再重复介绍了。 现在， 让我们来看看， 将一个新节点转变为某个主节点的复制品（也即是从节点）的方法。 举个例子， 如果我们打算让新节点成为 127.0.0.1:7005 的从节点， 那么我们只要用客户端连接上新节点， 然后执行以下命令就可以了： 1redis 127.0.0.1:7006&gt; cluster replicate 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 其中命令提供的 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 就是主节点 127.0.0.1:7005 的节点 ID 。 执行 cluster replicate 命令之后， 我们可以使用以下命令来确认 127.0.0.1:7006 已经成为了 ID 为 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 的节点的从节点： 123$ redis-cli -p 7000 cluster nodes | grep slave | grep 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912ef093c80dde814da99c5cf72a7dd01590792b783b 127.0.0.1:7006 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617702 3 connected2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617198 3 connected 3c3a0c… 现在有两个从节点， 一个从节点的端口号为 7002 ， 而另一个从节点的端口号为 7006 。 移除一个节点未完待续。 键空间通知（keyspace notification）功能概览键空间通知使得客户端可以通过订阅频道或模式， 来接收那些以某种方式改动了 Redis 数据集的事件。 以下是一些键空间通知发送的事件的例子： 所有修改键的命令。 所有接收到 LPUSH key value [value …] 命令的键。 0 号数据库中所有已过期的键。 事件通过 Redis 的订阅与发布功能（pub/sub）来进行分发， 因此所有支持订阅与发布功能的客户端都可以在无须做任何修改的情况下， 直接使用键空间通知功能。 因为 Redis 目前的订阅与发布功能采取的是发送即忘（fire and forget）策略， 所以如果你的程序需要可靠事件通知（reliable notification of events）， 那么目前的键空间通知可能并不适合你： 当订阅事件的客户端断线时， 它会丢失所有在断线期间分发给它的事件。 未来将会支持更可靠的事件分发， 这种支持可能会通过让订阅与发布功能本身变得更可靠来实现， 也可能会在 Lua 脚本中对消息（message）的订阅与发布进行监听， 从而实现类似将事件推入到列表这样的操作。 事件的类型对于每个修改数据库的操作，键空间通知都会发送两种不同类型的事件。 比如说，对 0 号数据库的键 mykey 执行 DEL key [key …] 命令时， 系统将分发两条消息， 相当于执行以下两个 PUBLISH channel message 命令：12PUBLISH __keyspace@0__:mykey delPUBLISH __keyevent@0__:del mykey 订阅第一个频道 __keyspace@0__:mykey 可以接收 0 号数据库中所有修改键 mykey 的事件， 而订阅第二个频道 __keyevent@0__:del 则可以接收 0 号数据库中所有执行 del 命令的键。 以 keyspace 为前缀的频道被称为键空间通知（key-space notification）， 而以 keyevent 为前缀的频道则被称为键事件通知（key-event notification）。 当 del mykey 命令执行时： 键空间频道的订阅者将接收到被执行的事件的名字，在这个例子中，就是 del 。 键事件频道的订阅者将接收到被执行事件的键的名字，在这个例子中，就是 mykey 。 配置因为开启键空间通知功能需要消耗一些 CPU ， 所以在默认配置下， 该功能处于关闭状态。 可以通过修改 redis.conf 文件， 或者直接使用 CONFIG SET 命令来开启或关闭键空间通知功能： 当 notify-keyspace-events 选项的参数为空字符串时，功能关闭。 另一方面，当参数不是空字符串时，功能开启。 notify-keyspace-events 的参数可以是以下字符的任意组合， 它指定了服务器该发送哪些类型的通知： 字符 发送的通知 K 键空间通知，所有通知以 __keyspace@\&lt;db>__ 为前缀 E 键事件通知，所有通知以 __keyevent@\&lt;db>__ 为前缀 g DEL 、 EXPIRE 、 RENAME 等类型无关的通用命令的通知 $ 字符串命令的通知 l 列表命令的通知 s 集合命令的通知 h 哈希命令的通知 z 有序集合命令的通知 x 过期事件：每当有过期键被删除时发送 e 驱逐(evict)事件：每当有键因为 maxmemory 政策而被删除时发送 A 参数 g$lshzxe 的别名 输入的参数中至少要有一个 K 或者 E ， 否则的话， 不管其余的参数是什么， 都不会有任何通知被分发。 举个例子， 如果只想订阅键空间中和列表相关的通知， 那么参数就应该设为 Kl ， 诸如此类。 将参数设为字符串 “AKE” 表示发送所有类型的通知。 命令产生的通知以下列表记录了不同命令所产生的不同通知： DEL key [key …] 命令为每个被删除的键产生一个 del 通知。 RENAME key newkey 产生两个通知：为来源键（source key）产生一个 rename_from 通知，并为目标键（destination key）产生一个 rename_to 通知。 EXPIRE key seconds 和 EXPIREAT key timestamp 在键被正确设置过期时间时产生一个 expire 通知。当 EXPIREAT key timestamp 设置的时间已经过期，或者 EXPIRE key seconds 传入的时间为负数值时，键被删除，并产生一个 del 通知。 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] 在命令带有 STORE 参数时产生一个 sortstore 事件。如果 STORE 指示的用于保存排序结果的键已经存在，那么程序还会发送一个 del 事件。 SET key value [EX seconds] [PX milliseconds] [NX|XX] 以及它的所有变种（SETEX key seconds value 、 SETNX key value 和 GETSET key value）都产生 set 通知。其中 SETEX key seconds value 还会产生 expire 通知。 MSET key value [key value …] 为每个键产生一个 set 通知。 SETRANGE key offset value 产生一个 setrange 通知。 INCR key 、 DECR key 、 INCRBY key increment 和 DECRBY key decrement 都产生 incrby 通知。 INCRBYFLOAT key increment 产生 incrbyfloat 通知。 APPEND key value 产生 append 通知。 LPUSH key value [value …] 和 LPUSHX key value 都产生单个 lpush 通知，即使有多个输入元素时，也是如此。 RPUSH key value [value …] 和 RPUSHX key value 都产生单个 rpush 通知，即使有多个输入元素时，也是如此。 RPOP key 产生 rpop 通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个 del 通知。 LPOP key 产生 lpop 通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个 del 通知。 LINSERT key BEFORE|AFTER pivot value 产生一个 linsert 通知。 LSET key index value 产生一个 lset 通知。 LTRIM key start stop 产生一个 ltrim 通知。如果 LTRIM key start stop 执行之后，列表键被清空，那么还会产生一个 del 通知。 RPOPLPUSH source destination 和 BRPOPLPUSH source destination timeout 产生一个 rpop 通知，以及一个 lpush 通知。两个命令都会保证 rpop 的通知在 lpush 的通知之前分发。如果从键弹出元素之后，被弹出的列表键被清空，那么还会产生一个 del 通知。 HSET hash field value 、 HSETNX hash field value 和 HMSET 都只产生一个 hset 通知。 HINCRBY 产生一个 hincrby 通知。 HINCRBYFLOAT 产生一个 hincrbyfloat 通知。 HDEL 产生一个 hdel 通知。如果执行 HDEL 之后，哈希键被清空，那么还会产生一个 del 通知。 SADD key member [member …] 产生一个 sadd 通知，即使有多个输入元素时，也是如此。 SREM key member [member …] 产生一个 srem 通知，如果执行 SREM key member [member …] 之后，集合键被清空，那么还会产生一个 del 通知。 SMOVE source destination member 为来源键（source key）产生一个 srem 通知，并为目标键（destination key）产生一个 sadd 事件。 SPOP key 产生一个 spop 事件。如果执行 SPOP key 之后，集合键被清空，那么还会产生一个 del 通知。 SINTERSTORE destination key [key …] 、 SUNIONSTORE destination key [key …] 和 SDIFFSTORE destination key [key …] 分别产生 sinterstore 、 sunionostore 和 sdiffstore 三种通知。如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 ZINCRBY key increment member 产生一个 zincr 通知。（译注：非对称，请注意。） ZADD key score member [[score member] [score member] …] 产生一个 zadd 通知，即使有多个输入元素时，也是如此。 ZREM key member [member …] 产生一个 zrem 通知，即使有多个输入元素时，也是如此。如果执行 ZREM key member [member …] 之后，有序集合键被清空，那么还会产生一个 del 通知。 ZREMRANGEBYSCORE key min max 产生一个 zrembyscore 通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 ZREMRANGEBYRANK key start stop 产生一个 zrembyrank 通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 ZINTERSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 和 ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 分别产生 zinterstore 和 zunionstore 两种通知。如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 每当一个键因为过期而被删除时，产生一个 expired 通知。 每当一个键因为 maxmemory 政策而被删除以回收内存时，产生一个 evicted 通知。 Note 所有命令都只在键真的被改动了之后，才会产生通知。 比如说，当 SREM key member [member …] 试图删除不存在于集合的元素时，删除操作会执行失败，因为没有真正的改动键，所以这一操作不会发送通知。 如果对命令所产生的通知有疑问， 最好还是使用以下命令， 自己来验证一下：1234$ redis-cli config set notify-keyspace-events KEA$ redis-cli --csv psubscribe '__key*__:*'Reading messages... (press Ctrl-C to quit)"psubscribe","__key*__:*",1 然后， 只要在其他终端里用 Redis 客户端发送命令， 就可以看到产生的通知了：123"pmessage","__key*__:*","__keyspace@0__:foo","set""pmessage","__key*__:*","__keyevent@0__:set","foo"... 过期通知的发送时间Redis 使用以下两种方式删除过期的键： 当一个键被访问时，程序会对这个键进行检查，如果键已经过期，那么该键将被删除。 底层系统会在后台渐进地查找并删除那些过期的键，从而处理那些已经过期、但是不会被访问到的键。 当过期键被以上两个程序的任意一个发现、 并且将键从数据库中删除时， Redis 会产生一个 expired 通知。 Redis 并不保证生存时间（TTL）变为 0 的键会立即被删除： 如果程序没有访问这个过期键， 或者带有生存时间的键非常多的话， 那么在键的生存时间变为 0 ， 直到键真正被删除这中间， 可能会有一段比较显著的时间间隔。 因此， Redis 产生 expired 通知的时间为过期键被删除的时候， 而不是键的生存时间变为 0 的时候。 通信协议（protocol）Redis 协议在以下三个目标之间进行折中： 易于实现 可以高效地被计算机分析（parse） 可以很容易地被人类读懂 网络层客户端和服务器通过 TCP 连接来进行数据交互， 服务器默认的端口号为 6379 。 客户端和服务器发送的命令或数据一律以 \r\n （CRLF）结尾。 请求Redis 服务器接受命令以及命令的参数。 服务器会在接到命令之后，对命令进行处理，并将命令的回复传送回客户端。 新版统一请求协议新版统一请求协议在 Redis 1.2 版本中引入， 并最终在 Redis 2.0 版本成为 Redis 服务器通信的标准方式。 你的 Redis 客户端应该按照这个新版协议来进行实现。 在这个协议中， 所有发送至 Redis 服务器的参数都是二进制安全（binary safe）的。 以下是这个协议的一般形式：123456*&lt;参数数量&gt; CR LF$&lt;参数 1 的字节数量&gt; CR LF&lt;参数 1 的数据&gt; CR LF...$&lt;参数 N 的字节数量&gt; CR LF&lt;参数 N 的数据&gt; CR LF Note 译注：命令本身也作为协议的其中一个参数来发送。 举个例子， 以下是一个命令协议的打印版本：1234567*3$3SET$5mykey$7myvalue 这个命令的实际协议值如下：1"*3\r\n$3\r\nSET\r\n$5\r\nmykey\r\n$7\r\nmyvalue\r\n" 稍后我们会看到， 这种格式除了用作命令请求协议之外， 也用在命令的回复协议中： 这种只有一个参数的回复格式被称为批量回复（Bulk Reply）。 统一协议请求原本是用在回复协议中， 用于将列表的多个项返回给客户端的， 这种回复格式被称为多条批量回复（Multi Bulk Reply）。 一个多条批量回复以 *\&lt;argc>\r\n 为前缀， 后跟多条不同的批量回复， 其中 argc 为这些批量回复的数量。 回复Redis 命令会返回多种不同类型的回复。 通过检查服务器发回数据的第一个字节， 可以确定这个回复是什么类型： 状态回复（status reply）的第一个字节是 “+” 错误回复（error reply）的第一个字节是 “-“ 整数回复（integer reply）的第一个字节是 “:” 批量回复（bulk reply）的第一个字节是 “$” 多条批量回复（multi bulk reply）的第一个字节是 “*” 状态回复一个状态回复（或者单行回复，single line reply）是一段以 “+” 开始、 “\r\n” 结尾的单行字符串。 以下是一个状态回复的例子：1+OK 客户端库应该返回 “+” 号之后的所有内容。 比如在在上面的这个例子中， 客户端就应该返回字符串 “OK” 。 状态回复通常由那些不需要返回数据的命令返回，这种回复不是二进制安全的，它也不能包含新行。 状态回复的额外开销非常少，只需要三个字节（开头的 “+” 和结尾的 CRLF）。 错误回复错误回复和状态回复非常相似， 它们之间的唯一区别是， 错误回复的第一个字节是 “-“ ， 而状态回复的第一个字节是 “+” 。 错误回复只在某些地方出现问题时发送： 比如说， 当用户对不正确的数据类型执行命令， 或者执行一个不存在的命令， 等等。 一个客户端库应该在收到错误回复时产生一个异常。 以下是两个错误回复的例子：12-ERR unknown command 'foobar'-WRONGTYPE Operation against a key holding the wrong kind of value 在 “-“ 之后，直到遇到第一个空格或新行为止，这中间的内容表示所返回错误的类型。 ERR 是一个通用错误，而 WRONGTYPE 则是一个更特定的错误。 一个客户端实现可以为不同类型的错误产生不同类型的异常， 或者提供一种通用的方式， 让调用者可以通过提供字符串形式的错误名来捕捉（trap）不同的错误。 不过这些特性用得并不多， 所以并不是特别重要， 一个受限的（limited）客户端可以通过简单地返回一个逻辑假（false）来表示一个通用的错误条件。 整数回复整数回复就是一个以 “:” 开头， CRLF 结尾的字符串表示的整数。 比如说， “:0\r\n” 和 “:1000\r\n” 都是整数回复。 返回整数回复的其中两个命令是 INCR key 和 LASTSAVE 。 被返回的整数没有什么特殊的含义， INCR key 返回键的一个自增后的整数值， 而 LASTSAVE 则返回一个 UNIX 时间戳， 返回值的唯一限制是这些数必须能够用 64 位有符号整数表示。 整数回复也被广泛地用于表示逻辑真和逻辑假： 比如 EXISTS key 和 SISMEMBER key member 都用返回值 1 表示真， 0 表示假。 其他一些命令， 比如 SADD key member [member …] 、 SREM key member [member …] 和 SETNX key value ， 只在操作真正被执行了的时候， 才返回 1 ， 否则返回 0 。 以下命令都返回整数回复： SETNX key value 、 DEL key [key …] 、 EXISTS key 、 INCR key 、 INCRBY key increment 、 DECR key 、 DECRBY key decrement 、 DBSIZE 、 LASTSAVE 、 RENAMENX key newkey 、 MOVE key db 、 LLEN key 、 SADD key member [member …] 、 SREM key member [member …] 、 SISMEMBER key member 、 SCARD key 。 批量回复服务器使用批量回复来返回二进制安全的字符串，字符串的最大长度为 512 MB 。12客户端：GET mykey服务器：foobar 服务器发送的内容中： 第一字节为 “$” 符号 接下来跟着的是表示实际回复长度的数字值 之后跟着一个 CRLF 再后面跟着的是实际回复数据 最末尾是另一个 CRLF 对于前面的 GET key 命令，服务器实际发送的内容为：1"$6\r\nfoobar\r\n" 如果被请求的值不存在， 那么批量回复会将特殊值 -1 用作回复的长度值， 就像这样：12客户端：GET non-existing-key服务器：$-1 这种回复称为空批量回复（NULL Bulk Reply）。 当请求对象不存在时，客户端应该返回空对象，而不是空字符串： 比如 Ruby 库应该返回 nil ， 而 C 库应该返回 NULL （或者在回复对象中设置一个特殊标志）， 诸如此类。 多条批量回复像 LRANGE key start stop 这样的命令需要返回多个值， 这一目标可以通过多条批量回复来完成。 多条批量回复是由多个回复组成的数组， 数组中的每个元素都可以是任意类型的回复， 包括多条批量回复本身。 多条批量回复的第一个字节为 “*” ， 后跟一个字符串表示的整数值， 这个值记录了多条批量回复所包含的回复数量， 再后面是一个 CRLF 。12345678910客户端： LRANGE mylist 0 3服务器： *4服务器： $3服务器： foo服务器： $3服务器： bar服务器： $5服务器： Hello服务器： $5服务器： World 在上面的示例中，服务器发送的所有字符串都由 CRLF 结尾。 正如你所见到的那样， 多条批量回复所使用的格式， 和客户端发送命令时使用的统一请求协议的格式一模一样。 它们之间的唯一区别是： 统一请求协议只发送批量回复。 而服务器应答命令时所发送的多条批量回复，则可以包含任意类型的回复。 以下例子展示了一个多条批量回复， 回复中包含四个整数值， 以及一个二进制安全字符串：1234567*5\r\n:1\r\n:2\r\n:3\r\n:4\r\n$6\r\nfoobar\r\n 在回复的第一行， 服务器发送 *5\r\n ， 表示这个多条批量回复包含 5 条回复， 再后面跟着的则是 5 条回复的正文。 多条批量回复也可以是空白的（empty）， 就像这样：12客户端： LRANGE nokey 0 1服务器： *0\r\n 无内容的多条批量回复（null multi bulk reply）也是存在的， 比如当 BLPOP key [key …] timeout 命令的阻塞时间超过最大时限时， 它就返回一个无内容的多条批量回复， 这个回复的计数值为 -1 ：12客户端： BLPOP key 1服务器： *-1\r\n 客户端库应该区别对待空白多条回复和无内容多条回复： 当 Redis 返回一个无内容多条回复时， 客户端库应该返回一个 null 对象， 而不是一个空数组。 多条批量回复中的空元素多条批量回复中的元素可以将自身的长度设置为 -1 ， 从而表示该元素不存在， 并且也不是一个空白字符串（empty string）。 当 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] 命令使用 GET pattern 选项对一个不存在的键进行操作时， 就会发生多条批量回复中带有空白元素的情况。 以下例子展示了一个包含空元素的多重批量回复：123456服务器： *3服务器： $3服务器： foo服务器： $-1服务器： $3服务器： bar 其中， 回复中的第二个元素为空。 对于这个回复， 客户端库应该返回类似于这样的回复：1["foo", nil, "bar"] 多命令和流水线客户端可以通过流水线， 在一次写入操作中发送多个命令： 在发送新命令之前， 无须阅读前一个命令的回复。 多个命令的回复会在最后一并返回。 内联命令当你需要和 Redis 服务器进行沟通， 但又找不到 redis-cli ， 而手上只有 telnet 的时候， 你可以通过 Redis 特别为这种情形而设的内联命令格式来发送命令。 以下是一个客户端和服务器使用内联命令来进行交互的例子：12客户端： PING服务器： +PONG 以下另一个返回整数值的内联命令的例子：12客户端： EXISTS somekey服务器： :0 因为没有了统一请求协议中的 “*” 项来声明参数的数量， 所以在 telnet 会话输入命令的时候， 必须使用空格来分割各个参数， 服务器在接收到数据之后， 会按空格对用户的输入进行分析（parse）， 并获取其中的命令参数。 高性能 Redis 协议分析器尽管 Redis 的协议非常利于人类阅读， 定义也很简单， 但这个协议的实现性能仍然可以和二进制协议一样快。 因为 Redis 协议将数据的长度放在数据正文之前， 所以程序无须像 JSON 那样， 为了寻找某个特殊字符而扫描整个 payload ， 也无须对发送至服务器的 payload 进行转义（quote）。 程序可以在对协议文本中的各个字符进行处理的同时， 查找 CR 字符， 并计算出批量回复或多条批量回复的长度， 就像这样：12345678910111213141516#include &lt;stdio.h&gt;int main(void) &#123; unsigned char *p = "$123\r\n"; int len = 0; p++; while(*p != '\r') &#123; len = (len*10)+(*p - '0'); p++; &#125; /* Now p points at '\r', and the len is in bulk_len. */ printf("%d\n", len); return 0;&#125; 得到了批量回复或多条批量回复的长度之后， 程序只需调用一次 read 函数， 就可以将回复的正文数据全部读入到内存中， 而无须对这些数据做任何的处理。 在回复最末尾的 CR 和 LF 不作处理，丢弃它们。 Redis 协议的实现性能可以和二进制协议的实现性能相媲美， 并且由于 Redis 协议的简单性， 大部分高级语言都可以轻易地实现这个协议， 这使得客户端软件的 bug 数量大大减少。 复制（Replication）Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(master server)的精确复制品。 以下是关于 Redis 复制功能的几个重要方面： Redis 使用异步复制。 从 Redis 2.8 开始， 从服务器会以每秒一次的频率向主服务器报告复制流（replication stream）的处理进度。 一个主服务器可以有多个从服务器。 不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个图状结构。 复制功能不会阻塞主服务器： 即使有一个或多个从服务器正在进行初次同步， 主服务器也可以继续处理命令请求。 复制功能也不会阻塞从服务器： 只要在 redis.conf 文件中进行了相应的设置， 即使从服务器正在进行初次同步， 服务器也可以使用旧版本的数据集来处理命令查询。 不过， 在从服务器删除旧版本数据集并载入新版本数据集的那段时间内， 连接请求会被阻塞。 你还可以配置从服务器， 让它在与主服务器之间的连接断开时， 向客户端发送一个错误。 复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] 命令可以交给附属节点去运行。 可以通过复制功能来让主服务器免于执行持久化操作： 只要关闭主服务器的持久化功能， 然后由从服务器去执行持久化操作即可。 关闭主服务器持久化时，复制功能的数据安全当配置Redis复制功能时，强烈建议打开主服务器的持久化功能。 否则的话，由于延迟等问题，部署的服务应该要避免自动拉起。 为了帮助理解主服务器关闭持久化时自动拉起的危险性，参考一下以下会导致主从服务器数据全部丢失的例子： 假设节点A为主服务器，并且关闭了持久化。 并且节点B和节点C从节点A复制数据 节点A崩溃，然后由自动拉起服务重启了节点A. 由于节点A的持久化被关闭了，所以重启之后没有任何数据 节点B和节点C将从节点A复制数据，但是A的数据是空的， 于是就把自身保存的数据副本删除。 在关闭主服务器上的持久化，并同时开启自动拉起进程的情况下，即便使用Sentinel来实现Redis的高可用性，也是非常危险的。 因为主服务器可能拉起得非常快，以至于Sentinel在配置的心跳时间间隔内没有检测到主服务器已被重启，然后还是会执行上面的数据丢失的流程。 无论何时，数据安全都是极其重要的，所以应该禁止主服务器关闭持久化的同时自动拉起。 复制功能的运作原理无论是初次连接还是重新连接， 当建立一个从服务器时， 从服务器都将向主服务器发送一个 SYNC 命令。 接到 SYNC 命令的主服务器将开始执行 BGSAVE ， 并在保存操作执行期间， 将所有新执行的写入命令都保存到一个缓冲区里面。 当 BGSAVE 执行完毕后， 主服务器将执行保存操作所得的 .rdb 文件发送给从服务器， 从服务器接收这个 .rdb 文件， 并将文件中的数据载入到内存中。 之后主服务器会以 Redis 命令协议的格式， 将写命令缓冲区中积累的所有内容都发送给从服务器。 你可以通过 telnet 命令来亲自验证这个同步过程： 首先连上一个正在处理命令请求的 Redis 服务器， 然后向它发送 SYNC 命令， 过一阵子， 你将看到 telnet 会话（session）接收到服务器发来的大段数据（.rdb 文件）， 之后还会看到， 所有在服务器执行过的写命令， 都会重新发送到 telnet 会话来。 即使有多个从服务器同时向主服务器发送 SYNC ， 主服务器也只需执行一次 BGSAVE 命令， 就可以处理所有这些从服务器的同步请求。 从服务器可以在主从服务器之间的连接断开时进行自动重连， 在 Redis 2.8 版本之前， 断线之后重连的从服务器总要执行一次完整重同步（full resynchronization）操作， 但是从 Redis 2.8 版本开始， 从服务器可以根据主服务器的情况来选择执行完整重同步还是部分重同步（partial resynchronization）。 部分重同步从 Redis 2.8 开始， 在网络连接短暂性失效之后， 主从服务器可以尝试继续执行原有的复制进程（process）， 而不一定要执行完整重同步操作。 这个特性需要主服务器为被发送的复制流创建一个内存缓冲区（in-memory backlog）， 并且主服务器和所有从服务器之间都记录一个复制偏移量（replication offset）和一个主服务器 ID （master run id）， 当出现网络连接断开时， 从服务器会重新连接， 并且向主服务器请求继续执行原来的复制进程： 如果从服务器记录的主服务器 ID 和当前要连接的主服务器的 ID 相同， 并且从服务器记录的偏移量所指定的数据仍然保存在主服务器的复制流缓冲区里面， 那么主服务器会向从服务器发送断线时缺失的那部分数据， 然后复制工作可以继续执行。 否则的话， 从服务器就要执行完整重同步操作。 Redis 2.8 的这个部分重同步特性会用到一个新增的 PSYNC master_run_id offset 内部命令， 而 Redis 2.8 以前的旧版本只有 SYNC 命令， 不过， 只要从服务器是 Redis 2.8 或以上的版本， 它就会根据主服务器的版本来决定到底是使用 PSYNC master_run_id offset 还是 SYNC ： 如果主服务器是 Redis 2.8 或以上版本，那么从服务器使用 PSYNC master_run_id offset 命令来进行同步。 如果主服务器是 Redis 2.8 之前的版本，那么从服务器使用 SYNC 命令来进行同步。 配置配置一个从服务器非常简单， 只要在配置文件中增加以下的这一行就可以了：1slaveof 192.168.1.1 6379 当然， 你需要将代码中的 192.168.1.1 和 6379 替换成你的主服务器的 IP 和端口号。 另外一种方法是调用 SLAVEOF host port 命令， 输入主服务器的 IP 和端口， 然后同步就会开始：12127.0.0.1:6379&gt; SLAVEOF 192.168.1.1 10086OK 只读从服务器从 Redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。 只读模式由 redis.conf 文件中的 slave-read-only 选项控制， 也可以通过 CONFIG SET parameter value 命令来开启或关闭这个模式。 只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。 即使从服务器是只读的， DEBUG 和 CONFIG 等管理式命令仍然是可以使用的， 所以我们还是不应该将服务器暴露给互联网或者任何不可信网络。 不过， 使用 redis.conf 中的命令改名选项， 我们可以通过禁止执行某些命令来提升只读从服务器的安全性。 你可能会感到好奇， 既然从服务器上的写数据会被重同步数据覆盖， 也可能在从服务器重启时丢失， 那么为什么要让一个从服务器变得可写呢？ 原因是， 一些不重要的临时数据， 仍然是可以保存在从服务器上面的。 比如说， 客户端可以在从服务器上保存主服务器的可达性（reachability）信息， 从而实现故障转移（failover）策略。 从服务器相关配置如果主服务器通过 requirepass 选项设置了密码， 那么为了让从服务器的同步操作可以顺利进行， 我们也必须为从服务器进行相应的身份验证设置。 对于一个正在运行的服务器， 可以使用客户端输入以下命令：1config set masterauth &lt;password&gt; 要永久地设置这个密码， 那么可以将它加入到配置文件中：1masterauth &lt;password&gt; 另外还有几个选项， 它们和主服务器执行部分重同步时所使用的复制流缓冲区有关， 详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。 主服务器只在有至少 N 个从服务器的情况下，才执行写操作从 Redis 2.8 开始， 为了保证数据的安全性， 可以通过配置， 让主服务器只在有至少 N 个当前已连接从服务器的情况下， 才执行写命令。 不过， 因为 Redis 使用异步复制， 所以主服务器发送的写数据并不一定会被从服务器接收到， 因此， 数据丢失的可能性仍然是存在的。 以下是这个特性的运作原理： 从服务器以每秒一次的频率 PING 主服务器一次， 并报告复制流的处理情况。 主服务器会记录各个从服务器最后一次向它发送 PING 的时间。 用户可以通过配置， 指定网络延迟的最大值 min-slaves-max-lag ， 以及执行写操作所需的至少从服务器数量 min-slaves-to-write 。 如果至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag 秒， 那么主服务器就会执行客户端请求的写操作。 你可以将这个特性看作 CAP 理论中的 C 的条件放宽版本： 尽管不能保证写操作的持久性， 但起码丢失数据的窗口会被严格限制在指定的秒数中。 另一方面， 如果条件达不到 min-slaves-to-write 和 min-slaves-max-lag 所指定的条件， 那么写操作就不会被执行， 主服务器会向请求执行写操作的客户端返回一个错误。 以下是这个特性的两个选项和它们所需的参数： min-slaves-to-write \ min-slaves-max-lag \ 详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。 事务（transaction）MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC 命令负责触发并执行事务中的所有命令： 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。 然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。 如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。 使用 redis-check-aof 程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。 从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作，具体信息请参考文档的后半部分。 用法MULTI 命令用于开启一个事务，它总是返回 OK 。 MULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC 命令被调用时， 所有队列中的命令才会被执行。 另一方面， 通过调用 DISCARD ， 客户端可以清空事务队列， 并放弃执行事务。 以下是一个事务例子， 它原子地增加了 foo 和 bar 两个键的值：123456789101112&gt; MULTIOK&gt; INCR fooQUEUED&gt; INCR barQUEUED&gt; EXEC1) (integer) 12) (integer) 1 EXEC 命令的回复是一个数组， 数组中的每个元素都是执行事务中的命令所产生的回复。 其中， 回复元素的先后顺序和命令发送的先后顺序一致。 当客户端处于事务状态时， 所有传入的命令都会返回一个内容为 QUEUED 的状态回复（status reply）， 这些被入队的命令将在 EXEC 命令被调用时执行。 事务中的错误使用事务时可能会遇上以下两种错误： 事务在执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。 命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。 对于发生在 EXEC 执行之前的错误，客户端以前的做法是检查命令入队所得的返回值：如果命令入队时返回 QUEUED ，那么入队成功；否则，就是入队失败。如果有命令在入队时失败，那么大部分客户端都会停止并取消这个事务。 不过，从 Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。 在 Redis 2.6.5 以前， Redis 只执行事务中那些入队成功的命令，而忽略那些入队失败的命令。 而新的处理方式则使得在流水线（pipeline）中包含事务变得简单，因为发送事务和读取事务的回复都只需要和服务器进行一次通讯。 至于那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。 从协议的角度来看这个问题，会更容易理解一些。 以下例子中， LPOP key 命令的执行将出错， 尽管调用它的语法是正确的：12345678910111213141516171819Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.MULTI+OKSET a 3abc+QUEUEDLPOP a+QUEUEDEXEC*2+OK-ERR Operation against a key holding the wrong kind of value EXEC 返回两条批量回复（bulk reply）： 第一条是 OK ，而第二条是 -ERR 。 至于怎样用合适的方法来表示事务中的错误， 则是由客户端自己决定的。 最重要的是记住这样一条， 即使事务中有某条/某些命令执行失败了， 事务队列中的其他命令仍然会继续执行 —— Redis 不会停止执行事务中的命令。 以下例子展示的是另一种情况， 当命令在入队时产生错误， 错误会立即被返回给客户端：12345MULTI+OKINCR a b c-ERR wrong number of arguments for 'incr' command 因为调用 INCR key 命令的参数格式不正确， 所以这个 INCR key 命令入队失败。 为什么 Redis 不支持回滚（roll back）如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。 以下是这种做法的优点： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR key 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR key ， 回滚是没有办法处理这些情况的。 鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。 放弃事务当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出：1234567891011121314redis&gt; SET foo 1OKredis&gt; MULTIOKredis&gt; INCR fooQUEUEDredis&gt; DISCARDOKredis&gt; GET foo"1" 使用 check-and-set 操作实现乐观锁WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已经失败。 举个例子， 假设我们需要原子性地为某个值进行增 1 操作（假设 INCR key 不存在）。 首先我们可能会这样做：123val = GET mykeyval = val + 1SET mykey val 上面的这个实现在只有一个客户端的时候可以执行得很好。 但是， 当多个客户端同时对同一个键进行这样的操作时， 就会产生竞争条件。 举个例子， 如果客户端 A 和 B 都读取了键原来的值， 比如 10 ， 那么两个客户端都会将键的值设为 11 ， 但正确的结果应该是 12 才对。 有了 WATCH ， 我们就可以轻松地解决这类问题了：12345678WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey valEXEC 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。 这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。 了解 WATCHWATCH 使得 EXEC 命令需要有条件地执行： 事务只能在所有被监视键都没有被修改的前提下执行， 如果这个前提不能满足的话，事务就不会被执行。 Note 如果你使用 WATCH 监视了一个带过期时间的键， 那么即使这个键过期了， 事务仍然可以正常执行， 关于这方面的详细情况，请看这个帖子： http://code.google.com/p/redis/issues/detail?id=270 WATCH 命令可以被调用多次。 对键的监视从 WATCH 执行之后开始生效， 直到调用 EXEC 为止。 用户还可以在单个 WATCH 命令中监视任意多个键， 就像这样：12redis&gt; WATCH key1 key2 key3OK 当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。 另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。 使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。 使用 WATCH 实现 ZPOPWATCH 可以用于创建 Redis 没有内置的原子操作。 举个例子， 以下代码实现了原创的 ZPOP 命令， 它可以原子地弹出有序集合中分值（score）最小的元素：12345WATCH zsetelement = ZRANGE zset 0 0MULTI ZREM zset elementEXEC 程序只要重复执行这段代码， 直到 EXEC 的返回值不是空多条回复（null multi-bulk reply）即可。 Redis 脚本和事务从定义上来说， Redis 中的脚本本身就是一种事务， 所以任何在事务里可以完成的事， 在脚本里面也能完成。 并且一般来说， 使用脚本要来得更简单，并且速度更快。 因为脚本功能是 Redis 2.6 才引入的， 而事务功能则更早之前就存在了， 所以 Redis 才会同时存在两种处理事务的方法。 不过我们并不打算在短时间内就移除事务功能， 因为事务提供了一种即使不使用脚本， 也可以避免竞争条件的方法， 而且事务本身的实现并不复杂。 不过在不远的将来， 可能所有用户都会只使用脚本来实现事务也说不定。 如果真的发生这种情况的话， 那么我们将废弃并最终移除事务功能。]]></content>
  </entry>
  <entry>
    <title><![CDATA[跨域问题及解决方案]]></title>
    <url>%2Fblog%2F%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html</url>
    <content type="text"><![CDATA[前言要了解跨域问题，我们先来了解下浏览器的同源策略。 浏览器的同源策略限制了从同一个源加载的文档或脚本与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。 同源的定义： 如果两个URL的协议、端口、IP（域名）都相同，则这两个URL具有相同的源。 根据这个定义，我们给出了下面的表格，可以简单看一下。 URL A URL B 是否同源 原因 http://abc.xyz.kk:8080/index.html http://abc.xyz.kk:8080/demo/demo.html 是 协议、端口、域名均相同 http://abc.xyz.kk:8080/api http://abc.xyz.kk:8888/index.html 否 端口不相同 http://abc.xyz.kk:8080/api http://xyz.abc.kk:8080/index.html 否 域名（IP）不相同 http://abc.xyz.kk:8080/index.html https://abc.xyz.kk:8888/index.html 否 协议不相同 http://abc.xyz.kk:8080/hello https://mmm.sss.ll:8888/hello 否 协议、端口、域名均不相同 跨域问题看到前言所说，同源策略提高了数据安全性，为什么我们还要研究跨域问题呢？ 如果单纯的Web网站，页面资源（html、js或jsp等）在服务端，我们是不用考虑跨域问题的，因为它们就在一个域下。 但是现在很多项目是前后端分离的，无论原生APP还是WebApp，由于IP、端口或者协议等的不同，它们的请求在访问后端系统时，如果不做些处理，就会受到浏览器同源策略的约束，进而出现403错误。 跨域方法我们目前有以下几种方法解决跨域问题，我们分别来看下吧。 Cross-Origin Resource Sharing (CORS)简介CORS是一个跨域资源共享方案，为了解决跨域问题，通过增加一系列HTTP请求头和响应头，规范安全地进行跨站数据传输。 请求头主要包括以下参数： 参数名 说明 Origin 用于在跨域请求或预先请求中，标明发起跨域请求的源域名 Access-Control-Request-Method 用于表明跨域请求使用的实际HTTP方法 Access-Control-Request-Headers 用于在预先请求时，告知服务器要发起的跨域请求中会携带的请求头信息 withCredentials 跨域请求是否携带凭据信息，如果设置为true，响应头的Access-Control-Allow-Origin必须指定具体域名，且Access-Control-Allow-Credentials参数为true 响应头主要包括以下参数： 参数名 说明 Access-Control-Allow-Origin 该参数中携带了服务器端验证后的允许的跨域请求域名，可以是一个具体的域名或是一个*（表示任意域名） Access-Control-Expose-Headers 该参数用于允许返回给跨域请求的响应头列表，在列表中的响应头的内容，才可以被浏览器访问 Access-Control-Max-Age 该参数用于告知浏览器可以将预先检查请求返回结果缓存的时间，在缓存有效期内，浏览器会使用缓存的预先检查结果判断是否发送跨域请求 Access-Control-Allow-Methods 该参数用于告知浏览器可以在实际发送跨域请求时，可以支持的请求方法，可以是一个具体的方法列表或是一个*（表示任意方法） Access-Control-Allow-Credentials 是否允许携带凭据信息。默认凭据信息 不包括在 CORS 请求之中 当我们给客户端添加符合的上述请求头参数，给服务端添加符合的响应头参数后，客户端对服务端的请求便可以实现跨越访问。 对CORS有更多兴趣的同学可以参考MDN的这篇文章。 Cross-Origin Resource Sharing (CORS) 例子我们使用前端JS+后台SpringBoot的例子来看一下。 前端部分代码： 123456789101112$("#test").click(function() &#123; $.ajax(&#123; type: "POST", url: "http://abc.xyz.kk/hello", data:&#123; name:"hello" &#125;, success: function(result) &#123; alert(result); &#125; &#125;);&#125;); 后端部分代码： 123456789101112131415161718192021222324@Configuration@Slf4jpublic class CorsConfig &#123; @Bean public CorsFilter corsFilter() &#123; UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); //允许所有类型请求头 corsConfiguration.addAllowedHeader("*"); //允许HEAD、POST和OPTIONS方法 corsConfiguration.setAllowedMethods(Arrays.asList("HEAD","POST", "OPTIONS")); //允许携带Cookie corsConfiguration.setAllowCredentials(true); //允许所有的源 corsConfiguration.setAllowedOrigins(Arrays.asList("*")); //时间设置为3600s corsConfiguration.setMaxAge(3600L); //跨域设置 //所有请求都允许跨域 source.registerCorsConfiguration("/**", corsConfiguration); return new CorsFilter(source); &#125;&#125; 一般出现跨域问题，如果使用CORS，只需要后台配置CORS过滤器（如上）即可实现跨域访问。 JSONP简介JSONP(JSON with Padding)是JSON的一种“使用模式”，可用于解决主流浏览器的跨域数据访问的问题。 JSONP的原理就是借助HTML中的&lt;script&gt;标签可以跨域引入资源。所以动态创建一个&lt;srcipt&gt;标签，src为目的接口 + get数据包 + 处理数据的函数名。后台收到GET请求后解析并返回函数名(数据)给前端，前端&lt;script&gt;标签动态执行处理函数。 &lt;script&gt;标签的src属性是没有跨域的限制的。这样说来，这种跨域方式其实与Ajax XmlHttpRequest协议无关了。 例子我们看一下JSONP方式实现跨域的前后端大致代码： 前端部分代码： 1234567&lt;script&gt; //jsonp回调方法，一定要写在jsonp请求之前 function jsonptest(result)&#123; alert(result); &#125;&lt;/script&gt;&lt;script src ="/hello/test?callback=jsonptest" type="text/javascript" &gt;&lt;/script&gt; 后端部分代码： 12345678910@Controller@RequestMapping("/hello")public class HelloController&#123; @RequestMapping(value="test",method=RequestMethod.GET) @ResponseBody public String jsonpTest(String callback)&#123; //do something return callback +"('Hello World!');"; &#125;&#125; 可以看到如果客户期望返回Hello World！ 实际收到的请求为 jsonptest(‘Hello World!’)，然后调用jsonptest函数获得实际想要的结果。 因为JSONP使用js的&lt;script&gt;标签进行传参，故该种方式只支持GET请求，这也是JSONP的一个缺点。 Nginx反向代理简介出现跨域限制的根本原因是浏览器同源问题的限制。 我们如果把前端项目和前端要请求的后台API接口地址放在同源下不就可以实现跨域请求了么？ 这样我们前后端都不需要做任何跨域配置处理。 例子比如我们有一个H5项目，部署在 http://abc.xyz.kk:8088 Nginx服务器上，后台地址为 http://abc.123.ss:8888 ，后台项目API接口地址为 http://abc.123.ss:8888/api/ 。 则Nginx的配置文件 nginx.conf 的 server部分配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; proxy_pass http://localhost:8000/; # 前端本机地址，实现自动更新 autoindex on; autoindex_exact_size on; autoindex_localtime on; &#125; location /api/ &#123; proxy_pass http://abc.123.ss:8888; # 后台API接口地址 proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125;&#125; 当我们访问了 http://abc.xyz.kk:8088 地址（代理前端地址），访问后台API时，通过反向代理，相当于访问 http://abc.xyz.kk:8088/api/ 这个地址，就不会出现跨域问题了。 其他跨域解决方案简介如果我们要通过A网页访问另一个域的B网页时，根据同源策略，也会出现跨域问题。 这种情况我们可以通过window.postMessage、window.name共享、window.location.hash共享等方法来解决，关于这块，我们简单的用window.postMessage来看下，其他的大家可以查询相关资料进行了解。 使用WebSocket也可以实现资源跨域访问，WebSocket是长连接，资源消耗较大，除在一些即时通讯等特殊场景，专门用来解决跨域问题还是少之又少的，这一块我们也不在详述。 例子我们用window.postMessage来进行举例。 下面是两个HTML，http://aaa.aaa.aa/A.html 和 http://bbb.bbb.bb/B.html 用 postMessage进行交互的例子。 A.html (发送端) 123456789101112131415161718192021222324252627&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;A.html&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;input id="test" type="text" value="B.html" /&gt; &lt;button id="send" &gt;A发送消息给B&lt;/button&gt;&lt;/div&gt;&lt;iframe id="receiver" src="http://bbb.bbb.bb/B.html" width="500" height="60"&gt; &lt;p&gt;你的浏览器不支持IFrame。&lt;/p&gt;&lt;/iframe&gt;&lt;script&gt; window.onload = function() &#123; var receiver = document.getElementById('receiver').contentWindow; var btn = document.getElementById('send'); btn.addEventListener('click', function (e) &#123; e.preventDefault(); var val = document.getElementById('test').value; receiver.postMessage("Hello "+val+"！", "http://aaa.aaa.aa/A.html"); &#125;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; B.html (接收端) 123456789101112131415161718192021222324&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;B.html&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="message"&gt; Hello World!&lt;/div&gt;&lt;script&gt; window.onload = function() &#123; var messageEle = document.getElementById('message'); window.addEventListener('message', function (e) &#123; alert(e.origin); if (e.origin !== "http://aaa.aaa.aa/A.html") &#123; return; &#125; messageEle.innerHTML = "从"+ e.origin +"收到消息： " + e.data; &#125;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 总结以上跨域解决方案，最常用的还是CORS和反向代理，其次是JSONP，其他很少会被使用。 在允许Ajax XmlHttpRequest的浏览器（高版本浏览器）并与后端交互的场景，CORS和反向代理应用最广。 如果浏览器不支持XmlHttpRequest（IE6、IE7….），可以考虑使用JSONP。 如果涉及到不同源网页交互，支持H5的浏览器可以采用window.postMessage，不支持的可以使用window.name共享、window.location.hash共享等。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>跨域</tag>
        <tag>CORS</tag>
        <tag>JSONP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK里那些有趣的代码（2）]]></title>
    <url>%2Fblog%2FJDK%E9%87%8C%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%882%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言接 JDK里那些有趣的代码这篇文章。 今天我们来看下另一个比较有意思的代码部分。 在说这个之前，我们先来研究一道比较有意思的题目。 使用Java程序 获取下一个最小的比入参n大的2的高次幂 这个题的意思就是：比如入参为10，则最小的比入参大的2的高次幂为 ${2}^{4} = {16}$；入参为100，则最小的比入参大的2的高次幂为${2}^{7}={128}$。 分析对于这种题目，我们如何处理呢？ 最简单的是想到循环，2，4，8…..逐渐增大值，并与入参进行对比，相关代码如下： 123456789public static int getNum1(int n)&#123; for(int i=0;i&lt;= 31;i++)&#123; int b = 1&lt;&lt;i; if(b &gt; n)&#123; return b; &#125; &#125; return -1;&#125; 如上方法 1&lt;&lt;i 是把1左移i位，每次左移一位就是乘以2，所以 1&lt;&lt;i 的结果是1乘以2的i次方。 当然我们也可以使用Java自带的Math.pow或者乘法算法方法，不过显然这种方法效率要低。 12345678910111213141516171819public static int getNum2(int n)&#123; for(int i=0;i&lt;= 31;i++)&#123; double b = Math.pow(2,i); if(b &gt; n)&#123; return (int)b; &#125; &#125; return -1;&#125;public static int getNum3(int n)&#123; int k = 1; for(int i=0;i&lt;= 31;i++)&#123; if(k &gt; n)&#123; return k; &#125; k *= 2; &#125; return -1;&#125; 或者我们可以想到将入参每次除以2，直到小于1，记录次数i，然后2的i次方即是我们所要求的值。 相关代码如下： 12345678public static int getNum4(int n)&#123; int i = 0; while (n &gt; 0) &#123; n = n &gt;&gt; 1; i++; &#125; return 1&lt;&lt;i;&#125; 可以看到我们仍使用了移位运算， n = n &gt;&gt; 1 每次将n向右移一位即除以2，当n &lt;= 0 时记录次数 i,并使用 1&lt;&lt;i 算出要求的值。 当然我们也可以使用普通的除法算法，但这种效率要低些，代码如下： 12345678public static int getNum5(int n)&#123; int i=1; while (n&gt;1)&#123; n /=2; i++; &#125; return 1&lt;&lt;i;&#125; 其实上面几种情况原理都是类似的。 还有什么别的方法么？ 正文很巧，在JDK相关源码中也有类似的问题，即获取下一个最小的比入参n大的2的高次幂。 在哪儿会用到呢？ 当然是HashMap了，HashMap在扩容时，扩容指定的大小就是下一个最小的比入参n大的2的高次幂。 下面是具体tableSizeFor方法源码： 123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 我们可以在CourrentHashMap、ForkJoinPool中发现类似的处理逻辑，这种处理的优点体现在哪儿呢？ 我们把上面的代码在整理下，如下，对于入参n，该方法可以计算出最小的比入参n大的2的高次幂。 12345678public static int getNum6(int n)&#123; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return n &lt; 0 ? 1 : n + 1;&#125; PS：我们忽略源码中的int n = cap - 1; 这一步的作用是对于入参比如8，tableSizeFor方法会返回8，而getNum6会返回16，其实主要看题目怎么出，这儿我们找的是比入参n大的数，不包括n。 我们先来手动计算一下，以32和2000为例。 1234567// 32 = 100000 = 0100000n|=n&gt;&gt;&gt;1;// n=n|(n&gt;&gt;&gt;1) = 0100000|0010000 = 0110000 = 48n|=n&gt;&gt;&gt;2;// n=n|(n&gt;&gt;&gt;2) = 0110000|0001100 = 0111100 = 60n|=n&gt;&gt;&gt;4;// n=n|(n&gt;&gt;&gt;4) = 0111100|0000011 = 0111111 = 63n|=n&gt;&gt;&gt;8;// n=n|(n&gt;&gt;&gt;8) = 0111111|0000000 = 0111111 = 63n|=n&gt;&gt;&gt;16;// n=n|(n&gt;&gt;&gt;16) = 0111111|0000000 = 0111111 = 63// n + 1 =64 1234567// 2000 = 11111010000 = 11111010000n|=n&gt;&gt;&gt;1;// n=n|(n&gt;&gt;&gt;1) = 11111010000|01111101000 = 11111111000 = 2040n|=n&gt;&gt;&gt;2;// n=n|(n&gt;&gt;&gt;2) = 11111111000|00111111110 = 11111111110 = 2046n|=n&gt;&gt;&gt;4;// n=n|(n&gt;&gt;&gt;4) = 11111111110|00001111111 = 11111111111 = 2047n|=n&gt;&gt;&gt;8;// n=n|(n&gt;&gt;&gt;8) = 11111111111|00000000111 = 11111111111 = 2047n|=n&gt;&gt;&gt;16;// n=n|(n&gt;&gt;&gt;16) = 11111111111|00000000000 = 11111111111 = 2047// n + 1 =2048 计算过程比较简单，只要明白以下两点： n&gt;&gt;&gt;i 是指二进制的n的值向右移i位； i|k指的是i和k进行位或运算，| 是把某两个二进制数中, 只要其中一个的某一位为1，则结果的该位就为1，与&amp;运算相反。 我们来分析一下： 首先，如果是2的整数次方数，其除最高位（指第一个不为0的数）外，其他位必然是0。比如 ${2}^{11}={2048}$，其二进制为 $100000000000$。 则2的整数次方数-1必定最高位为0，其他位必然为1。大致如下： 1234567892 -1 = 000000010 -1 = 0000000014 -1 = 000000100 -1 = 0000000118 -1 = 000001000 -1 = 00000011116 -1 = 000010000 -1 = 00000111132 -1 = 000100000 -1 = 00001111164 -1 = 001000000 -1 = 000111111128 -1 = 010000000 -1 = 001111111256 -1 = 100000000 -1 = 011111111...... 我们对于任意数，如21，二进制为 000010101，当使用移位+位或运算时，最终该值会逐渐增大到 000011111，而这个值加1就是我们要找的值。其实质是右传播最左侧的一位，来找到最大值。 问：为什么右移位要按照1、2、4、8、16这样移动呢？而不是其他数字呢？ 答：这很好理解，我们拿 $128 （010000000）$来举例，比它大的最小的2的高次幂是256，则需要得到255。 12345010000000011000000 右移1位+位或011110000 右移2位+位或011111111 右移4位+位或...... 可以看到我们用了一个最小值128，得到255，只需要最左侧的1右移（1、2、4）并进行位或操作。int最大32位，故右移最大16位即可保证最高位的1对右边的0进行全覆盖（位或操作）。 测试到底高不高效还是取决于测试结果，我们写一个简单的测试方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static void main(String[] args) &#123; //生成若干数量的随机数，找到它们的最小的2的高次幂 int [] a = new int[100000000]; Random random = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = random.nextInt(1073741824); &#125; //方法1 long start1 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum1(a[i]); &#125; System.out.println("方法1耗时:"+(System.currentTimeMillis()-start1)+"ms"); //方法2 long start2 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum2(a[i]); &#125; System.out.println("方法2耗时:"+(System.currentTimeMillis()-start2)+"ms"); //方法3 long start3 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum3(a[i]); &#125; System.out.println("方法3耗时:"+(System.currentTimeMillis()-start3)+"ms"); //方法4 long start4 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum4(a[i]); &#125; System.out.println("方法4耗时:"+(System.currentTimeMillis()-start4)+"ms"); //方法5 long start5 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum5(a[i]); &#125; System.out.println("方法5耗时:"+(System.currentTimeMillis()-start5)+"ms"); //方法6 long start6 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum6(a[i]); &#125; System.out.println("方法6耗时:"+(System.currentTimeMillis()-start6)+"ms");&#125; 某次结果如下： 123456方法1耗时:1064ms//由于方法2耗时实在无法接受，便不再展示。调用Math.pow方法，同学们可实际测试下。方法3耗时:1097ms方法4耗时:2232ms方法5耗时:3885ms方法6耗时:155ms 经过多次测试其结果相差不大。可以看到方法6（也就是JDK里的tableSizeFor方法）确实高效。 结语该方法在 Hacker’s Delight （高效程序的奥秘）一书 3.2节中有一些介绍，有兴趣的同学也可以去看看。 通过上面的讲解，我们可以看到JDK源码中使用高效算法的艺术，多读源码，对我们也受益匪浅。 今天就先到这里，有时间我们在分析JDK源码中比较有趣的一些代码。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.7与JDK1.8中ConcurrentHashMap的一些变化]]></title>
    <url>%2Fblog%2FJDK1-7%E4%B8%8EJDK1-8%E4%B8%ADConcurrentHashMap%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8F%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[前言今天我们来了解下ConcurrentHashMap的设计，并看下它在JDK1.7和JDK1.8中的一些改变。 说到ConcurrentHashMap，或许大家并不陌生，都知道它可以在并发访问的情况下使用，可以保证线程数据安全，相对于Hashtable和线程同步的HashMap-&gt;Collections.synchronizedMap(new HashMap&lt;&gt;()),它的效率更高。 在学习ConcurrentHashMap时，大家最好先对HashMap有一些认识，可以看一下我之前的一篇文章。HashMap实现原理 JDK1.7和JDK1.8 ConcurrentHashMap的设计实现是不同的，我们分别来看下，以进行对比。 正文JDK1.7的ConcurrentHashMap1.7的ConcurrentHashMap设计思路1.7的 ConcurrentHashMap的设计是通过分段锁的方式实现的，提高了并发度。分段是一开始就确定的了，后期不能再进行扩容。 所谓分段锁，主要是根据Segment段来实现的。 其中的段Segment继承了重入锁ReentrantLock，有了锁的功能，同时含有类似HashMap中的数组加链表结构（这里没有使用红黑树）。 虽然Segment的个数是不能扩容的，但是单个Segment里面的数组是可以扩容的。 整体概览ConcurrentHashMap有3个参数： initialCapacity：初始总容量，默认16 loadFactor：加载因子，默认0.75 concurrencyLevel：并发级别，默认16 然后我们需要知道的是： Segment的个数即ssize：取大于等于并发级别的最小的2的幂次。如concurrencyLevel=16，那么sszie=16,如concurrencyLevel=10，那么ssize=16。 单个Segment的初始容量cap：c=initialCapacity/ssize,并且可能需要+1。如15/7=2，那么c要取3，如16/8=2，那么c取2，c可能是一个任意值，那么同上述一样，cap取的值就是大于等于c的最下2的幂次。最小值要求是MIN_SEGMENT_TABLE_CAPACITY = 2。 单个Segment的阈值threshold：threshold=cap*loadFactor。 所以默认情况下，Segment的个数sszie=16,每个Segment的初始容量cap=2，单个Segment的阈值threshold=1。 如下图： 通过上图我们可以算出上述数据。 put过程 首先根据key计算出一个hash值，找到对应的Segment； 调用Segment的lock方法，为后面的put操作加锁； 根据key计算出hash值，找到Segment中数组中对应index的链表，并将该数据放置到该链表中； 判断当前Segment包含元素的数量大于阈值，则Segment进行扩容。 代码逻辑如下图源码： 其中Segment的put过程源码如下图： 我们看一下加锁方法： 可以看到如果不成功会尝试进行重试直到成功，同时如果找不到key，会返回一个新的node节点，如果key存在，会返回null。 扩容过程（rehash）这个扩容是在Segment的锁的保护下进行扩容的，不需要关注并发问题。 我们来看下相关源码： 我们看红框部分的内容，扩容的重点在于： 首先找到一个lastRun，lastRun之后的元素和lastRun是在同一个桶中，所以后面的不需要进行变动。 然后对开始到lastRun部分的元素，重新计算下设置到newTable中，每次都是将当前元素作为newTable的首元素，之前老的链表作为该首元素的next部分。 get过程 根据key计算出对应的Segment； 再根据key计算出对应Segment中数组的index； 最终遍历上述index位置的链表，查找出对应的key的value； 源码如下： remove过程 根据key值计算hash找到对应的Segment； 如果Segment不为空就调用Segment的remove方法； 对Segment段进行加锁，根据hash计算出index，找到链表（如果存在的话）； 对于找到的链表，循环找到key对应的值，并进行删除。 相关代码如下： size方法我们先来看下源码： 其大致原理如下： 使用一个循环，循环的退出条件是sum = last， 这次总数 = 上次总数，即Segment没有变化了； 每次循环，都记录 sum += modCount 和 size，如果超了int长度就返回最大int值； 循环一定次数（RETRIES_BEFORE_LOCK = 2）后如果Segment大小还在改变，就尝试对所有Segment加锁，来获取size； 最后要判断下所试次数（retries）是否大于RETRIES_BEFORE_LOCK，如果大于说明加过锁，还要对它们进行解锁。 其他方法大家可以参考下源码，不再详述。 JDK1.8的ConcurrentHashMap1.8的ConcurrentHashMap设计思路1.8的ConcurrentHashMap摒弃了1.7的Segment设计，而是在1.8HashMap的基础上实现了线程安全的版本，即也是采用数组+链表+红黑树的形式。 数组可以扩容，链表可以转化为红黑树。 整体概览有一个重要的参数sizeCtl，代表数组的大小； 用户可以设置一个初始容量initialCapacity给ConcurrentHashMap； sizeCtl = 大于（1.5倍initialCapacity+1）的最小的2的幂次， 即initialCapacity=20，则sizeCtl=32,如initialCapacity=24，则sizeCtl=64。 初始化的时候，会按照sizeCtl的大小创建出对应大小的数组。 相关代码如下： put过程 如果数组还未初始化，那么进行初始化，这里会通过一个CAS操作将sizeCtl设置为-1，设置成功的，可以进行初始化操作； 根据key的hash值找到对应的桶，如果桶还不存在，那么通过一个CAS操作来设置桶的第一个元素，失败的继续执行下面的逻辑即向桶中插入或更新； 如果找到的桶存在，但是桶中第一个元素的hash值是-1，说明此时该桶正在进行迁移操作，这一块会在下面的扩容中详细介绍； 如果找到的桶存在，那么要么是链表结构要么是红黑树结构，此时需要获取该桶的锁，在锁定的情况下执行链表或者红黑树的插入或更新； 如果桶中第一个元素的hash值大于0，说明是链表结构，则对链表插入或者更新； 如果桶中的第一个元素类型是TreeBin，说明是红黑树结构，则按照红黑树的方式进行插入或者更新； 在锁的保护下插入或者更新完毕后，如果是链表结构，需要判断链表中元素的数量是否超过8（默认），一旦超过就要考虑进行数组扩容或者是链表转红黑树。 如下图源码： initTable方法代码如下： 我们再来看下扩容过程。 扩容过程一旦链表中的元素个数超过了8个，那么可以执行数组扩容或者链表转为红黑树，这里依据的策略跟HashMap依据的策略是一致的。 当数组长度还未达到64个时，优先数组的扩容，否则选择链表转为红黑树。 源码如下所示： 重点来看看这个扩容过程，即看下上述tryPresize方法，也可以看到上述是2倍扩容的方式。 第一个执行的线程会首先设置sizeCtl属性为一个负值，然后执行transfer(tab, null)，其他晚进来的线程会检查当前扩容是否已经完成，没完成则帮助进行扩容，完成了则直接退出。 该ConcurrentHashMap的扩容操作可以允许多个线程并发执行，那么就要处理好任务的分配工作。每个线程获取一部分桶的迁移任务，如果当前线程的任务完成，查看是否还有未迁移的桶，若有则继续领取任务执行，若没有则退出。在退出时需要检查是否还有其他线程在参与迁移工作，如果有则自己什么也不做直接退出，如果没有了则执行最终的收尾工作。 Q1：当前线程如何感知其他线程也在参与迁移工作？ A1: 靠sizeCtl的值，它初始值是一个负值=(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)，每当一个线程参与进来执行迁移工作，则该值进行CAS自增，该线程的任务执行完毕要退出时对该值进行CAS自减操作，所以当sizeCtl的值等于上述初值则说明了此时未有其他线程还在执行迁移工作，可以去执行收尾工作了。见如下代码: Q2: 任务按照什么规则进行分片？ A2： 下图stride即是每个分片的大小，目前有最低要求16，即每个分片至少需要16个桶。stride的计算依赖于CPU的核数，如果只有1个核，那么此时就不用分片，即stride=n。其他情况就是 (n &gt;&gt;&gt; 3) / NCPU。 Q3：如何记录目前已经分出去的任务？ A3: ConcurrentHashMap含有一个属性transferIndex（初值为最后一个桶），表示从transferIndex开始到后面所有的桶的迁移任务已经被分配出去了。所以每次线程领取扩容任务，则需要对该属性进行CAS的减操作，即一般是transferIndex-stride。 Q4：每个线程如何处理分到的部分桶的迁移工作？ A4：第一个获取到分片的线程会创建一个新的数组，容量是之前的2倍。 遍历自己所分到的桶： 桶中元素不存在，则通过CAS操作设置桶中第一个元素为ForwardingNode，其Hash值为MOVED（-1）,同时该元素含有新的数组引用 此时若其他线程进行put操作，发现第一个元素的hash值为-1则代表正在进行扩容操作（并且表明该桶已经完成扩容操作了，可以直接在新的数组中重新进行hash和插入操作），该线程就可以去参与进去，或者没有任务则不用参与，此时可以去直接操作新的数组了 桶中元素存在且hash值为-1，则说明该桶已经被处理了（本不会出现多个线程任务重叠的情况，这里主要是该线程在执行完所有的任务后会再次进行检查，再次核对） 桶中为链表或者红黑树结构，则需要获取桶锁，防止其他线程对该桶进行put操作，然后处理方式同HashMap的处理方式一样，对桶中元素分为2类，分别代表当前桶中和要迁移到新桶中的元素。设置完毕后代表桶迁移工作已经完成，旧数组中该桶可以设置成ForwardingNode了 下面来看下详细的代码： get过程 根据k计算出hash值，找到对应的数组index; 如果该index位置无元素则直接返回null; 如果该index位置有元素: 如果第一个元素的hash值小于0，则该节点可能为ForwardingNode或者红黑树节点TreeBin; 如果是ForwardingNode（表示当前正在进行扩容），使用新的数组来进行查找; 如果是红黑树节点TreeBin，使用红黑树的查找方式来进行查找; 如果第一个元素的hash大于等于0，则为链表结构，依次遍历即可找到对应的元素。 详细代码如下： 其他方法过程ConcurrentHashMap的一些其它方法，如remove，size等也是十分复杂的。我们后面在详聊JDK1.8 ConcurrentHashMap的一些其它方法。 问题分析ConcurrentHashMap读为什么不需要锁？我们通常使用读写锁来保护对一堆数据的读写操作。读时加读锁，写时加写锁。在什么样的情况下可以不需要读锁呢？ 如果对数据的读写是一个原子操作，那么此时是可以不需要读锁的。如ConcurrentHashMap对数据的读写，写操作是不需要分2次写的（没有中间状态），读操作也是不需要2次读取的。假如一个写操作需要分多次写，必然会有中间状态，如果读不加锁，那么可能就会读到中间状态，那就不对了。 假如ConcurrentHashMap提供put(key1,value1,key2,value2)，写入的时候必然会存在中间状态即key1写完成，但是key2还未写，此时如果读不加锁，那么就可能读到key1是新数据而key2是老数据的中间状态。 虽然ConcurrentHashMap的读不需要锁，但是需要保证能读到最新数据，所以必须加volatile。即数组的引用需要加volatile，同时一个Node节点中的val和next属性也必须要加volatile。 ConcurrentHashMap是否可以在无锁的情况下进行迁移？目前1.8的ConcurrentHashMap迁移是在锁定旧桶的前提下进行迁移的，然而并没有去锁定新桶。那么就可能提出如下问题： Q1：在某个桶的迁移过程中，别的线程想要对该桶进行put操作怎么办？ A1: 一旦某个桶在迁移过程中了，必然要获取该桶的锁，所以其他线程的put操作要被阻塞，一旦迁移完毕，该桶中第一个元素就会被设置成ForwardingNode节点，所以其他线程put时需要重新判断下桶中第一个元素是否被更改了，如果被改了重新获取重新执行逻辑，如下代码： Q2: 某个桶已经迁移完成（其他桶还未完成），别的线程想要对该桶进行put操作怎么办？ A2: 该线程会首先检查是否还有未分配的迁移任务，如果有则先去执行迁移任务，如果没有即全部任务已经分发出去了，那么此时该线程可以直接对新的桶进行插入操作（映射到的新桶必然已经完成了迁移，所以可以放心执行操作）。 Q3: 从上面看到我们在迁移的时候还是需要对旧桶锁定的，能否在无锁的情况下实现迁移？ A3: 一旦扩容就涉及到迁移桶中元素的操作，将一个桶中的元素迁移到另一个桶中的操作不是一个原子操作，所以需要在锁的保护下进行迁移。如果扩容操作是移动桶的指向，那么就可以通过一个CAS操作来完成扩容操作。可以参考参考这篇论文Split-Ordered Lists: Lock-Free Extensible Hash Tables ConcurrentHashMap曾经的弱一致性曾经老版本的ConcurrentHashMap是弱一致的，大家可以参考相关文档或者较早的ConcurrentHashMap源码。 曾经引发弱一致性的原因： 对数组的引用是volatile来修饰的，但是数组中的元素并不是。即读取数组的引用总是能读取到最新的值，但是读取数组中某一个元素的时候并不一定能读到最新的值。所以说是弱一致性的。 要实现强一致性，可以这样： 对于新加的key，通过写入到链表的末尾即可。因为一个元素的next属性是volatile的，可以保证写入后立马看的到，如下1.8的方式； 或者对数组中元素的更新采用volatile写的方式，如下1.7的形式。 但是现在1.7版本的ConcurrentHashMap对于数组中元素的写也是加了volatile的，如下代码： 1.8的方式就是直接将新加入的元素写入next属性（含有volatile修饰）中而不是修改桶中的第一个元素，如下代码： 所以在1.7和1.8版本的ConcurrentHashMap中不再是弱一致性，写入的数据是可以立即被读到的。 结语本文介绍了JDK1.7和JDK1.8版本下的ConcurrentHashMap的一些差异，也了解了1.7和1.8下ConcurrentHashMap的一些原理及方法，让我们对ConcurrentHashMap有了更深刻的一些认识。 参考资料 jdk1.8的HashMap和ConcurrentHashMap (有改动) JDK1.8 ConcurrentHashMap源码 JDK1.7 ConcurrentHashMap源码]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）分布式系统下的CAP理论]]></title>
    <url>%2Fblog%2F%EF%BC%88%E8%BD%AC%EF%BC%89%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%8B%E7%9A%84CAP%E7%90%86%E8%AE%BA.html</url>
    <content type="text"><![CDATA[原文：分布式系统的CAP理论 2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。 无论你是一个系统架构师，还是一个普通开发，当你开发或者设计一个分布式系统的时候，CAP理论是无论如何也绕不过去的。本文就来介绍一下到底什么是CAP理论，如何证明CAP理论，以及CAP的权衡问题。 CAP理论概述CAP理论：一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 读者需要注意的的是，CAP理论中的CA和数据库事务中ACID的CA并完全是同一回事儿。两者之中的A都是C都是一致性(Consistency)。CAP中的A指的是可用性（Availability），而ACID中的A指的是原子性（Atomicity)，切勿混为一谈。 CAP的定义Consistency 一致性一致性指 “all nodes see the same data at the same time” ，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。 对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。 一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。 从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。 三种一致性策略 对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。 如果能容忍后续的部分或者全部访问不到，则是弱一致性。 如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。 CAP中说，不可能同时满足的这个一致性指的是强一致性。 Availability 可用性可用性指 “Reads and writes always succeed” ，即服务一直可用，而且是正常响应时间。 对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，一般我们在衡量一个系统的可用性的时候，都是通过停机时间来计算的。 可用性分类 可用水平（%） 年可容忍停机时间 容错可用性 99.9999 &lt;1 min 极高可用性 99.999 &lt;5 min 具有故障自动恢复能力的可用性 99.99 &lt;53 min 高可用性 99.9 &lt;8.8h 商品可用性 99 &lt;43.8 min 通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)36524*60 = 5.256 min，这是一个极高的要求。 好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。 Partition Tolerance分区容错性分区容错性指 “the system continues to operate despite arbitrary message loss or failure of part of the system” ，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。 简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。 CAP的证明 如上图，是我们证明CAP的基本场景，网络中有两个节点N1和N2，可以简单的理解N1和N2分别是两台计算机，他们之间网络可以连通，N1中有一个应用程序A，和一个数据库V，N2也有一个应用程序B2和一个数据库V。现在，A和B是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。 在满足一致性的时候，N1和N2中的数据是一样的，V0=V0。在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的正常运作。 如上图，是分布式系统正常运转的流程，用户向N1机器请求数据更新，程序A更新数据库Vo为V1，分布式系统将数据进行同步操作M，将V1同步的N2中V0，使得N2中的数据V0也更新为V1，N2中的数据再响应N2的请求。 这里，可以定义N1和N2的数据库V之间的数据是否一样为一致性；外部对N1和N2的请求响应为可用行；N1和N2之间的网络环境为分区容错性。这是正常运作的场景，也是理想的场景，然而现实是残酷的，当错误发生的时候，一致性和可用性还有分区容错性，是否能同时满足，还是说要进行取舍呢？ 作为一个分布式系统，它和单机系统的最大区别，就在于网络，现在假设一种极端情况，N1和N2之间的网络断开了，我们要支持这种网络异常，相当于要满足分区容错性，能不能同时满足一致性和响应性呢？还是说要对他们进行取舍。 假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1，由于网络是断开的，所以分布式系统同步操作M，所以N2中的数据依旧是V0；这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据V1，怎么办呢？ 有二种选择，第一，牺牲数据一致性，保证可用性。响应旧的数据V0给用户； 第二，牺牲可用性，保证数据一致性。阻塞等待，直到网络连接恢复，数据更新操作M完成之后，再给用户响应最新的数据V1。 这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。 CAP权衡通过CAP理论及前面的证明，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ 我们分三种情况来阐述一下。 CA without P这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。 比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。 其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到： 如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。 从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。 所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。 CP without A如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。 一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。 设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。 无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？ ZooKeeper是个CP（一致性+分区容错性）的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。 AP wihtout C要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。 这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。 你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。 但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。 适合的才是最好的上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。 对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CP，舍弃A。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。 对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。 总结无论你是一个架构师，还是一个普通开发，在设计或开发分布式系统的时候，不可避免的要在CAP中做权衡。需要根据自己的系统的实际情况，选择最适合自己的方案。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>CAP</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM堆内存及垃圾回收简介]]></title>
    <url>%2Fblog%2FJVM%E5%A0%86%E5%86%85%E5%AD%98%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言我们知道在JVM内存模型中，堆是十分重要的一块，堆是内存占用最大，管理最复杂的一个区域，其用途就是存放生成的对象实例，所有的对象都会在堆上进行分配使用。 JDK1.8后，字符串常量池从永久代剥离了出来，也存放在了堆上。 正文堆内存结构我们来看一下堆内存结构，JDK1.8后JVM堆内存结构如下图： 可以看到堆内存分为年轻代（Young Generation）、年老代（Old Generation）及元空间（MetaData Space）。 PS:JDK8 完全移除永久代(Permanent Generation), 取而代之的是元空间MetaData Space（JVM使用本地内存，存放类的元数据）。 年轻代（Young Generation）又分为 Eden Space 和 Survivor Space，其中Survivor区有两部分构成 Survivor 1 和 Survivor 2 。 JVM虚拟机默认Eden区和两块Survivor区的内存比例为8:1:1。 GC流程年轻代内存的大致使用过程为： 年轻代将内存分为Eden和2块Survivor区（分别叫from和to）。 一般情况下，新创建的对象都会被分配到Eden区(一些大对象特殊处理),这些对象经过第一次Minor GC后，如果仍然存活，将会被移到Survivor区。 对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度时，就会被移动到年老代中。 在GC开始的时候，对象只会存在于Eden区和名为“From”的Survivor区，Survivor区“To”是空的。 紧接着进行GC，Eden区中所有存活的对象都会被复制到“To”，而在“From”区中，仍存活的对象会根据他们的年龄值来决定去向。年龄达到一定值的对象会被移动到年老代中，没有达到阈值的对象会被复制到“To”区域。 经过这次GC后，Eden区和From区已经被清空。这个时候，“From”和“To”会交换他们的角色，也就是新的“To”就是上次GC前的“From”，新的“From”就是上次GC前的“To”。不管怎样，都会保证名为To的Survivor区域是空的。 经过Minor GC之后，如果Survivor存放不下存活的对象，对象就会通过分配担保机制进入老年代，而如果老年代空间还不够，就会进行Full GC。 Minor GC会一直重复这样的过程，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。 因为年轻代中的对象基本都是朝生夕死的(80%以上)，所以在年轻代的垃圾回收算法使用的是复制算法，复制算法的基本思想就是将内存分为两块，每次只用其中一块，当这一块内存用完，就将还活着的对象复制到另外一块上面。复制算法不会产生内存碎片。 因此对象进入年老代有以下4种情况： 经过Minor GC后，Survivor区存放不下存活的对象进入年老代。 对象长期存活，当年龄达到一定阈值后进入年老代，默认15。年龄阈值，可以通过-XX:MaxTenuringThreshold来设置。 大对象直接进入年老代，通过 -XX:PretenureSizeThreshold 参数可以进行设置多大的对象直接在年老代进行分配，从而避免大对象在年轻代（Eden和Survivor区）发生大量内存赋值操作。 如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。（动态对象年龄绑定） GC的大致回收流程如下图： GC的回收分为垃圾的收集和回收两部分，收集和回收都涉及到一些算法逻辑，我们来整理下。 垃圾收集算法JVM中常用的垃圾收集算法大致有两种，引用计数法和根搜索法。 引用计数法 引用计数法本质是给对象添加引用计数器，当引用对象时计数器+1，引用失效时，计数器-1，当计数器等于0时，对象失效，内存可以被回收。 但会有一个问题，如果A对象引用B对象，同时B对象又引用A对象，但它们都不会再被系统使用，则它们可认为为垃圾，但是它们的引用计数是永不为0的，因此该方法永远也不会将其标位垃圾。 优点：实现简单高效。 缺点：对象之间的互相循环引用问题不好解决。 根搜索法 通过GC roots可达的对象路径称为引用链（reference chain），当一个对象没有引用链时（即从GC roots不可达）则视为不可用对象，内存可以被回收。 JVM主要使用根搜索法进行垃圾收集。 那在JVM中，哪些对象可以视为GC roots呢？ 虚拟机栈中（即栈帧中的本地变量）的引用对象； 本地方法栈中的引用对象； 方法区中的静态变量引用的对象和常量池中引用的对象。 垃圾回收算法 标记-清除算法 分两步进行，第一步标记出可以回收的对象，第二步统一清理可以回收的对象内存。 缺点：如果在被标记后直接对对象进行清除，会带来另一个新的问题——内存碎片化。如果下次有比较大的对象实例需要在堆上分配较大的内存空间时，可能会出现无法找到足够的连续内存而不得不再次触发垃圾回收。 复制算法 此GC算法实际上解决了标记-清除算法带来的“内存碎片化”问题。首先还是先标记处待回收内存和不用回收的内存，下一步将不用回收的内存复制到新的内存区域，这样旧的内存区域就可以全部回收，而新的内存区域则是连续的。 缺点：就是会损失掉部分系统内存，因为你总要腾出一部分内存用于复制。 标记-整理算法 标记-压缩算法首先还是“标记”，标记过后，将不用回收的内存对象压缩到内存一端，此时即可直接清除边界处的内存，这样就能避免复制算法带来的效率问题，同时也能避免内存碎片化的问题。 分代收集算法 对于JVM堆内存的垃圾回收，可以认为是分代收集算法。 对于年轻代，大部分对象都不会存活，所以在新生代中使用复制算法较为高效。 而对于年老代来讲，大部分对象可能会继续存活下去，如果此时还是利用复制算法，效率则会降低，此时使用标记-整理算法，不仅提高效率，更节约内存。 当然，具体使用哪种垃圾回收算法，也和垃圾收集器的实现有具体关系。 垃圾收集器再来看一下JVM的几种垃圾收集器。 目前JVM有7种作用于不同分代的垃圾收集器。如下图： 上图两个垃圾收集器之间的连线表示它们可以搭配使用。 Serial收集器 Serial收集器是最基本、发展历史最悠久的收集器。是单线程的收集器。它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集完成。 Serial收集器依然是虚拟机运行在Client模式下默认新生代（年轻代）收集器，对于运行在Client模式下的虚拟机来说是一个很好的选择。 ParNew收集器 ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial 收集器完全一样。 ParNew收集器是许多运行在Server模式下的虚拟机中首选新生代收集器，其中有一个与性能无关但很重要的原因是，除Serial收集器之外，目前只有ParNew它能与CMS收集器配合工作。 Parallel Scavenge（并行回收）收集器 Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器。 该收集器的目标是达到一个可控制的吞吐量（Throughput）。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即 吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间）。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可用高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 Parallel Scavenge收集器提供两个参数用于精确控制吞吐量，分别是控制最大垃圾收起停顿时间的 -XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数 Parallel Scavenge收集器还有一个参数：-XX:+UseAdaptiveSizePolicy。这是一个开关参数，当这个参数打开后，就不需要手工指定新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数，只需要把基本的内存数据设置好（如-Xmx设置最大堆），然后使用MaxGVPauseMillis参数或GCTimeRation参数给虚拟机设立一个优化目标。 自适应调节策略也是Parallel Scavenge收集器与ParNew收集器的一个重要区别。 Serial Old 收集器 Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记整理算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。 如果在Server模式下，主要两大用途： （1）在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用。 （2）作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。 Parallel Old 收集器 Parallel Old 是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。这个收集器在1.6中才开始提供。 CMS收集器 CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务器的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 CMS收集器是基于“标记-清除”算法实现的。它的运作过程相对前面几种收集器来说更复杂一些，整个过程分为4个步骤： （1）初始标记 （2）并发标记 （3）重新标记 （4）并发清除 其中，初始标记、重新标记这两个步骤仍然需要“Stop The World”. CMS收集器主要优点：并发收集，低停顿。 CMS三个明显的缺点： （1）CMS收集器对CPU资源非常敏感。CPU个数少于4个时，CMS对于用户程序的影响就可能变得很大，为了应付这种情况，虚拟机提供了一种称为“增量式并发收集器”的CMS收集器变种。所做的事情和单CPU年代PC机操作系统使用抢占式来模拟多任务机制的思想。 （2）CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。在JDK1.5的默认设置下，CMS收集器当老年代使用了68%的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高参数-XX:CMSInitiatingOccupancyFraction的值来提高触发百分比，以便降低内存回收次数从而获取更好的性能，在JDK1.6中，CMS收集器的启动阀值已经提升至92%。 （3）CMS是基于“标记-清除”算法实现的收集器，收集结束时会有大量空间碎片产生。空间碎片过多，可能会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发FullGC。为了解决这个问题，CMS收集器提供了一个-XX:+UseCMSCompactAtFullCollection开关参数（默认就是开启的），用于在CMS收集器顶不住要进行FullGC时开启内存碎片合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间变长了。虚拟机设计者还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction,这个参数是用于设置执行多少次不压缩的Full GC后，跟着来一次带压缩的Full GC（默认值为0，表示每次进入Full GC时都进行碎片整理）。 G1收集器 G1收集器的优势： （1）并行与并发 （2）分代收集 （3）空间整理 （标记——整理算法，复制算法） （4）可预测的停顿（G1除处理追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经实现Java（RTSJ）的垃圾收集器的特征） 备注： The Real-time Specification for Java (RTSJ) is an open specification that augments the Java language to open the door more widely to using the language to build real-time systems (see Related topics). Implementing the RTSJ requires support in the operating system, the JRE, and the Java Class Library (JCL). 详见：RTSJ 中的Garbage collection的规范。 使用G1收集器时，Java堆的内存布局是整个规划为多个大小相等的独立区域（Region）,虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region的集合。 G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获取的空间大小以及回收所需要的时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的又来）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽量可能高的收集效率。 G1 内存“化整为零”的思路： 在GC根节点的枚举范围中加入Remembered Set即可保证不对全堆扫描也不会遗漏。 如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为一下步骤： （1）初始标记 （2）并发标记 （3）最终标记 （4）筛选回收 参考：https://www.cnblogs.com/chengxuyuanzhilu/p/7088316.html JVM的一些参数我们再来看下JVM的一些常用参数设置。 JVM的基础参数 -Xmx2048m：设置JVM最大堆内存为2048M。 -Xms2048m：设置JVM初始堆内存为2048M。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 -Xss128k：设置每个线程的栈大小。JDK5.0以后每个线程栈大小为1M，之前每个线程栈大小为256K。应当根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。需要注意的是：当这个值被设置的较大（例如&gt;2MB）时将会在很大程度上降低系统的性能。 -Xmn1g：设置年轻代大小为1G。在整个堆内存大小确定的情况下，增大年轻代将会减小年老代，反之亦然。此值关系到JVM垃圾回收，对系统性能影响较大，官方推荐配置为整个堆大小的3/8。 -XX:NewSize=1024m：设置年轻代初始值为1024M。 -XX:MaxNewSize=1024m：设置年轻代最大值为1024M。 -XX:PermSize=256m：设置持久代初始值为256M。（1.7以下JDK版本有效） -XX:MaxPermSize=256m：设置持久代最大值为256M。（1.7以下JDK版本有效） -XX:MetaspaceSize=8m：初始元数据空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。（1.7以上JDK版本有效） -XX:MaxMetaspaceSize=50m：元数据最大空间大小，默认是没有限制的。（1.7以上JDK版本有效） -XX:NewRatio=4：设置年轻代（包括1个Eden和2个Survivor区）与年老代的比值。表示年轻代比年老代为1:4。 -XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的比值。表示2个Survivor区与1个Eden区的比值为2:4，即1个Survivor区占整个年轻代大小的1/6。 -XX:MaxTenuringThreshold=7：表示一个对象如果在Survivor区（救助空间）移动了7次还没有被垃圾回收就进入年老代。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代，对于需要大量常驻内存的应用，这样做可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象在年轻代存活时间，增加对象在年轻代被垃圾回收的概率，减少Full GC的频率，这样做可以在某种程度上提高服务稳定性。 -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集。（1.7以上JDK版本有效） -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集。（1.7以上JDK版本有效） PS:可以看到-Xmn，-XX:NewSize/-XX:MaxNewSize，-XX:NewRatio 3组参数都可以影响年轻代的大小，它们混合使用生效的优先级为： 高优先级：-XX:NewSize/-XX:MaxNewSize 中优先级：-Xmn（默认等效 -Xmn=-XX:NewSize=-XX:MaxNewSize=?） 低优先级：-XX:NewRatio 推荐使用-Xmn参数。 JVM垃圾回收参数 -XX:+UseSerialGC：设置串行收集器。 -XX:+UseParallelGC：设置为并行收集器。此配置仅对年轻代有效。即年轻代使用并行收集，而年老代仍使用串行收集。 -XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时有多少个线程一起进行垃圾回收。此值建议配置与CPU数目相等。 -XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0开始支持对年老代并行收集。 -XX:MaxGCPauseMillis=100：设置每次年轻代垃圾回收的最长时间（单位毫秒）。如果无法满足此时间，JVM会自动调整年轻代大小，以满足此时间。 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动调整年轻代Eden区大小和Survivor区大小的比例，以达成目标系统规定的最低响应时间或者收集频率等指标。此参数建议在使用并行收集器时，一直打开。 -XX:+UseConcMarkSweepGC：即CMS收集，设置年老代为并发收集。CMS收集是JDK1.4后期版本开始引入的新GC算法。它的主要适合场景是对响应时间的重要性需求大于对吞吐量的需求，能够承受垃圾回收线程和应用线程共享CPU资源，并且应用中存在比较多的长生命周期对象。CMS收集的目标是尽量减少应用的暂停时间，减少Full GC发生的几率，利用和应用程序线程并发的垃圾回收线程来标记清除年老代内存。 -XX:+UseParNewGC：设置年轻代为并发收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此参数。 -XX:CMSFullGCsBeforeCompaction=0：由于并发收集器不对内存空间进行压缩和整理，所以运行一段时间并行收集以后会产生内存碎片，内存使用效率降低。此参数设置运行0次Full GC后对内存空间进行压缩和整理，即每次Full GC后立刻开始压缩和整理内存。 -XX:+UseCMSCompactAtFullCollection：打开内存空间的压缩和整理，在Full GC后执行。可能会影响性能，但可以消除内存碎片。 -XX:+CMSIncrementalMode：设置为增量收集模式。一般适用于单CPU情况。 -XX:CMSInitiatingOccupancyFraction=70：表示年老代内存空间使用到70%时就开始执行CMS收集，以确保年老代有足够的空间接纳来自年轻代的对象，避免Full GC的发生。 -XX:+ScavengeBeforeFullGC：年轻代GC优于Full GC执行。 -XX:+DisableExplicitGC：不响应 System.gc() 代码。 -XX:+UseThreadPriorities：启用本地线程优先级API。即使 java.lang.Thread.setPriority() 生效，不启用则无效。 -XX:SoftRefLRUPolicyMSPerMB=0：软引用对象在最后一次被访问后能存活0毫秒（JVM默认为1000毫秒）。 -XX:TargetSurvivorRatio=90：允许90%的Survivor区被占用（JVM默认为50%）。提高对于Survivor区的使用率。 -XX:+UseG1GC: 设置使用G1垃圾回收器（1.7以上JDK版本有效） -XX:G1HeapRegionSize=n:设置g1 region大小，不设置的话自己会根据堆大小算，目标是根据最小堆内存划分2048个区域（1.7以上JDK版本有效） JVM其它参数 -XX:+CITime：打印消耗在JIT编译的时间。 -XX:ErrorFile=./hs_err_pid.log：保存错误日志或数据到指定文件中。 -XX:HeapDumpPath=./java_pid.hprof：指定Dump堆内存时的路径。 -XX:+HeapDumpOnOutOfMemoryError：当首次遭遇内存溢出时Dump出此时的堆内存。 -XX:OnError=”;”：出现致命ERROR后运行自定义命令。 -XX:OnOutOfMemoryError=”;”：当首次遭遇内存溢出时执行自定义命令。 -XX:+PrintClassHistogram：按下 Ctrl+Break 后打印堆内存中类实例的柱状信息，同JDK的 jmap -histo 命令。 -XX:+PrintConcurrentLocks：按下 Ctrl+Break 后打印线程栈中并发锁的相关信息，同JDK的 jstack -l 命令。 -XX:+PrintCompilation：当一个方法被编译时打印相关信息。 -XX:+PrintGC：每次GC时打印相关信息。 -XX:+PrintGCDetails：每次GC时打印详细信息。 -XX:+PrintGCTimeStamps：打印每次GC的时间戳。 -XX:+TraceClassLoading：跟踪类的加载信息。 -XX:+TraceClassLoadingPreorder：跟踪被引用到的所有类的加载信息。 -XX:+TraceClassResolution：跟踪常量池。 -XX:+TraceClassUnloading：跟踪类的卸载信息。 -client：设置JVM使用Client模式，特点是启动速度比较快，但运行时性能和内存管理效率不高，通常用于客户端应用程序或开发调试；在32位环境下直接运行Java程序默认启用该模式。 -server：设置JVM使Server模式，特点是启动速度比较慢，但运行时性能和内存管理效率很高，适用于生产环境。在具有64位能力的JDK环境下默认启用该模式。 PS：关于参数名称定义如下。 标准参数（-），所有JVM都必须支持这些参数的功能，而且向后兼容； 非标准参数（-X），默认JVM实现这些参数的功能，但是并不保证所有JVM实现都满足，且不保证向后兼容； 非稳定参数（-XX），此类参数各个JVM实现会有所不同，将来可能会不被支持，需要慎重使用； 参考：https://blog.csdn.net/kthq/article/details/8618052 各个区域的OOM我们来看下JVM各个区域的OOM。 堆的OOM 我们创建如下类，进行测试。 123456789101112131415public class JvmTest &#123; public void oomTest()&#123; List&lt;JvmTest&gt; list = new ArrayList&lt;&gt;(); try&#123; while (true)&#123; list.add(new JvmTest()); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; new JvmTest().oomTest(); &#125; &#125; 运行后可以看到抛出如下异常： 这也是非常常见的一种OOM异常。出现的原因可能是创建了大量大对象、一些流未及时关闭等，导致堆内存溢出。 出现这种情况，必须考虑程序的优化解决方法。而不是单纯的通过-Xmn参数增大内存来解决。 栈的OOM 当栈深度超过虚拟机分配给线程的栈大小时，就会出现栈的溢出异常。 我们创建测试类，来看一下。 123456789101112public class JvmTest &#123; public int stackOverTest(int n)&#123; if(n==1)&#123; return 8; &#125;else&#123; return stackOverTest(n-1)+2; &#125; &#125; public static void main(String[] args) &#123; new JvmTest().stackOverTest(200000); &#125; &#125; 运行后可以看到如下异常： 这种异常一般是调用递归或者死循环等产生的，导致栈深度超过虚拟机分配给线程的栈大小。 当然可以通过-Xss参数控制每个线程的栈大小来解决，但通常情况下，应检查程序，减少递归的使用。 关于Metaspace与PermGen（永久代） JDK1.8移除了PermGen（永久代），取而代之的是Metaspace（元空间），元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 我们在JDK1.8环境下，设置Metaspace的大小，进行测试。(-XX:MetaspaceSize=5M -XX:MaxMetaspaceSize=5M) 123456789101112131415161718192021public class JvmTest &#123; public void MetaSpaceOOMTest() &#123; while (true) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(JvmTest.class); enhancer.setUseCache(false); enhancer.setCallback( new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; return methodProxy.invokeSuper(o,objects); &#125; &#125; ); enhancer.create(); &#125; &#125; public static void main(String[] args) &#123; new JvmTest().MetaSpaceOOMTest(); &#125; &#125; 运行后可以看到如下异常： 这种问题出现较少，如果出现一般为动态代理生成大量class类引起的问题。 我们在JDK1.6环境下，设置PermGen（永久代）大小，进行测试。（-XX:PermSize=10m -XX:MaxPermSize=10m） 测试方法同上。 可以看到日志输出如下： 由于项目大多数JDK版本都在8及以上，故这种OOM异常已经很少在见到了。 JVM 日志我们再来看下JVM的垃圾回收日志，并简单解读下。我们这儿主要来看新的JVM（1.8及其后）的GC日志。 我们在运行时添加如下参数： -XX:-PrintGCDetails 我们用上面的 JvmTest类里的oomTest方法来进行测试。 可以看到如下一些GC运行日志和OOM的dump日志。 我们先来看下GC的运行日志部分： 12345678[GC (Allocation Failure) [PSYoungGen: 207268K-&gt;200192K(339968K)] 879015K-&gt;879170K(1489408K), 0.8137540 secs] [Times: user=2.95 sys=0.06, real=0.81 secs] [GC (Allocation Failure) [PSYoungGen: 339968K-&gt;243200K(339456K)] 1292739K-&gt;1292955K(1488896K), 0.8972164 secs] [Times: user=3.18 sys=0.20, real=0.90 secs] [Full GC (Ergonomics) [PSYoungGen: 243200K-&gt;0K(339456K)] [ParOldGen: 1049755K-&gt;1110054K(1722880K)] 1292955K-&gt;1110054K(2062336K), [Metaspace: 3502K-&gt;3502K(1056768K)], 7.8097561 secs] [Times: user=16.27 sys=0.19, real=7.81 secs] [GC (Allocation Failure) [PSYoungGen: 96256K-&gt;96384K(424448K)] 1206310K-&gt;1206438K(2147328K), 0.3838048 secs] [Times: user=1.39 sys=0.05, real=0.38 secs] [GC (Allocation Failure) [PSYoungGen: 192640K-&gt;192672K(425472K)] 1302694K-&gt;1302726K(2148352K), 0.6567791 secs] [Times: user=2.53 sys=0.00, real=0.66 secs] [GC (Allocation Failure) [PSYoungGen: 303776K-&gt;303840K(430080K)] 1824520K-&gt;1824584K(2152960K), 1.1635894 secs] [Times: user=4.29 sys=0.00, real=1.16 secs] [GC (Allocation Failure) [PSYoungGen: 414944K-&gt;347136K(496640K)] 1935688K-&gt;1935808K(2219520K), 1.6472200 secs] [Times: user=5.76 sys=0.19, real=1.65 secs] [Full GC (Ergonomics) [PSYoungGen: 347136K-&gt;0K(496640K)] [ParOldGen: 1588672K-&gt;1661667K(2083840K)] 1935808K-&gt;1661667K(2580480K), [Metaspace: 3502K-&gt;3502K(1056768K)], 10.5294207 secs] [Times: user=23.63 sys=0.09, real=10.53 secs] (Allocation Failure)：Allocation Failure表示向young generation(eden)给新对象申请空间，但是young generation(eden)剩余的合适空间不够所需的大小导致的GC。 [PSYoungGen: 207268K-&gt;200192K(339968K)] 879015K-&gt;879170K(1489408K), 0.8137540 secs] 这段分别表示 [年轻代: GC前内存容量 -&gt; GC后内存容量 (年轻代总容量)] GC前堆内存大小 -&gt; GC后堆内存大小(堆内存总大小),该内存区域GC耗时（与Times的real相等），单位是秒。 [Times: user=2.95 sys=0.06, real=0.81 secs] 这段分别表示用户态耗时，内核态耗时和总耗时。 Full GC (Ergonomics) 表明该次发生了Full GC，Ergonomics就是Full GC的原因，可以认为如果晋升到老生代的平均大小大于老生代的剩余大小，则认为需要一次full gc。某些垃圾回收器会负责自动的调解gc暂停时间和吞吐量之间的平衡，然后JVM虚拟机性能更好，因而会出现这种Full GC原因。 ParOldGen部分表示年老代的相关GC信息。 Metaspace部分表示元空间的相关GC信息。 我们在GC相关源码(openjdk源码中gcCause.cpp文件)中还可以看到多种GC原因，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include "precompiled.hpp"#include "gc/shared/gcCause.hpp"const char* GCCause::to_string(GCCause::Cause cause) &#123; switch (cause) &#123; case _java_lang_system_gc: return "System.gc()"; case _full_gc_alot: return "FullGCAlot"; case _scavenge_alot: return "ScavengeAlot"; case _allocation_profiler: return "Allocation Profiler"; case _jvmti_force_gc: return "JvmtiEnv ForceGarbageCollection"; case _gc_locker: return "GCLocker Initiated GC"; case _heap_inspection: return "Heap Inspection Initiated GC"; case _heap_dump: return "Heap Dump Initiated GC"; case _wb_young_gc: return "WhiteBox Initiated Young GC"; case _wb_conc_mark: return "WhiteBox Initiated Concurrent Mark"; case _wb_full_gc: return "WhiteBox Initiated Full GC"; case _update_allocation_context_stats_inc: case _update_allocation_context_stats_full: return "Update Allocation Context Stats"; case _no_gc: return "No GC"; case _allocation_failure: return "Allocation Failure"; case _tenured_generation_full: return "Tenured Generation Full"; case _metadata_GC_threshold: return "Metadata GC Threshold"; case _metadata_GC_clear_soft_refs: return "Metadata GC Clear Soft References"; case _cms_generation_full: return "CMS Generation Full"; case _cms_initial_mark: return "CMS Initial Mark"; case _cms_final_remark: return "CMS Final Remark"; case _cms_concurrent_mark: return "CMS Concurrent Mark"; case _old_generation_expanded_on_last_scavenge: return "Old Generation Expanded On Last Scavenge"; case _old_generation_too_full_to_scavenge: return "Old Generation Too Full To Scavenge"; case _adaptive_size_policy: return "Ergonomics"; case _g1_inc_collection_pause: return "G1 Evacuation Pause"; case _g1_humongous_allocation: return "G1 Humongous Allocation"; case _dcmd_gc_run: return "Diagnostic Command"; case _last_gc_cause: return "ILLEGAL VALUE - last gc cause - ILLEGAL VALUE"; default: return "unknown GCCause"; &#125; ShouldNotReachHere();&#125; 这儿就不在对上面的所有GC情况做详细介绍了，有兴趣的同学可以查阅相关资料了解。 gcCause相关资料： openjdk-gcCause.cpp jvm-gcCause-info 我们再来看下出现OOM后GC的dump日志部分。 123456789Heap PSYoungGen total 584192K, used 10301K [0x0000000780700000, 0x00000007ba880000, 0x00000007c0000000) eden space 257536K, 3% used [0x0000000780700000,0x000000078110f510,0x0000000790280000) from space 326656K, 0% used [0x00000007a6980000,0x00000007a6980000,0x00000007ba880000) to space 347136K, 0% used [0x0000000790280000,0x0000000790280000,0x00000007a5580000) ParOldGen total 2083840K, used 2054113K [0x0000000701400000, 0x0000000780700000, 0x0000000780700000) object space 2083840K, 98% used [0x0000000701400000,0x000000077e9f8790,0x0000000780700000) Metaspace used 3535K, capacity 4506K, committed 4864K, reserved 1056768K class space used 392K, capacity 394K, committed 512K, reserved 1048576K 它们打印的JVM终止时Heap（堆内存）的信息，从该日志中我们能分析出JVM终止的一些原因。 可以看到PSYoungGen（年轻代） eden区使用了3%，（两个Survivor）from和to区使用了0%，ParOldGen（年老代） object space（对象区）使用了98%，Metaspace（元空间） class space（类加载区）的使用情况。 因此明显由于创建了大量对象，一直存在，无法被垃圾回收，导致内存空间用尽，出现OOM异常。 JVM监控Demo现在有许多JVM监控工具，如JConsole、Java VisualVM等，我们这里不过多介绍。 我们自写一个监控Demo来看下JVM在内存使用过程中的一些变化特点。 来看下java.lang.management包下的一些类。 MemoryMXBean ： 它里面有两个方法 getHeapMemoryUsage （获取堆内存使用情况）和getNonHeapMemoryUsage（获取非堆内存使用情况），返回MemoryUsage对象。 MemoryUsage：包含init（初始化了多少内存）、used（使用了多少内存）、committed（申请了多少内存）、max（最大内存）信息。 MemoryPoolMXBean：这里包含young（eden和survivor）、old等内存区的使用情况，我们可以通过 ManagementFactory.getMemoryPoolMXBeans() 获取到一个 MemoryPoolMXBean 列表，MemoryPoolMXBean里还有一个getName方法可以获得当前区域的名称。 GarbageCollectorMXBean：这个是垃圾收集相关的Bean，可以通过ManagementFactory.getGarbageCollectorMXBeans()获取其列表。其getName方法可以获得垃圾收集器的名称，getCollectionCount可以获得当前已经进行了多少次垃圾收集，getCollectionTime返回垃圾收集时间。 我们写一个测试类来看一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class JVMMonitorMemoryTest &#123; //定时任务线程 private static final ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(1); private ScheduledFuture future = null; private JvmTest jvmTest = new JvmTest(); public void doMonitor()&#123; future = executorService.scheduleAtFixedRate(()-&gt;&#123; JSONObject jsonObject = new JSONObject(); MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean(); jsonObject.put("totalMaxMemery", memoryMXBean.getHeapMemoryUsage().getMax()&gt;&gt;10&gt;&gt;10); jsonObject.put("totalUsedMemery", memoryMXBean.getHeapMemoryUsage().getUsed()&gt;&gt;10&gt;&gt;10); jsonObject.put("totalInitMemery", memoryMXBean.getHeapMemoryUsage().getInit()&gt;&gt;10&gt;&gt;10); //这里会返回老年代，新生代等内存区的使用情况 List&lt;MemoryPoolMXBean&gt; memoryPoolMXBeans = ManagementFactory.getMemoryPoolMXBeans(); memoryPoolMXBeans.forEach((pool) -&gt; &#123; String poolName = pool.getName().trim(); long max = pool.getUsage().getMax()&gt;&gt;10&gt;&gt;10; long used = pool.getUsage().getUsed()&gt;&gt;10&gt;&gt;10; long init = pool.getUsage().getInit()&gt;&gt;10&gt;&gt;10; long maxPeak = pool.getPeakUsage().getMax()&gt;&gt;10&gt;&gt;10; long usedPeak = pool.getPeakUsage().getUsed()&gt;&gt;10&gt;&gt;10; long initPeak = pool.getPeakUsage().getInit()&gt;&gt;10&gt;&gt;10; JSONObject poolJSON = new JSONObject(); poolJSON.put("max", max); poolJSON.put("used", used); poolJSON.put("init", init); poolJSON.put("maxPeak", maxPeak); poolJSON.put("usedPeak", usedPeak); poolJSON.put("initPeak", initPeak); if("PS Eden Space".equalsIgnoreCase(poolName))&#123; jsonObject.put("eden", poolJSON); &#125;else if("PS Survivor Space".equalsIgnoreCase(poolName))&#123; jsonObject.put("survivor", poolJSON); &#125;else if("PS Old Gen".equalsIgnoreCase(poolName))&#123; jsonObject.put("old", poolJSON); &#125;else if("Metaspace".equalsIgnoreCase(poolName))&#123; jsonObject.put("metaspace",poolJSON); &#125; &#125;); //垃圾收集相关 List&lt;GarbageCollectorMXBean&gt; garbageCollectorMXBeans = ManagementFactory.getGarbageCollectorMXBeans(); garbageCollectorMXBeans.forEach(collector -&gt; &#123; String gcName = collector.getName(); long gcCount = collector.getCollectionCount(); long gcTime = collector.getCollectionTime(); JSONObject gcJSON = new JSONObject(); gcJSON.put("gcCount", gcCount); gcJSON.put("gcTime", gcTime); if(gcName.toLowerCase().contains("scavenge"))&#123; jsonObject.put("edenGc", gcJSON); &#125;else if(gcName.toLowerCase().contains("marksweep"))&#123; jsonObject.put("oldGc", gcJSON); &#125; &#125;); System.out.println(JSON.toJSONString(jsonObject)); &#125;, 1, 1, TimeUnit.SECONDS); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //调用生成对象的方法 jvmTest.test(); &#125; public static void main(String[] args) &#123; JVMMonitorMemoryTest jvmMonitorMemoryTest = new JVMMonitorMemoryTest(); jvmMonitorMemoryTest.doMonitor(); &#125;&#125;//创建一个方法不停生产对象public class JvmTest &#123; public void test() &#123; List&lt;JvmTest&gt; list = new ArrayList&lt;&gt;(); try&#123; while (true)&#123; list.add(new JvmTest()); if(list.size()&gt;10000)&#123; list = new ArrayList&lt;&gt;(); TimeUnit.MILLISECONDS.sleep(100); &#125; &#125; &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 我们运行后可以看到相关输出信息，即JVM堆内存变化情况及垃圾收集情况。 数字数据不是很直观，我们结合Echart图表，动态展示JVM相关信息，因此我们把项目改造下，结合WebSocket来实现。 项目大致结构如下： 说一下里面的关键部分，MonitorJVMMemory.java（监控JVM内存变化类）和jvm-echart.js（Echart前端动态展示）。 MonitorJVMMemory相关代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135@ServerEndpoint("/websocket/jvm/monitor")public class MonitorJVMMemory implements ServletContextListener &#123; /** * 日志 */ private static final Logger logger = LoggerFactory.getLogger(MonitorJVMMemory.class); //定时任务线程 private static final ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(1); private ScheduledFuture future = null; private JvmTest jvmTest = new JvmTest(); /** * websocket会话 */ private Session session; @OnOpen public void init(Session session) &#123; this.session = session; future = executorService.scheduleAtFixedRate(() -&gt; &#123; JSONObject jsonObject = new JSONObject(); MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean(); jsonObject.put("totalMaxMemery", memoryMXBean.getHeapMemoryUsage().getMax() &gt;&gt; 10 &gt;&gt; 10); jsonObject.put("totalUsedMemery", memoryMXBean.getHeapMemoryUsage().getUsed() &gt;&gt; 10 &gt;&gt; 10); jsonObject.put("totalInitMemery", memoryMXBean.getHeapMemoryUsage().getInit() &gt;&gt; 10 &gt;&gt; 10); //这里会返回老年代，新生代等内存区的使用情况，按需自取就好 List&lt;MemoryPoolMXBean&gt; memoryPoolMXBeans = ManagementFactory.getMemoryPoolMXBeans(); memoryPoolMXBeans.forEach((pool) -&gt; &#123; String poolName = pool.getName().trim(); long max = pool.getUsage().getMax() &gt;&gt; 10 &gt;&gt; 10; long used = pool.getUsage().getUsed() &gt;&gt; 10 &gt;&gt; 10; long init = pool.getUsage().getInit() &gt;&gt; 10 &gt;&gt; 10; long maxPeak = pool.getPeakUsage().getMax() &gt;&gt; 10 &gt;&gt; 10; long usedPeak = pool.getPeakUsage().getUsed() &gt;&gt; 10 &gt;&gt; 10; long initPeak = pool.getPeakUsage().getInit() &gt;&gt; 10 &gt;&gt; 10; JSONObject poolJSON = new JSONObject(); poolJSON.put("max", max); poolJSON.put("used", used); poolJSON.put("init", init); poolJSON.put("maxPeak", maxPeak); poolJSON.put("usedPeak", usedPeak); poolJSON.put("initPeak", initPeak); if ("PS Eden Space".equalsIgnoreCase(poolName)) &#123; jsonObject.put("eden", poolJSON); &#125; else if ("PS Survivor Space".equalsIgnoreCase(poolName)) &#123; jsonObject.put("survivor", poolJSON); &#125; else if ("PS Old Gen".equalsIgnoreCase(poolName)) &#123; jsonObject.put("old", poolJSON); &#125; &#125;); //垃圾收集 List&lt;GarbageCollectorMXBean&gt; garbageCollectorMXBeans = ManagementFactory.getGarbageCollectorMXBeans(); garbageCollectorMXBeans.forEach(collector -&gt; &#123; String gcName = collector.getName(); long gcCount = collector.getCollectionCount(); long gcTime = collector.getCollectionTime(); JSONObject gcJSON = new JSONObject(); gcJSON.put("gcCount", gcCount); gcJSON.put("gcTime", gcTime); if (gcName.toLowerCase().contains("scavenge")) &#123; jsonObject.put("edenGc", gcJSON); &#125; else if (gcName.toLowerCase().contains("marksweep")) &#123; jsonObject.put("oldGc", gcJSON); &#125; &#125;); try &#123; session.getBasicRemote().sendText(jsonObject.toJSONString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;, 1, 1, TimeUnit.SECONDS); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; jvmTest.test(); &#125; /** * 接收信息 */ @OnMessage public void acceptMessage(String message) &#123; logger.info("Accept&gt;&gt;&gt;" + message); &#125; /** * 关闭会话 */ @OnClose public void closeSession(CloseReason closeReason) &#123; this.destory(); logger.info(closeReason.getReasonPhrase()); &#125; /** * 异常处理 */ @OnError public void errorHandler(Throwable e) &#123; this.destory(); logger.info("MonitorJVMMemory websocket error ：" + e.getMessage()); &#125; /** * 关闭资源 */ private void destory() &#123; try &#123; if (future != null &amp;&amp; !future.isCancelled()) &#123; future.cancel(true); &#125; if (session != null) &#123; session.close(); &#125; &#125; catch (Exception e) &#123; logger.error("destory", e); &#125; &#125; @Override public void contextInitialized(ServletContextEvent servletContextEvent) &#123; &#125; @Override public void contextDestroyed(ServletContextEvent servletContextEvent) &#123; jvmTest.stop(); executorService.shutdownNow(); &#125;&#125; 可以看到我们使用了Websocket，当连接Open后，使用定长线程池，里面维护一个每隔1s调用一次的方法，来查看当前内存情况，并使用jvmTest.test()来生成测试对象。 线程池里运行的线程执行的就是我们上面JVMMonitorMemoryTest类的doMonitor方法。 再看下jvm-echart.js： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134//echartvar memoryEchart = echarts.init(document.getElementById('memory_main'));var memoryData = new Array(3);//定义图表样式var memoryOption = &#123; tooltip : &#123; trigger: 'axis', axisPointer : &#123; // 坐标轴指示器，坐标轴触发有效 type : 'shadow' // 默认为直线，可选为：'line' | 'shadow' &#125;, formatter: function (params) &#123; var dataIndex = params[0].dataIndex; var res = params[0].axisValue; if(dataIndex==0 || dataIndex==1)&#123; res += '&lt;br/&gt;累计回收次数：' + params[0].data; res += '&lt;br/&gt;累计回收时间：' + params[1].data + "ms"; res += '&lt;br/&gt;平均回收时间：' + parseInt(params[1].data/params[0].data) + "ms"; &#125;else&#123; res += '&lt;br/&gt;已用内存量：' + params[0].data + "MB"; if(params[0].axisValue!='峰值内存消耗')&#123; var maxData = memoryData[2]; res += '&lt;br/&gt;可用内存量：' + params[1].data + "MB"; res += '&lt;br/&gt;最大内存量：' + maxData[dataIndex] + "MB"; &#125; &#125; return res; &#125; &#125;, color: ['#ff0000','#91C7AE'], legend: &#123; data: ['已用内存(MB)', '可用内存(MB)'] &#125;, grid: &#123; left: '3%', right: '4%', bottom: '3%', containLabel: true &#125;, xAxis: &#123; type: 'value' &#125;, yAxis: &#123; type: 'category', data: ['OldGenGC','EdenGC','Old Gen','Survivor Space','Eden Space','峰值内存消耗','JVM总内存'] &#125;, series: [ &#123; name: '已用内存(MB)', type: 'bar', stack: '总量', barWidth: 60, label: &#123; normal: &#123; show: true, position: 'insideRight' &#125; &#125;, data: memoryData[0] &#125;, &#123; name: '可用内存(MB)', type: 'bar', stack: '总量', barWidth: 60, label: &#123; normal: &#123; show: true, position: 'insideRight' &#125; &#125;, data: memoryData[1] &#125; ]&#125;;memoryEchart.setOption(memoryOption);//刷新图表数据function refreshMemoryData() &#123; memoryEchart.setOption(&#123; series: [&#123; data: memoryData[0] &#125;,&#123; data: memoryData[1] &#125;] &#125;);&#125;//与websocket建立连接var memorySocket;function initMemorySocket() &#123; if(memorySocket!=undefined || memorySocket!=null)&#123; memorySocket.close("3000", "断开连接"); &#125; var wsUrl = 'ws://'+$('#hid_host').val()+'/websocket/jvm/monitor'; console.log(wsUrl); memorySocket = new WebSocket(wsUrl); memorySocket.onopen = function (evt) &#123; console.log("Connection the jvm monitor server success!!!"); &#125;; memorySocket.onmessage = function (evt) &#123; var memory = $.parseJSON(evt.data); var peakUsed = memory.old.usedPeak + memory.eden.usedPeak; var usedData = [memory.oldGc.gcCount, memory.edenGc.gcCount, memory.old.used, memory.survivor.used, memory.eden.used, peakUsed, memory.totalUsedMemery]; var peakMax = parseInt((memory.old.maxPeak + memory.eden.maxPeak)*0.8); var usable = [memory.oldGc.gcTime, memory.edenGc.gcTime, memory.old.max-memory.old.used, memory.survivor.max-memory.survivor.used, memory.eden.max-memory.eden.used, 0, memory.totalMaxMemery-memory.totalUsedMemery]; var maxData = [memory.oldGc.gcTime, memory.edenGc.gcTime, memory.old.max, memory.survivor.max, memory.eden.max, 0, memory.totalMaxMemery]; memoryData[0] = usedData; memoryData[1] = usable; memoryData[2] = maxData; refreshMemoryData(); &#125;; memorySocket.onerror = function (evt) &#123; memorySocket.close(); &#125;;&#125;//断开监控连接function closeMemoryMonitor() &#123; if(memorySocket!=undefined || memorySocket!=null)&#123; memorySocket.close("3000", "断开连接"); &#125; memoryData[0] = []; memoryData[1] = []; memoryData[2] = []; refreshMemoryData(); $.messager.show(&#123; title: '系统提示', msg: '已断开监控连接！'&#125;); console.log("Disconnect the jvm monitor server success!!!");&#125;$(function () &#123; initMemorySocket();&#125;); 这个就是在解析后台数据构造Echart图表，这儿就不详细介绍了。 详细源码可以在 jvm-monitor-memory 看到。 我们可以简单看下运行效果图，可以看到JVM进行垃圾回收后内存的变化情况。 总结通过这篇文章，我们了解了JVM垃圾回收的一些运行原理，对JVM堆内存有了更深入的认识。了解虚拟机内存及垃圾回收的一些特性，有助于我们在工作过程中排查定位问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Lookup注解]]></title>
    <url>%2Fblog%2FSpring-Lookup%E6%B3%A8%E8%A7%A3.html</url>
    <content type="text"><![CDATA[前言今天我们看下Spring的Lookup注解，这个注解可以使Spring替换一个bean原有的，获取其它对象具体的方法，并自动返回在容器中的查找结果。 正文在了解它之前，我们先来看下一个例子。 我们有一个Bean，TestClassB，它是多例的。大致如下： 1234567@Component@Scope(value = SCOPE_PROTOTYPE)public class TestClassB &#123; public void printClass() &#123; System.out.println("This is TestClass B: " + this); &#125;&#125; 现在一个单例Bean，TestClassA，使用到了TestClassB，代码大致如下： 12345678910@Componentpublic class TestClassA &#123; @Autowired private TestClassB testClassB; public void printClass() &#123; System.out.println("This is TestClass A: " + this); testClassB.printClass(); &#125;&#125; 我们进行下测试，可以发现一些问题。 123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class LookUpTests &#123; @Autowired private TestClassA testClassA; @Test public void test()&#123; for (int i=0;i&lt;5;i++)&#123; testClassA.printClass(); &#125; &#125;&#125; 可以看到输出结果： 12345678910This is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2a 对于TestClassA，它因为是单例，所以一直是一个实例，我们是可以理解的，但是对于TestClassB，我们明明设置了多例，但是我们发现它仍是一个实例，相当于单例。 对于这种情况的产生，很好理解，因为TestClassA为单例，因此TestClassB只有一次注入的机会，即在生成单例TestClassA的时候，因此导致了TestClassB的多例不体现，仍相当于个单例模式。 如果我们要求TestClassB必须为多例的，那么上面这种情况是会出现问题的。 如何解决这种问题呢？ 其实我们最常用的一种方法是拿到SpringContext，然后手动获取Bean。代码大致如下： 1234567891011121314@Componentpublic class SpringUtils implements ApplicationContextAware &#123; private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; SpringUtils.applicationContext=applicationContext; &#125; public static ApplicationContext getApplicationContext()&#123; return applicationContext; &#125; public static &lt;T&gt; T getBean(Class&lt;T&gt; clazz)&#123; return applicationContext.getBean(clazz); &#125;&#125; 同时TestClassA里的方法如下： 1234567@Componentpublic class TestClassA &#123; public void printClass()&#123; System.out.println("This is TestClass A: " + this); SpringUtils.getBean(TestClassB.class).printClass(); &#125;&#125; 可以看到输出结果,TestClassB已经是多例的了。 12345678910This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@1640c151This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@5d5b5fa7This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@2a32fb6This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@6107165This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@164a62bf 当然也可以让TestClassA继承ApplicationContextAware直接拿到ApplicationContext，然后获取TestClassB。 这种方式在项目使用中还是比较多的，也是很方便的。 还有其他方法吗？ 下面我们来说另一种方式，当然就是我们今天的主角，Lookup注解。那具体如何使用呢，我们来看下。 1234567891011@Componentpublic class TestClassA &#123; @Lookup public TestClassB getTestClassB() &#123; return null; &#125; public void printClass() &#123; System.out.println("This is TestClass A: " + this); getTestClassB().printClass(); &#125;&#125; 我们运行测试类，可以看到输出结果： 12345678910This is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@1aac188dThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@7026b7eeThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@2d23faefThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@7cb8437dThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@62a4417 发现TestClassB是多例的，可是我们根据上面的代码，感觉TestClassB返回应该不是null么。 当然，这就要来了解下Lookup注解了，我们分析一下它的源码： 123456@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Lookup &#123; String value() default "";&#125; 可以看到该注解作用于方法上，有一个参数value，这个值可以指定要look up的Bean的名字。如果不指定，就会默认方法返回的类型寻找Bean并进行Look up。 我们在Spring源码中寻找下该注解 @Lookup，会发现只有一个地方使用到了该注解。 在AutowiredAnnotationBeanPostProcessor类的determineCandidateConstructors方法里。该方法部分内容如下： 12345678910111213141516171819202122232425262728293031323334public Constructor&lt;?&gt;[] determineCandidateConstructors(Class&lt;?&gt; beanClass, final String beanName) throws BeanCreationException &#123; // Let's check for lookup methods here.. if (!this.lookupMethodsChecked.contains(beanName)) &#123; try &#123; ReflectionUtils.doWithMethods(beanClass, new ReflectionUtils.MethodCallback() &#123; @Override public void doWith(Method method) throws IllegalArgumentException, IllegalAccessException &#123; Lookup lookup = method.getAnnotation(Lookup.class); if (lookup != null) &#123; LookupOverride override = new LookupOverride(method, lookup.value()); try &#123; RootBeanDefinition mbd = (RootBeanDefinition) beanFactory.getMergedBeanDefinition(beanName); mbd.getMethodOverrides().addOverride(override); &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanCreationException(beanName, "Cannot apply @Lookup to beans without corresponding bean definition"); &#125; &#125; &#125; &#125;); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, "Lookup method resolution failed", ex); &#125; catch (NoClassDefFoundError err) &#123; throw new BeanCreationException(beanName, "Failed to introspect bean class [" + beanClass.getName() + "] for lookup method metadata: could not find class that it depends on", err); &#125; this.lookupMethodsChecked.add(beanName); &#125; //部分代码略...... 可以看到Spring会首先判断该Bean是否有Lookup注解的方法，现在缓存里看，缓存没有的话会尝试获取方法上的Lookup注解，如果存在，拿到需要重写（覆盖）的方法信息放入LookupOverride，最后为RootBeanDefinition添加LookupOverride的属性。这个方法最终会被AbstractAutowireCapableBeanFactory类中的createBeanInstance方法调用，去生成新的Bean并重写，实现改变Bean的效果。 因此原理大致为：方法执行返回的对象，使用 Spring 内原有的这类对象替换，通过改变方法返回值来动态改变方法。内部实现为使用 cglib 方法，重新生成子类，重写配置的方法和返回对象，达到动态改变的效果。因此Bean的多列特性也被体现了。 总结通过解决一个单例Bean（无状态Bean）调用多例Bean（有状态Bean）的问题，我们了解了Lookup注解的一些简单用法，对Spring也有了一些深入的认识。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>Lookup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringRetry框架简介]]></title>
    <url>%2Fblog%2FSpringRetry%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言今天我们来聊一聊Spring Retry框架。 Spring Retry提供了一个关于重试失败操作的抽象，强调对流程和基于策略的行为的声明性控制，易于扩展和定制。例如，对于一个操作，如果它失败了，我们可以根据异常的类型，使用一个固定的或指数级的回退来重试它。 并不是所有的异常失败都适合重试，比如参数校验错误，显然不适合重试，而Spring Retry可以指定要重试的异常类型，对于指定类型的异常进行重试。 考虑到网络原因，可能一些方法失败后不立即进行下一次重试，而等待若干时间后再进行，Spring Retry里也支持此种类型的重试。 可能所有的重试都不成功，此时需要返回一个程序默认值或者直接抛出异常等，Spring Retry的兜底函数可以解决此类问题。 另外Spring Retry还支持简单的熔断策略。 正文说了这么多，我们来看下Spring Retry吧。 要使用Spring Retry，首先要引入相关jar包，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt; &lt;version&gt;1.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 先简单的写一个例子。 123456789101112131415161718192021222324252627282930313233343536public class Test &#123; public static void main(String[] args) &#123; RetryTemplate template = new RetryTemplate(); //重试策略：次数重试策略 SimpleRetryPolicy retryPolicy = new SimpleRetryPolicy(); retryPolicy.setMaxAttempts(3); template.setRetryPolicy(retryPolicy); //退避策略：固定退避策略 FixedBackOffPolicy backOffPolicy = new FixedBackOffPolicy(); backOffPolicy.setBackOffPeriod(4*1000L); template.setBackOffPolicy(backOffPolicy); String str = ""; //当重试失败后，执行RecoveryCallback String result2 = template.execute((retryCallback)-&gt;&#123; System.out.println("retry count:" + retryCallback.getRetryCount()); return getStr(str); &#125;, (recoveryCallback)-&gt; &#123; System.out.println("所有重试均失败!!"); return ""; &#125;); System.out.println("返回值为："+result2); &#125; /** * 测试方法 * @param str * @return */ public static String getStr(String str)&#123; if(StringUtils.isBlank(str))&#123; throw new RuntimeException("数据为空！"); &#125; return str; &#125;&#125; 上面的代码，当我们传入空或者空字符串时，可以看到程序会重试3次（每隔4s），均不成功，最后返回recoveryCallback的数据。 12345678910111218:05:20.879 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=0retry count:018:05:24.889 [main] DEBUG org.springframework.retry.support.RetryTemplate - Checking for rethrow: count=118:05:24.889 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=1retry count:118:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Checking for rethrow: count=218:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=2retry count:218:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Checking for rethrow: count=318:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry failed last attempt: count=3所有重试均失败!!返回值为：失败 我们来看下例子中涉及到的一些东西。 可以看到，要使用重试功能，首先要创建一个RetryTemplate，并设置它的两个重要参数：重试策略（RetryPolicy）和退避策略（BackOffPolicy）。 重试策略这两个策略还是比较好理解的，对于重试策略，指的就是请求不成功后下次请求的策略。很明显我们可以看到它是一个接口RetryPolicy。 这个接口里比较重要的一个方法为canRetry，它的返回值决定下一次是否重试。 1boolean canRetry(RetryContext context); 对于这个接口，可以看到它目前有8种重试策略。 NeverRetryPolicy 只调用被执行方法一次，不会进行重试操作。 我们可以看到它的canRetry方法。可以看到这个方法会一直返回false。 123456789101112131415161718 public boolean canRetry(RetryContext context) &#123; return !((NeverRetryContext) context).isFinished();&#125; private static class NeverRetryContext extends RetryContextSupport &#123; private boolean finished = false; public NeverRetryContext(RetryContext parent) &#123; super(parent); &#125; public boolean isFinished() &#123; return finished; &#125; public void setFinished() &#123; this.finished = true; &#125;&#125; AlwaysRetryPolicy 如果被执行方法调用不成功会一直重试，这种方法如果操作不当会出现死循环的情况，应当注意。 我们可以看到它里面的canRetry方法一直返回true，即如果调用失败，会一直重试直到成功。 123 public boolean canRetry(RetryContext context) &#123; return true;&#125; SimpleRetryPolicy 固定次数重试策略，默认最多重试3次，我们可以通过指定其maxAttempts参数的值来规定最多重试多少次。 它的canRetry方法可以看到和当前已重试次数做了比较来确定下一次是否重试。 1234 public boolean canRetry(RetryContext context) &#123; Throwable t = context.getLastThrowable(); return (t == null || retryForException(t)) &amp;&amp; context.getRetryCount() &lt; maxAttempts;&#125; TimeoutRetryPolicy 超时重试策略，只有在超时时间内才可以重试，超过后就不会再进行重试，超时时间可以认为是在第一次请求开始时计数。默认超时时间1000ms，我们可以通过设置timeout的值来指定超时时间。 它的canRetry方法，可以看到时间的对比来确定是否进行重试。 123456789101112131415161718 public boolean canRetry(RetryContext context) &#123; return ((TimeoutRetryContext) context).isAlive();&#125; private static class TimeoutRetryContext extends RetryContextSupport &#123; private long timeout; private long start; public TimeoutRetryContext(RetryContext parent, long timeout) &#123; super(parent); this.start = System.currentTimeMillis(); this.timeout = timeout; &#125; public boolean isAlive() &#123; return (System.currentTimeMillis() - start) &lt;= timeout; &#125;&#125; CompositeRetryPolicy 组合重试策略，有乐观重试和悲观重试两种情况。可以看到它有两个参数，optimistic和policies。 optimistic表示是否乐观，默认false。 policies表示所有传入的重试策略。 我们根据它的canRetry方法，可以清楚的知道，如果乐观情况下，有一个策略（policies[i]）canRetry为true就可以进行重试，悲观情况下只有所有的传入的重试策略canRetry为true才可以进行重试。 123456789101112131415161718192021222324 public boolean canRetry(RetryContext context) &#123; RetryContext[] contexts = ((CompositeRetryContext) context).contexts; RetryPolicy[] policies = ((CompositeRetryContext) context).policies; boolean retryable = true; if(this.optimistic) &#123; retryable = false; for (int i = 0; i &lt; contexts.length; i++) &#123; if (policies[i].canRetry(contexts[i])) &#123; retryable = true; &#125; &#125; &#125; else &#123; for (int i = 0; i &lt; contexts.length; i++) &#123; if (!policies[i].canRetry(contexts[i])) &#123; retryable = false; &#125; &#125; &#125; return retryable;&#125; ExpressionRetryPolicy 异常重试策略，会对抛出指定异常的情况下进行重试，继承SimpleRetryPolicy。可以指定要重试的异常参数expression，也可以指定异常的全名字符串，会被转化为指定异常。 我们看一下它的canRetry方法。 12345678910 public boolean canRetry(RetryContext context) &#123; Throwable lastThrowable = context.getLastThrowable(); if (lastThrowable == null) &#123; return super.canRetry(context); &#125; else &#123; return super.canRetry(context) &amp;&amp; this.expression.getValue(this.evaluationContext, lastThrowable, Boolean.class); &#125;&#125; 可以看到除了使用了SimpleRetryPolicy的canRetry判断还有对是不是当前异常的判断，来确定是否重试。 当然这个策略也是可以指定最大重试次数maxAttempts的。 ExceptionClassifierRetryPolicy 根据最新的异常动态的适应注入的策略，需要设置参数exceptionClassifier。 比如第一次重试时，抛出异常A，对应传入策略A，当第二次重试时，抛出异常B，则对应传入的策略B。 1234 public boolean canRetry(RetryContext context) &#123; RetryPolicy policy = (RetryPolicy) context; return policy.canRetry(context);&#125; 可以看到它的canRetry返回值取决于当前使用的策略的canRetry方法的返回值，而策略的动态切换由ExceptionClassifierRetryContext这个类来处理，这儿不再过多介绍。 CircuitBreakerRetryPolicy 带有熔断的重试策略，该策略提供过载保护功能，它的canRetry代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 public boolean canRetry(RetryContext context) &#123; CircuitBreakerRetryContext circuit = (CircuitBreakerRetryContext) context; //如果熔断器处于打开状态，就直接短路，返回失败 if (circuit.isOpen()) &#123; circuit.incrementShortCircuitCount(); return false; &#125; else &#123; //重置熔断器 circuit.reset(); &#125; return this.delegate.canRetry(circuit.context);&#125; //------- isOpen方法如下 public boolean isOpen() &#123; long time = System.currentTimeMillis() - this.start; boolean retryable = this.policy.canRetry(this.context); //当前不允许重试 if (!retryable) &#123; //如果已经超过重置时间，重新闭合，关闭熔断器 if (time &gt; this.timeout) &#123; logger.trace("Closing"); this.context = createDelegateContext(policy, getParent()); this.start = System.currentTimeMillis(); retryable = this.policy.canRetry(this.context); &#125; // 如果小于熔断器打开时间，读取关闭状态，如果熔断器是关闭的，就打开熔断器，重置熔断计时器 else if (time &lt; this.openWindow) &#123; if ((Boolean) getAttribute(CIRCUIT_OPEN) == false) &#123; logger.trace("Opening circuit"); setAttribute(CIRCUIT_OPEN, true); &#125; this.start = System.currentTimeMillis(); return true; &#125; &#125; //允许重试 else &#123; //判断是否在openWindow熔断器电路打开的超时时间之外，超过打开时间，就重置上下文，并且返回false if (time &gt; this.openWindow) &#123; logger.trace("Resetting context"); this.start = System.currentTimeMillis(); this.context = createDelegateContext(policy, getParent()); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace("Open: " + !retryable); &#125; setAttribute(CIRCUIT_OPEN, !retryable); return !retryable;&#125; 它接受三个参数，delegate、resetTimeout和openTimeout。 delegate指使用的重试策略，默认使用SimpleRetryPolicy。 resetTimeout表示重置线路超时时间(以毫秒为单位)。当线路打开后，它会在此时间过后重新关闭，上下文将重新启动。 openTimeout表示断开线路的超时时间。如果委托策略无法重试，则自上下文启动以来经过的时间小于此时间，则打开线路。 退避策略我们再来看一下退避策略（BackOffPolicy）。 退避策略接口（BackOffPolicy）目前有5种已实现策略。如下图： 我们来分别看一下它们。 要实现退避策略，重要的是实现接口的backoff方法。 1234public interface BackOffPolicy &#123; BackOffContext start(RetryContext context); void backOff(BackOffContext backOffContext) throws BackOffInterruptedException;&#125; 这个方法的实现有两个主要类，抽象类StatelessBackOffPolicy和实现类ExponentialBackOffPolicy。 如图： StatelessBackOffPolicy 这是用于在调用之间不维护任何状态的退避策略实现的简单基类，它的backoff方法调用了子类的doBackOff方法。 123public final void backOff(BackOffContext backOffContext) throws BackOffInterruptedException &#123;doBackOff();&#125; 可以看到它的三个实现并简单分析，如下图。 NoBackOffPolicy 无任何退避策略，可以看到doBackOff方法什么也没做。 12 protected void doBackOff() throws BackOffInterruptedException &#123;&#125; 这种情况下，如果一次重试不成功，下一次会直接再进行重试。 FixedBackOffPolicy 固定退避策略，这种情况下，一次重试不成功，下一次会间隔一段时间后在进行重试。 可以看到它可以通过设置backOffPeriod（退避间隔）来指定与下一次重试的间隔时间。这个值默认为1000ms。 这个类里面另一个比较重要的参数为Sleeper（休眠器），它可以指定程序的休眠方式，默认使用ThreadWaitSleeper休眠器。 可以看到它的doBackOff方法直接调用了休眠器的sleep方法休眠一段时间。 12345678 protected void doBackOff() throws BackOffInterruptedException &#123; try &#123; sleeper.sleep(backOffPeriod); &#125; catch (InterruptedException e) &#123; throw new BackOffInterruptedException("Thread interrupted while sleeping", e); &#125;&#125; UniformRandomBackOffPolicy 随机休眠退避策略，当一次重试失败后，下一次重试之前，这个策略会随机退避一段时间。 看到这个我们明显就知道它会有minBackOffPeriod（最小退避时间）和maxBackOffPeriod（最大退避时间）两个值了。最小退避值默认500ms，最大退避值默认1500ms。 除了上面两个参数，它里面比较重要的两个参数一个是取值器和休眠器。 12 private Random random = new Random(System.currentTimeMillis());private Sleeper sleeper = new ThreadWaitSleeper(); 上面代码可以看到它们的值（random取值器不可人为修改）。 再来看下doBackOff方法。 123456789 protected void doBackOff() throws BackOffInterruptedException &#123; try &#123; long delta = maxBackOffPeriod==minBackOffPeriod ? 0 : random.nextInt((int) (maxBackOffPeriod - minBackOffPeriod)); sleeper.sleep(minBackOffPeriod + delta ); &#125; catch (InterruptedException e) &#123; throw new BackOffInterruptedException("Thread interrupted while sleeping", e); &#125;&#125; 也是比较好理解的，可以看到当最大时间和最小时间相等时，delta=0，即每次重试之前都休眠minBackOffPeriod时间。 ExponentialBackOffPolicy 指数型退避策略，顾名思义，它的退避时间是指数增长的。 我们来看下它的三个参数，initialInterval 初始时间间隔，maxInterval 最大时间间隔，multiplier指数因子。 来看一下它的backOff方法： 1234567891011121314 public void backOff(BackOffContext backOffContext) throws BackOffInterruptedException &#123; ExponentialBackOffContext context = (ExponentialBackOffContext) backOffContext; try &#123; long sleepTime = context.getSleepAndIncrement(); if (logger.isDebugEnabled()) &#123; logger.debug("Sleeping for " + sleepTime); &#125; sleeper.sleep(sleepTime); &#125; catch (InterruptedException e) &#123; throw new BackOffInterruptedException("Thread interrupted while sleeping", e); &#125;&#125; 以及它涉及到的下面的方法： 1234567891011121314 public synchronized long getSleepAndIncrement() &#123; long sleep = this.interval; if (sleep &gt; maxInterval) &#123; sleep = maxInterval; &#125; else &#123; this.interval = getNextInterval(); &#125; return sleep;&#125;protected long getNextInterval() &#123; return (long) (this.interval * this.multiplier);&#125; 可以看到逻辑很好理解，默认退避时间为interval，如果interval超过maxInterval，退避时间就为maxInterval，否则就获取下一次的interval时间，这个时间就是interval*multiplier，所以退避时间会以指数增长。 它的另一个参数Sleeper（休眠器）默认也是ThreadWaitSleeper。 initialInterval初始时间默认值为100ms，maxInterval最大时间默认为30000ms，multiplier指数因子默认为2. ExponentialRandomBackOffPolicy 随机指数退避策略，对于上面的指数策略，这儿不一样的就是指数因子会随机变化。 我们大致看一下这个策略的源码。 12345678910111213141516171819202122 public class ExponentialRandomBackOffPolicy extends ExponentialBackOffPolicy &#123; //部分代码略 @Overridestatic class ExponentialRandomBackOffContext extends ExponentialBackOffPolicy.ExponentialBackOffContext &#123; private final Random r = new Random(); public ExponentialRandomBackOffContext(long expSeed, double multiplier, long maxInterval) &#123; super(expSeed, multiplier, maxInterval); &#125; @Override public synchronized long getSleepAndIncrement() &#123; long next = super.getSleepAndIncrement(); next = (long) (next * (1 + r.nextFloat() * (getMultiplier() - 1))); return next; &#125;&#125; //部分代码略 &#125; 可以看到它继承了ExponentialBackOffPolicy，并重写了ExponentialBackOffContext里的getSleepAndIncrement方法，原来的指数因子改为随机的了。 其它与ExponentialBackOffPolicy一致，这儿不再介绍。 RetryTemplate再来看下重试模板RetryTemplate，除了上面说到RetryPolicy和BackOffPolicy，它还有几个比较重要的参数。 RetryListener ：可以传入一个listener数组，主要功能是用于监控重试行为。 RetryCallback ：重试回调，用户包装业务流，第一次执行和产生重试执行都会调用这个callback代码。 RecoveryCallback ：当所有重试都失败后，回调该接口，提供给业务重试回复机制。 RetryState ：重试状态，对于一些有事务的方法，如果出现某些异常，可能需要回滚而不是进行重试，这个参数可以完成这一功能。 RetryContext ： 重试上下文，每次重试都会将其作为参数传入RetryCallback中使用。 然后我们大致来看下RetryTemplate的部分关键代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192protected &lt;T, E extends Throwable&gt; T doExecute(RetryCallback&lt;T, E&gt; retryCallback, RecoveryCallback&lt;T&gt; recoveryCallback, RetryState state) throws E, ExhaustedRetryException &#123; //拿到重试策略和退避策略 RetryPolicy retryPolicy = this.retryPolicy; BackOffPolicy backOffPolicy = this.backOffPolicy; //初始化重试上下文 RetryContext context = open(retryPolicy, state); RetrySynchronizationManager.register(context); Throwable lastException = null; boolean exhausted = false; try &#123; //执行切面方法，在执行业务之前可以通过listener进行监控 boolean running = doOpenInterceptors(retryCallback, context); if (!running) &#123; throw new TerminatedRetryException( "Retry terminated abnormally by interceptor before first attempt"); &#125; //确定退避上下文环境 BackOffContext backOffContext = null; Object resource = context.getAttribute("backOffContext"); if (resource instanceof BackOffContext) &#123; backOffContext = (BackOffContext) resource; &#125; if (backOffContext == null) &#123; backOffContext = backOffPolicy.start(context); if (backOffContext != null) &#123; context.setAttribute("backOffContext", backOffContext); &#125; &#125; //开始重试循环 //如果重试策略认为可以重试 while (canRetry(retryPolicy, context) &amp;&amp; !context.isExhaustedOnly()) &#123; try &#123; //清空上次的异常 lastException = null; //执行业务方法 return retryCallback.doWithRetry(context); &#125; catch (Throwable e) &#123; //出现异常，最新异常就是此次异常 lastException = e; try &#123; //将异常信息通知到retryPolicy、state和context registerThrowable(retryPolicy, state, context, e); &#125; catch (Exception ex) &#123; throw new TerminatedRetryException("Could not register throwable", ex); &#125; finally &#123; //对于重试出现的异常，我们使用切面listener进行监听 doOnErrorInterceptors(retryCallback, context, e); &#125; //如果重试策略认为还可以重试 if (canRetry(retryPolicy, context) &amp;&amp; !context.isExhaustedOnly()) &#123; try &#123; //执行退避策略 backOffPolicy.backOff(backOffContext); &#125; catch (BackOffInterruptedException ex) &#123; lastException = e; throw ex; &#125; &#125; //确认下是否需要重新抛出（对于有事务的逻辑，重新抛出指定异常方便事务回滚） if (shouldRethrow(retryPolicy, context, state)) &#123; throw RetryTemplate.&lt;E&gt;wrapIfNecessary(e); &#125; &#125; if (state != null &amp;&amp; context.hasAttribute(GLOBAL_STATE)) &#123; break; &#125; &#125; //重试完成后，执行recoveryCallback操作 exhausted = true; return handleRetryExhausted(recoveryCallback, context, state); &#125; catch (Throwable e) &#123; throw RetryTemplate.&lt;E&gt;wrapIfNecessary(e); &#125; finally &#123; //关闭 close(retryPolicy, context, state, lastException == null || exhausted); //使用切面listener进行监控关闭等流程 doCloseInterceptors(retryCallback, context, lastException); RetrySynchronizationManager.clear(); &#125; &#125; 上述代码中用到的一些方法如下： 1234567891011121314151617181920212223242526272829303132 //该异常是否抛出protected boolean shouldRethrow(RetryPolicy retryPolicy, RetryContext context, RetryState state) &#123; return state != null &amp;&amp; state.rollbackFor(context.getLastThrowable());&#125; //监听open操作private &lt;T, E extends Throwable&gt; boolean doOpenInterceptors( RetryCallback&lt;T, E&gt; callback, RetryContext context) &#123; boolean result = true; for (RetryListener listener : this.listeners) &#123; result = result &amp;&amp; listener.open(context, callback); &#125; return result;&#125; //监听close操作private &lt;T, E extends Throwable&gt; void doCloseInterceptors( RetryCallback&lt;T, E&gt; callback, RetryContext context, Throwable lastException) &#123; for (int i = this.listeners.length; i-- &gt; 0;) &#123; this.listeners[i].close(context, callback, lastException); &#125;&#125; //监听error操作private &lt;T, E extends Throwable&gt; void doOnErrorInterceptors( RetryCallback&lt;T, E&gt; callback, RetryContext context, Throwable throwable) &#123; for (int i = this.listeners.length; i-- &gt; 0;) &#123; this.listeners[i].onError(context, callback, throwable); &#125;&#125; 根据上面的描述，RetryTemplate的执行流程大致如下： 其它我们把开始提到的例子复杂化下。引入Listener和RetryState参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Test &#123; public static void main(String[] args) &#123; RetryTemplate template = new RetryTemplate(); //重试策略：次数重试策略 SimpleRetryPolicy retryPolicy = new SimpleRetryPolicy(); retryPolicy.setMaxAttempts(3); template.setRetryPolicy(retryPolicy); //退避策略：固定退避策略 FixedBackOffPolicy backOffPolicy = new FixedBackOffPolicy(); backOffPolicy.setBackOffPeriod(1000L); template.setBackOffPolicy(backOffPolicy); //设置有状态重试 BinaryExceptionClassifier classifier = new BinaryExceptionClassifier( Collections.singleton(NullPointerException.class) ); RetryState state = new DefaultRetryState("rollbackKey", false, classifier); //设置监听 DefaultStatisticsRepository defaultStatisticsRepository =new DefaultStatisticsRepository(); template.setListeners(new RetryListener[]&#123;new StatisticsListener(defaultStatisticsRepository)&#125;); String str = "1"; //当重试失败后，执行RecoveryCallback String result2 = template.execute((retryCallback)-&gt;&#123; System.out.println("retry count:" + retryCallback.getRetryCount()); retryCallback.setAttribute(RetryContext.NAME,"method.key"); return getStr(str); &#125;, (recoveryCallback)-&gt; &#123; System.out.println("所有重试均失败!!"); return "失败"; &#125;,state); RetryStatistics statistics = defaultStatisticsRepository.findOne("method.key"); System.out.println(statistics); &#125; public static String getStr(String str)&#123; if(StringUtils.isBlank(str))&#123; throw new NullPointerException("数据为空！"); &#125; if("1".equals(str))&#123; throw new RuntimeException("参数为1！！！"); &#125; return str; &#125;&#125; 我们对str赋值1和””，可以清楚的看到输出的日志。当赋值””时，执行一次后直接抛出空指针异常，不会再进行重试。如果调用的方法有事务，可以进行回滚等操作，这就是有状态的重试。 当str=”1”时，可以看到监听分析的结果： 1DefaultRetryStatistics [name=method.key, startedCount=0, completeCount=0, recoveryCount=1, errorCount=3, abortCount=0] 重试注解Spring Retry也支持使用注解的形式标注。如下： EnableRetry12345678@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@EnableAspectJAutoProxy(proxyTargetClass = false)@Import(RetryConfiguration.class)@Documentedpublic @interface EnableRetry &#123; boolean proxyTargetClass() default false;&#125; proxyTargetClass指是否使用CGLIB增强代理，默认false。 这个注解作用在类上，如果想要某个方法可以进行重试，则这个方法所在的类需要有EnableRetry注解。 Retryable内容如下： 123456789101112131415@Target(&#123; ElementType.METHOD, ElementType.TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Retryable &#123; String interceptor() default ""; Class&lt;? extends Throwable&gt;[] value() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] include() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] exclude() default &#123;&#125;; String label() default ""; boolean stateful() default false; int maxAttempts() default 3; String maxAttemptsExpression() default ""; Backoff backoff() default @Backoff(); String exceptionExpression() default "";&#125; 该注解作用在方法上，指定的方法会进行重试操作。 参数说明： interceptor：拦截器 value：可以重试的异常类型，如果为空并且exclude为空，则会重试所有异常，与include同义。 include：与value同义。 exclude：不需要重试的异常。 label：分析报告的名称，listener相关使用。 stateful：是否有状态重试，有的话指定的异常要抛出而不是重试。 maxAttempts：最大重试次数。 maxAttemptsExpression：最大重试次数表达式。 backoff：退避策略，详见BackOff注解。 exceptionExpression：异常表达式，要抛出的异常（有状态情况下）的表达式。 Backoff1234567891011121314@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Import(RetryConfiguration.class)@Documentedpublic @interface Backoff &#123; long value() default 1000; long delay() default 0; long maxDelay() default 0; double multiplier() default 0; String delayExpression() default ""; String maxDelayExpression() default ""; String multiplierExpression() default ""; boolean random() default false;&#125; 退避策略注解，使用方式见上面Retryable的backoff值。 主要参数说明： value：退避间隔，和delay同义。 delay：与value同义。在随机退避策略里表示最小值，在指数退避策略和随机指数退避策略里表示起始值。 maxDelay：在随机退避策略里表示最大值，在指数退避策略和随机指数退避策略里表示最大值。 multiplier：指数因子。 delayExpression：退避间隔表达式。 maxDelayExpression：最大值表达式。 multiplierExpression：指数因子表达式。 random：是否随机。 可以看到，如果什么也不设置，将使用NoBackOffPolicy。如果只设置value或者delay值，将使用FixedBackOffPolicy。如果还设置了maxDelay和random，将使用UniformRandomBackOffPolicy…… CircuitBreaker熔断注解。 12345678910111213@Target(&#123; ElementType.METHOD, ElementType.TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Retryable(stateful = true)public @interface CircuitBreaker &#123; Class&lt;? extends Throwable&gt;[] value() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] include() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] exclude() default &#123;&#125;; int maxAttempts() default 3; String label() default ""; long resetTimeout() default 20000; long openTimeout() default 5000;&#125; 主要参数与上面说的Retryable基本说明一样，它的其它两个参数resetTimeout和openTimeout上面已经讲过。 Recover123456@Target(&#123; ElementType.METHOD, ElementType.TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Import(RetryConfiguration.class)@Documentedpublic @interface Recover &#123;&#125; 这个注解也作用于方法上，表示所有重试失败后兜底的返回信息，这个作用的方法，应该有以下特性： 第一个参数是重试的程序抛出的异常（需要重试的异常）。 后面的参数应该与Retryable注释的入参一致，返回值也应一致。 第一个参数可选，但是如果不写，需要保证Retryable在没有其他的Recover匹配的情况下才会被调用。 我们使用注解来简单写个例子，如下： 123456789101112131415161718192021@Service@Slf4j@EnableRetrypublic class RetryTest &#123; @Transactional @Retryable(value = Exception.class,maxAttempts = 5,backoff = @Backoff(2000L)) public String retryTest(String str1,Integer integer1)&#123; if(StringUtils.isBlank(str1))&#123; throw new NullPointerException("str1参数为空！！"); &#125; if(integer1==null)&#123; throw new RuntimeException("integer1参数不正确！！！"); &#125; return str1+integer1; &#125; @Recover public String recover(Exception e,String str1,Integer integer1)&#123; log.info("所有重试均失败，返回兜底值",e); return ""; &#125;&#125; 可以看到我们创建了一个重试方法，这个方法最多重试5次，每重试一次之前都会退避2s后再进行，重试所有异常，当所有重试均不成功后会返回兜底值””。 总结通过对Spring Retry框架的理解，我们对重试框架有了一个更全面的认识，了解了它的一些简单实现原理，明白了它的一些关键参数。如果有方法有重试需求，可以适当进行Spring Retry框架的考虑。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringRetry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java将文件上传到云服务器]]></title>
    <url>%2Fblog%2FJava%E5%B0%86%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%88%B0%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8.html</url>
    <content type="text"><![CDATA[前言今天总结了一个云上传的小demo特地来跟大家分享下。 这个demo可以将一些文件上传到云服务器。我们来看下吧。 正文我这里使用了阿里云、亚马逊S3和微软Azure这三种云上传做的demo。 要使用云上传，我们需要引入相关jar包，如下： 123456789101112131415161718192021222324&lt;!--阿里云OSS--&gt;&lt;dependency&gt; &lt;groupId&gt;com.aliyun.oss&lt;/groupId&gt; &lt;artifactId&gt;aliyun-sdk-oss&lt;/artifactId&gt; &lt;version&gt;2.2.3&lt;/version&gt;&lt;/dependency&gt;&lt;!--微软Azure--&gt;&lt;dependency&gt; &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt; &lt;artifactId&gt;azure-storage&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--亚马逊S3--&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazonaws&lt;/groupId&gt; &lt;artifactId&gt;aws-java-sdk&lt;/artifactId&gt; &lt;version&gt;1.8.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 腾讯云cos --&gt;&lt;dependency&gt; &lt;groupId&gt;com.qcloud&lt;/groupId&gt; &lt;artifactId&gt;cos_api&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt; PS:如果只使用一种云上传方式，引入对应的jar包即可，不必全部引入。 我们可以提供一个通用的上传接口upload，而具体的上传逻辑让各个实现类去实现。 同时我们暴露公共方法出来供上传使用。 上传的文件有可能是本地文件，也有可能是前端传过来的Base64图片字符串，也有可能是MultipartFile等。 我们提供一个抽象的上传方法，大致如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144public abstract class UploadAbstractUtil &#123; public static final Logger logger = LoggerFactory.getLogger(UploadAbstractUtil.class); /** * 文件缓存路径 */ private String basedir; public UploadAbstractUtil(String basedir) &#123; this.basedir = basedir; &#125; /** * 上传文件到云 * @param tempFile * @param realName * @return */ protected abstract String upload(File tempFile, String realName); /** * 上传文件到云 * @param file * @return */ abstract String upload(MultipartFile file); /** * 上传文件到云 * @param bytes * @param contentType * @return */ protected abstract String upload(byte[] bytes,String contentType); /** * 尝试初始化客户端 */ protected abstract void initClient(); /** * 生成一个唯一的上传文件名 * @param file * @return */ protected String generateUploadFileName(MultipartFile file)&#123; String name = file.getOriginalFilename(); String ext = name.substring(name.lastIndexOf(".")); String uuid = UUID.randomUUID().toString(); // 生成唯一的key return uuid + ext; &#125; /** * base64转为文件后在进行上传 * @param base64Str * @return */ public String base64UploadUseTempFile(String base64Str)&#123; String realName = UUID.randomUUID().toString() + ".jpg"; File file = new File(basedir + "/" + realName); if (!file.exists()) &#123; try &#123; file.createNewFile(); &#125;catch (IOException e)&#123; logger.error("文件上传，尝试创建文件时失败！！！",e); throw new RuntimeException("文件上传失败！！！"); &#125; &#125; boolean flag = Base64Utils.Base64ToImage(base64Str, file.getPath()); if(!flag)&#123; throw new RuntimeException("base64转换为文件时发生错误！！！"); &#125; String url; try &#123; long startTime = System.currentTimeMillis(); url = upload(file, realName); logger.info("tempFile---上传服务耗时------time:[&#123;&#125;ms]", System.currentTimeMillis()-startTime); &#125; catch (Exception e) &#123; logger.error("文件上传，上传文件时发生异常！！！",e); throw new RuntimeException("文件上传失败!"); &#125; if (file.exists()) &#123; file.delete(); &#125; return url; &#125; /** * 通用文件上传 * @param filePath 文件路径 * @return */ public String fileUpload(String filePath)&#123; if(StringUtils.isBlank(filePath))&#123; throw new RuntimeException("请输入正确的文件路径！"); &#125; File file = new File(filePath); if(!file.exists())&#123; throw new RuntimeException("请输入正确的文件路径！"); &#125; int position = filePath.lastIndexOf("."); String fileSuffix = ""; if(position &gt; 0)&#123; fileSuffix = filePath.substring(position); &#125; //上传到云上的文件名 String realName = UUID.randomUUID().toString() + fileSuffix; String url; try &#123; long startTime = System.currentTimeMillis(); url = upload(file, realName); logger.info("filePath---上传服务耗时------time:[&#123;&#125;ms]", System.currentTimeMillis()-startTime); &#125; catch (Exception e) &#123; logger.error("文件上传，上传文件时发生异常！！！",e); throw new RuntimeException("文件上传失败!"); &#125; return url; &#125; /** * 使用流来进行文件上传 * @param base64Str * @return */ public String base64UploadUseInputStream(String base64Str)&#123; byte[] bytes = Base64Utils.Base64ToByte(base64Str); String url; try &#123; long startTime = System.currentTimeMillis(); url = upload(bytes,"image/jpeg"); logger.info("Stream---上传服务耗时------time:[&#123;&#125;ms]", System.currentTimeMillis()-startTime); &#125; catch (Exception e) &#123; logger.error("文件上传，上传文件时发生异常！！！",e); throw new RuntimeException("文件上传失败!"); &#125; return url; &#125;&#125; PS： 这个类看着比较多……其实都是对文件进行处理，生成上传文件名，然后交给上传方法，开始写的时候代码较少，在学习优化的过程中不断添加新功能，导致了该结果。 上面的上传抽象类大致逻辑如下： 针对图片文件： 如果在服务器上的，可以直接获取到文件后进行上传。（fileUpload方法） 如果APP端传过来的Base64编码的图片文件，可以把它生成临时文件，然后进行上传，也可以直接把Base64转换为流后进行上传。（base64UploadUseTempFile方法和base64UploadUseInputStream方法） 如果APP端传过来MultipartFile文件，直接将其进行转换并上传。（upload(MultipartFile file)方法） 对于每种云上传，各个实现类具体如下： 阿里云OSS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133public class AliOssUploadUtil extends UploadAbstractUtil&#123; public static final Logger logger = LoggerFactory.getLogger(AliOssUploadUtil.class); /** * 阿里云accessKey */ private String aliyunaccessKey; /** * 阿里云secretKey */ private String aliyunsecretKey; /** * 阿里云endpoint */ private String aliyunendpoint; /** * 阿里云endpointexternal */ private String aliyunendpointexternal; /** * 阿里云bucket */ private String aliyunbucket; /** * ossClient */ private OSSClient ossClient; /** * 构造器 * @param basedir * @param aliyunaccessKey * @param aliyunsecretKey * @param aliyunendpoint * @param aliyunendpointexternal * @param aliyunbucket */ public AliOssUploadUtil(String basedir, String aliyunaccessKey, String aliyunsecretKey, String aliyunendpoint, String aliyunendpointexternal, String aliyunbucket) &#123; super(basedir); this.aliyunaccessKey = aliyunaccessKey; this.aliyunsecretKey = aliyunsecretKey; this.aliyunendpoint = aliyunendpoint; this.aliyunendpointexternal = aliyunendpointexternal; this.aliyunbucket = aliyunbucket; &#125; /** * 阿里云文件上传 * @param tempFile * @param realName * @return */ @Override protected String upload(File tempFile,String realName)&#123; initClient(); ossClient.putObject(aliyunbucket, realName, tempFile); URL url = ossClient.generatePresignedUrl(aliyunbucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("阿里云OSS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(aliyunendpoint, aliyunendpointexternal); logger.info("阿里云OSS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125; /** * 阿里云文件上传 * @param file * @return */ @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try (InputStream is = new ByteArrayInputStream(file.getBytes())) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(file.getSize()); metadata.setContentType(file.getContentType()); // 上传 ossClient.putObject(aliyunbucket,key, is, metadata); URL url = ossClient.generatePresignedUrl(aliyunbucket, key, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("阿里云OSS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(aliyunendpoint, aliyunendpointexternal); logger.info("阿里云OSS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用阿里云OSS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 尝试初始化client */ @Override protected void initClient() &#123; if(ossClient == null)&#123; ossClient = new OSSClient(aliyunendpoint, aliyunaccessKey, aliyunsecretKey); &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try (InputStream is = new ByteArrayInputStream(bytes)) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(is.available()); metadata.setContentType(contentType); // 上传 ossClient.putObject(aliyunbucket,realName, is, metadata); URL url = ossClient.generatePresignedUrl(aliyunbucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("阿里云OSS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(aliyunendpoint, aliyunendpointexternal); logger.info("阿里云OSS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用阿里云OSS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * shutdown Client */ public void shutdown()&#123; ossClient.shutdown(); &#125;&#125; 亚马逊S3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class AmazonS3UploadUtil extends UploadAbstractUtil&#123; /** * S3 accessKey */ private String s3accessKey; /** * S3 secretKey */ private String s3secretKey; /** * S3 endpoint */ private String s3endpoint; /** * S3 bucket */ private String s3bucket; /** * 协议 */ private static Protocol protocol = Protocol.HTTP; /** * 亚马逊s3 client */ private AmazonS3 client; /** * 构造器 * @param basedir * @param s3accessKey * @param s3secretKey * @param s3endpoint * @param s3bucket */ public AmazonS3UploadUtil(String basedir, String s3accessKey, String s3secretKey, String s3endpoint, String s3bucket) &#123; super(basedir); this.s3accessKey = s3accessKey; this.s3secretKey = s3secretKey; this.s3endpoint = s3endpoint; this.s3bucket = s3bucket; &#125; /** * 上传文件到 Amazon S3 * @param tempFile * @param realName * @return */ @Override protected String upload(File tempFile, String realName)&#123; try &#123; initClient(); client.setEndpoint(s3endpoint); client.setS3ClientOptions(new S3ClientOptions().withPathStyleAccess(true)); client.putObject(new PutObjectRequest(s3bucket, realName, tempFile) .withCannedAcl(CannedAccessControlList.AuthenticatedRead)); String imageUrl = "http://" + s3endpoint + "/" + s3bucket + "/" + realName; logger.info("亚马逊S3上传服务------图片url:[&#123;&#125;]", imageUrl); return imageUrl; &#125; catch (AmazonClientException e) &#123; logger.error("使用亚马逊S3上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 上传文件到S3 * @param file * @return */ @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try (InputStream is = new ByteArrayInputStream(file.getBytes()))&#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(file.getSize()); metadata.setContentType(file.getContentType()); PutObjectRequest mall = new PutObjectRequest(s3bucket, key, is, metadata) .withCannedAcl(CannedAccessControlList.AuthenticatedRead); // 上传 client.putObject(mall); return "http://" + s3endpoint + "/" + s3bucket + "/" + key; &#125;catch (IOException e)&#123; logger.error("使用亚马逊S3上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 尝试初始化S3Client */ @Override protected void initClient() &#123; if(client == null)&#123; AWSCredentials credential = new BasicAWSCredentials(s3accessKey, s3secretKey); ClientConfiguration clientConfig = new ClientConfiguration(); clientConfig.setProtocol(protocol); client = new AmazonS3Client(credential, clientConfig); &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try (InputStream is = new ByteArrayInputStream(bytes))&#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(is.available()); metadata.setContentType(contentType); PutObjectRequest mall = new PutObjectRequest(s3bucket, realName, is, metadata) .withCannedAcl(CannedAccessControlList.AuthenticatedRead); // 上传 client.putObject(mall); return "http://" + s3endpoint + "/" + s3bucket + "/" + realName; &#125;catch (IOException e)&#123; logger.error("使用亚马逊S3上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125;&#125; 微软Azure123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class AzureUploadUtil extends UploadAbstractUtil&#123; /** * 用户名 */ private String accountName; /** * 密码 */ private String accountKey; /** * endPoint */ private String endPoint; /** * containerName */ private String containerName; /** * 连接串 */ private String storageConnectionString; /** * Azure client */ private CloudBlobClient blobClient; /** * 构造器 * @param accountName * @param accountKey * @param endPoint * @param containerName */ public AzureUploadUtil(String basedir,String accountName, String accountKey, String endPoint, String containerName) &#123; super(basedir); this.accountName = accountName; this.accountKey = accountKey; this.endPoint = endPoint; this.containerName = containerName; this.storageConnectionString = "DefaultEndpointsProtocol=https;AccountName="+ accountName +";AccountKey="+ accountKey +";EndpointSuffix=" + endPoint; &#125; /** * 上传文件到Azure * @param * @return */ @Override protected String upload(File tempFile,String realName) &#123; try &#123; initClient(); CloudBlobContainer container = blobClient.getContainerReference(containerName); // Create the container if it does not exist with public access. container.createIfNotExists(BlobContainerPublicAccessType.CONTAINER, new BlobRequestOptions(), new OperationContext()); //Getting a blob reference CloudBlockBlob blob = container.getBlockBlobReference(tempFile.getName()); //Creating blob and uploading file to it blob.uploadFromFile(tempFile.getAbsolutePath()); String imageUrl = "https://"+ accountName +".blob."+ endPoint +"/"+ containerName +"/" + tempFile.getName(); logger.info("微软Azure上传服务------图片url:[&#123;&#125;]", imageUrl); return imageUrl; &#125; catch (IOException| StorageException | URISyntaxException e) &#123; logger.error("使用微软Azure上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 上传文件到Azure * @param file * @return */ @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try(InputStream is = new ByteArrayInputStream(file.getBytes()))&#123; CloudBlobContainer container = blobClient.getContainerReference(containerName); // Create the container if it does not exist with public access. container.createIfNotExists(BlobContainerPublicAccessType.CONTAINER, new BlobRequestOptions(), new OperationContext()); CloudBlockBlob blob = container.getBlockBlobReference(key); //Creating blob and uploading file to it blob.upload(is, is.available()); return "https://"+ accountName +".blob."+ endPoint +"/"+ containerName +"/" + key; &#125;catch (IOException| StorageException | URISyntaxException e)&#123; logger.error("使用微软Azure上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 尝试初始化client */ @Override protected void initClient()&#123; if(blobClient == null)&#123; try&#123; CloudStorageAccount storageAccount = CloudStorageAccount.parse(storageConnectionString); blobClient = storageAccount.createCloudBlobClient(); &#125;catch (URISyntaxException|InvalidKeyException e)&#123; logger.error("使用微软Azure初始化client失败！",e); throw new RuntimeException(e); &#125; &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try(InputStream is = new ByteArrayInputStream(bytes))&#123; CloudBlobContainer container = blobClient.getContainerReference(containerName); // Create the container if it does not exist with public access. container.createIfNotExists(BlobContainerPublicAccessType.CONTAINER, new BlobRequestOptions(), new OperationContext()); CloudBlockBlob blob = container.getBlockBlobReference(realName); //Creating blob and uploading file to it blob.upload(is, is.available()); return "https://"+ accountName +".blob."+ endPoint +"/"+ containerName +"/" + realName; &#125;catch (IOException| StorageException | URISyntaxException e)&#123; logger.error("使用微软Azure上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125;&#125; 腾讯云COS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class TencentCOSUploadUtil extends UploadAbstractUtil &#123; public static final Logger logger = LoggerFactory.getLogger(TencentCOSUploadUtil.class); /** * 腾讯云COS AccessKey */ private String qAccessKey; /** * 腾讯云COS SecretKey */ private String qSecretKey; /** * 腾讯云COS bucket */ private String qBucket; /** * 腾讯云COS region */ private String qRegion; /** * 腾讯云COS qEndpoint */ private String qEndpoint; /** * 腾讯云COS qEndpointExternal */ private String qEndpointExternal; /** * COSClient */ private COSClient cosClient; public TencentCOSUploadUtil(String basedir, String qAccessKey, String qSecretKey, String qBucket, String qRegion, String qEndpoint, String qEndpointExternal) &#123; super(basedir); this.qAccessKey = qAccessKey; this.qSecretKey = qSecretKey; this.qBucket = qBucket; this.qRegion = qRegion; this.qEndpoint = qEndpoint; this.qEndpointExternal = qEndpointExternal; &#125; @Override protected String upload(File tempFile, String realName) &#123; initClient(); cosClient.putObject(qBucket,realName,tempFile); URL url =cosClient.generatePresignedUrl(qBucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("腾讯云COS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(qEndpoint, qEndpointExternal); logger.info("腾讯云COS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125; @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try (InputStream is = new ByteArrayInputStream(file.getBytes())) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(file.getSize()); metadata.setContentType(file.getContentType()); // 上传 cosClient.putObject(qBucket,key, is, metadata); URL url = cosClient.generatePresignedUrl(qBucket, key, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("腾讯云COS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(qEndpoint, qEndpointExternal); logger.info("腾讯云COS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用腾讯云COS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try (InputStream is = new ByteArrayInputStream(bytes)) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(is.available()); metadata.setContentType(contentType); // 上传 cosClient.putObject(qBucket,realName, is, metadata); URL url = cosClient.generatePresignedUrl(qBucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("腾讯云COS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(qEndpoint, qEndpointExternal); logger.info("腾讯云COS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用腾讯云COS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * @Description: 初始化COSCilent * @Date: 2019/6/2 9:42 AM * @Params: [] * @Return: void */ @Override protected void initClient() &#123; if(cosClient==null)&#123; COSCredentials cosCredentials = new BasicCOSCredentials(qAccessKey,qSecretKey); ClientConfig clientConfig = new ClientConfig(new Region(qRegion)); cosClient = new COSClient(cosCredentials,clientConfig); &#125; &#125; /** * shutdown Client */ public void shutdown()&#123; cosClient.shutdown(); &#125;&#125; 用到的相关工具类如下： 1234567891011121314151617181920212223242526272829303132333435/** * base64字符串转换成图片 * 对字节数组字符串进行Base64解码并生成图片 * @param imgStr base64字符串 * @param imgFilePath 图片存放路径 * @return **/public static boolean Base64ToImage(String imgStr,String imgFilePath) &#123; // 图像数据为空 if (StringUtils.isEmpty(imgStr)) &#123; return false; &#125; //如果包含 data:image/jpeg;base64, 前缀需要去掉 if(imgStr.contains(","))&#123; imgStr = imgStr.split(",")[1]; &#125; BASE64Decoder decoder = new BASE64Decoder(); try &#123; // Base64解码 byte[] b = decoder.decodeBuffer(imgStr); for (int i = 0; i &lt; b.length; ++i) &#123; // 调整异常数据 if (b[i] &lt; 0) &#123; b[i] += 256; &#125; &#125; OutputStream out = new FileOutputStream(imgFilePath); out.write(b); out.flush(); out.close(); return true; &#125; catch (Exception e) &#123; return false; &#125;&#125; 结合SpringBoot，引入自动配置，生成相关上传Bean，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495@Configurationpublic class OssUtilsConfiguration &#123; @Value("$&#123;file.cache.dir&#125;") private String basedir; @Value("$&#123;file.upload.server.type&#125;") private String uploadServerType; @Autowired private Environment environment; /** * 亚马逊S3配置 */ private String s3accessKey; private String s3secretKey; private String s3endpoint; private String s3bucket; /** * 阿里云OSS配置 */ private String aliyunAccessKey; private String aliyunSecretKey; private String aliyunBucket; private String aliyunEndpoint; private String aliyunEndpointexternal; /** * 微软Azure配置 */ private String azureAccountName; private String azureAccountKey; private String azureEndpointSuffix; private String azureContainerName; /** * 腾讯云COS配置 */ private String qAccessKey; private String qSecretKey; private String qBucket; private String qRegion; private String qEndpoint; private String qEndpointExternal; /** * 根据配置的 file.upload.server.type 选择一个上传服务器 * @return */ @Bean public UploadAbstractUtil uploadAbstractUtil()&#123; //可以根据枚举进行配置 使用阿里云或者亚马逊S3或者Azure UploadServerEnum uploadServerEnum = UploadServerEnum.getEnum(uploadServerType); UploadAbstractUtil uploadAbstractUtil; switch (uploadServerEnum)&#123; //亚马逊s3 case AMAZON: s3accessKey = environment.getRequiredProperty("s3.accessKey"); s3secretKey = environment.getRequiredProperty("s3.secretKey"); s3endpoint = environment.getRequiredProperty("s3.endpoint"); s3bucket = environment.getRequiredProperty("s3.bucket"); uploadAbstractUtil = new AmazonS3UploadUtil(basedir,s3accessKey,s3secretKey,s3endpoint,s3bucket); return uploadAbstractUtil; //阿里云OSS case ALIOSS: aliyunAccessKey = environment.getRequiredProperty("aliyun.accessKey"); aliyunSecretKey = environment.getRequiredProperty("aliyun.secretKey"); aliyunBucket = environment.getRequiredProperty("aliyun.bucket"); aliyunEndpoint = environment.getRequiredProperty("aliyun.endpoint"); aliyunEndpointexternal = environment.getRequiredProperty("aliyun.endpointexternal"); uploadAbstractUtil = new AliOssUploadUtil(basedir,aliyunAccessKey,aliyunSecretKey,aliyunEndpoint,aliyunEndpointexternal,aliyunBucket); return uploadAbstractUtil; //微软Azure case AZURE: azureAccountName = environment.getRequiredProperty("azure.accountName"); azureAccountKey = environment.getRequiredProperty("azure.accountKey"); azureEndpointSuffix = environment.getRequiredProperty("azure.endpointSuffix"); azureContainerName = environment.getRequiredProperty("azure.containerName"); uploadAbstractUtil = new AzureUploadUtil(basedir,azureAccountName,azureAccountKey,azureEndpointSuffix,azureContainerName); return uploadAbstractUtil; case TENCENTCOS: qAccessKey = environment.getRequiredProperty("tencent.accessKey"); qSecretKey = environment.getRequiredProperty("tencent.secretKey"); qBucket = environment.getRequiredProperty("tencent.bucket"); qEndpoint = environment.getRequiredProperty("tencent.endpoint"); qRegion = environment.getRequiredProperty("tencent.region"); qEndpointExternal = environment.getRequiredProperty("tencent.endpointexternal"); uploadAbstractUtil = new TencentCOSUploadUtil(basedir,qAccessKey,qSecretKey,qBucket,qRegion,qEndpoint,qEndpointExternal); return uploadAbstractUtil; default: throw new RuntimeException("暂不支持其他类型的云上传！！！"); &#125; &#125;&#125; UploadServerEnum 枚举如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public enum UploadServerEnum &#123; /** * 阿里云OSS */ ALIOSS("aliyun_oss","阿里云OSS"), /** * 亚马逊s3 */ AMAZON("amazon_s3","亚马逊S3"), /** * 微软azure */ AZURE("azure","微软Azure"), /** * 腾讯云cos */ TENCENTCOS("tencent_cos","腾讯云cos"); private String value; private String desc; UploadServerEnum(String value, String desc) &#123; this.value = value; this.desc = desc; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public String getDesc() &#123; return desc; &#125; public void setDesc(String desc) &#123; this.desc = desc; &#125; /** * 获取一个枚举 * @param value * @return */ public static UploadServerEnum getEnum(String value)&#123; return Arrays.stream(UploadServerEnum.values()).filter(e-&gt;e.value.equals(value)).findFirst().get(); &#125;&#125; 我们看到代码比较多…… 其实几种云上传的核心只要理解，便非常清楚了。它们的大致步骤如下： 根据配置信息创建上传client 上传文件（有多种方式，直接上传文件或根据文件流来上传等） 上传结果，获取上传文件路径等等。 如需关闭client，需要关闭client。 总结通过学习如何进行文件云上传，我们掌握了云上传的方法，也可以体验到一些封装、继承、多态的好处，总的来说是蛮不错的一次体验。 项目地址： ossutils-spring-boot]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>云上传</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java序列化和反序列化的几种方式]]></title>
    <url>%2Fblog%2FJava%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言在Java中，我们知道可以随意创建对象，只要对象未被GC回收，我们都可以继续在程序里使用，但这些对象只是存在于JVM内存中的，我们JVM一旦停止，这些对象就消失不见了。 经常有些时候，我们需要把这些对象持久化下来，再次需要时，再重新把对象读取出来，Java中有一种机制，对象序列化机制（object serialization）便可以帮我们完成相关功能。 对象序列化，可以方便的把对象状态保存为字节数组，可以通过字节流进行远程网络传输等，接收到字节流，通过反序列化机制，可以将字节数组转换为相关对象。 常说的RPC远程调用，相关传输对象的生成类就必须实现序列化以便在网络间传输。 正文Serializable接口在Java中，我们最常用的实现序列化和反序列化的方法就是相关类实现 java.io.Serializable 接口了，这也是Java给我们提供的一个方便的API。 我们创建一个Apple类，实现序列化接口，通过测试，可以看到相关对象生成的字节码和反序列化后的对象。 123456789101112131415161718192021222324252627public class Apple implements Serializable &#123; private String color; private int weight; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; public int getWeight() &#123; return weight; &#125; public void setWeight(int weight) &#123; this.weight = weight; &#125; public Apple(String color, int weight) &#123; this.color = color; this.weight = weight; &#125; @Override public String toString() &#123; return "Apple&#123;" + "color='" + color + '\'' + ", weight=" + weight + '&#125;'; &#125;&#125; 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) throws IOException,ClassNotFoundException&#123; byte [] bytes = null; Apple apple = new Apple("red",150); //序列化 try(ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(baos) )&#123; oos.writeObject(apple); bytes = baos.toByteArray(); for (byte b : baos.toByteArray()) &#123; System.out.print(Byte.toString(b) + " "); &#125; &#125; System.out.println(); //反序列化 try ( ByteArrayInputStream bais = new ByteArrayInputStream(bytes); ObjectInputStream ois = new ObjectInputStream(bais) ) &#123; System.out.println(ois.readObject().toString()); &#125; &#125;&#125; 测试输出结果： 当我们去掉Apple类的Serializable接口后，执行测试会抛出异常，说明对象无法被序列化。 序列化ID（serialVersionUID）JVM虚拟机是否可以对某个对象进行反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的点是两个类的序列化ID是否一致（就是 private static final long serialVersionUID)。 序列化ID有两种生成策略，一个是固定的 1L，一个是随机生成一个不重复的 long 类型数据（实际上是使用 JDK 工具生成），在这里有一个建议，如果没有特殊需求，就是用默认的 1L 就可以，这样可以确保代码一致时反序列化成功。那么随机生成的序列化ID有什么作用呢，有些时候，通过改变序列化 ID 可以用来限制某些用户的使用。 序列化的实现方式在Java中，我们还可以利用其它方式对对象进行序列化，我总结了几种序列化方式如下。 让我们一起来看一下： 我们提供一个序列化与反序列化通用接口 123456789101112131415161718192021public interface Serializer &#123; /** * 序列化名称 * @return */ String name(); /** * 序列化 * @param obj * @return * @throws IOException */ byte[] serialize(Object obj) throws IOException ; /** * 反序列化 * @param bytes * @return * @throws IOException */ Object deserialize(byte[] bytes) throws IOException ;&#125; 标准的Java序列化123456789101112131415161718192021222324252627282930public class JavaSerializer implements Serializer &#123; @Override public String name() &#123; return "java"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; try( ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(baos); )&#123; oos.writeObject(obj); return baos.toByteArray(); &#125; &#125; @Override public Object deserialize(byte[] bits) throws IOException&#123; if(bits == null || bits.length == 0) &#123; return null; &#125; try ( ByteArrayInputStream bais = new ByteArrayInputStream(bits); ObjectInputStream ois = new ObjectInputStream(bais); )&#123; return ois.readObject(); &#125;catch (ClassNotFoundException e)&#123; throw new RuntimeException(e); &#125; &#125;&#125; 可以看到序列化就是我们将对象通过ObjectOutputStream转化为ByteArrayOutputStream字节流，反序列化就是将字节流转换为对象流并读取。 FST实现序列化需要引入相关jar包 12345&lt;dependency&gt; &lt;groupId&gt;de.ruedigermoeller&lt;/groupId&gt; &lt;artifactId&gt;fst&lt;/artifactId&gt; &lt;version&gt;2.57&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930public class FSTSerializer implements Serializer &#123; @Override public String name() &#123; return "fst"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; try ( ByteArrayOutputStream out = new ByteArrayOutputStream(); FSTObjectOutput fout = new FSTObjectOutput(out); )&#123; fout.writeObject(obj); fout.flush(); return out.toByteArray(); &#125; &#125; @Override public Object deserialize(byte[] bytes) throws IOException &#123; if(bytes == null || bytes.length == 0) &#123; return null; &#125; try ( FSTObjectInput in = new FSTObjectInput(new ByteArrayInputStream(bytes)); )&#123; return in.readObject(); &#125; catch (ClassNotFoundException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; Kryo实现序列化需要引入相关jar包 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;5.0.0-RC4&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223242526272829public class KryoSerializer implements Serializer &#123; private final static Kryo kryo = new Kryo(); @Override public String name() &#123; return "kryo"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; try (ByteArrayOutputStream baos = new ByteArrayOutputStream(); Output output = new Output(baos)) &#123; kryo.register(obj.getClass()); kryo.writeClassAndObject(output, obj); output.flush(); return baos.toByteArray(); &#125; &#125; @Override public Object deserialize(byte[] bits) throws IOException &#123; if (bits == null || bits.length == 0) &#123; return null; &#125; try (ByteArrayInputStream bais = new ByteArrayInputStream(bits); Input ois = new Input(bais) ) &#123; return kryo.readClassAndObject(ois); &#125; &#125;&#125; KryoPool实现序列化由于kryo创建的代价相对较高，我们可以使用一个KryoPool池来管理Kryo，使用空间换取时间，提高运行效率。 我们使用一个双端队列来对Kryo进行管理，相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155public class KryoPoolSerializer implements Serializer&#123; /** * Kryo 的包装 */ private static class KryoHolder &#123; private Kryo kryo; static final int BUFFER_SIZE = 1024; /** * reuse */ private Output output = new Output(BUFFER_SIZE, -1); private Input input = new Input(); KryoHolder(Kryo kryo) &#123; this.kryo = kryo; &#125; &#125; interface KryoPool &#123; /** * get o kryo object * @return KryoHolder instance */ KryoHolder get(); /** * return object * @param kryo holder */ void offer(KryoHolder kryo); &#125; /** * 由于kryo创建的代价相对较高 ，这里使用空间换时间 * 对KryoHolder对象进行重用 */ public static class KryoPoolImpl implements KryoPool &#123; /** * default is 1500 * online server limit 3K */ /** * thread safe list */ private final Deque&lt;KryoHolder&gt; kryoHolderDeque=new ConcurrentLinkedDeque&lt;KryoHolder&gt;(); private KryoPoolImpl() &#123; &#125; /** * @return KryoPool instance */ public static KryoPool getInstance() &#123; return Singleton.pool; &#125; /** * get o KryoHolder object * * @return KryoHolder instance */ @Override public KryoHolder get() &#123; // Retrieves and removes the head of the queue represented by this table KryoHolder kryoHolder = kryoHolderDeque.pollFirst(); return kryoHolder == null ? creatInstnce() : kryoHolder; &#125; /** * create a new kryo object to application use * @return KryoHolder instance */ public KryoHolder creatInstnce() &#123; Kryo kryo = new Kryo(); kryo.setReferences(false); return new KryoHolder(kryo); &#125; /** * return object * Inserts the specified element at the tail of this queue. * * @param kryoHolder ... */ @Override public void offer(KryoHolder kryoHolder) &#123; kryoHolderDeque.addLast(kryoHolder); &#125; /** * creat a Singleton */ private static class Singleton &#123; private static final KryoPool pool = new KryoPoolImpl(); &#125; &#125; @Override public String name() &#123; return "Kryo_Pool"; &#125; /** * Serialize object * @param obj what to serialize * @return return serialize data */ @Override public byte[] serialize(Object obj) throws IOException &#123; KryoHolder kryoHolder = null; if (obj == null)&#123; throw new RuntimeException("obj can not be null"); &#125; try &#123; kryoHolder = KryoPoolImpl.getInstance().get(); kryoHolder.kryo.register(obj.getClass()); //reset Output --&gt;每次调用的时候 重置 kryoHolder.output.reset(); kryoHolder.kryo.writeClassAndObject(kryoHolder.output, obj); // 无法避免拷贝 ~~~ return kryoHolder.output.toBytes(); &#125; catch (RuntimeException e) &#123; e.printStackTrace(); throw new RuntimeException("Serialize obj exception"); &#125; finally &#123; KryoPoolImpl.getInstance().offer(kryoHolder); //GC obj = null; &#125; &#125; /** * Deserialize data * @param bytes what to deserialize * @return object */ @Override public Object deserialize(byte[] bytes) throws IOException &#123; KryoHolder kryoHolder = null; if (bytes == null)&#123; throw new RuntimeException("bytes can not be null"); &#125; try &#123; kryoHolder = KryoPoolImpl.getInstance().get(); //call it ,and then use input object ,discard any array kryoHolder.input.setBuffer(bytes, 0, bytes.length); return kryoHolder.kryo.readClassAndObject(kryoHolder.input); &#125; catch (RuntimeException e) &#123; throw new RuntimeException("Deserialize bytes exception"); &#125; finally &#123; KryoPoolImpl.getInstance().offer(kryoHolder); // for gc bytes = null; &#125; &#125;&#125; Jackson 实现序列化和反序列化Jackson也可以实现相关序列化和反序列化功能，需要引入jackson 的jar包。 使用writeValueAsBytes和readValue方法即可完成相关功能。 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt;&lt;/dependency&gt; 代码如下： 123456789101112131415161718public class JacksonSerializer implements Serializer&#123; private static final ObjectMapper mapper = new ObjectMapper(); @Override public String name() &#123; return "Jackson"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; return mapper.writeValueAsBytes(obj); &#125; @Override public Object deserialize(byte[] bytes) throws IOException &#123; return mapper.readValue(bytes,Object.class); &#125;&#125; FastJson实现序列化和反序列化FastJson实现序列化与反序列化，需要引入相关jar包，如下： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.58&lt;/version&gt;&lt;/dependency&gt; 相关代码如下： 12345678910111213141516public class FastJsonSerializer implements Serializer&#123; @Override public String name() &#123; return "FastJson"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; return JSON.toJSONString(obj, SerializerFeature.WriteClassName).getBytes(); &#125; @Override public Object deserialize(byte[] bytes) throws IOException&#123; return JSON.parse(new String(bytes), Feature.SupportAutoType); &#125;&#125; 以上的序列化与反序列化的对象都需要实现Serializable接口。 我们对上述代码进行相关测试： 12345678910111213141516171819202122232425public class SerializationUtils &#123; public static void main(String[] args) throws IOException &#123; Apple apple =new Apple(); apple.setColor("red"); apple.setWeight(100); printData(new JavaSerializer(),apple); printData(new FSTSerializer(),apple); printData(new KryoSerializer(),apple); printData(new KryoPoolSerializer(),apple); printData(new JacksonSerializer(),apple); printData(new FastJsonSerializer(),apple); &#125; public static void printData(Serializer serializer,Apple apple) throws IOException&#123; long start = System.currentTimeMillis(); byte[] bits = serializer.serialize(apple); System.out.println(serializer.name()+"序列化所需时间："+(System.currentTimeMillis()-start)+"ms"); System.out.println(serializer.name()+"序列化后字节码长度："+bits.length); long start1 = System.currentTimeMillis(); Object obj = serializer.deserialize(bits); System.out.println(serializer.name()+"反序列化所需时间："+(System.currentTimeMillis()-start1)+"ms"); System.out.println(serializer.name()+"反序列化后对象："+obj.toString()); &#125;&#125; 可以看到输出后的结果： 根据结果判断正确性后，也大致能看出各种序列化方式的一些优点和缺点。 JavaSerializer 明显的优点是不用引用包，也是Java程序默认的序列化方式，但是其序列化后占用空间是几种序列化方式里最大的，如果遇到大对象序列化，处理起来可能就比较力不从心了。 FSTSerializer、JacksonSerializer、FastJsonSerializer 它们是一种比较适中的序列化方式，序列化后的字节比Java方法少，时间也差不多。 KryoSerializer 是一种比较优异的序列化方式，可以看到它的序列化后的字节很短，占用空间少，且序列化和反序列化时间短。 KryoPoolSerializer 这种相当于KryoSerializer的改进版，利用了一部分内存空间，进一步降低了序列化和反序列化的时间。 正因为Kryo如此高效的序列化和反序列化性能，因此在大数据领域应用广泛。如Apache的spark、hive等。 如果需要更准确的结果比较各种序列化方式的性能，可以创建大量对象并对它们进行序列化记录时间等参数比较，这儿就不过多讨论了。 结语通过对序列化和反序列化的简单介绍，并比较了一些常用的序列化方式，我们对对象的序列化与反序列化有了更进一步的认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>序列化与反序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产者和消费者模式]]></title>
    <url>%2Fblog%2F%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言今天我们来学习下生产者与消费者模式。 生产者和消费者模式可以解决绝大多数并发问题，一般由生产者、数据缓冲区、消费者构成。 如下图，其原理是将原来的直接调用（消费者-&gt;生产者）变为了生产者生产数据放入缓存区，消费者从缓存区获取数据并消费这种模式。 可以知道MQ就是生产者与消费者模式的典型代表。 我们可以举例比如一个定时任务，每天要批处理数据，比如上传文件，每天如果要上传1000个文件或者更多，这时候我们使用平常的循环上传方法，明显大部分时间均浪费在了上传的时间上。 如果按照每个文件处理需要3s，1000个文件则至少需要3000s时间。 如果我们引入生产者和消费者模式，生产者部分负责查询组装数据并把它们放入数据缓存区，消费者部分负责处理数据并上传，可以大大提高并发性能。 使用生产者与消费者模式的典型优点如下： 并发支持 可以看到，如果消费者处理比较耗时，我们可以使用多个生产者生产数据或者消费者去处理队列数据，从而提高系统并发性能。即消费者和生产者可以为两个独立的并发主体。 解耦 我们将生产者和消费者分开后，即使生产者部分处理数据的逻辑有变化，也不会影响到消费者部分，而相比之前在一起的逻辑，我们可能需要改动整个业务部分以完成数据处理。即生产者和消费者没有过分的依赖关系，只要保证传输数据格式的正确性即可。 解决忙闲不均问题 可以看到生产者和消费者模式可以完美解决忙闲不均的问题，当生产者数据过多时，进入数据缓存区等待消费者慢慢处理，生产者数据少时，由于缓存区的数据，也不至于消费者无事可做。即无论生产者或者消费者谁快谁慢，我们总可以通过对他们的数量控制来均衡资源的分配。 正文我们通过上面的例子来实践下消费者和生产者模式。 我们正常逻辑可能如下： 123456789101112131415161718public static void main(String[] args) throws Exception&#123; //1. 组装数据 //数据库查询、组装数据过程略，由for循环插入数据代替 List&lt;String&gt; list = new ArrayList&lt;&gt;(); for(int i = 0;i&lt;1000;i++)&#123; //假设处理每条数据花费平均10ms时间 Thread.sleep(10); list.add(i+""); &#125; //2. 上传数据/文件 //上传过程略 for (int i =0;i&lt;list.size();i++)&#123; //假设每个文件平均耗时1s Thread.sleep(1000); System.out.println(i); &#125;&#125; 可以看到这个过程是非常耗时的，我们使用生产者和消费者模式来设计下这个业务场景。 我们数据缓存区使用队列来暂存数据，生产者组生产数据时会将数据放入队列，消费者消费数据时会从队列中获取数据。 我们用阻塞队列LinkedBlockingQueue来作为数据缓存区，写一个生产者放入数据和消费者取出数据的方法。 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class Context&lt;E&gt; &#123; private static final Logger log = LoggerFactory.getLogger(Context.class); //阻塞队列用来暂存数据 private final LinkedBlockingQueue&lt;E&gt; consumptionQueue = new LinkedBlockingQueue&lt;E&gt;(2500); // 生产线程的状态 private volatile ThreadState producersThreadState; // 消费线程的状态 private volatile ThreadState consumersThreadState; /** * 获取队列大小 * @return * @throws Exception */ int getConsumptionQueueSize() &#123; return consumptionQueue.size(); &#125; /** * 将指定元素插入到此队列的尾部，如有必要（队列空间已满且消费线程未停止运行），则等待空间变得可用。 * @param e * @return boolean true:插入成功;false:插入失败（消费线程已停止运行） * @throws Exception */ public boolean offerDataToConsumptionQueue(E e) throws Exception &#123; //设置生产者线程为运行 setProducersThreadState(ThreadState.RUNNING); // 如果消费线程停止了，不再生产数据 if (ThreadState.DEAD == this.getConsumersThreadState())&#123; return false; &#125; //一直尝试将数据放入队列 while (true) &#123; //将数据放入队列，如果成功返回成功 if (consumptionQueue.offer(e, 2, TimeUnit.SECONDS))&#123; return true; &#125; // 添加元素失败，很有可能是队列已满，再次检查消费线程是否工作中 // 如果消费线程停止了，不再生产数据 if (ThreadState.DEAD == this.getConsumersThreadState()) &#123; return false; &#125; &#125; &#125; /** * 获取并移除此队列的头，如果此队列为空且生产线程已停止，则返回 null * @return E 队列的头元素，如果队列为空且生产线程已停止则返回null * @throws Exception */ public E pollDataFromConsumptionQueue() throws Exception &#123; //设置消费者线程为运行 setConsumersThreadState(ThreadState.RUNNING); //一直尝试从队列里获取数据 while (true) &#123; //尝试从队列里获取数据 E e = consumptionQueue.poll(20, TimeUnit.MILLISECONDS); if (e != null)&#123; return e; &#125; // 没有从队列里获取到元素，并且生产线程已停止，则返回null if (ThreadState.DEAD == this.getProducersThreadState())&#123; return null; &#125; log.debug("demand exceeds supply(供不应求，需生产数据)..."); Thread.sleep(50); &#125; &#125; /** * 获取 producersThreadState * @return producersThreadState */ ThreadState getProducersThreadState() &#123; return producersThreadState; &#125; /** * 设置 producersThreadState * @param producersThreadState */ void setProducersThreadState(ThreadState producersThreadState) &#123; this.producersThreadState = producersThreadState; &#125; /** * 获取 consumersThreadState * @return consumersThreadState */ ThreadState getConsumersThreadState() &#123; return consumersThreadState; &#125; /** * 设置 consumersThreadState * @param consumersThreadState */ void setConsumersThreadState(ThreadState consumersThreadState) &#123; this.consumersThreadState = consumersThreadState; &#125;&#125; 线程状态枚举：新线程(NEW)、可运行的(RUNNABLE)、运行中(RUNNING)、死亡(DEAD)、阻塞(BLOCKED)。 123enum ThreadState &#123; NEW, RUNNABLE, RUNNING, DEAD, BLOCKED;&#125; 然后我们构造两个模板接口，一个生产者模板接口一个消费者模板接口，分别提供生产者产生数据的方法和消费者消费数据的方法。具体实现有各自的业务实现类实现即可。 123456789101112131415161718192021222324/** * 生产者模板 * @param &lt;C_E&gt; */public interface ProducerTemplate&lt;C_E&gt; &#123; /** * 生产数据 * @param context * @throws Exception */ void production(Context&lt;C_E&gt; context) throws Exception;&#125;/** * 消费者模板 * @param &lt;C_E&gt; */public interface ConsumerTemplate&lt;C_E&gt; &#123; /** * 消费数据 * @param context * @throws Exception */ void consumption(Context&lt;C_E&gt; context) throws Exception;&#125; 创建一个生产者与消费者的协调者类，用来启动生产者或者消费者。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * 生产与消费协调者 */public class Coordinator &#123; private static final Logger log = LoggerFactory.getLogger(Coordinator.class); private final Lock lock = new ReentrantLock(); private final Condition enabledConsumers = lock.newCondition(); private volatile boolean isEnabledForConsumers; private final Context&lt;?&gt; context; // 是否等待生产及消费完成 private boolean isWaitingToFinish; // 最大消费线程数 private int consumersMaxTotal; public Coordinator(Context&lt;?&gt; context, int consumersMaxTotal) &#123; this(context, consumersMaxTotal, true); &#125; public Coordinator(Context&lt;?&gt; context, int consumersMaxTotal, boolean isWaitingToFinish) &#123; this.context = context; this.consumersMaxTotal = consumersMaxTotal; this.isWaitingToFinish = isWaitingToFinish; &#125; /** *启动生产、消费 * @param producerTemplate 生产者模板 * @param consumerTemplate 消费者模板 */ public void start(ProducerTemplate&lt;?&gt; producerTemplate,ConsumerTemplate&lt;?&gt; consumerTemplate) throws Exception &#123; if (context.getConsumersThreadState() != null || context.getProducersThreadState() != null)&#123; return; &#125; ProducersThreadUnit producersThreadUnit = new ProducersThreadUnit(producerTemplate, "production", context); ConsumersThreadUnit consumersThreadUnit = new ConsumersThreadUnit(consumerTemplate, "consumption", context); this.start(producersThreadUnit, consumersThreadUnit); &#125; /** * 启动生产、消费（适用于生产函数、消费函数不在一个类里实现，或者一个类里有多对生产、消费组合，或者方法入参列表复杂） * @param producersThreadUnit * @param consumersThreadUnit */ public void start(ProducersThreadUnit producersThreadUnit, ConsumersThreadUnit consumersThreadUnit) throws Exception &#123; if (context.getConsumersThreadState() != ThreadState.NEW || context.getProducersThreadState() != ThreadState.NEW)&#123; return; &#125; long time = System.currentTimeMillis(); try &#123; //启动生产者 Thread startProducersThread = this.startProducers(producersThreadUnit); //启动消费者 Thread startConsumersThread = this.startConsumers(consumersThreadUnit); if (!this.isWaitingToFinish)&#123; return; &#125; startProducersThread.join(); if (startConsumersThread != null)&#123; startConsumersThread.join(); &#125; &#125; catch (Exception e) &#123; log.error("start worker error...", e); throw e; &#125; log.info(String.format("processing is completed... man-hour(millisecond)=[%s]", System.currentTimeMillis() - time)); &#125; /** * 启动生产 * @param producersThreadUnit * @return */ private Thread startProducers(ProducersThreadUnit producersThreadUnit) throws Exception &#123; Thread thread = new Thread(producersThreadUnit); thread.start(); return thread; &#125; /** * 启动消费 * @param consumersThreadUnit * @return */ private Thread startConsumers(ConsumersThreadUnit consumersThreadUnit) throws Exception &#123; lock.lock(); try &#123; log.info("wating for producers..."); while (!isEnabledForConsumers)&#123; // 等待生产（造成当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态）,假定可能发生虚假唤醒（这并非是因为等待超时），因此总是在一个循环中等待 // 间隔检查，防止意外情况下线程没能被成功唤醒（机率小之又小,导致线程无限挂起） enabledConsumers.await(5, TimeUnit.SECONDS); &#125; if (context.getConsumptionQueueSize() == 0)&#123; return null; &#125; log.info("start consumers before..."); Thread thread = new Thread(consumersThreadUnit); thread.start(); return thread; &#125; catch (Exception e) &#123; log.error("start consumers error...", e); throw e; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 生产者和消费者的线程单元如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156 /** * 生产线程 */ public class ProducersThreadUnit implements Runnable &#123; private Object targetObject; private String targetMethodName; private Object[] targetMethodParameters; private ExecutorService executorService = Executors.newFixedThreadPool(1); public ProducersThreadUnit(Object targetObject, String targetMethodName, Object... targetMethodParameters) &#123; this.targetObject = targetObject; this.targetMethodName = targetMethodName; this.targetMethodParameters = targetMethodParameters; context.setProducersThreadState(ThreadState.NEW); &#125; @Override public void run() &#123; try &#123; executorService.execute(new RunnableThreadUnit(targetObject, targetMethodName, targetMethodParameters)); context.setProducersThreadState(ThreadState.RUNNABLE); executorService.shutdown(); // 阻塞线程，直到生产中（消费队列不为空）或者停止生产 while (!executorService.isTerminated() &amp;&amp; context.getConsumptionQueueSize() == 0)&#123; Thread.sleep(20); &#125; log.info("production the end or products have been delivered,ready to inform consumers..."); this.wakeConsumers(); log.info("wait until the production is complete..."); while (!executorService.isTerminated())&#123; // 等待生产完毕 Thread.sleep(200); &#125; &#125; catch (Exception e) &#123; log.error(String.format("production error... targetObject=[%s],targetMethodName=[%s],targetMethodParameters=[%s]", targetObject, targetMethodName, targetMethodParameters), e); if (!executorService.isShutdown())&#123; executorService.shutdown(); &#125; &#125; finally &#123; log.info("production the end..."); context.setProducersThreadState(ThreadState.DEAD); // 无论在何种情况下，必须确保能够结束挂起中的消费者线程 isEnabledForConsumers = true; &#125; &#125; /** * 向消费者发送信号 */ private void wakeConsumers() &#123; // 即使唤醒消费者线程失败，也可以使用该句柄结束挂起中的消费者线程 isEnabledForConsumers = true; lock.lock(); try &#123; enabledConsumers.signal(); &#125; catch (Exception e) &#123; log.error("inform to consumers error...", e); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; /** * 消费线程 */ public class ConsumersThreadUnit implements Runnable &#123; private Object targetObject; private String targetMethodName; private Object[] targetMethodParameters; public ConsumersThreadUnit(Object targetObject, String targetMethodName, Object... targetMethodParameters) &#123; this.targetObject = targetObject; this.targetMethodName = targetMethodName; this.targetMethodParameters = targetMethodParameters; context.setConsumersThreadState(ThreadState.NEW); &#125; @Override public void run() &#123; ThreadPoolExecutor threadPoolExecutor = null; int concurrencyMaxTotal = Coordinator.this.consumersMaxTotal; try &#123; threadPoolExecutor = new ThreadPoolExecutor(0, concurrencyMaxTotal, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); while (concurrencyMaxTotal &gt; 0) &#123; if (threadPoolExecutor.getPoolSize() &gt; context.getConsumptionQueueSize()) &#123; if (ThreadState.DEAD == context.getProducersThreadState()) &#123; // 无须再提交新任务 break; &#125;else &#123; Thread.sleep(50); // 再次检查是否有必要提交新任务 continue; &#125; &#125; RunnableThreadUnit consumers = new RunnableThreadUnit(targetObject, targetMethodName, targetMethodParameters); threadPoolExecutor.execute(consumers); context.setConsumersThreadState(ThreadState.RUNNABLE); log.info("submit consumption task..."); concurrencyMaxTotal--; &#125; threadPoolExecutor.shutdown(); while (!threadPoolExecutor.isTerminated()) &#123; // 等待消费完毕 Thread.sleep(100); &#125; &#125; catch (Exception e) &#123; log.error(String.format("consumption error... targetObject=[%s],targetMethodName=[%s],targetMethodParameters=[%s]", targetObject, targetMethodName, targetMethodParameters), e); if (threadPoolExecutor != null &amp;&amp; !threadPoolExecutor.isShutdown()) &#123; threadPoolExecutor.shutdown(); &#125; &#125; finally &#123; log.info("consumption the end..."); context.setConsumersThreadState(ThreadState.DEAD); &#125; &#125; &#125;/** *线程单元（无返回值） */public class RunnableThreadUnit implements Runnable &#123; private final static Logger logger = LoggerFactory.getLogger(RunnableThreadUnit.class); private Object object; private String methodName; private Object[] methodParameters; public RunnableThreadUnit(Object object, String methodName, Object... methodParameters) &#123; if (object == null || StringUtils.isBlank(methodName) || methodParameters == null) &#123; throw new RuntimeException("init runnable thread unit error..."); &#125; this.object = object; this.methodName = methodName; this.methodParameters = methodParameters; &#125; @Override public void run() &#123; try &#123; Class&lt;?&gt;[] classes = new Class[methodParameters.length]; for (int i = 0; i &lt; methodParameters.length; i++) &#123; classes[i] = methodParameters[i].getClass(); &#125; Method method = object.getClass().getMethod(methodName, classes); method.invoke(object, methodParameters); &#125; catch (Exception e) &#123; logger.error(String.format("execute runnable thread unit error... service=[%s],invokeMethodName=[%s]", object, methodName), e); &#125; &#125;&#125; 可以看到我们使用反射获取了production和consumption方法，并执行它们。启动了两个线程，生产者线程和消费者线程去处理业务，其中消费者线程利用了线程池，可以放置concurrencyMaxTotal个子线程去消费任务。 我们创建一个测试类进行测试，如下： 123456789101112131415161718192021222324252627282930313233public class CourrentTest implements ProducerTemplate&lt;String&gt;, ConsumerTemplate&lt;String&gt;&#123; @Override public void production(Context&lt;String&gt; context) throws Exception &#123; //1. 组装数据 //数据库查询、组装数据过程略，由for循环插入数据代替 for(int i = 0;i&lt;1000;i++)&#123; Thread.sleep(10); //插入不成功，说明可能是消费者线程死亡或者队列已满 if(!context.offerDataToConsumptionQueue(i+""))&#123; return; &#125; &#125; &#125; @Override public void consumption(Context&lt;String&gt; context) throws Exception &#123; //2. 上传数据/文件 //消费者消费数据 while (true) &#123; String str = context.pollDataFromConsumptionQueue(); if (str == null) &#123; break; &#125; //假设每个文件上传消耗1s时间 Thread.sleep(1000); System.out.println(str); &#125; &#125; //测试 public static void main(String[] args) throws Exception&#123; CourrentTest courrentTest = new CourrentTest(); new Coordinator(new Context&lt;String&gt;(),10).start(courrentTest,courrentTest); &#125;&#125; 运行后可以看到输出的结果。 这儿我们可以看到对比较耗时的上传方法（消费者端）进行了并发处理以提高效率，生产端如果保证了数据的安全性，我们可以使用并行流等放入数据以提高放入数据的效率。 其实我们看到这儿，可以理解线程池也是一个类似于生产者消费者模式的东西。线程池里面有任务就会去执行，相当于消费者，线程池里的队列相当于缓存区，而生产者就是我们一个个放入线程的Runable方法。 上述代码的运行原理图大致如下： PS: 上述代码可以在我的GitHub项目里找到。 https://github.com/JavaZWT/framework-base 另外提供了一个简易模板SimpleTemplate可以适用生产者方法和消费者方法在一个类里的情况，只继承这一个方法即可。不用分别继承ConsumerTemplate和ProducerTemplate接口了。 总结通过对上面一个列子使用生产者和消费者模式，我们了解了这种模式的一些适用情形和优点。 当然也了解了它的一些缺点，对于解决并发问题的方案，最要重视的应该就是数据安全问题了。 我们在平时工作中也可以考虑什么样的场景下可以使用这种模式，其实这种模式的适用场景还是蛮多的，对于一些处理较耗时的操作，文件上传、图片生成转换等都可以考虑这种模式。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
        <tag>生产者与消费者模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis在项目中的一些应用]]></title>
    <url>%2Fblog%2FRedis%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言今天我们来总结下Redis在项目中的一些应用。 Redis在实际项目中除了可以作为缓存或者持久化数据库外，还能解决项目中遇到的一些棘手的问题。 正文限流/防高频问题这基本上属于项目中一个比较经典的问题了，我们以防止用户高频访问来举例，大多数的项目中都是通过Redis来解决高频访问问题的。 我们知道，对于高频访问问题，要有以下3要素：单位时间、单位时间限制访问次数、超频后的限制访问时间长。 我们定义一个Bean，控制这几个参数，当然也可以直接使用配置文件的方式进行配置等。 123456789101112131415161718public class LimitRule &#123; /** * 单位访问时间 */ private int seconds; /** * 单位时间内限制的访问次数 */ private int limitCount; /** * 单位时间超过访问次数后的锁定时间 */ private int lockTime; //Get Set略 public boolean enableLimitLock() &#123; return getLockTime() &gt; 0 &amp;&amp; getLimitCount() &gt; 0; &#125;&#125; Redis有一种数据结构，名字为Zset，可以通过方法zadd添加元素，通过zcount统计记录数，我们可以用Zset的有序集的value来存放访问时间，判断超频时，只需要用zcount判断单位时间seconds内Zset里的元素数据是否超过limitCount即可，超过后即为该用户添加一个锁定lockTime的Redis key。 因此，我们代码大致如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class HighFreqLimit &#123; //记录用户行为并判断高频访问的zset private static final String REDIS_VISIT_KEY_FORMAT = "redis.visit:user:zset:%s"; //高频访问用户key private static final String REDIS_LIMIT_KEY_FORMAT = "redis.limit:user:%s"; //Redis 工具 private RedisUtil redisUtil; public HighFreqLimit(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; /** * 方案一：使用Zset来判断高频访问情况 */ /** * 判断用户是否高频访问 * 高频访问抛出异常 * @param userNo 用户唯一的编号 * @param limitRule 访问规则 这个规则可以用配置文件的方式处理 * @return */ public void checkLimit1(String userNo,LimitRule limitRule)&#123; //判断是否启用了高频访问 if(!limitRule.enableLimitLock())&#123; return; &#125; String redisKeyUserVisitZset = String.format(REDIS_VISIT_KEY_FORMAT,userNo); String redisUserFreqLimitKey = String.format(REDIS_LIMIT_KEY_FORMAT,userNo); //如果存在高频访问key说明已经高频访问了 if (redisUtil.exists(redisUserFreqLimitKey)) &#123; throw new RuntimeException("您操作的太快了，请稍后访问"); &#125; long currentTimeMillis=System.currentTimeMillis(); //访问信息，可以根据具体业务定制 String visitInfo = userNo +":"+ System.currentTimeMillis(); //将信息添加到zset里 redisUtil.zadd(redisKeyUserVisitZset,System.currentTimeMillis(), visitInfo); //设置过期时间为单位时间 redisUtil.expire(redisKeyUserVisitZset, limitRule.getSeconds()); long startTimeMillis = currentTimeMillis - limitRule.getSeconds() * 1000; //统计单位时间内Zset的元素个数 long visitCount = redisUtil.zcount(redisKeyUserVisitZset, startTimeMillis, currentTimeMillis); //超过阈值则成为高频用户 if (visitCount &gt; limitRule.getLimitCount()) &#123; redisUtil.setString(redisUserFreqLimitKey, visitInfo); redisUtil.expire(redisUserFreqLimitKey, limitRule.getLockTime()); throw new RuntimeException("您操作的太快了，请稍后访问"); &#125; &#125;&#125; Redis在2.6版本后支持Lua表达式，因此我们也可以构建使用Lua表达式来解决上述问题。 构建Lua表达式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344private static final String REDIS_VISIT_KEY_FORMAT2 = "redis.visit:user:lua:%s"; /** * 方案二：使用Lua表达式来判断高频访问情况 */ /** * 判断用户是否高频访问 * 高频访问抛出异常 * @param userNo * @param limitRule */ public void checkLimit2(String userNo, LimitRule limitRule) &#123; String redisKeyUserVisit = String.format(REDIS_VISIT_KEY_FORMAT2,userNo); long count; List&lt;String&gt; keys = new ArrayList&lt;String&gt;(); keys.add(redisKeyUserVisit); List&lt;String&gt; args = new ArrayList&lt;String&gt;(); args.add(limitRule.getLimitCount() + ""); args.add(limitRule.getSeconds() + ""); args.add(limitRule.getLockTime() + ""); count = Long.parseLong(redisUtil.getJedisFactory().getJedisCluster().eval(buildLuaScript(limitRule), keys, args) + ""); if(count &gt; limitRule.getLimitCount())&#123; throw new RuntimeException("您操作的太快了，请稍后访问"); &#125; &#125; private String buildLuaScript(LimitRule limitRule) &#123; StringBuilder lua = new StringBuilder(); lua.append("\nlocal c"); lua.append("\nc = redis.call('get',KEYS[1])"); lua.append("\nif c and tonumber(c) &gt; tonumber(ARGV[1]) then"); lua.append("\nreturn c;"); lua.append("\nend"); lua.append("\nc = redis.call('incr',KEYS[1])"); lua.append("\nif tonumber(c) == 1 then"); lua.append("\nredis.call('expire',KEYS[1],ARGV[2])"); lua.append("\nend"); if (limitRule.enableLimitLock()) &#123; lua.append("\nif tonumber(c) &gt; tonumber(ARGV[1]) then"); lua.append("\nredis.call('expire',KEYS[1],ARGV[3])"); lua.append("\nend"); &#125; lua.append("\nreturn c;"); return lua.toString(); &#125; 对于上述表达式，KEYS[1]即为redisKeyUserVisit，可以看到先进行取值，如果有值并且值比limitCount大就返回了，根据后面count &gt; limitRule.getLimitCount()的判断说明已经超频了，如果不大于该值，则进行自增，如果该值是1，说明单位时间第一次访问，就设置它的单位时间过期，然后如果该值超频后会这是这个Key的过期时间为lockTime。 序列号生成问题项目中另一种常见的情况就是流水号的生成了，很多业务流水号有如下格式 XXXX2019040100001 等，我最近的一个项目就有类似的复杂需求，这种情况下我们可以使用Redis来生成某一天的自增流水号，大致如下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class SequenceUtils &#123; private RedisUtil redisUtil; public SequenceUtils(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; //业务规定序号为00001 ，00002 这种5位格式 private static final int DEFAULT_LENGTH = 5; //缓存时长 private static final int ONE_DAY_TIME = 24*60*60; private static final String REDIS_CACHE_KEY = "redis.serialnumber:%s:%s"; /** * 获取自增数字的字符串形式,包含0前缀 * @param seq * @return */ private String getSequenceWithZeroPrefix(long seq) &#123; String str = String.valueOf(seq); int len = str.length(); if (len &gt;= DEFAULT_LENGTH) &#123; throw new RuntimeException("Sequence generate failed!"); &#125; int rest = DEFAULT_LENGTH - len; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; rest; i++) &#123; sb.append('0'); &#125; sb.append(str); return sb.toString(); &#125; /** * 获取自增数字的字符串形式,不包含0前缀 * @param seq * @return */ private String getSequenceNoZeroPrefix(long seq)&#123; return String.valueOf(seq); &#125; /** * 序列号生成器 * @param bizCode 业务码 * @param needZero 是否需要0前缀 * @return */ public String generate(String bizCode,boolean needZero)&#123; String date = DateFormatUtils.format(new Date(),"yyyyMMdd"); //redis key String key = String.format(REDIS_CACHE_KEY,bizCode,date); //自增并设置过期时间 long sequence = redisUtil.incr(key); redisUtil.expire(key,ONE_DAY_TIME); String seq; if(needZero)&#123; seq = getSequenceWithZeroPrefix(sequence); &#125;else&#123; seq = getSequenceNoZeroPrefix(sequence); &#125; StringBuffer sb = new StringBuffer(); sb.append(bizCode).append(date).append(seq); return sb.toString(); &#125;&#125; 为保证绝对可靠，还可以进行改善，当Redis拿不到值时可以去数据库初始化今天的起始流水号等，这儿不再过多介绍，可以看到主要就是利用了Redis的自增incr和指定时间过期expire这两个关键方法。 分布式锁还可以使用Redis做分布式锁，相比较之前说的Zookeeper实现分布式锁，使用Redis实现分布式锁，最明显的优点就是指令为内存操作，速度较快，性能较高；但缺点也比较明显，使用Redis实现分布式锁较为复杂，需要考虑超时、原子性、误删等情形，较为复杂，且由于没有等待锁的队列，等待锁只能依靠客户端自旋，效率较为低下。反观ZK实现的分布式锁，有等待锁的队列，但是添加删除节点性能较低。 我们使用Redis来简单实现一个分布式锁。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class RedisLock &#123; //锁的key前缀 private final static String LOCK_PREFIX="redis.lock:%s"; private RedisUtil redisUtil; public RedisLock(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; /** * 尝试获得锁 * @param key String key * @param lockTimeOut long 超时时间(毫秒) * @return 大于 0 获得到锁并,等于0获取锁失败 */ public long tryLock(String key,long lockTimeOut)&#123; key= String.format(LOCK_PREFIX,key); long expireTime = 0; expireTime = System.currentTimeMillis() + lockTimeOut +1; if(redisUtil.setStringIfNotExists(key, String.valueOf(expireTime))==1)&#123; return expireTime; &#125;else &#123; String curLockTimeStr = redisUtil.getString(key); //判断是否过期 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr)) &#123; expireTime = System.currentTimeMillis() + lockTimeOut +1; curLockTimeStr = redisUtil.getSet(key, String.valueOf(expireTime)); //仍然过期,则得到锁 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr))&#123; return expireTime; &#125;else &#123; return 0; &#125; &#125;else &#123; return 0; &#125; &#125; &#125; /** * 一直等待获得锁 * @param key String key * @param lockTimeOut long 超时时间(毫秒) * @param perSleep long 获得锁循环等待休眠时间 * @return 大于 0 获得到锁并,等于0获取锁失败 * @throws InterruptedException */ public long lock(String key,long lockTimeOut,long perSleep) throws InterruptedException&#123; key= String.format(LOCK_PREFIX,key); long starttime = System.currentTimeMillis(); long sleep = (perSleep==0 ? lockTimeOut/ 10 : perSleep); //得到锁后设置的过期时间，未得到锁返回0 long expireTime = 0; for (;;) &#123; expireTime = System.currentTimeMillis() + lockTimeOut +1; if (redisUtil.setStringIfNotExists(key, String.valueOf(expireTime)) == 1) &#123; //得到了锁返回 return expireTime; &#125;else &#123; String curLockTimeStr = redisUtil.getString(key); //判断是否过期 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr)) &#123; expireTime = System.currentTimeMillis() + lockTimeOut +1; curLockTimeStr = redisUtil.getSet(key, String.valueOf(expireTime)); //仍然过期,则得到锁 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr))&#123; return expireTime; &#125;else &#123; Thread.sleep(sleep); &#125; &#125;else &#123; Thread.sleep(sleep); &#125; &#125; if (lockTimeOut &gt; 0 &amp;&amp; ((System.currentTimeMillis() - starttime) &gt;= lockTimeOut)) &#123; expireTime = 0; return expireTime; &#125; &#125; &#125; /** * 释放锁 * @param key String key * @param expireTime long 超时时间(毫秒) */ public void unlock(String key,long expireTime)&#123; key= String.format(LOCK_PREFIX,key); if (System.currentTimeMillis()-expireTime&gt;0) &#123; return ; &#125; String curLockTimeStr = redisUtil.getString(key); if (StringUtils.isNotBlank(curLockTimeStr) &amp;&amp; Long.valueOf(curLockTimeStr)&gt;System.currentTimeMillis()) &#123; redisUtil.delKey(key); &#125; &#125;&#125; 可以看到在等待锁的阶段，如果设置超时时间，则客户端只能自旋等待锁，如果在指定时间内未获得锁，就会超时。 可以看到主要逻辑是，首先使用setStringIfNotExists （对应Redis的set str NX 命令）方法尝试设置key，如果成功说明获得锁，返回超时时间；如果不成功说明已经有程序在使用该锁，需要判断剩余过期时间，如果没有剩余过期时间，再尝试获得锁，否则线程sleep剩余过期时间。 可以看到，我们需要指定锁的使用时间，如果不指定时间，有可能会造成死锁等问题。 配置中心Redis也可以用来实现配置中心的相关功能。 Redis 做配置中心，需要结合数据库来实现以确保稳定性。 数据库设计一张配置表用于存储配置数据，在Redis可以将数据存入哈希表来进行处理。 当然我们新增修改或者删除数据时需要同时对Redis和数据库进行操作。 并可以添加数据从Redis刷新到数据库和从数据库刷新到Redis等功能。 部分代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105public class RedisConfigCenter &#123; /** * 配置中心key */ public static final String CONFIG_CENTER_KEY = "redis.configcenter:hash:key"; //Redis 工具 private RedisUtil redisUtil; public RedisConfigCenter(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; /** * 配置中心添加数据 * 可以添加一个或多个 * @return */ public boolean insertData(Map&lt;String,String&gt; insertData)&#123; //放入Redis hash表 boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(exists)&#123; //拿到key对应的Redis hash表数据 Map&lt;String,String&gt; redisMap = redisUtil.hashGetAll(CONFIG_CENTER_KEY); insertData.putAll(redisMap); &#125; //写入Redis hash表 redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,insertData); //数据库处理部分代码略 return true; &#125; /** * 配置中心更新数据 * @param updateData * @return */ public boolean updateData(Map&lt;String,String&gt; updateData)&#123; boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(!exists)&#123; throw new RuntimeException("请先新增数据!"); &#125; //拿到key对应的Redis hash表数据 Map&lt;String,String&gt; redisMap = redisUtil.hashGetAll(CONFIG_CENTER_KEY); redisMap.putAll(updateData); //写入Redis hash表 redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,updateData); //数据库处理部分代码略 return true; &#125; /** * 配置中心删除数据 * @param deleteKeys * @return */ public boolean deleteData(List&lt;String&gt; deleteKeys)&#123; boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(!exists)&#123; throw new RuntimeException("请先新增数据!"); &#125; //拿到key对应的Redis hash表数据 Map&lt;String,String&gt; redisMap = redisUtil.hashGetAll(CONFIG_CENTER_KEY); deleteKeys.forEach(key-&gt;&#123; redisMap.remove(key); &#125;); //写入Redis hash表 redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,redisMap); //数据库处理部分代码略 return true; &#125; /** * 查询数据列表 * @return */ public Map&lt;String,String&gt; selectData()&#123; Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(!exists)&#123; return map; &#125; map = redisUtil.hashGetAll(CONFIG_CENTER_KEY); return map; &#125; /** * 将数据库数据刷新到Redis * @return */ public boolean refreshToRedis()&#123; //数据库获取到数据集合 略 Map&lt;String,String&gt; map = new HashMap&lt;&gt;();//TODO redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,map); return true; &#125; /** * Redis刷新到数据库 * @return */ public boolean refreshToDataBase()&#123; Map&lt;String,String&gt; map = redisUtil.hashGetAll(CONFIG_CENTER_KEY); //数据库操作，略 return true; &#125;&#125; 当然，为保证数据可靠性，可以启动两个线程，指定时间从Redis刷新到数据库或者从数据库刷新到Redis的功能。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738/** * 可以改为配置的形式 */ private static boolean refreshRedisThreadStop = false; private static boolean refreshDataBaseThreadStop = false; /** * 刷新数据的线程 */ ExecutorService executorService = Executors.newFixedThreadPool(2); /** * 初始化 */ public void init() &#123; // refresh thread executorService.submit(() -&gt;&#123; while (!refreshRedisThreadStop) &#123; try &#123; //30 min 刷新一次 TimeUnit.MINUTES.sleep(30); refreshToRedis(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); executorService.submit(() -&gt;&#123; while (!refreshDataBaseThreadStop) &#123; try &#123; //30 min 刷新一次 TimeUnit.MINUTES.sleep(30); refreshToDataBase(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 结语通过以上对Redis在应用中的各个功能的应用，让我们对Redis的应用有了更深入的了解，以及对Redis的应用场景有了更加深刻的认识。 Redis 在项目中基本上已经是很平常的存在了，如何使用好它，解决棘手问题乃是我们的重中之重。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发中遇到的一些问题与解决思路（1）]]></title>
    <url>%2Fblog%2F%E5%BC%80%E5%8F%91%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF-1.html</url>
    <content type="text"><![CDATA[前言整理了一下开发中遇到的一些问题及解决思路，特地分享下。 正文Mysql临时变量（局部变量）的使用Mysql临时变量又称为局部变量，其带有@符号，使用时不需要声明，只在局部起作用。 用法 set @num=1; 或set @num:=1; 这里要使用变量来保存数据，直接使用@num变量即可 select @num:=1; 或 select @num:=字段名 from 表名 where …… 注意上面两种赋值符号，使用set时可以用“=”或“：=”，但是使用select时必须用“：=赋值” 巧妙的使用临时变量，可以解决我们遇到的一些比较棘手的问题。 例子假设有一张客户表，customer ，其有字段 id（表示唯一id），字段 locale （表示国家，0001 中国，0002 美国，0003 日本 等），字段 create_time 表示这个用户的创建日期（yyyy-MM-dd hh:MM:ss）形式，等其他字段。 现在这张表已经上线一段时间，有了大量数据，由于某些原因（需求），需要新增字段 客户号 customer_no ，客户号的生成遵循一定规则，比如 2018-11-12 日当天第一个中国用户，则客户号为 CN201811120001 这种格式，第20个中国用户客户号为 CN201811120020，即用户号的生成规则是 国家简称 + 日期yyyyMMdd + 0001 递增，当天第一个美国用户客户号为 AM201811120001 这种形式，如果日期变为2018-11-13 则这一天的第一个中国用户为 CN201811130001 这种形式。 这儿就不讨论客户号之后的生成逻辑，现在这张表已经有一定量的数据，新增了客户号字段，需要维护旧数据的此字段，手动修改显然是不现实的…… 这时候我们可以借助临时变量来实现对旧数据的更新维护，locale为中国（CN）的代码更新可以如下（其它国家类似）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152-- 临时自增变量SET @rownum = 0;-- 初始值，需要手动设置，选择一个日期SET @startdate = '20181227';-- 更新语句UPDATE customer pSET p.customer_no = ( SELECT b.num FROM ( SELECT CASE DATE_FORMAT(a.create_time, "%Y%m%d") WHEN @startdate THEN @rownum :=@rownum + 1 ELSE @rownum := 1 END as temp, CONCAT( 'CN', DATE_FORMAT(a.create_time, "%Y%m%d"), CASE WHEN @rownum &gt; 999 THEN '' WHEN @rownum &gt; 99 THEN '0' WHEN @rownum &gt; 9 THEN '00' ELSE '000' END, @rownum ) AS num, ( CASE DATE_FORMAT(a.create_time, "%Y%m%d") WHEN @startdate THEN @startdate ELSE @startdate := DATE_FORMAT(a.create_time, "%Y%m%d") END ) AS helpdata, a.id AS id FROM customer a WHERE 1 = 1 ORDER BY p.create_time asc ) AS b WHERE b.id = p.id)WHERE p.locale = '0001'; 可以看到这个SQL超级长，其实不算复杂，其customer_no的赋值就是借助了两个临时变量进行的，可以看到第一个@rownum临时变量，用来进行自增，拼接为0001、0010等等这种形式。 为什么还有个@startdate临时变量呢？ 这个变量主要用来记录上一条的时间跟要更新的这一条的时间是否一致，一致的话@rownum自增1，不一致说明是新的时间，@rownum重新从1开始，这样做的前提保证是数据要根据create_time进行排序，即 ORDER BY p.create_time asc的作用。 同时我们还可以看到我们借助了中间表b实现了对@startdate的赋值。 其实主要思想可以如下分解，更新表里的旧数据，需要将customer_no赋值，则可以先写一个select语句查询出 CN201811120001 这种形式的一张表，通过b.id = p.id实现关联到指定的要更新的数据，然后进行数据update。 这属于一个较复杂的Mysql临时变量的使用例子，临时变量虽然在Mysql查询等语句中使用的不多，但是关键时候还是有很大作用的。 使用并行流时，必须保证操作对象的线程安全性举例： 12345678910111213141516171819public static void main(String[] args) throws Exception&#123; //构造一个源数据List List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for(int i = 0;i&lt;100000;i++)&#123; list.add(i); &#125; //线程同步的ArrayList List&lt;Integer&gt; list1 = Collections.synchronizedList(new ArrayList&lt;&gt;()); //普通的ArrayList List&lt;Integer&gt; list2 = new ArrayList&lt;&gt;(); //并行流处理 list.parallelStream().forEach(e-&gt;&#123; list1.add(e); list2.add(e); &#125;); //输出结果 System.out.println(list1.size()); System.out.println(list2.size());&#125; 输出结果： 1210000099785 可以看到list2 数据不正常，多运行几次，可以发现list2有时候正常，有时候不正常，有时候甚至会出现数组下标越界异常等情况。 而list1的数据结果一直没有问题。 所以在操作ArrayList的不安全操作时（或者其他不安全数据结构），如add，delete等，如果使用并行流，必须保证被操作对象的线程安全性。 上述情况一般有两种解决办法： 使用线程安全的对象，如ArrayList使用Collections.synchronizedList 方法变为同步的List，或者使用Vector等。 抛弃并行流，使用串行流或者其他解决办法。 Java8 lambda表达式无法抛出受检查异常的问题我们知道Java异常分为两类，受检查异常（Checked Exception）跟非受检异常（UnChecked Exception）。 对于受检查异常，代码中必须显式处理该异常，不然编译不通过，如IOException等。 而对于非受检查异常，或称为运行时异常，可以不用处理，如RunTimeException等。 对于一个方法，如果方法内部抛出受检查异常，则方法本身也要显式抛出异常。 我们来看下下面两个例子。 1234567891011121314public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("1"); list.stream().forEach(e-&gt;&#123; throw new RuntimeException("Exception"); &#125;);&#125;public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("1"); list.stream().forEach(e-&gt;&#123; throw new IOException("Exception"); &#125;);&#125; 可以看到第二个main方法是编译不通过的，因为抛出了受检查异常，而它lambda表达式如下 1Consumer consumer = e-&gt;&#123;throw new IOException("Exception");&#125;; 由于Consumer执行的accept方法，如下： 1234@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; void accept(T t);&#125; 故需要该方法抛出受检查异常才可以，显然我们是无法修改Java源码的。 对于其它函数式接口lambda表达式亦是如此。 那如何处理这种情况呢？ 在程序代码想使用lambda表达式的情况下，大致有如下两种办法： 将受检查异常包装成非受检查异常。 123list.stream().forEach(e-&gt;&#123; throw new RuntimeException(new IOException("Exception"));&#125;); 这样虽然解决了问题，但是破坏了异常结构，代码也不是很美观，在涉及到一些事务的方法上，还会导致不能正确捕捉异常进行回滚而产生一些问题等。 不推荐使用。 包装泛型异常 12345678910public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("1"); list.stream().forEach(e-&gt;&#123; doThrow(new IOException()); &#125;);&#125;public static &lt;E extends Exception&gt; void doThrow(Exception e) throws E &#123; throw (E)e;&#125; 可以看到我们利用异常泛型包装了异常处理，这时候编译器不能明确异常类型，因此编译通过。 这时候我们在测试，可以看到lambda表达式运行后抛出了我们期望的IOException。 结语以后再有开发相关方面的问题，值得记录和分享的，我会在更新。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PDF.js插件]]></title>
    <url>%2Fblog%2FPDF-js%E6%8F%92%E4%BB%B6.html</url>
    <content type="text"><![CDATA[前言今天我们来学习下一款非常有意思的插件PDF.js，正如它的名字一样，它是由Mozilla开源的，用来在Web浏览器上预览PDF文件的一款插件。 我们知道，目前很多浏览器已经支持了PDF的在线预览及下载功能，但是展示的样式各异，并且一部分手机浏览器并不支持预览，在开发WebApp遇到预览PDF的功能，可能需要下载下来借助手机第三方软件打开，显然用户体验不够友好。 而PDF.js恰恰解决了以上问题。 如果说PDF.js的缺点，那大概就是它不支持IE 8 及以下浏览器。（PS:PDF.js使用了HTML 5的相关技术，如canvas，理论上不支持HTML 5的浏览器均不能使用） 项目地址：https://mozilla.github.io/pdf.js/ 正文分析我们来了解并使用下这款插件。 根据上面地址，我们可以下载PDF.js的预编译版本和源码。 PDF.js提供的预编译版本是可以直接使用的，当然我们也可以使用源码自己编译生成PDF.js插件。 我们来简单说下PDF.js预编译版本的基本构成，截止当前，我使用的是 Stable(v2.0.943) 的预编译版本。 它的结构如下： 12345678910111213141516├── build/│ ├── pdf.js - display layer│ ├── pdf.js.map - display layer&apos;s source map│ ├── pdf.worker.js - core layer│ └── pdf.worker.js.map - core layer&apos;s source map├── web/│ ├── cmaps/ - character maps (required by core)│ ├── compressed.tracemonkey-pldi-09.pdf - PDF file for testing purposes│ ├── debugger.js - helpful debugging features│ ├── images/ - images for the viewer and annotation icons│ ├── locale/ - translation files│ ├── viewer.css - viewer style sheet│ ├── viewer.html - viewer layout│ ├── viewer.js - viewer layer│ └── viewer.js.map - viewer layer&apos;s source map└── LICENSE 主要由build和web包构成，build包里的pdf.worker.js是PDF.js的核心处理包，web包里的viewer.js viewer.html viewer.css 用来在Web页面上展示渲染PDF（边框，工具栏等）。 images文件夹里存放一些工具栏图标等内容，locale文件夹里存放各地区语言包。 debugger.js 是debug相关js，我们在使用时可以开启debug输出某些信息来进行调试。 compressed.tracemonkey-pldi-09.pdf 是一个PDF测试类，当我们没有加载自己定义的PDF时，会默认加载此文件。 使用集成PDF.js插件我们在SpringBoot项目下引入PDF.js插件并简单使用。 我们新建SpringBoot项目，引入Web模块，生成项目后，我们只需在项目的static文件夹下引入预编译版本的PDF.js即可。 如图： PS: 为方便管理，我把PDF.js插件放到了一个pdfViewer文件夹里。 我们启动项目，通过浏览器访问viewer.html，即 http://localhost:8080/pdfViewer/web/viewer.html 可以看到成功打开了我们的测试PDF。 我们如果想打开自己的PDF应该如何操作呢？ 我们可以在上述网络地址上加上file参数指向我们的PDF。 http://localhost:8080/pdfViewer/web/viewer.html?file=..... 的形式。 我们在项目static文件夹中新建pdf包，放入我们的PDF，如下： PDF文件在本项目中，访问有两种方式，网络路径访问和相对路径访问。 相对路径访问。 通过 http://localhost:8080/pdfViewer/web/viewer.html?file=../../pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 进行访问。 可以看到file后面跟的路径是PDF文件相对于viewer.html的路径。 网络路径访问。 通过 http://localhost:8080/pdfViewer/web/viewer.html?file=http://localhost:8080/pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 进行访问。 file后面跟的参数是PDF所在的网络地址。 最后预览效果如下图： 如果PDF文件不在本项目中呢？那一定会出现跨域问题，我们来看一下。 访问 http://localhost:8080/pdfViewer/web/viewer.html?file=http://localhost:8081/pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 可以看到文件不存在。 我们F12查看信息，可以看到PDF.js插件出现如下错误：Uncaught (in promise) Error: file origin does not match viewer’s 说明出现了跨域问题。 如何解决呢？ 毕竟不是所有的PDF文件都会放在项目中的。 这就需要解决跨域问题，通常有两种方法： 我们如果仍使用路径的方式，则需要对PDF.js进行配置，通过刚才报错的信息，我们很容易在viewer.js 里找到如下内容。 这段js很好理解，设置允许跨域的路径，我们把我们的服务器网络路径添加到HOSTED_VIEWER_ORIGINS对象里，http://localhost:8080。 PS: 这里要注意，viewer.js.map 文件里的 HOSTED_VIEWER_ORIGINS 对象也要改变（预编译版本），否则你会看到很奇怪的缓存问题。 但这样仍是不行的，我们尝试访问 http://localhost:8080/pdfViewer/web/viewer.html?file=http://localhost:8081/pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 可以看到跨域问题仍然存在。 因为要解决跨域问题，服务器也需要进行设置，我们找到8081服务器，添加跨域设置，主要内容如下（SpringBoot项目）： 12345678910111213141516171819202122@Configurationpublic class CorsConfig &#123; @Bean public CorsFilter corsFilter() &#123; UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); String apiAllowOrigins = "http://localhost:8080"; if(StringUtils.isEmpty(apiAllowOrigins))&#123; corsConfiguration.addAllowedOrigin("*"); &#125;else&#123; corsConfiguration.setAllowedOrigins(Arrays.asList(apiAllowOrigins.split(","))); &#125; corsConfiguration.addAllowedHeader("*"); corsConfiguration.setAllowedMethods(Arrays.asList("HEAD","POST", "OPTIONS","GET")); corsConfiguration.setAllowCredentials(true); corsConfiguration.setMaxAge(3600L); //跨域设置 source.registerCorsConfiguration("/**", corsConfiguration); return new CorsFilter(source); &#125;&#125; 这段代码很好理解，apiAllowOrigins设置成我们请求的服务器地址即可。 这时候我们在访问上面的PDF地址，可以看到PDF被加载出来了。 我们也可以使用流的方式来加载远程的PDF文件，当然也需要设置跨域属性。 上面的CorsConfig.java 在8081服务器上保持不变，我们在该项目里新增一个类PDFController，用于解析文件并返回流。 123456789101112131415161718@Controllerpublic class PDFController &#123; @RequestMapping("/getPDF") public void getPDF(@RequestParam("fileName") String fileName, HttpServletRequest request, HttpServletResponse response) throws Exception&#123; response.setStatus(HttpServletResponse.SC_OK); response.setContentType("application/pdf;charset=UTF-8"); //PDF文件在服务器上的位置，可以通过程序获取，略，我直接写死了 try(FileInputStream input =new FileInputStream("E:\\WorkSpace\\spring-cloud\\pdf1-service\\src\\main\\resources\\static\\pdf\\"+fileName); ServletOutputStream out = response.getOutputStream())&#123; byte buffBytes[] = new byte[1024]; int read = 0; while ((read = input.read(buffBytes)) != -1) &#123; out.write(buffBytes, 0, read); &#125; out.flush(); &#125; &#125;&#125; 我这里传入了fileName属性来获取PDF流。 这样在调用PDF.js 插件时，需要先通过ajax获取PDF流拿到PDF，并提供给PDF.js插件。 我们打开viewer.js 找到如下代码。 这个方法是webView初始化方法，如果没有传file属性，就加载默认的PDF文件。 我们把 file = ‘file’ in params ? params.file : _app_options.AppOptions.get(‘defaultUrl’); 这段代码换成如下： 1file = 'file' in params ? params.file : DEFAULT_URL; 很好理解，就是不用它的默认值，我们传入一个DEFAULT_URL值。 我们新建helper.js，如下： 12345678910111213141516171819var DEFAULT_URL = "";var PDFData = "";$.ajax(&#123; type:"post", async:false, mimeType: 'text/plain; charset=x-user-defined', url:"http://localhost:8081/getPDF", data:&#123;fileName:"IT入职指南.pdf"&#125;, success:function(data)&#123; PDFData = data; &#125;&#125;);var rawLength = PDFData.length;//转换成pdf.js能直接解析的Uint8Array类型var array = new Uint8Array(new ArrayBuffer(rawLength));for(i = 0; i &lt; rawLength; i++) &#123; array[i] = PDFData.charCodeAt(i) &amp; 0xff;&#125;DEFAULT_URL = array; 并在PDF.js插件的web包下引入helper.js 和 jquery.js。 PS: jquery.js 的引用就使用了一个ajax，如果由其他的ajax方案可以不引入jquery。 并在viewer.html里引入依赖，需要在viewer.js 之前引入。 12345678.....部分代码略&lt;script src="../build/pdf.js"&gt;&lt;/script&gt;&lt;script src="jquery-3.3.1.min.js"&gt;&lt;/script&gt;&lt;script src="helper.js"&gt;&lt;/script&gt; &lt;script src="viewer.js"&gt;&lt;/script&gt; &lt;/head&gt;.......部分代码略 设置好后，我们重新启动下服务器，可以看到PDF文件被加载了。 PS:根据上面，我们可以看到当不传入file参数时，PDF.js会加载默认配置，我们可以通过js等控制这个默认配置，让PDF.js每次打开的都是 http://localhost:8080/pdfViewer/web/viewer.html ，但是PDF文件变化，以实现不显示file参数的需求。 PDF.js 插件的个性化需求我们使用了PDF.js 插件后，可以看到功能很全，什么工具栏，放大缩小，打印，下载等等功能一应俱全，但现实中我们有可能是不需要这么多的。 尤其在WebApp中，我们可能仅仅可以查看PDF即可，偶尔可以有个放大缩小功能，这样应该如何处理呢。 这项功能的实现是非常简单的，我们找到该功能的按钮的button，直接添加hidden属性即可。 12345678910111213......&lt;button id="sidebarToggle" class="toolbarButton" title="Toggle Sidebar" tabindex="11" data-l10n-id="toggle_sidebar" hidden&gt; &lt;span data-l10n-id="toggle_sidebar_label"&gt;Toggle Sidebar&lt;/span&gt;&lt;/button&gt;&lt;div class="toolbarButtonSpacer"&gt;&lt;/div&gt;&lt;button id="viewFind" class="toolbarButton" title="Find in Document" tabindex="12" data-l10n-id="findbar" hidden&gt; &lt;span data-l10n-id="findbar_label"&gt;Find&lt;/span&gt;&lt;/button&gt;......&lt;button id="secondaryToolbarToggle" class="toolbarButton" title="Tools" tabindex="36" data-l10n-id="tools" hidden&gt; &lt;span data-l10n-id="tools_label"&gt;Tools&lt;/span&gt;&lt;/button&gt;...... PDF.js 踩坑记 关于过大PDF无法显示的问题 曾经遇到过测试环境PDF文件预览正常但是生产环境部分过大PDF文件无法打开的问题，后查看请求发现GET请求的range范围为0-65535，判断可能是服务器对 Range 进行了某些特殊限制。 而PDF.js 插件里有一项参数是可以禁止使用Range的，在viewer.js defaultOptions 对象里，如下： 123456var defaultOptions = &#123; disableRange: &#123; value: false, kind: OptionKind.API &#125;&#125; 我们将属性设置为true得以解决问题。（如果不生效，可能是map文件存在的缘故，需要重新生成viewer.js.map文件） PDF无法正常显示 如果相对路径无法加载，可尝试网络路径。 如果使用的是网络路径扔无法加载，可以对网络路径进行encode编码在返回给前端调用。 如果仍不可以，可以考虑使用流传输的方式。 关于PDF.js 插件的缓存问题 使用PDF.js插件过程中，你会发现它会缓存看过的PDF的阅读位置，这本是一项人性化的设定，但如果你就是想每次打开PDF文件后从头开始看起，请使用 disableHistory 参数。 它也在defaultOptions对象里，默认false，改为true后每次再打开这个PDF文件时就会从第一页看起。 viewer.js defaultOptions 对象 可以看到PDF.js 插件 defaultOptions 对象里还有很多很多的默认参数，我们都是可以对其进行设置以实现相关功能或者禁用相关功能的。 关于它们的用法，可以参考一些相关文档，这儿就不做过多叙述了。 结语今天主要介绍了PDF.js 插件的使用，这是一款非常优秀的插件，在WebApp 预览PDF文件时经常会被使用，而且 Mozilla 以后有意将该插件集成到 FireFox 浏览器里，并且通过该插件立志于打造一项Web浏览PDF文件的标准。 让我们拭目以待吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>PDF.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（3）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%EF%BC%883%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言接 搭建自己的Hexo博客（2） 这篇文章，时隔一段时间，我对自己的博客陆陆续续又进行了部分优化，特地总结了下。 正文图片存储问题原来blog的图片也随静态页面托管在Github Pages 上的，后来页面访问加载速度很慢，而且图片本来较大，上传至Github上也是比较费劲的。 就考虑到将其存储到OSS服务上，由于我的域名没有经过网络备案，因此无法开启CDN服务，开启CDN服务后可以进一步提高图片加载速度。 我的域名在阿里云申请的，进而选择了阿里云的OSS服务。 PS：当然，对于阿里云，CDN服务和OSS服务都是要收取一定费用的。 我们登陆阿里云，选择对象存储OSS服务，如未开通请选择开通，而后新建一个我们blog的Bucket，找到文件管理选项，选择上传文件，上传我们的blog图片，上传成功后，可以通过复制图片URL的方式拿到图片地址，将博文中的图片地址指向此路径即可。 因为上传到OSS图片并在blog中使用后，图片的地址会被暴露在网页上，为了防止图片地址被盗用，我们可以设置只允许我们的域名可以访问图片，以进一步确保安全，如下： 添加相册功能我们为blog添加相册功能，进一步美化blog。 我们可以为相册增加一个标签，如图所示，点击进入到相册页面。 在 next 主题的config文件里添加相册(photos)标签，如下： 1234567891011menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat support: /support/ || support photos: /photos/ || photo PS: 如果不显示中文（相册），只显示photos，需要在next主题里的languages/zh-Hans.yml 里配置 photos: 相册。 添加完标签后，回到hexo的source文件夹，在里面新建photos文件夹，并在该文件夹里新建index.md 文件，内容如下： 1234567title: 相册comments: falsetype: photosnoDate: true---&lt;div class="ImageGrid"&gt;&lt;/div&gt; ImageGrid的div即用来存放相册的一个div。 我们在博客的根目录下新建photosTool.js，如下： 123456789101112131415161718192021222324252627282930313233343536"use strict";const fs = require("fs");const sizeOf = require('image-size');const path = "source/images_backup/photos";const output = "source/photos/photos.json";var dimensions;fs.readdir(path, function(err, files) &#123; if (err) &#123; return; &#125; let arr = []; (function iterator(index) &#123; if (index == files.length) &#123; fs.writeFile(output, JSON.stringify(arr, null, "\t")); return; &#125; fs.stat(path + "/" + files[index], function(err, stats) &#123; if (!files[index].endsWith(".jpg") || files[index].endsWith(".jpeg")) &#123; &#125; if (err) &#123; return; &#125; if (stats.isFile()) &#123; if (files[index].endsWith(".jpg") || files[index].endsWith(".jpeg") || files[index].endsWith(".mp4") || files[index].endsWith(".png") || files[index].endsWith(".gif")) &#123; dimensions = sizeOf(path + "/" + files[index]); console.log(dimensions.width, dimensions.height); arr.push(dimensions.width + '.' + dimensions.height + ' ' + files[index]); &#125; &#125; iterator(index + 1); &#125;) &#125;(0));&#125;); 这个js用于获取图片的长和宽，以便可以获得更好的展示效果，同时，她会生成一个photos.json的文件，里面保存这图片的长和宽，如下： 123456789101112131415161718192021[ photo-1-1.jpg, "3200.2000 photo-10.jpg", "1920.1200 photo-11.jpg", "1920.1080 photo-12.jpg", "1920.1200 photo-13.jpg", "1920.1080 photo-14.jpg", "1920.1080 photo-15.jpg", "1920.1200 photo-16.jpg", "1920.1080 photo-17.jpg", "1920.1186 photo-18.jpg", "1920.1080 photo-19.jpg", "1024.610 photo-2.jpg", "2200.1400 photo-3.jpg", "1200.750 photo-4.jpg", "3500.1898 photo-5.jpg", "1080.1920 photo-6.jpg", "1600.1120 photo-7.jpg", "1728.1080 photo-8.jpg", "1920.1080 photo-9.jpg"] 这个文件我生成在了photos文件夹下。 可以看到它们的图片是从source/images_backup/photos路径下获取的。 为提高相应速度，需要将这些图片放到阿里云OSS上，存放到上面后，我们需要一个photos.js来解析图片并展示。 找到themes/next/source/js/src文件夹，新建photos.js文件，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647photo = &#123; page: 1, offset: 20, init: function() &#123; var that = this; $.getJSON("/photos/photos.json", function(data) &#123; that.render(that.page, data); //that.scroll(data); &#125;); &#125;, render: function(page, data) &#123; var begin = (page - 1) * this.offset; var end = page * this.offset; if (begin &gt;= data.length) return; var html, imgNameWithPattern, imgName, imageSize, imageX, imageY, li = ""; for (var i = begin; i &lt; end &amp;&amp; i &lt; data.length; i++) &#123; imgNameWithPattern = data[i].split(' ')[1]; imgName = imgNameWithPattern.split('.')[0] imageSize = data[i].split(' ')[0]; imageX = imageSize.split('.')[0]; imageY = imageSize.split('.')[1]; li += '&lt;div class="card" style="width:330px"&gt;' + '&lt;div class="ImageInCard" style="height:' + 330 * imageY / imageX + 'px"&gt;' + '&lt;a data-fancybox="gallery" href="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + imgNameWithPattern + '?raw=true" data-caption="' + imgName + '"&gt;' + '&lt;img src="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + imgNameWithPattern + '?raw=true"/&gt;' + '&lt;/a&gt;' + '&lt;/div&gt;' + // '&lt;div class="TextInCard"&gt;' + imgName + '&lt;/div&gt;' + '&lt;/div&gt;' &#125; $(".ImageGrid").append(li); $(".ImageGrid").lazyload(); this.minigrid(); &#125;, minigrid: function() &#123; var grid = new Minigrid(&#123; container: '.ImageGrid', item: '.card', gutter: 12 &#125;); grid.mount(); $(window).resize(function() &#123; grid.mount(); &#125;); &#125;&#125;photo.init(); 可以看到，我们主要是解析原来生成的photos.json文件，获得长和宽，并将其放到card框里，同时图片是通过OSS获取的。 要使该js生效，需要被引用并加载，打开themes/next/layout/_scripts/commons.swig文件，添加photos.js的引用，如下： 123456789101112131415161718192021&#123;% if page.type ===&apos;photos&apos; %&#125;&#123;% set js_commons = [ &apos;src/utils.js&apos;, &apos;src/motion.js&apos;, &apos;src/minigrid.min.js&apos;, &apos;src/photos.js&apos; ]%&#125;&#123;% else %&#125;&#123;% set js_commons = [ &apos;src/utils.js&apos;, &apos;src/motion.js&apos; ]%&#125;&#123;% endif %&#125;&#123;% for common in js_commons %&#125; &lt;script type=&quot;text/javascript&quot; src=&quot;&#123;&#123; url_for(theme.js) &#125;&#125;/&#123;&#123; common &#125;&#125;?v=&#123;&#123; theme.version &#125;&#125;&quot;&gt;&lt;/script&gt;&#123;% endfor %&#125; PS:一些自己想实现的其它功能，如果有新的js文件，也可以做如下的添加方式，另使photos.js生效不仅仅只可以添加到这儿，也可以通过其它的地方引入，考虑到以后维护的方便，我们可以统一添加到了这里。 可以为相册相框添加缩放效果及一些其它效果，我的相框的css样式如下： 12345678910111213141516171819202122232425262728293031323334353637383940//图片鼠标靠近放大效果.fancybox img &#123; display: block; border: 0; width: 100%; transform: scale(1); transition: all 1s ease 0s; -webkit-transform: scale(1); -webkit-transform: all 1s ease 0s;&#125; .fancybox:hover img &#123; transform: scale(1.1); transition: all 1s ease 0s; -webkit-transform: scale(1.1); -webkit-transform: all 1s ease 0s;&#125;.ImageGrid &#123; width: 100%; max-width: 1040px; margin: 0 auto; text-align: center;&#125;.card &#123; overflow: hidden; transition: .3s ease-in-out; border-radius: 8px; background-color: #ddd;&#125;.ImageInCard img &#123; padding: 0 0 0 0;&#125;.TextInCard &#123; line-height: 54px; background-color: #ffffff; font-size: 24px;&#125; 这个css内容我们添加到themes/next/source/css/_custom/custom.styl文件夹里。 最终效果如下： SEO 相关优化这一块的优化还是蛮多的，我挑几个比较重要的说下。 我们知道，我们的blog有了域名后，相当于发布在了外网，为了提高blog阅读量，需要对blog或者其它方面做些工作，这都可以称谓博客的SEO优化。 编写质量高的原创博文这一条基本不用多说，好的文章从不缺少流量。关键在于作者提高自身的写作水平。 更改博文生成结构使用hexo生成博文时，博文的生成方式(permalink)为blog/:title:year:month:day/这种格式，这种是不便于搜索的，因为相当于四级目录，我们可以改为blog/:title.html这种格式。 在hexo 的 config.yml文件里进行修改，并重新运行命令生成博文。 1234url: https://www.sakuratears.toproot: /permalink: blog/:title.html ## SEO优化permalink_defaults: 禁止外链我们在写作博文时，有可能引用到其它博文的文章，会有指向其它博文的链接，这样是不便于SEO的，我们又不能去掉该外链而影响到博文功能。因此我们可以引入 hexo-autonofollow 插件，在package.json文件里配置并安装该插件后，在hexo的config.yml文件里添加如下配置。 12345## nofollow SEO 优化nofollow: enable: true exclude: # 除外的链接 - ‘https://www.sakuratears.top’ 可以使网页爬虫更好的获取我们的页面数据，提高SEO。 人为发布博客外链这个就比较简单了，我们可以在一些平台等地方发布我们的博客地址，提高曝光度，提高SEO。 使百度、谷歌、必应等网站收录你的网站这项是一项漫长而又复杂的过程，但也是效果最明显的一个。 我们如果有百度账号的话，可以登陆百度站长平台，将我们的站点添加百度收录。 如果网站已备案，添加备案号可以加快收录速度。 我们可以在链接提交-自动提交-sitemap里添加我们网站的sitemap.xml并提交，以加快百度对我们网站的收录。 同时在自动推送里，我们看到我们可以使用js进行自动推送，由于hexo已经集成了该功能，我们简单设置下就行。 12345678910111213# ---------------------------------------------------------------# SEO Settings# ---------------------------------------------------------------# Canonical, set a canonical link tag in your hexo, you could use it for your SEO of blog.# See: https://support.google.com/webmasters/answer/139066# Tips: Before you open this tag, remember set up your URL in hexo _config.yml ( ex. url: http://yourdomain.com )canonical: true# Change headers hierarchy on site-subtitle (will be main site description) and on all post/pages titles for better SEO-optimization.seo: true# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true 可以看到hexo里面集成的baidu_push.swig内容和百度提供的js是一样的。 12345678910111213141516&#123;% if theme.baidu_push %&#125;&lt;script&gt;(function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);&#125;)();&lt;/script&gt;&#123;% endif %&#125; 我们可以使用手动抓取功能对我们的网站进行抓取，如下效果图。 PS：由于网络等一些原因，有可能抓取失败。 弄好后，过一段时间，可以看到数据的变化。 我们可以通过 site:www.sakuratears.top来检查百度是否收录了我们的网站，如图。 PS:我在bing上进行测试，发现必应收录的比百度要快。 后来查询了解得知Github屏蔽掉了百度爬虫，所以一些blog作者在国内托管到Coding或者Gitee上，在国外托管到GitHub上，以加快博客访问速度，提高SEO优化。 有关这一块内容，我准备后面在处理。 结语今天差不多就这些了，关于博客的一些其它优化内容我会在整理一下。 一些其它的想法: 原来blog的音乐播放器直接使用的网易云的，但是我们公司居然把网易云墙了，而且考虑到歌曲以后可能放在那边会404等情况，下阶段优化考虑使用Hexo的aplayer去处理，并将音乐放到OSS存储上。 关于托管到Coding或者Gitee上的问题，也要研究下。 每次看博客是都会出现live2D妹子，可能并不是所有人都喜欢，也有时候觉得碍事，可能考虑新增一个关闭按钮，可以主动开启或关闭live2D妹子。 部分页面需要美化下，SEO的部分还要看看有没有需要总结的，毕竟SEO优化是条漫长的道路。 以上。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka简介及简单使用]]></title>
    <url>%2Fblog%2FEureka%E7%AE%80%E4%BB%8B%E5%8F%8A%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言Eureka是Spring Cloud Netflix微服务套件中的一部分，一般与SpringBoot构建的微服务进行整合。它基于Netflix Eureka做了二次封装，主要负责完成微服务架构中的服务治理功能。 服务治理可以说是微服务架构中最为核心和基础的模块。服务治理通常包含服务注册和服务发现两部分。 Eureka包含客户端和服务端部分。 Eureka服务端，我们也称为服务注册中心。它支持高可用配置。它依托于强一致性提供良好的服务实例可用性，可以应对多种不同的故障场景。如果Eureka以集群模式部署，当集群中有分片出现故障时，那么Eureka就转入自我保护模式。它允许在分片故障期间继续提供服务的发现和注册，当故障分片恢复运行时，集群中的其他分片就会把它们的状态再次同步回来。不同可用区域的服务注册中心通过异步模式互相复制各自的状态，这意味着在任意给定的时间点每个实例关于所有服务的状态是有细微差别的。 Eureka客户端，主要处理服务的注册与发现。客户端服务通过注解和参数配置的方式，嵌在客户端应用程序的代码中，在应用程序运行时，Eureka客户端向注册中心注册自身提供的服务并周期性的发送心跳来更新它的服务租约。同时，它也能从服务端查询当前注册的服务信息并把它们缓存到本地并进行周期性的刷新服务状态。 正文我们今天先简单通过例子了解一下Eureka，后面在对其进行一些深入研究。 Eureka服务端我们创建一个Eureka服务端，如下： 新建SpringBoot项目，命名为eureka-server 选择Eureka Server依赖 Eureka Server 的配置 首先我们需要启用EurekaServer，如下： 12345678@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 在application.properties里进行相关配置，如下： 123456789101112131415161718# 应用名称spring.application.name=eureka-server# 实例端口server.port=8001# server地址server.address=172.30.13.173# 实例名称eureka.instance.hostname=test1# 实例IDeureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125;# 是否向注册中心注册自己eureka.client.register-with-eureka=false# 是否需要检索服务eureka.client.fetch-registry=false# 使用IP地址定义主机名eureka.instance.prefer-ip-address=true# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/ 我们启动EurekaServer，通过 http://172.30.13.173:8001/ 进行访问，可以看到如下图，Eureka注册中心中尚未有可以使用的实例。 Eureka 客户端我们再新创建一个项目sakura-service，并为其添加web模块和eureka-client模块，以使其注册到eureka中心上。 启用服务发现，@EnableDiscoveryClient。 123456789@EnableDiscoveryClient@SpringBootApplicationpublic class SakuraServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SakuraServiceApplication.class, args); &#125;&#125; application.properties进行如下相关配置。 123456# 实例端口server.port=9001# 应用名称spring.application.name=sakura-service# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/ 我们在该项目中新建一个SakuraController类，如下： 1234567891011121314151617181920212223242526@RestControllerpublic class SakuraController &#123; // 服务注册 @Qualifier("eurekaRegistration") @Autowired private Registration registration; @Autowired private DiscoveryClient client; @RequestMapping(value = "/hello",method = RequestMethod.GET) public String index()&#123; ServiceInstance instance = serviceInstance(); System.out.println("Host:"+instance.getHost()+";ServiceID:"+instance.getServiceId()); return "Hello World"; &#125; public ServiceInstance serviceInstance() &#123; List&lt;ServiceInstance&gt; list = client.getInstances(registration.getServiceId()); if (list != null &amp;&amp; list.size() &gt; 0) &#123; return list.stream().findAny().get(); &#125; return null; &#125;&#125; 在启动此项目，可以看到此项目已被注册到了注册中心上。 我们在创建一个服务消费者用于消费刚才我们创建的服务Service。 需要添加web模块，eureka-client模块和ribbon模块，Ribbon是一个基于HTTP和TCP的客户端负载均衡器，它可以通过客户端配置的ribbonServerList服务端列表去轮询达到负载均衡的目的，和Eureka联合使用时，Ribbon会从Eureka的注册中心获取服务列表去轮询。关于Ribbon的内容，我们后面在研究。 启用服务发现，@EnableDiscoveryClient。并添加RestTemplate，如下： 123456789101112@EnableDiscoveryClient@SpringBootApplicationpublic class SakuraConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SakuraConsumerApplication.class, args); &#125; @Bean @LoadBalanced RestTemplate restTemplate()&#123; return new RestTemplate(); &#125;&#125; application.properties进行如下相关配置。 123456# 实例端口server.port=7001# 应用名称spring.application.name=sakura-consumer# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/ 我们在项目中新建SakuraController用于对sakura-service的hello服务进行调用，如下： 12345678910@RestControllerpublic class SakuraController &#123; @Autowired RestTemplate restTemplate; @RequestMapping(value = "/consumer",method = RequestMethod.GET) public String hello()&#123; return restTemplate.getForEntity("http://SAKURA-SERVICE/hello",String.class).getBody(); &#125;&#125; 启动此项目，可以发现此项目也被注册到了Eureka注册中心。 我们尝试访问消费者，http://172.30.13.173:7001/consumer 可以看到打印了 sakura-service的 Hello World 字样，同时 sakura-service项目日志中输出相关信息。 1Host:DELL-3020-PC;ServiceID:SAKURA-SERVICE 高可用Eureka注册中心上面的例子，当我们关掉Eureka服务端时，可以发现两个项目之间已经无法进行相互调用了。因此，单节点的服务注册中心是不合理的。 所以作为注册中心，应具备高可用性的特征。 Eureka Server在设计的一开始就考虑了高可用的问题，在Eureka的服务治理设计中，所有节点既是服务提供方，也是服务消费方，服务注册中心也不例外。 我们在配置单节点的注册中心时，设置过如下两个参数，让服务注册中心不注册自己。 1234# 是否向注册中心注册自己eureka.client.register-with-eureka=false# 是否需要检索服务eureka.client.fetch-registry=false Eureka Server 的高可用实际上就是将自己作为服务向其他服务注册中心注册自己，这样就形成一组相互注册的服务注册中心，以实现服务清单的相互同步，达到高可用的效果。 下面我们来尝试搭建高可用的服务注册中心集群，我们构建一个三节点的服务注册中心集群。 在搭建之前，我们需要知道SpringBoot是支持多环境配置的，其命名格式需要满足 application-{profile}.properties的格式，其中{profile}对应环境标识，如下： application-dev.properties: 开发环境 application-test.properties: 测试环境 application-prod.properties: 生产环境 至于要加载那个配置文件，需要在application.properties 文件中通过spring.profiles.active 属性来设置，spring.profiles.active=dev 就会加载 application-dev.properties配置。 另外我们在启动项目时，可以通过 java -jar xxxxx.jar –spring.profiles.active = test 来指定使用哪个配置。 根据上面所述，我们可以在原来eureka-server项目里创建3份properties文件，如下： application-test1.properties 123456789101112131415161718# 应用名称spring.application.name=eureka-server# 实例端口server.port=8001# server地址server.address=172.30.13.173# 实例名称eureka.instance.hostname=test1# 实例IDeureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125;# 是否向注册中心注册自己eureka.client.register-with-eureka=true# 是否需要检索服务eureka.client.fetch-registry=true# 使用IP地址定义主机名eureka.instance.prefer-ip-address=true# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8002/eureka/,http://172.30.13.173:8003/eureka/ application-test2.properties 123456789101112131415161718# 应用名称spring.application.name=eureka-server# 实例端口server.port=8002# server地址server.address=172.30.13.173# 实例名称eureka.instance.hostname=test2# 实例IDeureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125;# 是否向注册中心注册自己eureka.client.register-with-eureka=true# 是否需要检索服务eureka.client.fetch-registry=true# 使用IP地址定义主机名eureka.instance.prefer-ip-address=true# eureka服务地址eureka.client.service-url.defaultZone=http://172.30.13.173:8001/eureka/,http://172.30.13.173:8003/eureka/ application-test3.properties 123456789101112131415161718# 应用名称 spring.application.name=eureka-server # 实例端口 server.port=8003 # server地址 server.address=172.30.13.173 # 实例名称 eureka.instance.hostname=test3 # 实例ID eureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125; # 是否向注册中心注册自己 eureka.client.register-with-eureka=true # 是否需要检索服务 eureka.client.fetch-registry=true # 使用IP地址定义主机名 eureka.instance.prefer-ip-address=true # eureka服务地址 eureka.client.service-url.defaultZone=http://172.30.13.173:8002/eureka/,http://172.30.13.173:8001/eureka/ 可以看到我们在一台机器上使用了3个不同端口（8001，8002，8003）来搭建了一个Eureka服务注册中心集群。 PS: application.properties 里可以配置 spring.profiles.active=test1 让其默认使用 test1的配置。 我们使用Maven 打包生成 eureka-server-1.0.0-SNAPSHOT.jar 包，并使用 java -jar eureka-server-1.0.0-SNAPSHOT.jar –spring.profiles.active=test1 启动test1 ,相同的方式启动test2和test3。 完成后访问eureka注册中心界面（http://172.30.13.173:8001/ 或者 http://172.30.13.173:8002/ 或者 http://172.30.13.173:8003/），如图： 可以看到Eureka形成了一组相互注册的服务注册中心。 那它的高可用性能被保证吗？ 我们来测试下，我们根据上面所述，对sakura-service做成两个微服务，如下： sakura-service项目的配置文件 application-test1.properties 123server.port=9001spring.application.name=sakura-serviceeureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/,http://172.30.13.173:8002/eureka/,http://172.30.13.173:8003/eureka/ application-test2.properties 123server.port=9002spring.application.name=sakura-serviceeureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/,http://172.30.13.173:8002/eureka/,http://172.30.13.173:8003/eureka/ 启动它们，可以看到服务注册中心已经有了它们的实例。 我们启动消费者项目sakura-consumer将其也注册到注册中心。 访问 http://172.30.13.173:7001/consumer 可以看到两个注册的sakura-service交替输出信息，这也是使用ribbon可以做到负载均衡的体现。 这时候我们停止掉 eureka-server 的test1 ，可以看到 使用consumer调用 sakura-service的服务仍然正常。 可以看到test2和test3仍然正常进行服务，我们也可以继续停掉test2，可以看到test3正常服务，保证注册中心的服务正常进行。 总结今天我们研究了Eureka注册中心的使用及如何创建一个高可用的注册中心，后面会结合Eureka的源码详细了解Eureka使用及特点。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>Eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql数据库知识点总结(1)]]></title>
    <url>%2Fblog%2FMySql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93(1).html</url>
    <content type="text"><![CDATA[前言最近在使用MySql数据库过程中，遇到了一些问题，并抽时间总结了下来，也是结合Java语言和MyBatis的方方面面，在此分享给大家。 正文中文姓名排序问题比如一张有中文姓名的表，我们要对中文姓名按拼音进行排序，在没有姓名拼音字段（插入姓名时顺带插入了姓名拼音）的帮助下，可以使用如下语法。 如果MySql的排序字段采用的GBK字符集，那可以直接使用 order by ‘字段名’ asc 语法。 如果MySql的排序字段采用的不是GBK字符集，那么可以使用如下方法。 1select * from '表名称' order by convert('字段名' using gbk) asc; 使用该语法，姓名字段名无法使用索引（如果有的话），需要注意。 多个字段去重并统计数量这个问题还是比较常见的，比如有一张用户表，有用户姓名 user_name 用户身份证号 id_card_no 等字段，现在我们要把姓名和身份证号一致的认为一个用户，其它情况均为两个用户，则可以使用以下语法。 可以使用DISTINCT，虽然DISTINCT只能去重一行，但是可以使用字段合并功能来进行处理。 1select count(DISTINCT(c.user_name + IFNULL(c.id_card_no,''))) from user c where c.invite_id = 'xxxxx'; 注意： 两列的类型最好为varchar类型，其他类型不推荐使用此方法，比如char和int相加，会得到意想不到的结果从而产生问题。 使用GROUP BY语句，需要有子查询。 1select count(*) from (select * from user u where u.invite_id = 'xxxxx' GROUP BY u.user_name,IFNULL(u.id_card_no,'')) a ; 注意：子查询得到的表必须有别名，不然SQL语句报错。 查询条件字段有多个条件的查询这个问题是这样，比如我们有一群用户在user表，他们有个字段表示所属平台platform，比如有A、B、C、D等几种平台，现在要查询所属平台为A或B平台的用户。 我们可以明显看到用in即可以解决。在MyBatis里，对于这个字段，则需要传入一个List形式的数组，并在xml文件里通过循环赋值给SQL语句进行查询，也是比较常用的方法。 123456&lt;select id="findUser" parameterType="com.xxx.NewUser"&gt; select * from user u where u.invite_id= #&#123;condition.inviteID&#125; and u.platform in &lt;foreach collection="list" item="condition.platform" index="index" open="(" close=")" separator=","&gt; #&#123;condition.platform&#125; &lt;/foreach&gt;&lt;/select&gt; 上面的NewUser对象里的platform字段是个List，最后组成的SQL语句如下。 1select * from user u where u.invite_id= 'xxxxxx' and u.platform in ('A','B'); 我们还可以利用FIND_IN_SET函数，同时在MyBatis里传入的参数直接为String即可，但是A、B平台需要用逗号隔开。 123&lt;select id="findUser" parameterType="com.xxx.User"&gt; select * from user u where u.invite_id= #&#123;condition.inviteID&#125; and FIND_IN_SET(u.platform,#&#123;condition.platform&#125;)&lt;/select&gt; 上面的User对象里的platform字段是个String，平台之间用逗号分割，最后SQL如下。 1select * from user u where u.invite_id= 'xxxxxx' and FIND_IN_SET(u.platform,'A,B'); 这儿需要注意的一点是，FIND_IN_SET无法使用u.platform字段的索引。 虽然无法使用索引，但如果SQL语句本身其它条件索引检索后数据量不大，或者被FIND_IN_SET的字段没有索引，也是可以使用的。 如果传入Mybatis里的实体类字段本身就是逗号分割的，我们在拆成一个一个数据最后再使用in也是非常费事的，可以使用FIND_IN_SET轻松搞定。 使用程序对MySql批量处理数据我们知道，对于MySql批量处理数据，我们可以使用MyBatis的批量处理方法。 大致如下： 1234567&lt;insert id="insertBatch" parameterType="java.util.List"&gt; insert into student (id, stu_no, stu_name, stu_sex, stu_birthday, stu_class) values &lt;foreach collection="list" item="item" index="index" separator=","&gt; (#&#123;item.id,jdbcType=INTEGER&#125;, #&#123;item.stuNo,jdbcType=VARCHAR&#125;, #&#123;item.stuName,jdbcType=VARCHAR&#125;, #&#123;item.stuSex,jdbcType=CHAR&#125;, #&#123;item.stuBirthday,jdbcType=DATE&#125;, #&#123;item.stuClass,jdbcType=CHAR&#125;) &lt;/foreach&gt;&lt;/insert&gt; 这种方法比普通的程序循环一条条插入要快很多，下面是一个例子。 1234567891011121314151617181920212223242526272829303132333435@Testpublic void test3() throws Exception&#123; Thread.sleep(2000); long start1 =System.currentTimeMillis(); for(int i=0;i&lt;1000;i++)&#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i+""); studentModel.setStuClass("1"); studentModel.setStuNo(i+""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); studentMapper.insert(studentModel); &#125; long end1 =System.currentTimeMillis(); System.out.println(end1-start1); Thread.sleep(2000); long start2 =System.currentTimeMillis(); List&lt;StudentModel&gt; list3 = new ArrayList&lt;&gt;(); for(int i=1000;i&lt;2000;i++)&#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i+""); studentModel.setStuClass("1"); studentModel.setStuNo(i+""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); list3.add(studentModel); &#125; studentMapper.insertBatch(list3); long end2 =System.currentTimeMillis(); System.out.println(end2-start2);&#125; 运行结果： 12345672019-02-25 14:46:14.851 INFO 9360 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2019-02-25 14:46:15.031 INFO 9360 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed.1206012602019-02-25 14:48:17.666 INFO 9360 --- [ Thread-2] o.s.w.c.s.GenericWebApplicationContext : Closing org.springframework.web.context.support.GenericWebApplicationContext@2ddc9a9f: startup date [Mon Feb 25 14:46:09 CST 2019]; root of context hierarchy2019-02-25 14:48:17.671 INFO 9360 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...2019-02-25 14:48:17.675 INFO 9360 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. 可以看到批处理要比单条处理快了N倍，这儿不再过多介绍。 Tips：如果数据量过大，如100w，1000w等，可以将数据分成多份循环去批处理插入。 关于批量的处理的另外一点思考： 当我们需要对多张表进行批量处理时，如果想提高运行效率，可以考虑使用多线程处理，比如下面的例子。 我们对4张表插入1w数据，正常的处理逻辑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Testpublic void test4() throws Exception &#123; Thread.sleep(2000); long start1 = System.currentTimeMillis(); List&lt;ScoreModel&gt; list1 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; ScoreModel scoreModel = new ScoreModel(); scoreModel.setId(i); scoreModel.setScore(new BigDecimal(100)); scoreModel.setCouNo(i + ""); scoreModel.setStuNo(i + ""); list1.add(scoreModel); &#125; scoreMapper.insertBatch(list1); List&lt;TeacherModel&gt; list2 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; TeacherModel teacherModel = new TeacherModel(); teacherModel.setId(i); teacherModel.setTeachNo(i + ""); teacherModel.setTeachName(i + ""); teacherModel.setTeachBirthday(new Date()); teacherModel.setTeachDepart(i + ""); teacherModel.setTeachSex("M"); teacherModel.setTeachProf(i + ""); list2.add(teacherModel); &#125; teacherMapper.insertBatch(list2); List&lt;StudentModel&gt; list3 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i + ""); studentModel.setStuClass("1"); studentModel.setStuNo(i + ""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); list3.add(studentModel); &#125; studentMapper.insertBatch(list3); List&lt;CourseModel&gt; list4 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; CourseModel courseModel = new CourseModel(); courseModel.setId(i); courseModel.setCouName(i + ""); courseModel.setCouNo(i + ""); courseModel.setTeachNo(i + ""); list4.add(courseModel); &#125; courseMapper.insertBatch(list4); long end1 = System.currentTimeMillis(); System.out.println(end1 - start1);&#125; 运行结果： 1234562019-02-25 15:01:23.688 INFO 9576 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2019-02-25 15:01:23.850 INFO 9576 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed.43482019-02-25 15:01:27.689 INFO 9576 --- [ Thread-2] o.s.w.c.s.GenericWebApplicationContext : Closing org.springframework.web.context.support.GenericWebApplicationContext@298a5e20: startup date [Mon Feb 25 15:01:18 CST 2019]; root of context hierarchy2019-02-25 15:01:27.691 INFO 9576 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...2019-02-25 15:01:27.700 INFO 9576 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. 我们用线程池对四张表分别进行批量操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Testpublic void test2() throws Exception&#123; Thread.sleep(2000); long start1 =System.currentTimeMillis(); ExecutorService executorService = Executors.newFixedThreadPool(4); Future&lt;Boolean&gt; future1= executorService.submit(()-&gt;&#123; List&lt;ScoreModel&gt; list1 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; ScoreModel scoreModel =new ScoreModel(); scoreModel.setId(i); scoreModel.setScore(new BigDecimal(100)); scoreModel.setCouNo(i+""); scoreModel.setStuNo(i+""); list1.add(scoreModel); &#125; scoreMapper.insertBatch(list1); return true; &#125;); Future&lt;Boolean&gt; future2= executorService.submit(()-&gt;&#123; List&lt;TeacherModel&gt; list2 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; TeacherModel teacherModel =new TeacherModel(); teacherModel.setId(i); teacherModel.setTeachNo(i+""); teacherModel.setTeachName(i+""); teacherModel.setTeachBirthday(new Date()); teacherModel.setTeachDepart(i+""); teacherModel.setTeachSex("M"); teacherModel.setTeachProf(i+""); list2.add(teacherModel); &#125; teacherMapper.insertBatch(list2); return true; &#125;); Future&lt;Boolean&gt; future3= executorService.submit(()-&gt;&#123; List&lt;StudentModel&gt; list3 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i+""); studentModel.setStuClass("1"); studentModel.setStuNo(i+""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); list3.add(studentModel); &#125; studentMapper.insertBatch(list3); return true; &#125;); Future&lt;Boolean&gt; future4= executorService.submit(()-&gt;&#123; List&lt;CourseModel&gt; list4 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; CourseModel courseModel = new CourseModel(); courseModel.setId(i); courseModel.setCouName(i+""); courseModel.setCouNo(i+""); courseModel.setTeachNo(i+""); list4.add(courseModel); &#125; courseMapper.insertBatch(list4); return true; &#125;); future1.get(); future2.get(); future3.get(); future4.get(); long end1 = System.currentTimeMillis(); System.out.println(end1-start1);&#125; 运行结果： 1234562019-02-25 15:04:46.623 INFO 8284 --- [pool-1-thread-3] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2019-02-25 15:04:47.084 INFO 8284 --- [pool-1-thread-3] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed.19932019-02-25 15:04:47.986 INFO 8284 --- [ Thread-2] o.s.w.c.s.GenericWebApplicationContext : Closing org.springframework.web.context.support.GenericWebApplicationContext@298a5e20: startup date [Mon Feb 25 15:04:40 CST 2019]; root of context hierarchy2019-02-25 15:04:47.990 INFO 8284 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...2019-02-25 15:04:48.004 INFO 8284 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. 可以看到速度提高了1倍，当数据量更大时，提高效果更明显。 注意 对于上面多线程多表批量插入，如果需要保证事务，单独的每张表的事务是可以保证的（将每张表的批量插入提出来，形成一个方法，并加上事务属性，如果有错误就会回滚），但是如果要同时保证4张表的事务（这几张表要么全成功，要么全不成功），是无法满足的，由于多线程的特殊性。所以这种情况下请使用第一种同步方法，并加上事务，才能保证4张表批处理要么全成功，要么全不成功。 对于一张表的批处理，如果数据量过大时，可以使用多线程同时插入这一张表吗？ 答案是否定的，对于MySQL InnoDB数据库，默认是行锁，前提条件是建立在索引之上的。如果筛选条件没有建立索引，会降级到表锁。即如果where条件中的字段都加了索引，则加的是行锁；否则加的是表锁。 当为表锁时，瓶颈在数据库，多线程是无法提高对同一张表的插入效率的； 当为行锁时，看起来可以insert A行时同时insert B行，确实可以提高效率，但有数据冲突的错误情况，一般也不会使用。 结语我还会在工作学习过程中不断总结，此文章类型也会不断更新，今天就先到这里吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis命令使用参考手册]]></title>
    <url>%2Fblog%2FRedis%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E5%8F%82%E8%80%83%E6%89%8B%E5%86%8C.html</url>
    <content type="text"><![CDATA[前言我们来详细了解下Redis，及其一些命令的具体使用方法，并学会如何使用 Redis 的事务、持久化、复制、Sentinel、集群等功能。 主要命令我们知道Redis支持五种数据类型：string（字符串），hash（哈希表），list（列表），set（集合）及zset(sorted set：有序集合)。先来看下它们的一些操作命令。 字符串SETSET key value [EX seconds] [PX milliseconds] [NX|XX]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将字符串值 value 关联到 key 。 如果 key 已经持有其他值， SET 就覆写旧值， 无视类型。 当 SET 命令对一个带有生存时间（TTL）的键进行设置之后， 该键原有的 TTL 将被清除。 可选参数从 Redis 2.6.12 版本开始， SET 命令的行为可以通过一系列参数来修改： EX seconds ： 将键的过期时间设置为 seconds 秒。 执行 SET key value EX seconds 的效果等同于执行 SETEX key seconds value 。 PX milliseconds ： 将键的过期时间设置为 milliseconds 毫秒。 执行 SET key value PX milliseconds 的效果等同于执行 PSETEX key milliseconds value 。 NX ： 只在键不存在时， 才对键进行设置操作。 执行 SET key value NX 的效果等同于执行 SETNX key value 。 XX ： 只在键已经存在时， 才对键进行设置操作。 Note 因为 SET 命令可以通过参数来实现 SETNX 、 SETEX 以及 PSETEX 命令的效果， 所以 Redis 将来的版本可能会移除并废弃 SETNX 、 SETEX 和 PSETEX 这三个命令。 返回值在 Redis 2.6.12 版本以前， SET 命令总是返回 OK 。 从 Redis 2.6.12 版本开始， SET 命令只在设置操作成功完成时才返回 OK ； 如果命令使用了 NX 或者 XX 选项， 但是因为条件没达到而造成设置操作未执行， 那么命令将返回空批量回复（NULL Bulk Reply）。 代码示例对不存在的键进行设置： 12345redis&gt; SET key "value"OKredis&gt; GET key"value" 对已存在的键进行设置：12345redis&gt; SET key "new-value"OKredis&gt; GET key"new-value" 使用 EX 选项：12345678redis&gt; SET key-with-expire-time "hello" EX 10086OKredis&gt; GET key-with-expire-time"hello"redis&gt; TTL key-with-expire-time(integer) 10069 使用 PX 选项：12345678redis&gt; SET key-with-pexpire-time "moto" PX 123321OKredis&gt; GET key-with-pexpire-time"moto"redis&gt; PTTL key-with-pexpire-time(integer) 111939 使用 NX 选项：1234567891011redis&gt; SET not-exists-key "value" NXOK # 键不存在，设置成功redis&gt; GET not-exists-key"value"redis&gt; SET not-exists-key "new-value" NX(nil) # 键已经存在，设置失败redis&gt; GEt not-exists-key"value" # 维持原值不变 使用 XX 选项：1234567891011121314redis&gt; EXISTS exists-key(integer) 0redis&gt; SET exists-key "value" XX(nil) # 因为键不存在，设置失败redis&gt; SET exists-key "value"OK # 先给键设置一个值redis&gt; SET exists-key "new-value" XXOK # 设置新值成功redis&gt; GET exists-key"new-value" SETNXSETNX key value可用版本： &gt;= 1.0.0 时间复杂度： O(1) 只在键 key 不存在的情况下， 将键 key 的值设置为 value 。 若键 key 已经存在， 则 SETNX 命令不做任何动作。 SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 返回值命令在设置成功时返回 1 ， 设置失败时返回 0 。 代码示例1234567891011redis&gt; EXISTS job # job 不存在(integer) 0redis&gt; SETNX job "programmer" # job 设置成功(integer) 1redis&gt; SETNX job "code-farmer" # 尝试覆盖 job ，失败(integer) 0redis&gt; GET job # 没有被覆盖"programmer" SETEXSETEX key seconds value可用版本： &gt;= 2.0.0 时间复杂度： O(1) 将键 key 的值设置为 value ， 并将键 key 的生存时间设置为 seconds 秒钟。 如果键 key 已经存在， 那么 SETEX 命令将覆盖已有的值。 SETEX 命令的效果和以下两个命令的效果类似： SET key value EXPIRE key seconds # 设置生存时间 SETEX 和这两个命令的不同之处在于 SETEX 是一个原子（atomic）操作， 它可以在同一时间内完成设置值和设置过期时间这两个操作， 因此 SETEX 命令在储存缓存的时候非常实用。 返回值命令在设置成功时返回 OK 。 当 seconds 参数不合法时， 命令将返回一个错误。 代码示例在键 key 不存在的情况下执行 SETEX ：12345678redis&gt; SETEX cache_user_id 60 10086OKredis&gt; GET cache_user_id # 值"10086"redis&gt; TTL cache_user_id # 剩余生存时间(integer) 49 键 key 已经存在， 使用 SETEX 覆盖旧值：1234567891011redis&gt; SET cd "timeless"OKredis&gt; SETEX cd 3000 "goodbye my love"OKredis&gt; GET cd"goodbye my love"redis&gt; TTL cd(integer) 2997 PSETEXPSETEX key milliseconds value可用版本： &gt;= 2.6.0 时间复杂度： O(1) 这个命令和 SETEX 命令相似， 但它以毫秒为单位设置 key 的生存时间， 而不是像 SETEX 命令那样以秒为单位进行设置。 返回值命令在设置成功时返回 OK 。 代码示例12345678redis&gt; PSETEX mykey 1000 "Hello"OKredis&gt; PTTL mykey(integer) 999redis&gt; GET mykey"Hello" GETGET key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回与键 key 相关联的字符串值。 返回值如果键 key 不存在， 那么返回特殊值 nil ； 否则， 返回键 key 的值。 如果键 key 的值并非字符串类型， 那么返回一个错误， 因为 GET 命令只能用于字符串值。 代码示例对不存在的键 key 或是字符串类型的键 key 执行 GET 命令：12345678redis&gt; GET db(nil)redis&gt; SET db redisOKredis&gt; GET db"redis" 对不是字符串类型的键 key 执行 GET 命令：12345678redis&gt; DEL db(integer) 1redis&gt; LPUSH db redis mongodb mysql(integer) 3redis&gt; GET db(error) ERR Operation against a key holding the wrong kind of value GETSETGETSET key value可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将键 key 的值设为 value ， 并返回键 key 在被设置之前的旧值。 返回值返回给定键 key 的旧值。 如果键 key 没有旧值， 也即是说， 键 key 在被设置之前并不存在， 那么命令返回 nil 。 当键 key 存在但不是字符串类型时， 命令返回一个错误。 代码示例1234567891011redis&gt; GETSET db mongodb # 没有旧值，返回 nil(nil)redis&gt; GET db"mongodb"redis&gt; GETSET db redis # 返回旧值 mongodb"mongodb"redis&gt; GET db"redis" STRLENSTRLEN key可用版本： &gt;= 2.2.0 复杂度： O(1) 返回键 key 储存的字符串值的长度。 返回值STRLEN 命令返回字符串值的长度。 当键 key 不存在时， 命令返回 0 。 当 key 储存的不是字符串值时， 返回一个错误。 代码示例获取字符串值的长度：12345redis&gt; SET mykey "Hello world"OKredis&gt; STRLEN mykey(integer) 11 不存在的键的长度为 0 ：12redis&gt; STRLEN nonexisting(integer) 0 APPENDAPPEND key value可用版本： &gt;= 2.0.0 时间复杂度： 平摊O(1) 如果键 key 已经存在并且它的值是一个字符串， APPEND 命令将把 value 追加到键 key 现有值的末尾。 如果 key 不存在， APPEND 就简单地将键 key 的值设为 value ， 就像执行 SET key value 一样。 返回值追加 value 之后， 键 key 的值的长度。 示例代码对不存在的 key 执行 APPEND ：12345redis&gt; EXISTS myphone # 确保 myphone 不存在(integer) 0redis&gt; APPEND myphone "nokia" # 对不存在的 key 进行 APPEND ，等同于 SET myphone "nokia"(integer) 5 # 字符长度 对已存在的字符串进行 APPEND ：12345redis&gt; APPEND myphone " - 1110" # 长度从 5 个字符增加到 12 个字符(integer) 12redis&gt; GET myphone"nokia - 1110" SETRANGESETRANGE key offset value可用版本： &gt;= 2.2.0 时间复杂度：对于长度较短的字符串，命令的平摊复杂度O(1)；对于长度较大的字符串，命令的复杂度为 O(M) ，其中 M 为 value 的长度。 从偏移量 offset 开始， 用 value 参数覆写(overwrite)键 key 储存的字符串值。 不存在的键 key 当作空白字符串处理。 SETRANGE 命令会确保字符串足够长以便将 value 设置到指定的偏移量上， 如果键 key 原来储存的字符串长度比偏移量小(比如字符串只有 5 个字符长，但你设置的 offset 是 10 )， 那么原字符和偏移量之间的空白将用零字节(zerobytes, “\x00” )进行填充。 因为 Redis 字符串的大小被限制在 512 兆(megabytes)以内， 所以用户能够使用的最大偏移量为 2^29-1(536870911) ， 如果你需要使用比这更大的空间， 请使用多个 key 。 Warning 当生成一个很长的字符串时， Redis 需要分配内存空间， 该操作有时候可能会造成服务器阻塞(block)。 在2010年出产的Macbook Pro上， 设置偏移量为 536870911(512MB 内存分配)将耗费约 300 毫秒， 设置偏移量为 134217728(128MB 内存分配)将耗费约 80 毫秒， 设置偏移量 33554432(32MB 内存分配)将耗费约 30 毫秒， 设置偏移量为 8388608(8MB 内存分配)将耗费约 8 毫秒。 返回值SETRANGE 命令会返回被修改之后， 字符串值的长度。 代码示例对非空字符串执行 SETRANGE 命令：12345678redis&gt; SET greeting "hello world"OKredis&gt; SETRANGE greeting 6 "Redis"(integer) 11redis&gt; GET greeting"hello Redis" 对空字符串/不存在的键执行 SETRANGE 命令：12345678redis&gt; EXISTS empty_string(integer) 0redis&gt; SETRANGE empty_string 5 "Redis!" # 对不存在的 key 使用 SETRANGE(integer) 11redis&gt; GET empty_string # 空白处被"\x00"填充"\x00\x00\x00\x00\x00Redis!" GETRANGEGETRANGE key start end可用版本： &gt;= 2.4.0 时间复杂度： O(N)，其中 N 为被返回的字符串的长度。 返回键 key 储存的字符串值的指定部分， 字符串的截取范围由 start 和 end 两个偏移量决定 (包括 start 和 end 在内)。 负数偏移量表示从字符串的末尾开始计数， -1 表示最后一个字符， -2 表示倒数第二个字符， 以此类推。 GETRANGE 通过保证子字符串的值域(range)不超过实际字符串的值域来处理超出范围的值域请求。 Note GETRANGE 命令在 Redis 2.0 之前的版本里面被称为 SUBSTR 命令。 返回值GETRANGE 命令会返回字符串值的指定部分。 代码示例1234567891011121314151617redis&gt; SET greeting "hello, my friend"OKredis&gt; GETRANGE greeting 0 4 # 返回索引0-4的字符，包括4。"hello"redis&gt; GETRANGE greeting -1 -5 # 不支持回绕操作""redis&gt; GETRANGE greeting -3 -1 # 负数索引"end"redis&gt; GETRANGE greeting 0 -1 # 从第一个到最后一个"hello, my friend"redis&gt; GETRANGE greeting 0 1008611 # 值域范围不超过实际字符串，超过部分自动被符略"hello, my friend" INCRBYINCRBY key increment可用版本： &gt;= 1.0.0 时间复杂度： O(1) 为键 key 储存的数字值加上增量 increment 。 如果键 key 不存在， 那么键 key 的值会先被初始化为 0 ， 然后再执行 INCRBY 命令。 如果键 key 储存的值不能被解释为数字， 那么 INCRBY 命令将返回一个错误。 本操作的值限制在 64 位(bit)有符号数字表示之内。 返回值在加上增量 increment 之后， 键 key 当前的值。 代码示例键存在，并且值为数字：12345678redis&gt; SET rank 50OKredis&gt; INCRBY rank 20(integer) 70redis&gt; GET rank"70" 键不存在：12345678redis&gt; EXISTS counter(integer) 0redis&gt; INCRBY counter 30(integer) 30redis&gt; GET counter"30" 键存在，但值无法被解释为数字：12345redis&gt; SET book "long long ago..."OKredis&gt; INCRBY book 200(error) ERR value is not an integer or out of range INCRBYFLOATINCRBYFLOAT key increment可用版本： &gt;= 2.6.0 时间复杂度： O(1) 为键 key 储存的值加上浮点数增量 increment 。 如果键 key 不存在， 那么 INCRBYFLOAT 会先将键 key 的值设为 0 ， 然后再执行加法操作。 如果命令执行成功， 那么键 key 的值会被更新为执行加法计算之后的新值， 并且新值会以字符串的形式返回给调用者。 无论是键 key 的值还是增量 increment ， 都可以使用像 2.0e7 、 3e5 、 90e-2 那样的指数符号(exponential notation)来表示， 但是， 执行 INCRBYFLOAT 命令之后的值总是以同样的形式储存， 也即是， 它们总是由一个数字， 一个（可选的）小数点和一个任意长度的小数部分组成（比如 3.14 、 69.768 ，诸如此类)， 小数部分尾随的 0 会被移除， 如果可能的话， 命令还会将浮点数转换为整数（比如 3.0 会被保存成 3 ）。 此外， 无论加法计算所得的浮点数的实际精度有多长， INCRBYFLOAT 命令的计算结果最多只保留小数点的后十七位。 当以下任意一个条件发生时， 命令返回一个错误： 键 key 的值不是字符串类型(因为 Redis 中的数字和浮点数都以字符串的形式保存，所以它们都属于字符串类型）； 键 key 当前的值或者给定的增量 increment 不能被解释(parse)为双精度浮点数。 返回值在加上增量 increment 之后， 键 key 的值。 代码示例12345678redis&gt; GET decimal"3.0"redis&gt; INCRBYFLOAT decimal 2.56"5.56"redis&gt; GET decimal"5.56" DECRDECR key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 为键 key 储存的数字值减去一。 如果键 key 不存在， 那么键 key 的值会先被初始化为 0 ， 然后再执行 DECR 操作。 如果键 key 储存的值不能被解释为数字， 那么 DECR 命令将返回一个错误。 本操作的值限制在 64 位(bit)有符号数字表示之内。 返回值DECR 命令会返回键 key 在执行减一操作之后的值。 代码示例对储存数字值的键 key 执行 DECR 命令：12345redis&gt; SET failure_times 10OKredis&gt; DECR failure_times(integer) 9 对不存在的键执行 DECR 命令：12345redis&gt; EXISTS count(integer) 0redis&gt; DECR count(integer) -1 DECRBYDECRBY key decrement可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将键 key 储存的整数值减去减量 decrement 。 如果键 key 不存在， 那么键 key 的值会先被初始化为 0 ， 然后再执行 DECRBY 命令。 如果键 key 储存的值不能被解释为数字， 那么 DECRBY 命令将返回一个错误。 本操作的值限制在 64 位(bit)有符号数字表示之内。 返回值DECRBY 命令会返回键在执行减法操作之后的值。 代码示例对已经存在的键执行 DECRBY 命令：12345redis&gt; SET count 100OKredis&gt; DECRBY count 20(integer) 80 对不存在的键执行 DECRBY 命令：12345redis&gt; EXISTS pages(integer) 0redis&gt; DECRBY pages 10(integer) -10 MSETMSET key value [key value …]可用版本： &gt;= 1.0.1 时间复杂度： O(N)，其中 N 为被设置的键数量。 同时为多个键设置值。 如果某个给定键已经存在， 那么 MSET 将使用新值去覆盖旧值， 如果这不是你所希望的效果， 请考虑使用 MSETNX 命令， 这个命令只会在所有给定键都不存在的情况下进行设置。 MSET 是一个原子性(atomic)操作， 所有给定键都会在同一时间内被设置， 不会出现某些键被设置了但是另一些键没有被设置的情况。 返回值MSET 命令总是返回 OK 。 代码示例同时对多个键进行设置：1234567redis&gt; MSET date "2012.3.30" time "11:00 a.m." weather "sunny"OKredis&gt; MGET date time weather1) "2012.3.30"2) "11:00 a.m."3) "sunny" 覆盖已有的值：12345678910redis&gt; MGET k1 k21) "hello"2) "world"redis&gt; MSET k1 "good" k2 "bye"OKredis&gt; MGET k1 k21) "good"2) "bye" MSETNXMSETNX key value [key value …]可用版本： &gt;= 1.0.1 时间复杂度： O(N)， 其中 N 为被设置的键数量。 当且仅当所有给定键都不存在时， 为所有给定键设置值。 即使只有一个给定键已经存在， MSETNX 命令也会拒绝执行对所有键的设置操作。 MSETNX 是一个原子性(atomic)操作， 所有给定键要么就全部都被设置， 要么就全部都不设置， 不可能出现第三种状态。 返回值当所有给定键都设置成功时， 命令返回 1 ； 如果因为某个给定键已经存在而导致设置未能成功执行， 那么命令返回 0 。 代码示例对不存在的键执行 MSETNX 命令：1234567redis&gt; MSETNX rmdbs "MySQL" nosql "MongoDB" key-value-store "redis"(integer) 1redis&gt; MGET rmdbs nosql key-value-store1) "MySQL"2) "MongoDB"3) "redis" 对某个已经存在的键进行设置：12345678redis&gt; MSETNX rmdbs "Sqlite" language "python" # rmdbs 键已经存在，操作失败(integer) 0redis&gt; EXISTS language # 因为 MSETNX 命令没有成功执行(integer) 0 # 所以 language 键没有被设置redis&gt; GET rmdbs # rmdbs 键也没有被修改"MySQL" MGETMGET key [key …]可用版本： &gt;= 1.0.0 时间复杂度： O(N) ，其中 N 为给定键的数量。 返回给定的一个或多个字符串键的值。 如果给定的字符串键里面， 有某个键不存在， 那么这个键的值将以特殊值 nil 表示。 返回值MGET 命令将返回一个列表， 列表中包含了所有给定键的值。 代码示例1234567891011121314redis&gt; SET redis redis.comOKredis&gt; SET mongodb mongodb.orgOKredis&gt; MGET redis mongodb1) "redis.com"2) "mongodb.org"redis&gt; MGET redis mongodb mysql # 不存在的 mysql 返回 nil1) "redis.com"2) "mongodb.org"3) (nil) 哈希表HSETHSET hash field value可用版本： &gt;= 2.0.0 时间复杂度： O(1) 将哈希表 hash 中域 field 的值设置为 value 。 如果给定的哈希表并不存在， 那么一个新的哈希表将被创建并执行 HSET 操作。 如果域 field 已经存在于哈希表中， 那么它的旧值将被新值 value 覆盖。 返回值 当 HSET 命令在哈希表中新创建 field 域并成功为它设置值时， 命令返回 1 ； 如果域 field 已经存在于哈希表， 并且 HSET 命令成功使用新值覆盖了它的旧值， 那么命令返回 0 。 代码示例设置一个新域：12345redis&gt; HSET website google "www.g.cn"(integer) 1redis&gt; HGET website google"www.g.cn" 对一个已存在的域进行更新：12345redis&gt; HSET website google "www.google.com"(integer) 0redis&gt; HGET website google"www.google.com" HSETNXHSETNX hash field value可用版本： &gt;= 2.0.0 时间复杂度： O(1) 当且仅当域 field 尚未存在于哈希表的情况下， 将它的值设置为 value 。 如果给定域已经存在于哈希表当中， 那么命令将放弃执行设置操作。 如果哈希表 hash 不存在， 那么一个新的哈希表将被创建并执行 HSETNX 命令。 返回值HSETNX 命令在设置成功时返回 1 ， 在给定域已经存在而放弃执行设置操作时返回 0 。 代码示例域尚未存在， 设置成功：12345redis&gt; HSETNX database key-value-store Redis(integer) 1redis&gt; HGET database key-value-store"Redis" 域已经存在， 设置未成功， 域原有的值未被改变：12345redis&gt; HSETNX database key-value-store Riak(integer) 0redis&gt; HGET database key-value-store"Redis" HGETHGET hash field可用版本： &gt;= 2.0.0 时间复杂度： O(1) 返回哈希表中给定域的值。 返回值HGET 命令在默认情况下返回给定域的值。 如果给定域不存在于哈希表中， 又或者给定的哈希表并不存在， 那么命令返回 nil 。 代码示例域存在的情况：12345redis&gt; HSET homepage redis redis.com(integer) 1redis&gt; HGET homepage redis"redis.com" 域不存在的情况：12redis&gt; HGET site mysql(nil) HEXISTSHEXISTS hash field可用版本： &gt;= 2.0.0 时间复杂度： O(1) 检查给定域 field 是否存在于哈希表 hash 当中。 返回值HEXISTS 命令在给定域存在时返回 1 ， 在给定域不存在时返回 0 。 代码示例给定域不存在：12redis&gt; HEXISTS phone myphone(integer) 0 给定域存在：12345redis&gt; HSET phone myphone nokia-1110(integer) 1redis&gt; HEXISTS phone myphone(integer) 1 HDELHDEL key field [field …]可用版本：&gt;= 2.0.0 时间复杂度:O(N)， N 为要删除的域的数量。 删除哈希表 key 中的一个或多个指定域，不存在的域将被忽略。 Note 在Redis2.4以下的版本里， HDEL 每次只能删除单个域，如果你需要在一个原子时间内删除多个域，请将命令包含在 MULTI / EXEC 块内。 返回值被成功移除的域的数量，不包括被忽略的域。 代码示例1234567891011121314151617181920212223242526# 测试数据redis&gt; HGETALL abbr1) "a"2) "apple"3) "b"4) "banana"5) "c"6) "cat"7) "d"8) "dog"# 删除单个域redis&gt; HDEL abbr a(integer) 1# 删除不存在的域redis&gt; HDEL abbr not-exists-field(integer) 0# 删除多个域redis&gt; HDEL abbr b c(integer) 2redis&gt; HGETALL abbr1) "d"2) "dog" HLENHLEN key时间复杂度：O(1) 返回哈希表 key 中域的数量。 返回值哈希表中域的数量。 当 key 不存在时，返回 0 。 代码示例1234567891011121314redis&gt; HSET db redis redis.com(integer) 1redis&gt; HSET db mysql mysql.com(integer) 1redis&gt; HLEN db(integer) 2redis&gt; HSET db mongodb mongodb.org(integer) 1redis&gt; HLEN db(integer) 3 HSTRLENHSTRLEN key field可用版本：&gt;= 3.2.0 时间复杂度：O(1) 返回哈希表 key 中， 与给定域 field 相关联的值的字符串长度（string length）。 如果给定的键或者域不存在， 那么命令返回 0 。 返回值一个整数。 代码示例1234567891011redis&gt; HMSET myhash f1 "HelloWorld" f2 "99" f3 "-256"OKredis&gt; HSTRLEN myhash f1(integer) 10redis&gt; HSTRLEN myhash f2(integer) 2redis&gt; HSTRLEN myhash f3(integer) 4 HINCRBYHINCRBY key field increment可用版本：&gt;= 2.0.0 时间复杂度：O(1) 为哈希表 key 中的域 field 的值加上增量 increment 。 增量也可以为负数，相当于对给定域进行减法操作。 如果 key 不存在，一个新的哈希表被创建并执行 HINCRBY 命令。 如果域 field 不存在，那么在执行命令前，域的值被初始化为 0 。 对一个储存字符串值的域 field 执行 HINCRBY 命令将造成一个错误。 本操作的值被限制在 64 位(bit)有符号数字表示之内。 返回值执行 HINCRBY 命令之后，哈希表 key 中域 field 的值。 代码示例1234567891011121314151617181920212223242526272829303132# increment 为正数redis&gt; HEXISTS counter page_view # 对空域进行设置(integer) 0redis&gt; HINCRBY counter page_view 200(integer) 200redis&gt; HGET counter page_view"200"# increment 为负数redis&gt; HGET counter page_view"200"redis&gt; HINCRBY counter page_view -50(integer) 150redis&gt; HGET counter page_view"150"# 尝试对字符串值的域执行HINCRBY命令redis&gt; HSET myhash string hello,world # 设定一个字符串值(integer) 1redis&gt; HGET myhash string"hello,world"redis&gt; HINCRBY myhash string 1 # 命令执行失败，错误。(error) ERR hash value is not an integerredis&gt; HGET myhash string # 原值不变"hello,world" HINCRBYFLOATHINCRBYFLOAT key field increment可用版本：&gt;= 2.6.0 时间复杂度：O(1) 为哈希表 key 中的域 field 加上浮点数增量 increment 。 如果哈希表中没有域 field ，那么 HINCRBYFLOAT 会先将域 field 的值设为 0 ，然后再执行加法操作。 如果键 key 不存在，那么 HINCRBYFLOAT 会先创建一个哈希表，再创建域 field ，最后再执行加法操作。 当以下任意一个条件发生时，返回一个错误： 域 field 的值不是字符串类型(因为 redis 中的数字和浮点数都以字符串的形式保存，所以它们都属于字符串类型） 域 field 当前的值或给定的增量 increment 不能解释(parse)为双精度浮点数(double precision floating point number) 返回值执行加法操作之后 field 域的值。 代码示例1234567891011121314151617181920212223242526272829303132# 值和增量都是普通小数redis&gt; HSET mykey field 10.50(integer) 1redis&gt; HINCRBYFLOAT mykey field 0.1"10.6"# 值和增量都是指数符号redis&gt; HSET mykey field 5.0e3(integer) 0redis&gt; HINCRBYFLOAT mykey field 2.0e2"5200"# 对不存在的键执行 HINCRBYFLOATredis&gt; EXISTS price(integer) 0redis&gt; HINCRBYFLOAT price milk 3.5"3.5"redis&gt; HGETALL price1) "milk"2) "3.5"# 对不存在的域进行 HINCRBYFLOATredis&gt; HGETALL price1) "milk"2) "3.5"redis&gt; HINCRBYFLOAT price coffee 4.5 # 新增 coffee 域"4.5"redis&gt; HGETALL price1) "milk"2) "3.5"3) "coffee"4) "4.5" HMSETHMSET key field value [field value …]可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为 field-value 对的数量。 同时将多个 field-value (域-值)对设置到哈希表 key 中。 此命令会覆盖哈希表中已存在的域。 如果 key 不存在，一个空哈希表被创建并执行 HMSET 操作。 返回值如果命令执行成功，返回 OK 。 当 key 不是哈希表(hash)类型时，返回一个错误。 代码示例12345678redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HGET website google"www.google.com"redis&gt; HGET website yahoo"www.yahoo.com" HMGETHMGET key field [field …]可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为给定域的数量。 返回哈希表 key 中，一个或多个给定域的值。 如果给定的域不存在于哈希表，那么返回一个 nil 值。 因为不存在的 key 被当作一个空哈希表来处理，所以对一个不存在的 key 进行 HMGET 操作将返回一个只带有 nil 值的表。 返回值一个包含多个给定域的关联值的表，表值的排列顺序和给定域参数的请求顺序一样。 代码示例1234567redis&gt; HMSET pet dog "doudou" cat "nounou" # 一次设置多个域OKredis&gt; HMGET pet dog cat fake_pet # 返回值的顺序和传入参数的顺序一样1) "doudou"2) "nounou"3) (nil) HKEYSHKEYS key可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为哈希表的大小。 返回哈希表 key 中的所有域。 返回值一个包含哈希表中所有域的表。当 key 不存在时，返回一个空表。 示例代码1234567891011121314# 哈希表非空redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HKEYS website1) "google"2) "yahoo"# 空哈希表/key不存在redis&gt; EXISTS fake_key(integer) 0redis&gt; HKEYS fake_key(empty list or set) HVALSHVALS key可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为哈希表的大小。 返回哈希表 key 中所有域的值。 返回值一个包含哈希表中所有值的表。 当 key 不存在时，返回一个空表。 代码示例1234567891011121314# 非空哈希表redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HVALS website1) "www.google.com"2) "www.yahoo.com"# 空哈希表/不存在的keyredis&gt; EXISTS not_exists(integer) 0redis&gt; HVALS not_exists(empty list or set) HGETALLHGETALL key可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为哈希表的大小。 返回哈希表 key 中，所有的域和值。 在返回值里，紧跟每个域名(field name)之后是域的值(value)，所以返回值的长度是哈希表大小的两倍。 返回值以列表形式返回哈希表的域和域的值。 若 key 不存在，返回空列表。 代码示例1234567891011redis&gt; HSET people jack "Jack Sparrow"(integer) 1redis&gt; HSET people gump "Forrest Gump"(integer) 1redis&gt; HGETALL people1) "jack" # 域2) "Jack Sparrow" # 值3) "gump"4) "Forrest Gump" HSCANHSCAN key cursor [MATCH pattern] [COUNT count]具体信息请参考 SCAN cursor [MATCH pattern] [COUNT count] 命令。 列表LPUSHLPUSH key value [value …]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将一个或多个值 value 插入到列表 key 的表头 如果有多个 value 值，那么各个 value 值按从左到右的顺序依次插入到表头： 比如说，对空列表 mylist 执行命令 LPUSH mylist a b c ，列表的值将是 c b a ，这等同于原子性地执行 LPUSH mylist a 、 LPUSH mylist b 和 LPUSH mylist c 三个命令。 如果 key 不存在，一个空列表会被创建并执行 LPUSH 操作。 当 key 存在但不是列表类型时，返回一个错误。 Note 在Redis 2.4版本以前的 LPUSH 命令，都只接受单个 value 值。 返回值执行 LPUSH 命令后，列表的长度。 代码示例1234567891011121314151617181920# 加入单个元素redis&gt; LPUSH languages python(integer) 1# 加入重复元素redis&gt; LPUSH languages python(integer) 2redis&gt; LRANGE languages 0 -1 # 列表允许重复元素1) "python"2) "python"# 加入多个元素redis&gt; LPUSH mylist a b c(integer) 3redis&gt; LRANGE mylist 0 -11) "c"2) "b"3) "a" LPUSHXLPUSHX key value可用版本： &gt;= 2.2.0 时间复杂度： O(1) 将值 value 插入到列表 key 的表头，当且仅当 key 存在并且是一个列表。 和 LPUSH key value [value …] 命令相反，当 key 不存在时， LPUSHX 命令什么也不做。 返回值LPUSHX 命令执行之后，表的长度。 代码示例1234567891011121314151617# 对空列表执行 LPUSHXredis&gt; LLEN greet # greet 是一个空列表(integer) 0redis&gt; LPUSHX greet "hello" # 尝试 LPUSHX，失败，因为列表为空(integer) 0# 对非空列表执行 LPUSHXredis&gt; LPUSH greet "hello" # 先用 LPUSH 创建一个有一个元素的列表(integer) 1redis&gt; LPUSHX greet "good morning" # 这次 LPUSHX 执行成功(integer) 2redis&gt; LRANGE greet 0 -11) "good morning"2) "hello" RPUSHRPUSH key value [value …]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将一个或多个值 value 插入到列表 key 的表尾(最右边)。 如果有多个 value 值，那么各个 value 值按从左到右的顺序依次插入到表尾：比如对一个空列表 mylist 执行 RPUSH mylist a b c ，得出的结果列表为 a b c ，等同于执行命令 RPUSH mylist a 、 RPUSH mylist b 、 RPUSH mylist c 。 如果 key 不存在，一个空列表会被创建并执行 RPUSH 操作。 当 key 存在但不是列表类型时，返回一个错误。 Note 在 Redis 2.4 版本以前的 RPUSH 命令，都只接受单个 value 值。 返回值执行 RPUSH 操作后，表的长度。 代码示例1234567891011121314151617181920# 添加单个元素redis&gt; RPUSH languages c(integer) 1# 添加重复元素redis&gt; RPUSH languages c(integer) 2redis&gt; LRANGE languages 0 -1 # 列表允许重复元素1) "c"2) "c"# 添加多个元素redis&gt; RPUSH mylist a b c(integer) 3redis&gt; LRANGE mylist 0 -11) "a"2) "b"3) "c" RPUSHXRPUSHX key value可用版本： &gt;= 2.2.0 时间复杂度： O(1) 将值 value 插入到列表 key 的表尾，当且仅当 key 存在并且是一个列表。 和 RPUSH key value [value …] 命令相反，当 key 不存在时， RPUSHX 命令什么也不做。 返回值RPUSHX 命令执行之后，表的长度。 代码示例1234567891011121314151617# key不存在redis&gt; LLEN greet(integer) 0redis&gt; RPUSHX greet "hello" # 对不存在的 key 进行 RPUSHX，PUSH 失败。(integer) 0# key 存在且是一个非空列表redis&gt; RPUSH greet "hi" # 先用 RPUSH 插入一个元素(integer) 1redis&gt; RPUSHX greet "hello" # greet 现在是一个列表类型，RPUSHX 操作成功。(integer) 2redis&gt; LRANGE greet 0 -11) "hi"2) "hello" LPOPLPOP key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 移除并返回列表 key 的头元素。 返回值列表的头元素。 当 key 不存在时，返回 nil 。 代码示例1234567891011redis&gt; LLEN course(integer) 0redis&gt; RPUSH course algorithm001(integer) 1redis&gt; RPUSH course c++101(integer) 2redis&gt; LPOP course # 移除头元素"algorithm001" RPOPRPOP key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 移除并返回列表 key 的尾元素。 返回值列表的尾元素。 当 key 不存在时，返回 nil 。 代码示例123456789101112131415redis&gt; RPUSH mylist "one"(integer) 1redis&gt; RPUSH mylist "two"(integer) 2redis&gt; RPUSH mylist "three"(integer) 3redis&gt; RPOP mylist # 返回被弹出的元素"three"redis&gt; LRANGE mylist 0 -1 # 列表剩下的元素1) "one"2) "two" RPOPLPUSHRPOPLPUSH source destination可用版本： &gt;= 1.2.0 时间复杂度： O(1) 命令 RPOPLPUSH 在一个原子时间内，执行以下两个动作： 将列表 source 中的最后一个元素(尾元素)弹出，并返回给客户端。 将 source 弹出的元素插入到列表 destination ，作为 destination 列表的的头元素。 举个例子，你有两个列表 source 和 destination ， source 列表有元素 a, b, c ， destination 列表有元素 x, y, z ，执行 RPOPLPUSH source destination 之后， source 列表包含元素 a, b ， destination 列表包含元素 c, x, y, z ，并且元素 c 会被返回给客户端。 如果 source 不存在，值 nil 被返回，并且不执行其他动作。 如果 source 和 destination 相同，则列表中的表尾元素被移动到表头，并返回该元素，可以把这种特殊情况视作列表的旋转(rotation)操作。 返回值被弹出的元素。 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# source 和 destination 不同redis&gt; LRANGE alpha 0 -1 # 查看所有元素1) "a"2) "b"3) "c"4) "d"redis&gt; RPOPLPUSH alpha reciver # 执行一次 RPOPLPUSH 看看"d"redis&gt; LRANGE alpha 0 -11) "a"2) "b"3) "c"redis&gt; LRANGE reciver 0 -11) "d"redis&gt; RPOPLPUSH alpha reciver # 再执行一次，证实 RPOP 和 LPUSH 的位置正确"c"redis&gt; LRANGE alpha 0 -11) "a"2) "b"redis&gt; LRANGE reciver 0 -11) "c"2) "d"# source 和 destination 相同redis&gt; LRANGE number 0 -11) "1"2) "2"3) "3"4) "4"redis&gt; RPOPLPUSH number number"4"redis&gt; LRANGE number 0 -1 # 4 被旋转到了表头1) "4"2) "1"3) "2"4) "3"redis&gt; RPOPLPUSH number number"3"redis&gt; LRANGE number 0 -1 # 这次是 3 被旋转到了表头1) "3"2) "4"3) "1"4) "2" 模式： 安全的队列Redis的列表经常被用作队列(queue)，用于在不同程序之间有序地交换消息(message)。一个客户端通过 LPUSH key value [value …] 命令将消息放入队列中，而另一个客户端通过 RPOP key 或者 BRPOP key [key …] timeout 命令取出队列中等待时间最长的消息。 不幸的是，上面的队列方法是『不安全』的，因为在这个过程中，一个客户端可能在取出一个消息之后崩溃，而未处理完的消息也就因此丢失。 使用 RPOPLPUSH 命令(或者它的阻塞版本 BRPOPLPUSH source destination timeout )可以解决这个问题：因为它不仅返回一个消息，同时还将这个消息添加到另一个备份列表当中，如果一切正常的话，当一个客户端完成某个消息的处理之后，可以用 LREM key count value 命令将这个消息从备份表删除。 最后，还可以添加一个客户端专门用于监视备份表，它自动地将超过一定处理时限的消息重新放入队列中去(负责处理该消息的客户端可能已经崩溃)，这样就不会丢失任何消息了。 模式：循环列表通过使用相同的 key 作为 RPOPLPUSH 命令的两个参数，客户端可以用一个接一个地获取列表元素的方式，取得列表的所有元素，而不必像 LRANGE key start stop 命令那样一下子将所有列表元素都从服务器传送到客户端中(两种方式的总复杂度都是 O(N))。 以上的模式甚至在以下的两个情况下也能正常工作： 有多个客户端同时对同一个列表进行旋转(rotating)，它们获取不同的元素，直到所有元素都被读取完，之后又从头开始。 有客户端在向列表尾部(右边)添加新元素。 这个模式使得我们可以很容易实现这样一类系统：有 N 个客户端，需要连续不断地对一些元素进行处理，而且处理的过程必须尽可能地快。一个典型的例子就是服务器的监控程序：它们需要在尽可能短的时间内，并行地检查一组网站，确保它们的可访问性。 注意，使用这个模式的客户端是易于扩展(scala)且安全(reliable)的，因为就算接收到元素的客户端失败，元素还是保存在列表里面，不会丢失，等到下个迭代来临的时候，别的客户端又可以继续处理这些元素了。 LREMLREM key count value可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为列表的长度。 根据参数 count 的值，移除列表中与参数 value 相等的元素。 count 的值可以是以下几种： count &gt; 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。 count &lt; 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。 count = 0 : 移除表中所有与 value 相等的值。 返回值被移除元素的数量。 因为不存在的 key 被视作空表(empty list)，所以当 key 不存在时， LREM 命令总是返回 0 。 代码示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 先创建一个表，内容排列是# morning hello morning helllo morningredis&gt; LPUSH greet "morning"(integer) 1redis&gt; LPUSH greet "hello"(integer) 2redis&gt; LPUSH greet "morning"(integer) 3redis&gt; LPUSH greet "hello"(integer) 4redis&gt; LPUSH greet "morning"(integer) 5redis&gt; LRANGE greet 0 4 # 查看所有元素1) "morning"2) "hello"3) "morning"4) "hello"5) "morning"redis&gt; LREM greet 2 morning # 移除从表头到表尾，最先发现的两个 morning(integer) 2 # 两个元素被移除redis&gt; LLEN greet # 还剩 3 个元素(integer) 3redis&gt; LRANGE greet 0 21) "hello"2) "hello"3) "morning"redis&gt; LREM greet -1 morning # 移除从表尾到表头，第一个 morning(integer) 1redis&gt; LLEN greet # 剩下两个元素(integer) 2redis&gt; LRANGE greet 0 11) "hello"2) "hello"redis&gt; LREM greet 0 hello # 移除表中所有 hello(integer) 2 # 两个 hello 被移除redis&gt; LLEN greet(integer) 0 LLENLLEN key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回列表 key 的长度。 如果 key 不存在，则 key 被解释为一个空列表，返回 0 . 如果 key 不是列表类型，返回一个错误。 返回值列表 key 的长度。 代码示例12345678910111213# 空列表redis&gt; LLEN job(integer) 0# 非空列表redis&gt; LPUSH job "cook food"(integer) 1redis&gt; LPUSH job "have lunch"(integer) 2redis&gt; LLEN job(integer) 2 LINDEXLINDEX key index可用版本： &gt;= 1.0.0 时间复杂度：O(N)， N 为到达下标 index 过程中经过的元素数量。因此，对列表的头元素和尾元素执行 LINDEX 命令，复杂度为O(1)。 返回列表 key 中，下标为 index 的元素。 下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 如果 key 不是列表类型，返回一个错误。 返回值列表中下标为 index 的元素。 如果 index 参数的值不在列表的区间范围内(out of range)，返回 nil 。 代码示例1234567891011121314redis&gt; LPUSH mylist "World"(integer) 1redis&gt; LPUSH mylist "Hello"(integer) 2redis&gt; LINDEX mylist 0"Hello"redis&gt; LINDEX mylist -1"World"redis&gt; LINDEX mylist 3 # index不在 mylist 的区间范围内(nil) LINSERTLINSERT key BEFORE|AFTER pivot value可用版本： &gt;= 2.2.0 时间复杂度: O(N)， N 为寻找 pivot 过程中经过的元素数量。 将值 value 插入到列表 key 当中，位于值 pivot 之前或之后。 当 pivot 不存在于列表 key 时，不执行任何操作。 当 key 不存在时， key 被视为空列表，不执行任何操作。 如果 key 不是列表类型，返回一个错误。 返回值如果命令执行成功，返回插入操作完成之后，列表的长度。 如果没有找到 pivot ，返回 -1 。 如果 key 不存在或为空列表，返回 0 。 代码示例123456789101112131415161718192021222324redis&gt; RPUSH mylist "Hello"(integer) 1redis&gt; RPUSH mylist "World"(integer) 2redis&gt; LINSERT mylist BEFORE "World" "There"(integer) 3redis&gt; LRANGE mylist 0 -11) "Hello"2) "There"3) "World"# 对一个非空列表插入，查找一个不存在的 pivotredis&gt; LINSERT mylist BEFORE "go" "let's"(integer) -1 # 失败# 对一个空列表执行 LINSERT 命令redis&gt; EXISTS fake_list(integer) 0redis&gt; LINSERT fake_list BEFORE "nono" "gogogog"(integer) 0 # 失败 LSETLSET key index value可用版本： &gt;= 1.0.0 时间复杂度：对头元素或尾元素进行 LSET 操作，复杂度为 O(1)。其他情况下，为 O(N)， N 为列表的长度。 将列表 key 下标为 index 的元素的值设置为 value 。 当 index 参数超出范围，或对一个空列表( key 不存在)进行 LSET 时，返回一个错误。 关于列表下标的更多信息，请参考 LINDEX key index 命令。 返回值操作成功返回 ok ，否则返回错误信息。 代码示例1234567891011121314151617181920212223242526# 对空列表(key 不存在)进行 LSETredis&gt; EXISTS list(integer) 0redis&gt; LSET list 0 item(error) ERR no such key# 对非空列表进行 LSETredis&gt; LPUSH job "cook food"(integer) 1redis&gt; LRANGE job 0 01) "cook food"redis&gt; LSET job 0 "play game"OKredis&gt; LRANGE job 0 01) "play game"# index 超出范围redis&gt; LLEN list # 列表长度为 1(integer) 1redis&gt; LSET list 3 'out of range'(error) ERR index out of range LRANGELRANGE key start stop可用版本： &gt;= 1.0.0 时间复杂度: O(S+N)， S 为偏移量 start ， N 为指定区间内元素的数量。 返回列表 key 中指定区间内的元素，区间以偏移量 start 和 stop 指定。 下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 注意LRANGE命令和编程语言区间函数的区别 假如你有一个包含一百个元素的列表，对该列表执行 LRANGE list 0 10 ，结果是一个包含11个元素的列表，这表明 stop 下标也在 LRANGE 命令的取值范围之内(闭区间)，这和某些语言的区间函数可能不一致，比如Ruby的 Range.new 、 Array#slice 和Python的 range() 函数。 超出范围的下标 超出范围的下标值不会引起错误。 如果 start 下标比列表的最大下标 end ( LLEN list 减去 1 )还要大，那么 LRANGE 返回一个空列表。 如果 stop 下标比 end 下标还要大，Redis将 stop 的值设置为 end 。 返回值一个列表，包含指定区间内的元素。 代码示例123456789101112redis&gt; RPUSH fp-language lisp(integer) 1redis&gt; LRANGE fp-language 0 01) "lisp"redis&gt; RPUSH fp-language scheme(integer) 2redis&gt; LRANGE fp-language 0 11) "lisp"2) "scheme" LTRIMLTRIM key start stop可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 为被移除的元素的数量。 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 举个例子，执行命令 LTRIM list 0 2 ，表示只保留列表 list 的前三个元素，其余元素全部删除。 下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 当 key 不是列表类型时，返回一个错误。 LTRIM 命令通常和 LPUSH key value [value …] 命令或 RPUSH key value [value …] 命令配合使用，举个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172LPUSH log newest_logLTRIM log 0 99```bash这个例子模拟了一个日志程序，每次将最新日志 newest_log 放到 log 列表中，并且只保留最新的 100 项。注意当这样使用 LTRIM 命令时，时间复杂度是O(1)，因为平均情况下，每次只有一个元素被移除。**注意LTRIM命令和编程语言区间函数的区别** 假如你有一个包含一百个元素的列表 list ，对该列表执行 LTRIM list 0 10 ，结果是一个包含11个元素的列表，这表明 stop 下标也在 LTRIM 命令的取值范围之内(闭区间)，这和某些语言的区间函数可能不一致，比如Ruby的 Range.new 、 Array#slice 和Python的 range() 函数。**超出范围的下标** 超出范围的下标值不会引起错误。如果 start 下标比列表的最大下标 end ( LLEN list 减去 1 )还要大，或者 start &gt; stop ， LTRIM 返回一个空列表(因为 LTRIM 已经将整个列表清空)。如果 stop 下标比 end 下标还要大，Redis将 stop 的值设置为 end 。#### 返回值命令执行成功时，返回 ok 。#### 代码示例```bash# 情况 1： 常见情况， start 和 stop 都在列表的索引范围之内redis&gt; LRANGE alpha 0 -1 # alpha 是一个包含 5 个字符串的列表1) "h"2) "e"3) "l"4) "l"5) "o"redis&gt; LTRIM alpha 1 -1 # 删除 alpha 列表索引为 0 的元素OKredis&gt; LRANGE alpha 0 -1 # "h" 被删除了1) "e"2) "l"3) "l"4) "o"# 情况 2： stop 比列表的最大下标还要大redis&gt; LTRIM alpha 1 10086 # 保留 alpha 列表索引 1 至索引 10086 上的元素OKredis&gt; LRANGE alpha 0 -1 # 只有索引 0 上的元素 "e" 被删除了，其他元素还在1) "l"2) "l"3) "o"# 情况 3： start 和 stop 都比列表的最大下标要大，并且 start &lt; stopredis&gt; LTRIM alpha 10086 123321OKredis&gt; LRANGE alpha 0 -1 # 列表被清空(empty list or set)# 情况 4： start 和 stop 都比列表的最大下标要大，并且 start &gt; stopredis&gt; RPUSH new-alpha "h" "e" "l" "l" "o" # 重新建立一个新列表(integer) 5redis&gt; LRANGE new-alpha 0 -11) "h"2) "e"3) "l"4) "l"5) "o"redis&gt; LTRIM new-alpha 123321 10086 # 执行 LTRIMOKredis&gt; LRANGE new-alpha 0 -1 # 同样被清空(empty list or set) BLPOPBLPOP key [key …] timeout可用版本： &gt;= 2.0.0 时间复杂度： O(1) BLPOP 是列表的阻塞式(blocking)弹出原语。 它是 LPOP key 命令的阻塞版本，当给定列表内没有任何元素可供弹出的时候，连接将被 BLPOP 命令阻塞，直到等待超时或发现可弹出元素为止。 当给定多个 key 参数时，按参数 key 的先后顺序依次检查各个列表，弹出第一个非空列表的头元素。 非阻塞行为当 BLPOP 被调用时，如果给定 key 内至少有一个非空列表，那么弹出遇到的第一个非空列表的头元素，并和被弹出元素所属的列表的名字一起，组成结果返回给调用者。 当存在多个给定 key 时， BLPOP 按给定 key 参数排列的先后顺序，依次检查各个列表。 假设现在有 job 、 command 和 request 三个列表，其中 job 不存在， command 和 request 都持有非空列表。考虑以下命令： BLPOP job command request 0 BLPOP 保证返回的元素来自 command ，因为它是按”查找 job -&gt; 查找 command -&gt; 查找 request “这样的顺序，第一个找到的非空列表。 123456789101112redis&gt; DEL job command request # 确保key都被删除(integer) 0redis&gt; LPUSH command "update system..." # 为command列表增加一个值(integer) 1redis&gt; LPUSH request "visit page" # 为request列表增加一个值(integer) 1redis&gt; BLPOP job command request 0 # job 列表为空，被跳过，紧接着 command 列表的第一个元素被弹出。1) "command" # 弹出元素所属的列表2) "update system..." # 弹出元素所属的值 阻塞行为如果所有给定 key 都不存在或包含空列表，那么 BLPOP 命令将阻塞连接，直到等待超时，或有另一个客户端对给定 key 的任意一个执行 LPUSH key value [value …] 或 RPUSH key value [value …] 命令为止。 超时参数 timeout 接受一个以秒为单位的数字作为值。超时参数设为 0 表示阻塞时间可以无限期延长(block indefinitely) 。12345678910111213redis&gt; EXISTS job # 确保两个 key 都不存在(integer) 0redis&gt; EXISTS command(integer) 0redis&gt; BLPOP job command 300 # 因为key一开始不存在，所以操作会被阻塞，直到另一客户端对 job 或者 command 列表进行 PUSH 操作。1) "job" # 这里被 push 的是 job2) "do my home work" # 被弹出的值(26.26s) # 等待的秒数redis&gt; BLPOP job command 5 # 等待超时的情况(nil)(5.66s) # 等待的秒数 相同的key被多个客户端同时阻塞 相同的 key 可以被多个客户端同时阻塞。 不同的客户端被放进一个队列中，按『先阻塞先服务』(first-BLPOP，first-served)的顺序为 key 执行 BLPOP 命令。 在MULTI/EXEC事务中的BLPOPBLPOP 可以用于流水线(pipline,批量地发送多个命令并读入多个回复)，但把它用在 MULTI / EXEC 块当中没有意义。因为这要求整个服务器被阻塞以保证块执行时的原子性，该行为阻止了其他客户端执行 LPUSH key value [value …] 或 RPUSH key value [value …] 命令。 因此，一个被包裹在 MULTI / EXEC 块内的 BLPOP 命令，行为表现得就像 LPOP key 一样，对空列表返回 nil ，对非空列表弹出列表元素，不进行任何阻塞操作。 1234567891011121314151617181920212223242526# 对非空列表进行操作redis&gt; RPUSH job programming(integer) 1redis&gt; MULTIOKredis&gt; BLPOP job 30QUEUEDredis&gt; EXEC # 不阻塞，立即返回1) 1) "job" 2) "programming"# 对空列表进行操作redis&gt; LLEN job # 空列表(integer) 0redis&gt; MULTIOKredis&gt; BLPOP job 30QUEUEDredis&gt; EXEC # 不阻塞，立即返回1) (nil) 返回值如果列表为空，返回一个 nil 。 否则，返回一个含有两个元素的列表，第一个元素是被弹出元素所属的 key ，第二个元素是被弹出元素的值。 模式：事件提醒有时候，为了等待一个新元素到达数据中，需要使用轮询的方式对数据进行探查。 另一种更好的方式是，使用系统提供的阻塞原语，在新元素到达时立即进行处理，而新元素还没到达时，就一直阻塞住，避免轮询占用资源。 对于 Redis ，我们似乎需要一个阻塞版的 SPOP key 命令，但实际上，使用 BLPOP 或者 BRPOP key [key …] timeout 就能很好地解决这个问题。 使用元素的客户端(消费者)可以执行类似以下的代码：123456LOOP forever WHILE SPOP(key) returns elements ... process elements ... END BRPOP helper_keyEND 添加元素的客户端(生产者)则执行以下代码：1234MULTI SADD key element LPUSH helper_key xEXEC BRPOPBRPOP key [key …] timeout可用版本： &gt;= 2.0.0 时间复杂度： O(1) BRPOP 是列表的阻塞式(blocking)弹出原语。 它是 RPOP key 命令的阻塞版本，当给定列表内没有任何元素可供弹出的时候，连接将被 BRPOP 命令阻塞，直到等待超时或发现可弹出元素为止。 当给定多个 key 参数时，按参数 key 的先后顺序依次检查各个列表，弹出第一个非空列表的尾部元素。 关于阻塞操作的更多信息，请查看 BLPOP key [key …] timeout 命令， BRPOP 除了弹出元素的位置和 BLPOP key [key …] timeout 不同之外，其他表现一致。 返回值假如在指定时间内没有任何元素被弹出，则返回一个 nil 和等待时长。 反之，返回一个含有两个元素的列表，第一个元素是被弹出元素所属的 key ，第二个元素是被弹出元素的值。 代码示例123456789101112redis&gt; LLEN course(integer) 0redis&gt; RPUSH course algorithm001(integer) 1redis&gt; RPUSH course c++101(integer) 2redis&gt; BRPOP course 301) "course" # 被弹出元素所属的列表键2) "c++101" # 被弹出的元素 BRPOPLPUSHBRPOPLPUSH source destination timeout可用版本： &gt;= 2.2.0 时间复杂度： O(1) BRPOPLPUSH 是 RPOPLPUSH source destination 的阻塞版本，当给定列表 source 不为空时， BRPOPLPUSH 的表现和 RPOPLPUSH source destination 一样。 当列表 source 为空时， BRPOPLPUSH 命令将阻塞连接，直到等待超时，或有另一个客户端对 source 执行 LPUSH key value [value …] 或 RPUSH key value [value …] 命令为止。 超时参数 timeout 接受一个以秒为单位的数字作为值。超时参数设为 0 表示阻塞时间可以无限期延长(block indefinitely) 。 更多相关信息，请参考 RPOPLPUSH source destination 命令。 返回值假如在指定时间内没有任何元素被弹出，则返回一个 nil 和等待时长。 反之，返回一个含有两个元素的列表，第一个元素是被弹出元素的值，第二个元素是等待时长。 代码示例123456789101112131415# 非空列表redis&gt; BRPOPLPUSH msg reciver 500"hello moto" # 弹出元素的值(3.38s) # 等待时长redis&gt; LLEN reciver(integer) 1redis&gt; LRANGE reciver 0 01) "hello moto"# 空列表redis&gt; BRPOPLPUSH msg reciver 1(nil)(1.34s) 模式：安全队列参考 RPOPLPUSH source destination 命令的《安全队列》一节。 模式：循环列表参考 RPOPLPUSH source destination 命令的《循环列表》一节。 集合SADDSADD key member [member …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是被添加的元素的数量。 将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的 member 元素将被忽略。 假如 key 不存在，则创建一个只包含 member 元素作成员的集合。 当 key 不是集合类型时，返回一个错误。 Note在Redis2.4版本以前， SADD 只接受单个 member 值。 返回值被添加到集合中的新元素的数量，不包括被忽略的元素。 代码示例12345678910111213141516# 添加单个元素redis&gt; SADD bbs "discuz.net"(integer) 1# 添加重复元素redis&gt; SADD bbs "discuz.net"(integer) 0# 添加多个元素redis&gt; SADD bbs "tianya.cn" "groups.google.com"(integer) 2redis&gt; SMEMBERS bbs1) "discuz.net"2) "groups.google.com"3) "tianya.cn" SISMEMBERSISMEMBER key member可用版本： &gt;= 1.0.0 时间复杂度: O(1) 判断 member 元素是否集合 key 的成员。 返回值如果 member 元素是集合的成员，返回 1 。 如果 member 元素不是集合的成员，或 key 不存在，返回 0 。 代码示例12345678910redis&gt; SMEMBERS joe's_movies1) "hi, lady"2) "Fast Five"3) "2012"redis&gt; SISMEMBER joe's_movies "bet man"(integer) 0redis&gt; SISMEMBER joe's_movies "Fast Five"(integer) 1 SPOPSPOP key可用版本： &gt;= 1.0.0 时间复杂度: O(1) 移除并返回集合中的一个随机元素。 如果只想获取一个随机元素，但不想该元素从集合中被移除的话，可以使用 SRANDMEMBER key [count] 命令。 返回值被移除的随机元素。 当 key 不存在或 key 是空集时，返回 nil 。 代码示例1234567891011121314151617redis&gt; SMEMBERS db1) "MySQL"2) "MongoDB"3) "Redis"redis&gt; SPOP db"Redis"redis&gt; SMEMBERS db1) "MySQL"2) "MongoDB"redis&gt; SPOP db"MySQL"redis&gt; SMEMBERS db1) "MongoDB" SRANDMEMBERSRANDMEMBER key [count]可用版本： &gt;= 1.0.0 时间复杂度: 只提供 key 参数时为 O(1) 。如果提供了 count 参数，那么为 O(N) ，N 为返回数组的元素个数。 如果命令执行时，只提供了 key 参数，那么返回集合中的一个随机元素。 从 Redis 2.6 版本开始， SRANDMEMBER 命令接受可选的 count 参数： 如果 count 为正数，且小于集合基数，那么命令返回一个包含 count 个元素的数组，数组中的元素各不相同。如果 count 大于等于集合基数，那么返回整个集合。如果 count 为负数，那么命令返回一个数组，数组中的元素可能会重复出现多次，而数组的长度为 count 的绝对值。该操作和 SPOP key 相似，但 SPOP key 将随机元素从集合中移除并返回，而 SRANDMEMBER 则仅仅返回随机元素，而不对集合进行任何改动。 返回值只提供 key 参数时，返回一个元素；如果集合为空，返回 nil 。 如果提供了 count 参数，那么返回一个数组；如果集合为空，返回空数组。 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 添加元素redis&gt; SADD fruit apple banana cherry(integer) 3# 只给定 key 参数，返回一个随机元素redis&gt; SRANDMEMBER fruit"cherry"redis&gt; SRANDMEMBER fruit"apple"# 给定 3 为 count 参数，返回 3 个随机元素# 每个随机元素都不相同redis&gt; SRANDMEMBER fruit 31) "apple"2) "banana"3) "cherry"# 给定 -3 为 count 参数，返回 3 个随机元素# 元素可能会重复出现多次redis&gt; SRANDMEMBER fruit -31) "banana"2) "cherry"3) "apple"redis&gt; SRANDMEMBER fruit -31) "apple"2) "apple"3) "cherry"# 如果 count 是整数，且大于等于集合基数，那么返回整个集合redis&gt; SRANDMEMBER fruit 101) "apple"2) "banana"3) "cherry"# 如果 count 是负数，且 count 的绝对值大于集合的基数# 那么返回的数组的长度为 count 的绝对值redis&gt; SRANDMEMBER fruit -101) "banana"2) "apple"3) "banana"4) "cherry"5) "apple"6) "apple"7) "cherry"8) "apple"9) "apple"10) "banana"# SRANDMEMBER 并不会修改集合内容redis&gt; SMEMBERS fruit1) "apple"2) "cherry"3) "banana"# 集合为空时返回 nil 或者空数组redis&gt; SRANDMEMBER not-exists(nil)redis&gt; SRANDMEMBER not-eixsts 10(empty list or set) SREMSREM key member [member …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 为给定 member 元素的数量。 移除集合 key 中的一个或多个 member 元素，不存在的 member 元素会被忽略。 当 key 不是集合类型，返回一个错误。 Note在 Redis 2.4 版本以前， SREM 只接受单个 member 值。 返回值被成功移除的元素的数量，不包括被忽略的元素。 代码示例123456789101112131415161718192021# 测试数据redis&gt; SMEMBERS languages1) "c"2) "lisp"3) "python"4) "ruby"# 移除单个元素redis&gt; SREM languages ruby(integer) 1# 移除不存在元素redis&gt; SREM languages non-exists-language(integer) 0# 移除多个元素redis&gt; SREM languages lisp python c(integer) 3redis&gt; SMEMBERS languages(empty list or set) SMOVESMOVE source destination member可用版本： &gt;= 1.0.0 时间复杂度: O(1) 将 member 元素从 source 集合移动到 destination 集合。 SMOVE 是原子性操作。 如果 source 集合不存在或不包含指定的 member 元素，则 SMOVE 命令不执行任何操作，仅返回 0 。否则， member 元素从 source 集合中被移除，并添加到 destination 集合中去。 当 destination 集合已经包含 member 元素时， SMOVE 命令只是简单地将 source 集合中的 member 元素删除。 当 source 或 destination 不是集合类型时，返回一个错误。 返回值如果 member 元素被成功移除，返回 1 。 如果 member 元素不是 source 集合的成员，并且没有任何操作对 destination 集合执行，那么返回 0 。 代码示例123456789101112131415redis&gt; SMEMBERS songs1) "Billie Jean"2) "Believe Me"redis&gt; SMEMBERS my_songs(empty list or set)redis&gt; SMOVE songs my_songs "Believe Me"(integer) 1redis&gt; SMEMBERS songs1) "Billie Jean"redis&gt; SMEMBERS my_songs1) "Believe Me" SCARDSCARD key可用版本： &gt;= 1.0.0 时间复杂度: O(1) 返回集合 key 的基数(集合中元素的数量)。 返回值集合的基数。 当 key 不存在时，返回 0 。 代码示例1234567891011redis&gt; SADD tool pc printer phone(integer) 3redis&gt; SCARD tool # 非空集合(integer) 3redis&gt; DEL tool(integer) 1redis&gt; SCARD tool # 空集合(integer) 0 SMEMBERSSMEMBERS key可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 为集合的基数。 返回集合 key 中的所有成员。 不存在的 key 被视为空集合。 返回值集合中的所有成员。 代码示例123456789101112131415# key 不存在或集合为空redis&gt; EXISTS not_exists_key(integer) 0redis&gt; SMEMBERS not_exists_key(empty list or set)# 非空集合redis&gt; SADD language Ruby Python Clojure(integer) 3redis&gt; SMEMBERS language1) "Python"2) "Ruby"3) "Clojure" SSCANSSCAN key cursor [MATCH pattern] [COUNT count]详细信息请参考 SCAN cursor [MATCH pattern] [COUNT count] 命令。 SINTERSINTER key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N * M)， N 为给定集合当中基数最小的集合， M 为给定集合的个数。 返回一个集合的全部成员，该集合是所有给定集合的交集。 不存在的 key 被视为空集。 当给定集合当中有一个空集时，结果也为空集(根据集合运算定律)。 返回值交集成员的列表。 代码示例1234567891011redis&gt; SMEMBERS group_11) "LI LEI"2) "TOM"3) "JACK"redis&gt; SMEMBERS group_21) "HAN MEIMEI"2) "JACK"redis&gt; SINTER group_1 group_21) "JACK" SINTERSTORESINTERSTORE destination key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N * M)， N 为给定集合当中基数最小的集合， M 为给定集合的个数。 这个命令类似于 SINTER key [key …] 命令，但它将结果保存到 destination 集合，而不是简单地返回结果集。 如果 destination 集合已经存在，则将其覆盖。 destination 可以是 key 本身。 返回值结果集中的成员数量。 代码示例12345678910111213redis&gt; SMEMBERS songs1) "good bye joe"2) "hello,peter"redis&gt; SMEMBERS my_songs1) "good bye joe"2) "falling"redis&gt; SINTERSTORE song_interset songs my_songs(integer) 1redis&gt; SMEMBERS song_interset1) "good bye joe" SUNIONSUNION key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 返回一个集合的全部成员，该集合是所有给定集合的并集。 不存在的 key 被视为空集。 返回值并集成员的列表。 代码示例123456789redis&gt; SMEMBERS songs1) "Billie Jean"redis&gt; SMEMBERS my_songs1) "Believe Me"redis&gt; SUNION songs my_songs1) "Billie Jean"2) "Believe Me" SUNIONSTORESUNIONSTORE destination key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 这个命令类似于 SUNION key [key …] 命令，但它将结果保存到 destination 集合，而不是简单地返回结果集。 如果 destination 已经存在，则将其覆盖。 destination 可以是 key 本身。 返回值结果集中的元素数量。 代码示例12345678910111213141516redis&gt; SMEMBERS NoSQL1) "MongoDB"2) "Redis"redis&gt; SMEMBERS SQL1) "sqlite"2) "MySQL"redis&gt; SUNIONSTORE db NoSQL SQL(integer) 4redis&gt; SMEMBERS db1) "MySQL"2) "sqlite"3) "MongoDB"4) "Redis" SDIFFSDIFF key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 返回一个集合的全部成员，该集合是所有给定集合之间的差集。 不存在的 key 被视为空集。 返回值一个包含差集成员的列表。 代码示例12345678910111213redis&gt; SMEMBERS peter's_movies1) "bet man"2) "start war"3) "2012"redis&gt; SMEMBERS joe's_movies1) "hi, lady"2) "Fast Five"3) "2012"redis&gt; SDIFF peter's_movies joe's_movies1) "bet man"2) "start war" SDIFFSTORESDIFFSTORE destination key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 这个命令的作用和 SDIFF key [key …] 类似，但它将结果保存到 destination 集合，而不是简单地返回结果集。 如果 destination 集合已经存在，则将其覆盖。 destination 可以是 key 本身。 返回值结果集中的元素数量。 代码示例12345678910111213141516redis&gt; SMEMBERS joe's_movies1) "hi, lady"2) "Fast Five"3) "2012"redis&gt; SMEMBERS peter's_movies1) "bet man"2) "start war"3) "2012"redis&gt; SDIFFSTORE joe_diff_peter joe's_movies peter's_movies(integer) 2redis&gt; SMEMBERS joe_diff_peter1) "hi, lady"2) "Fast Five" 有序集合ZADDZADD key score member [[score member] [score member] …]可用版本： &gt;= 1.2.0 时间复杂度: O(M*log(N))， N 是有序集的基数， M 为成功添加的新成员的数量。 将一个或多个 member 元素及其 score 值加入到有序集 key 当中。 如果某个 member 已经是有序集的成员，那么更新这个 member 的 score 值，并通过重新插入这个 member 元素，来保证该 member 在正确的位置上。 score 值可以是整数值或双精度浮点数。 如果 key 不存在，则创建一个空的有序集并执行 ZADD 操作。 当 key 存在但不是有序集类型时，返回一个错误。 Note在 Redis 2.4 版本以前， ZADD 每次只能添加一个元素。 返回值被成功添加的新成员的数量，不包括那些被更新的、已经存在的成员。 代码示例123456789101112131415161718192021222324252627282930313233343536373839# 添加单个元素redis&gt; ZADD page_rank 10 google.com(integer) 1# 添加多个元素redis&gt; ZADD page_rank 9 baidu.com 8 bing.com(integer) 2redis&gt; ZRANGE page_rank 0 -1 WITHSCORES1) "bing.com"2) "8"3) "baidu.com"4) "9"5) "google.com"6) "10"# 添加已存在元素，且 score 值不变redis&gt; ZADD page_rank 10 google.com(integer) 0redis&gt; ZRANGE page_rank 0 -1 WITHSCORES # 没有改变1) "bing.com"2) "8"3) "baidu.com"4) "9"5) "google.com"6) "10"# 添加已存在元素，但是改变 score 值redis&gt; ZADD page_rank 6 bing.com(integer) 0redis&gt; ZRANGE page_rank 0 -1 WITHSCORES # bing.com 元素的 score 值被改变1) "bing.com"2) "6"3) "baidu.com"4) "9"5) "google.com"6) "10" ZSCOREZSCORE key member可用版本： &gt;= 1.2.0 时间复杂度: O(1) 返回有序集 key 中，成员 member 的 score 值。 如果 member 元素不是有序集 key 的成员，或 key 不存在，返回 nil 。 返回值member 成员的 score 值，以字符串形式表示。 代码示例12345678910redis&gt; ZRANGE salary 0 -1 WITHSCORES # 测试数据1) "tom"2) "2000"3) "peter"4) "3500"5) "jack"6) "5000"redis&gt; ZSCORE salary peter # 注意返回值是字符串"3500" ZINCRBYZINCRBY key increment member可用版本： &gt;= 1.2.0 时间复杂度: O(log(N)) 为有序集 key 的成员 member 的 score 值加上增量 increment 。 可以通过传递一个负数值 increment ，让 score 减去相应的值，比如 ZINCRBY key -5 member ，就是让 member 的 score 值减去 5 。 当 key 不存在，或 member 不是 key 的成员时， ZINCRBY key increment member 等同于 ZADD key increment member 。 当 key 不是有序集类型时，返回一个错误。 score 值可以是整数值或双精度浮点数。 返回值member 成员的新 score 值，以字符串形式表示。 代码示例12345redis&gt; ZSCORE salary tom"2000"redis&gt; ZINCRBY salary 2000 tom # tom 加薪啦！"4000" ZCARDZCARD key可用版本： &gt;= 1.2.0 时间复杂度: O(1) 返回有序集 key 的基数。 返回值当 key 存在且是有序集类型时，返回有序集的基数。 当 key 不存在时，返回 0 。 代码示例1234567891011121314151617redis &gt; ZADD salary 2000 tom # 添加一个成员(integer) 1redis &gt; ZCARD salary(integer) 1redis &gt; ZADD salary 5000 jack # 再添加一个成员(integer) 1redis &gt; ZCARD salary(integer) 2redis &gt; EXISTS non_exists_key # 对不存在的 key 进行 ZCARD 操作(integer) 0redis &gt; ZCARD non_exists_key(integer) 0 ZCOUNTZCOUNT key min max可用版本： &gt;= 2.0.0 时间复杂度: O(log(N))， N 为有序集的基数。 返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。 关于参数 min 和 max 的详细使用方法，请参考 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 命令。 返回值score 值在 min 和 max 之间的成员的数量。 代码示例12345678910111213redis&gt; ZRANGE salary 0 -1 WITHSCORES # 测试数据1) "jack"2) "2000"3) "peter"4) "3500"5) "tom"6) "5000"redis&gt; ZCOUNT salary 2000 5000 # 计算薪水在 2000-5000 之间的人数(integer) 3redis&gt; ZCOUNT salary 3000 5000 # 计算薪水在 3000-5000 之间的人数(integer) 2 ZRANGEZRANGE key start stop [WITHSCORES]可用版本： &gt;= 1.2.0 时间复杂度: O(log(N)+M)， N 为有序集的基数，而 M 为结果集的基数。 返回有序集 key 中，指定区间内的成员。 其中成员的位置按 score 值递增(从小到大)来排序。 具有相同 score 值的成员按字典序(lexicographical order )来排列。 如果你需要成员按 score 值递减(从大到小)来排列，请使用 ZREVRANGE key start stop [WITHSCORES] 命令。 下标参数 start 和 stop 都以 0 为底，也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。 你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。 超出范围的下标并不会引起错误。 比如说，当 start 的值比有序集的最大下标还要大，或是 start &gt; stop 时， ZRANGE 命令只是简单地返回一个空列表。 另一方面，假如 stop 参数的值比有序集的最大下标还要大，那么 Redis 将 stop 当作最大下标来处理。 可以通过使用 WITHSCORES 选项，来让成员和它的 score 值一并返回，返回列表以 value1,score1, …, valueN,scoreN 的格式表示。 客户端库可能会返回一些更复杂的数据类型，比如数组、元组等。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例123456789101112131415161718192021222324redis &gt; ZRANGE salary 0 -1 WITHSCORES # 显示整个有序集成员1) "jack"2) "3500"3) "tom"4) "5000"5) "boss"6) "10086"redis &gt; ZRANGE salary 1 2 WITHSCORES # 显示有序集下标区间 1 至 2 的成员1) "tom"2) "5000"3) "boss"4) "10086"redis &gt; ZRANGE salary 0 200000 WITHSCORES # 测试 end 下标超出最大下标时的情况1) "jack"2) "3500"3) "tom"4) "5000"5) "boss"6) "10086"redis &gt; ZRANGE salary 200000 3000000 WITHSCORES # 测试当给定区间不存在于有序集时的情况(empty list or set) ZREVRANGEZREVRANGE key start stop [WITHSCORES]可用版本： &gt;= 1.2.0 时间复杂度: O(log(N)+M)， N 为有序集的基数，而 M 为结果集的基数。 返回有序集 key 中，指定区间内的成员。 其中成员的位置按 score 值递减(从大到小)来排列。 具有相同 score 值的成员按字典序的逆序(reverse lexicographical order)排列。 除了成员按 score 值递减的次序排列这一点外， ZREVRANGE 命令的其他方面和 ZRANGE key start stop [WITHSCORES] 命令一样。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例123456789101112131415redis&gt; ZRANGE salary 0 -1 WITHSCORES # 递增排列1) "peter"2) "3500"3) "tom"4) "4000"5) "jack"6) "5000"redis&gt; ZREVRANGE salary 0 -1 WITHSCORES # 递减排列1) "jack"2) "5000"3) "tom"4) "4000"5) "peter"6) "3500" ZRANGEBYSCOREZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]可用版本： &gt;= 1.0.5 时间复杂度: O(log(N)+M)， N 为有序集的基数， M 为被结果集的基数。 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 具有相同 score 值的成员按字典序(lexicographical order)来排列(该属性是有序集提供的，不需要额外的计算)。 可选的 LIMIT 参数指定返回结果的数量及区间(就像SQL中的 SELECT LIMIT offset, count )，注意当 offset 很大时，定位 offset 的操作可能需要遍历整个有序集，此过程最坏复杂度为 O(N) 时间。 可选的 WITHSCORES 参数决定结果集是单单返回有序集的成员，还是将有序集成员及其 score 值一起返回。 该选项自 Redis 2.0 版本起可用。 区间及无限min 和 max 可以是 -inf 和 +inf ，这样一来，你就可以在不知道有序集的最低和最高 score 值的情况下，使用 ZRANGEBYSCORE 这类命令。 默认情况下，区间的取值使用闭区间 (小于等于或大于等于)，你也可以通过给参数前增加 ( 符号来使用可选的开区间 (小于或大于)。 举个例子：ZRANGEBYSCORE zset (1 5返回所有符合条件 1 &lt; score &lt;= 5 的成员，而 ZRANGEBYSCORE zset (5 (10则返回所有符合条件 5 &lt; score &lt; 10 的成员。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例12345678910111213141516171819202122232425262728redis&gt; ZADD salary 2500 jack # 测试数据(integer) 0redis&gt; ZADD salary 5000 tom(integer) 0redis&gt; ZADD salary 12000 peter(integer) 0redis&gt; ZRANGEBYSCORE salary -inf +inf # 显示整个有序集1) "jack"2) "tom"3) "peter"redis&gt; ZRANGEBYSCORE salary -inf +inf WITHSCORES # 显示整个有序集及成员的 score 值1) "jack"2) "2500"3) "tom"4) "5000"5) "peter"6) "12000"redis&gt; ZRANGEBYSCORE salary -inf 5000 WITHSCORES # 显示工资 &lt;=5000 的所有成员1) "jack"2) "2500"3) "tom"4) "5000"redis&gt; ZRANGEBYSCORE salary (5000 400000 # 显示工资大于 5000 小于等于 400000 的成员1) "peter" ZREVRANGEBYSCOREZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]可用版本： &gt;= 2.2.0 时间复杂度: O(log(N)+M)， N 为有序集的基数， M 为结果集的基数。 返回有序集 key 中， score 值介于 max 和 min 之间(默认包括等于 max 或 min )的所有的成员。有序集成员按 score 值递减(从大到小)的次序排列。 具有相同 score 值的成员按字典序的逆序(reverse lexicographical order )排列。 除了成员按 score 值递减的次序排列这一点外， ZREVRANGEBYSCORE 命令的其他方面和 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 命令一样。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例12345678910111213141516171819redis &gt; ZADD salary 10086 jack(integer) 1redis &gt; ZADD salary 5000 tom(integer) 1redis &gt; ZADD salary 7500 peter(integer) 1redis &gt; ZADD salary 3500 joe(integer) 1redis &gt; ZREVRANGEBYSCORE salary +inf -inf # 逆序排列所有成员1) "jack"2) "peter"3) "tom"4) "joe"redis &gt; ZREVRANGEBYSCORE salary 10000 2000 # 逆序排列薪水介于 10000 和 2000 之间的成员1) "peter"2) "tom"3) "joe" ZRANKZRANK key member可用版本： &gt;= 2.0.0 时间复杂度: O(log(N)) 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。 排名以 0 为底，也就是说， score 值最小的成员排名为 0 。 使用 ZREVRANK key member 命令可以获得成员按 score 值递减(从大到小)排列的排名。 返回值如果 member 是有序集 key 的成员，返回 member 的排名。 如果 member 不是有序集 key 的成员，返回 nil 。 代码示例12345678910redis&gt; ZRANGE salary 0 -1 WITHSCORES # 显示所有成员及其 score 值1) "peter"2) "3500"3) "tom"4) "4000"5) "jack"6) "5000"redis&gt; ZRANK salary tom # 显示 tom 的薪水排名，第二(integer) 1 ZREVRANKZREVRANK key member可用版本： &gt;= 2.0.0 时间复杂度: O(log(N)) 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递减(从大到小)排序。 排名以 0 为底，也就是说， score 值最大的成员排名为 0 。 使用 ZRANK key member 命令可以获得成员按 score 值递增(从小到大)排列的排名。 返回值如果 member 是有序集 key 的成员，返回 member 的排名。 如果 member 不是有序集 key 的成员，返回 nil 。 代码示例12345678910111213redis 127.0.0.1:6379&gt; ZRANGE salary 0 -1 WITHSCORES # 测试数据1) "jack"2) "2000"3) "peter"4) "3500"5) "tom"6) "5000"redis&gt; ZREVRANK salary peter # peter 的工资排第二(integer) 1redis&gt; ZREVRANK salary tom # tom 的工资最高(integer) 0 ZREMZREM key member [member …]可用版本： &gt;= 1.2.0 时间复杂度: O(M*log(N))， N 为有序集的基数， M 为被成功移除的成员的数量。 移除有序集 key 中的一个或多个成员，不存在的成员将被忽略。 当 key 存在但不是有序集类型时，返回一个错误。 Note在 Redis 2.4 版本以前， ZREM 每次只能删除一个元素。 返回值被成功移除的成员的数量，不包括被忽略的成员。 代码示例1234567891011121314151617181920212223242526272829# 测试数据redis&gt; ZRANGE page_rank 0 -1 WITHSCORES1) "bing.com"2) "8"3) "baidu.com"4) "9"5) "google.com"6) "10"# 移除单个元素redis&gt; ZREM page_rank google.com(integer) 1redis&gt; ZRANGE page_rank 0 -1 WITHSCORES1) "bing.com"2) "8"3) "baidu.com"4) "9"# 移除多个元素redis&gt; ZREM page_rank baidu.com bing.com(integer) 2redis&gt; ZRANGE page_rank 0 -1 WITHSCORES(empty list or set)# 移除不存在元素redis&gt; ZREM page_rank non-exists-element(integer) 0 ZREMRANGEBYRANKZREMRANGEBYRANK key start stop可用版本： &gt;= 2.0.0 时间复杂度: O(log(N)+M)， N 为有序集的基数，而 M 为被移除成员的数量。 移除有序集 key 中，指定排名(rank)区间内的所有成员。 区间分别以下标参数 start 和 stop 指出，包含 start 和 stop 在内。 下标参数 start 和 stop 都以 0 为底，也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。 你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。 返回值被移除成员的数量。 代码示例12345678910111213redis&gt; ZADD salary 2000 jack(integer) 1redis&gt; ZADD salary 5000 tom(integer) 1redis&gt; ZADD salary 3500 peter(integer) 1redis&gt; ZREMRANGEBYRANK salary 0 1 # 移除下标 0 至 1 区间内的成员(integer) 2redis&gt; ZRANGE salary 0 -1 WITHSCORES # 有序集只剩下一个成员1) "tom"2) "5000" ZREMRANGEBYSCOREZREMRANGEBYSCORE key min max可用版本： &gt;= 1.2.0 时间复杂度： O(log(N)+M)， N 为有序集的基数，而 M 为被移除成员的数量。 移除有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。 自版本2.1.6开始， score 值等于 min 或 max 的成员也可以不包括在内，详情请参见 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 命令。 返回值被移除成员的数量。 代码示例1234567891011121314redis&gt; ZRANGE salary 0 -1 WITHSCORES # 显示有序集内所有成员及其 score 值1) "tom"2) "2000"3) "peter"4) "3500"5) "jack"6) "5000"redis&gt; ZREMRANGEBYSCORE salary 1500 3500 # 移除所有薪水在 1500 到 3500 内的员工(integer) 2redis&gt; ZRANGE salary 0 -1 WITHSCORES # 剩下的有序集成员1) "jack"2) "5000" ZRANGEBYLEXZRANGEBYLEX key min max [LIMIT offset count]可用版本： &gt;= 2.8.9 时间复杂度：O(log(N)+M)， 其中 N 为有序集合的元素数量， 而 M 则是命令返回的元素数量。 如果 M 是一个常数（比如说，用户总是使用 LIMIT 参数来返回最先的 10 个元素）， 那么命令的复杂度也可以看作是 O(log(N)) 。 当有序集合的所有成员都具有相同的分值时， 有序集合的元素会根据成员的字典序（lexicographical ordering）来进行排序， 而这个命令则可以返回给定的有序集合键 key 中， 值介于 min 和 max 之间的成员。 如果有序集合里面的成员带有不同的分值， 那么命令返回的结果是未指定的（unspecified）。 命令会使用 C 语言的 memcmp() 函数， 对集合中的每个成员进行逐个字节的对比（byte-by-byte compare）， 并按照从低到高的顺序， 返回排序后的集合成员。 如果两个字符串有一部分内容是相同的话， 那么命令会认为较长的字符串比较短的字符串要大。 可选的 LIMIT offset count 参数用于获取指定范围内的匹配元素 （就像 SQL 中的 SELECT LIMIT offset count 语句）。 需要注意的一点是， 如果 offset 参数的值非常大的话， 那么命令在返回结果之前， 需要先遍历至 offset 所指定的位置， 这个操作会为命令加上最多 O(N) 复杂度。 如何指定范围区间合法的 min 和 max 参数必须包含 ( 或者 [ ， 其中 ( 表示开区间（指定的值不会被包含在范围之内）， 而 [ 则表示闭区间（指定的值会被包含在范围之内）。 特殊值 + 和 - 在 min 参数以及 max 参数中具有特殊的意义， 其中 + 表示正无限， 而 - 表示负无限。 因此， 向一个所有成员的分值都相同的有序集合发送命令 ZRANGEBYLEX \&lt;zset> - + ， 命令将返回有序集合中的所有元素。 返回值数组回复：一个列表，列表里面包含了有序集合在指定范围内的成员。 代码示例123456789101112131415161718redis&gt; ZADD myzset 0 a 0 b 0 c 0 d 0 e 0 f 0 g(integer) 7redis&gt; ZRANGEBYLEX myzset - [c1) "a"2) "b"3) "c"redis&gt; ZRANGEBYLEX myzset - (c1) "a"2) "b"redis&gt; ZRANGEBYLEX myzset [aaa (g1) "b"2) "c"3) "d"4) "e"5) "f" ZLEXCOUNTZLEXCOUNT key min max可用版本： &gt;= 2.8.9 时间复杂度： O(log(N))，其中 N 为有序集合包含的元素数量。 对于一个所有成员的分值都相同的有序集合键 key 来说， 这个命令会返回该集合中， 成员介于 min 和 max 范围内的元素数量。 这个命令的 min 参数和 max 参数的意义和 ZRANGEBYLEX key min max [LIMIT offset count] 命令的 min 参数和 max 参数的意义一样。 返回值整数回复：指定范围内的元素数量。 代码示例1234567891011redis&gt; ZADD myzset 0 a 0 b 0 c 0 d 0 e(integer) 5redis&gt; ZADD myzset 0 f 0 g(integer) 2redis&gt; ZLEXCOUNT myzset - +(integer) 7redis&gt; ZLEXCOUNT myzset [b [f(integer) 5 ZREMRANGEBYLEXZREMRANGEBYLEX key min max可用版本： &gt;= 2.8.9 时间复杂度： O(log(N)+M)， 其中 N 为有序集合的元素数量， 而 M 则为被移除的元素数量。 对于一个所有成员的分值都相同的有序集合键 key 来说， 这个命令会移除该集合中， 成员介于 min 和 max 范围内的所有元素。 这个命令的 min 参数和 max 参数的意义和 ZRANGEBYLEX key min max [LIMIT offset count] 命令的 min 参数和 max 参数的意义一样。 返回值整数回复：被移除的元素数量。 代码示例1234567891011121314151617181920212223242526redis&gt; ZADD myzset 0 aaaa 0 b 0 c 0 d 0 e(integer) 5redis&gt; ZADD myzset 0 foo 0 zap 0 zip 0 ALPHA 0 alpha(integer) 5redis&gt; ZRANGE myzset 0 -11) "ALPHA"2) "aaaa"3) "alpha"4) "b"5) "c"6) "d"7) "e"8) "foo"9) "zap"10) "zip"redis&gt; ZREMRANGEBYLEX myzset [alpha [omega(integer) 6redis&gt; ZRANGE myzset 0 -11) "ALPHA"2) "aaaa"3) "zap"4) "zip" ZSCANZSCAN key cursor [MATCH pattern] [COUNT count]详细信息请参考 SCAN cursor [MATCH pattern] [COUNT count] 命令。 ZUNIONSTOREZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX]可用版本：&gt;= 2.0.0 时间复杂度: O(N)+O(M log(M))， N 为给定有序集基数的总和， M 为结果集的基数。 计算给定的一个或多个有序集的并集，其中给定 key 的数量必须以 numkeys 参数指定，并将该并集(结果集)储存到 destination 。 默认情况下，结果集中某个成员的 score 值是所有给定集下该成员 score 值之 和 。 WEIGHTS使用 WEIGHTS 选项，你可以为 每个 给定有序集 分别 指定一个乘法因子(multiplication factor)，每个给定有序集的所有成员的 score 值在传递给聚合函数(aggregation function)之前都要先乘以该有序集的因子。 如果没有指定 WEIGHTS 选项，乘法因子默认设置为 1 。 AGGREGATE使用 AGGREGATE 选项，你可以指定并集的结果集的聚合方式。 默认使用的参数 SUM ，可以将所有集合中某个成员的 score 值之 和 作为结果集中该成员的 score 值；使用参数 MIN ，可以将所有集合中某个成员的 最小 score 值作为结果集中该成员的 score 值；而参数 MAX 则是将所有集合中某个成员的 最大 score 值作为结果集中该成员的 score 值。 返回值保存到 destination 的结果集的基数。 代码示例1234567891011121314151617181920212223242526272829303132redis&gt; ZRANGE programmer 0 -1 WITHSCORES1) "peter"2) "2000"3) "jack"4) "3500"5) "tom"6) "5000"redis&gt; ZRANGE manager 0 -1 WITHSCORES1) "herry"2) "2000"3) "mary"4) "3500"5) "bob"6) "4000"redis&gt; ZUNIONSTORE salary 2 programmer manager WEIGHTS 1 3 # 公司决定加薪。。。除了程序员。。。(integer) 6redis&gt; ZRANGE salary 0 -1 WITHSCORES1) "peter"2) "2000"3) "jack"4) "3500"5) "tom"6) "5000"7) "herry"8) "6000"9) "mary"10) "10500"11) "bob"12) "12000" ZINTERSTOREZINTERSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX]可用版本： &gt;= 2.0.0 时间复杂度: O(N*K)+O(M*log(M))， N 为给定 key 中基数最小的有序集， K 为给定有序集的数量， M 为结果集的基数。 计算给定的一个或多个有序集的交集，其中给定 key 的数量必须以 numkeys 参数指定，并将该交集(结果集)储存到 destination 。 默认情况下，结果集中某个成员的 score 值是所有给定集下该成员 score 值之和. 关于 WEIGHTS 和 AGGREGATE 选项的描述，参见 ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 命令。 返回值保存到 destination 的结果集的基数。 代码示例123456789101112131415161718192021222324redis &gt; ZADD mid_test 70 "Li Lei"(integer) 1redis &gt; ZADD mid_test 70 "Han Meimei"(integer) 1redis &gt; ZADD mid_test 99.5 "Tom"(integer) 1redis &gt; ZADD fin_test 88 "Li Lei"(integer) 1redis &gt; ZADD fin_test 75 "Han Meimei"(integer) 1redis &gt; ZADD fin_test 99.5 "Tom"(integer) 1redis &gt; ZINTERSTORE sum_point 2 mid_test fin_test(integer) 3redis &gt; ZRANGE sum_point 0 -1 WITHSCORES # 显示有序集内所有成员及其 score 值1) "Han Meimei"2) "145"3) "Li Lei"4) "158"5) "Tom"6) "199" HyperLogLogPFADDPFADD key element [element …]可用版本： &gt;= 2.8.9 时间复杂度： 每添加一个元素的复杂度为 O(1) 。 将任意数量的元素添加到指定的 HyperLogLog 里面。 作为这个命令的副作用， HyperLogLog 内部可能会被更新， 以便反映一个不同的唯一元素估计数量（也即是集合的基数）。 如果 HyperLogLog 估计的近似基数（approximated cardinality）在命令执行之后出现了变化， 那么命令返回 1 ， 否则返回 0 。 如果命令执行时给定的键不存在， 那么程序将先创建一个空的 HyperLogLog 结构， 然后再执行命令。 调用 PFADD key element [element …] 命令时可以只给定键名而不给定元素： 如果给定键已经是一个 HyperLogLog ， 那么这种调用不会产生任何效果；但如果给定的键不存在， 那么命令会创建一个空的 HyperLogLog ， 并向客户端返回 1 。要了解更多关于 HyperLogLog 数据结构的介绍知识， 请查阅 PFCOUNT key [key …] 命令的文档。 返回值整数回复： 如果 HyperLogLog 的内部储存被修改了， 那么返回 1 ， 否则返回 0 。 代码示例1234567891011121314151617redis&gt; PFADD databases "Redis" "MongoDB" "MySQL"(integer) 1redis&gt; PFCOUNT databases(integer) 3redis&gt; PFADD databases "Redis" # Redis 已经存在，不必对估计数量进行更新(integer) 0redis&gt; PFCOUNT databases # 元素估计数量没有变化(integer) 3redis&gt; PFADD databases "PostgreSQL" # 添加一个不存在的元素(integer) 1redis&gt; PFCOUNT databases # 估计数量增一4 PFCOUNTPFCOUNT key [key …]可用版本： &gt;= 2.8.9 时间复杂度： 当命令作用于单个 HyperLogLog 时， 复杂度为 O(1) ， 并且具有非常低的平均常数时间。 当命令作用于 N 个 HyperLogLog 时， 复杂度为 O(N) ， 常数时间也比处理单个 HyperLogLog 时要大得多。 当 PFCOUNT key [key …] 命令作用于单个键时， 返回储存在给定键的 HyperLogLog 的近似基数， 如果键不存在， 那么返回 0 。 当 PFCOUNT key [key …] 命令作用于多个键时， 返回所有给定 HyperLogLog 的并集的近似基数， 这个近似基数是通过将所有给定 HyperLogLog 合并至一个临时 HyperLogLog 来计算得出的。 通过 HyperLogLog 数据结构， 用户可以使用少量固定大小的内存， 来储存集合中的唯一元素 （每个 HyperLogLog 只需使用 12k 字节内存，以及几个字节的内存来储存键本身）。 命令返回的可见集合（observed set）基数并不是精确值， 而是一个带有 0.81% 标准错误（standard error）的近似值。 举个例子， 为了记录一天会执行多少次各不相同的搜索查询， 一个程序可以在每次执行搜索查询时调用一次 PFADD key element [element …] ， 并通过调用 PFCOUNT key [key …] 命令来获取这个记录的近似结果。 返回值整数回复： 给定 HyperLogLog 包含的唯一元素的近似数量。 代码示例1234567891011121314151617redis&gt; PFADD databases "Redis" "MongoDB" "MySQL"(integer) 1redis&gt; PFCOUNT databases(integer) 3redis&gt; PFADD databases "Redis" # Redis 已经存在，不必对估计数量进行更新(integer) 0redis&gt; PFCOUNT databases # 元素估计数量没有变化(integer) 3redis&gt; PFADD databases "PostgreSQL" # 添加一个不存在的元素(integer) 1redis&gt; PFCOUNT databases # 估计数量增一4 PFMERGEPFMERGE destkey sourcekey [sourcekey …]可用版本： &gt;= 2.8.9 时间复杂度： O(N) ， 其中 N 为被合并的 HyperLogLog 数量， 不过这个命令的常数复杂度比较高。 将多个 HyperLogLog 合并（merge）为一个 HyperLogLog ， 合并后的 HyperLogLog 的基数接近于所有输入 HyperLogLog 的可见集合（observed set）的并集。 合并得出的 HyperLogLog 会被储存在 destkey 键里面， 如果该键并不存在， 那么命令在执行之前， 会先为该键创建一个空的 HyperLogLog 。 返回值字符串回复：返回 OK 。 代码示例1234567891011redis&gt; PFADD nosql "Redis" "MongoDB" "Memcached"(integer) 1redis&gt; PFADD RDBMS "MySQL" "MSSQL" "PostgreSQL"(integer) 1redis&gt; PFMERGE databases nosql RDBMSOKredis&gt; PFCOUNT databases(integer) 6 地理位置GEOADDGEOADD key longitude latitude member [longitude latitude member …]可用版本： &gt;= 3.2.0 时间复杂度： 每添加一个元素的复杂度为 O(log(N)) ， 其中 N 为键里面包含的位置元素数量。 将给定的空间元素（纬度、经度、名字）添加到指定的键里面。 这些数据会以有序集合的形式被储存在键里面， 从而使得像 GEORADIUS 和 GEORADIUSBYMEMBER 这样的命令可以在之后通过位置查询取得这些元素。 GEOADD 命令以标准的 x,y 格式接受参数， 所以用户必须先输入经度， 然后再输入纬度。 GEOADD 能够记录的坐标是有限的： 非常接近两极的区域是无法被索引的。 精确的坐标限制由 EPSG:900913 / EPSG:3785 / OSGEO:41001 等坐标系统定义， 具体如下： 有效的经度介于 -180 度至 180 度之间。有效的纬度介于 -85.05112878 度至 85.05112878 度之间。当用户尝试输入一个超出范围的经度或者纬度时， GEOADD 命令将返回一个错误。 返回值新添加到键里面的空间元素数量， 不包括那些已经存在但是被更新的元素。 代码示例123456789101112redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEODIST Sicily Palermo Catania"166274.15156960039"redis&gt; GEORADIUS Sicily 15 37 100 km1) "Catania"redis&gt; GEORADIUS Sicily 15 37 200 km1) "Palermo"2) "Catania" GEOPOSGEOPOS key member [member …]可用版本： &gt;= 3.2.0 时间复杂度： 获取每个位置元素的复杂度为 O(log(N)) ， 其中 N 为键里面包含的位置元素数量。 从键里面返回所有给定位置元素的位置（经度和纬度）。 因为 GEOPOS 命令接受可变数量的位置元素作为输入， 所以即使用户只给定了一个位置元素， 命令也会返回数组回复。 返回值GEOPOS 命令返回一个数组， 数组中的每个项都由两个元素组成： 第一个元素为给定位置元素的经度， 而第二个元素则为给定位置元素的纬度。 当给定的位置元素不存在时， 对应的数组项为空值。 代码示例123456789redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEOPOS Sicily Palermo Catania NonExisting1) 1) "13.361389338970184" 2) "38.115556395496299"2) 1) "15.087267458438873" 2) "37.50266842333162"3) (nil) GEODISTGEODIST key member1 member2 [unit]可用版本： &gt;= 3.2.0 复杂度： O(log(N)) 返回两个给定位置之间的距离。 如果两个位置之间的其中一个不存在， 那么命令返回空值。 指定单位的参数 unit 必须是以下单位的其中一个： m 表示单位为米。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 如果用户没有显式地指定单位参数， 那么 GEODIST 默认使用米作为单位。 GEODIST 命令在计算距离时会假设地球为完美的球形， 在极限情况下， 这一假设最大会造成 0.5% 的误差。 返回值计算出的距离会以双精度浮点数的形式被返回。 如果给定的位置元素不存在， 那么命令返回空值。 代码示例1234567891011121314redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEODIST Sicily Palermo Catania"166274.15156960039"redis&gt; GEODIST Sicily Palermo Catania km"166.27415156960038"redis&gt; GEODIST Sicily Palermo Catania mi"103.31822459492736"redis&gt; GEODIST Sicily Foo Bar(nil) GEORADIUSGEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [ASC|DESC] [COUNT count]可用版本： &gt;= 3.2.0 时间复杂度： O(N+log(M))， 其中 N 为指定半径范围内的位置元素数量， 而 M 则是被返回位置元素的数量。 以给定的经纬度为中心， 返回键包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素。 范围可以使用以下其中一个单位： m 表示单位为米。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 在给定以下可选项时， 命令会返回额外的信息： WITHDIST ： 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 距离的单位和用户给定的范围单位保持一致。 WITHCOORD ： 将位置元素的经度和维度也一并返回。 WITHHASH ： 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 命令默认返回未排序的位置元素。 通过以下两个参数， 用户可以指定被返回位置元素的排序方式： ASC ： 根据中心的位置， 按照从近到远的方式返回位置元素。 DESC ： 根据中心的位置， 按照从远到近的方式返回位置元素。 在默认情况下， GEORADIUS 命令会返回所有匹配的位置元素。 虽然用户可以使用 COUNT \&lt;count> 选项去获取前 N 个匹配元素， 但是因为命令在内部可能会需要对所有被匹配的元素进行处理， 所以在对一个非常大的区域进行搜索时， 即使只使用 COUNT 选项去获取少量元素， 命令的执行速度也可能会非常慢。 但是从另一方面来说， 使用 COUNT 选项去减少需要返回的元素数量， 对于减少带宽来说仍然是非常有用的。 返回值GEORADIUS 命令返回一个数组， 具体来说： 在没有给定任何 WITH 选项的情况下， 命令只会返回一个像 [“New York”,”Milan”,”Paris”] 这样的线性（linear）列表。 在指定了 WITHCOORD 、 WITHDIST 、 WITHHASH 等选项的情况下， 命令返回一个二层嵌套数组， 内层的每个子数组就表示一个元素。 在返回嵌套数组时， 子数组的第一个元素总是位置元素的名字。 至于额外的信息， 则会作为子数组的后续元素， 按照以下顺序被返回： 以浮点数格式返回的中心与位置元素之间的距离， 单位与用户指定范围时的单位一致。 geohash 整数。 由两个元素组成的坐标，分别为经度和纬度。 举个例子， GEORADIUS Sicily 15 37 200 km withcoord withdist 这样的命令返回的每个子数组都是类似以下格式的： [“Palermo”,”190.4424”,[“13.361389338970184”,”38.115556395496299”]] 代码示例1234567891011121314151617181920212223242526redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEORADIUS Sicily 15 37 200 km WITHDIST1) 1) "Palermo" 2) "190.4424"2) 1) "Catania" 2) "56.4413"redis&gt; GEORADIUS Sicily 15 37 200 km WITHCOORD1) 1) "Palermo" 2) 1) "13.361389338970184" 2) "38.115556395496299"2) 1) "Catania" 2) 1) "15.087267458438873" 2) "37.50266842333162"redis&gt; GEORADIUS Sicily 15 37 200 km WITHDIST WITHCOORD1) 1) "Palermo" 2) "190.4424" 3) 1) "13.361389338970184" 2) "38.115556395496299"2) 1) "Catania" 2) "56.4413" 3) 1) "15.087267458438873" 2) "37.50266842333162" GEORADIUSBYMEMBERGEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [ASC|DESC] [COUNT count]可用版本： &gt;= 3.2.0 时间复杂度： O(log(N)+M)， 其中 N 为指定范围之内的元素数量， 而 M 则是被返回的元素数量。 这个命令和 GEORADIUS 命令一样， 都可以找出位于指定范围内的元素， 但是 GEORADIUSBYMEMBER 的中心点是由给定的位置元素决定的， 而不是像 GEORADIUS 那样， 使用输入的经度和纬度来决定中心点。 返回值一个数组， 数组中的每个项表示一个范围之内的位置元素。 代码示例123456789redis&gt; GEOADD Sicily 13.583333 37.316667 "Agrigento"(integer) 1redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEORADIUSBYMEMBER Sicily Agrigento 100 km1) "Agrigento"2) "Palermo" GEOHASHGEOHASH key member [member …]可用版本： &gt;= 3.2.0 时间复杂度： 寻找每个位置元素的复杂度为 O(log(N)) ， 其中 N 为给定键包含的位置元素数量。 返回一个或多个位置元素的 Geohash 表示。 返回值一个数组， 数组的每个项都是一个 geohash 。 命令返回的 geohash 的位置与用户给定的位置元素的位置一一对应。 代码示例123456redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEOHASH Sicily Palermo Catania1) "sqc8b49rny0"2) "sqdtr74hyu0" 位图SETBITSETBIT key offset value可用版本： &gt;= 2.2.0 时间复杂度: O(1) 对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 位的设置或清除取决于 value 参数，可以是 0 也可以是 1 。 当 key 不存在时，自动生成一个新的字符串值。 字符串会进行伸展(grown)以确保它可以将 value 保存在指定的偏移量上。当字符串值进行伸展时，空白位置以 0 填充。 offset 参数必须大于或等于 0 ，小于 2^32 (bit 映射被限制在 512 MB 之内)。 Warning对使用大的 offset 的 SETBIT 操作来说，内存分配可能造成 Redis 服务器被阻塞。具体参考 SETRANGE key offset value 命令，warning(警告)部分。 返回值指定偏移量原来储存的位。 代码示例12345678redis&gt; SETBIT bit 10086 1(integer) 0redis&gt; GETBIT bit 10086(integer) 1redis&gt; GETBIT bit 100 # bit 默认被初始化为 0(integer) 0 GETBITGETBIT key offset可用版本： &gt;= 2.2.0 时间复杂度： O(1) 对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 当 offset 比字符串值的长度大，或者 key 不存在时，返回 0 。 返回值字符串值指定偏移量上的位(bit)。 代码示例12345678910111213# 对不存在的 key 或者不存在的 offset 进行 GETBIT， 返回 0redis&gt; EXISTS bit(integer) 0redis&gt; GETBIT bit 10086(integer) 0# 对已存在的 offset 进行 GETBITredis&gt; SETBIT bit 10086 1(integer) 0redis&gt; GETBIT bit 10086(integer) 1 BITCOUNTBITCOUNT key [start] [end]可用版本： &gt;= 2.6.0 时间复杂度： O(N) 计算给定字符串中，被设置为 1 的比特位的数量。 一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。 start 和 end 参数的设置和 GETRANGE key start end 命令类似，都可以使用负数值： 比如 -1 表示最后一个字节， -2 表示倒数第二个字节，以此类推。 不存在的 key 被当成是空字符串来处理，因此对一个不存在的 key 进行 BITCOUNT 操作，结果为 0 。 返回值被设置为 1 的位的数量。 代码示例1234567891011121314redis&gt; BITCOUNT bits(integer) 0redis&gt; SETBIT bits 0 1 # 0001(integer) 0redis&gt; BITCOUNT bits(integer) 1redis&gt; SETBIT bits 3 1 # 1001(integer) 0redis&gt; BITCOUNT bits(integer) 2 模式：使用 bitmap 实现用户上线次数统计Bitmap 对于一些特定类型的计算非常有效。 假设现在我们希望记录自己网站上的用户的上线频率，比如说，计算用户 A 上线了多少天，用户 B 上线了多少天，诸如此类，以此作为数据，从而决定让哪些用户参加 beta 测试等活动 —— 这个模式可以使用 SETBIT key offset value 和 BITCOUNT key [start] [end] 来实现。 比如说，每当用户在某一天上线的时候，我们就使用 SETBIT key offset value ，以用户名作为 key ，将那天所代表的网站的上线日作为 offset 参数，并将这个 offset 上的为设置为 1 。 举个例子，如果今天是网站上线的第 100 天，而用户 peter 在今天阅览过网站，那么执行命令 SETBIT peter 100 1 ；如果明天 peter 也继续阅览网站，那么执行命令 SETBIT peter 101 1 ，以此类推。 当要计算 peter 总共以来的上线次数时，就使用 BITCOUNT key [start] [end] 命令：执行 BITCOUNT peter ，得出的结果就是 peter 上线的总天数。 性能前面的上线次数统计例子，即使运行 10 年，占用的空间也只是每个用户 10*365 比特位(bit)，也即是每个用户 456 字节。对于这种大小的数据来说， BITCOUNT key [start] [end] 的处理速度就像 GET key 和 INCR key 这种 O(1) 复杂度的操作一样快。 如果你的 bitmap 数据非常大，那么可以考虑使用以下两种方法： 将一个大的 bitmap 分散到不同的 key 中，作为小的 bitmap 来处理。使用 Lua 脚本可以很方便地完成这一工作。 使用 BITCOUNT key [start] [end] 的 start 和 end 参数，每次只对所需的部分位进行计算，将位的累积工作(accumulating)放到客户端进行，并且对结果进行缓存 (caching)。 BITPOSBITPOS key bit [start] [end]可用版本： &gt;= 2.8.7 时间复杂度： O(N)，其中 N 为位图包含的二进制位数量 返回位图中第一个值为 bit 的二进制位的位置。 在默认情况下， 命令将检测整个位图， 但用户也可以通过可选的 start 参数和 end 参数指定要检测的范围。 返回值整数回复。 代码示例12345678127.0.0.1:6379&gt; SETBIT bits 3 1 # 1000(integer) 0127.0.0.1:6379&gt; BITPOS bits 0(integer) 0127.0.0.1:6379&gt; BITPOS bits 1(integer) 3 BITOPBITOP operation destkey key [key …] 可用版本： &gt;= 2.6.0 时间复杂度： O(N)对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 operation 可以是 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种： BITOP AND destkey key [key ...] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。 BITOP OR destkey key [key ...] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。 BITOP XOR destkey key [key ...] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey 。 BITOP NOT destkey key ，对给定 key 求逻辑非，并将结果保存到 destkey 。 除了 NOT 操作之外，其他操作都可以接受一个或多个 key 作为输入。 处理不同长度的字符串 当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0 。 空的 key 也被看作是包含 0 的字符串序列。 返回值保存到 destkey 的字符串的长度，和输入 key 中最长的字符串长度相等。 NoteBITOP 的复杂度为 O(N) ，当处理大型矩阵(matrix)或者进行大数据量的统计时，最好将任务指派到附属节点(slave)进行，避免阻塞主节点。 代码示例1234567891011121314151617181920212223242526272829redis&gt; SETBIT bits-1 0 1 # bits-1 = 1001(integer) 0redis&gt; SETBIT bits-1 3 1(integer) 0redis&gt; SETBIT bits-2 0 1 # bits-2 = 1011(integer) 0redis&gt; SETBIT bits-2 1 1(integer) 0redis&gt; SETBIT bits-2 3 1(integer) 0redis&gt; BITOP AND and-result bits-1 bits-2(integer) 1redis&gt; GETBIT and-result 0 # and-result = 1001(integer) 1redis&gt; GETBIT and-result 1(integer) 0redis&gt; GETBIT and-result 2(integer) 0redis&gt; GETBIT and-result 3(integer) 1 BITFIELDBITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL]可用版本： &gt;= 3.2.0 时间复杂度： 每个子命令的复杂度为 O(1) 。 BITFIELD 命令可以将一个 Redis 字符串看作是一个由二进制位组成的数组， 并对这个数组中储存的长度不同的整数进行访问 （被储存的整数无需进行对齐）。 换句话说， 通过这个命令， 用户可以执行诸如 “对偏移量 1234 上的 5 位长有符号整数进行设置”、 “获取偏移量 4567 上的 31 位长无符号整数”等操作。 此外， BITFIELD 命令还可以对指定的整数执行加法操作和减法操作， 并且这些操作可以通过设置妥善地处理计算时出现的溢出情况。 BITFIELD 命令可以在一次调用中同时对多个位范围进行操作： 它接受一系列待执行的操作作为参数， 并返回一个数组作为回复， 数组中的每个元素就是对应操作的执行结果。 比如以下命令就展示了如何对位于偏移量 100 的 8 位长有符号整数执行加法操作， 并获取位于偏移量 0 上的 4 位长无符号整数：123&gt; BITFIELD mykey INCRBY i8 100 1 GET u4 01) (integer) 12) (integer) 0 注意： 使用 GET 子命令对超出字符串当前范围的二进制位进行访问（包括键不存在的情况）， 超出部分的二进制位的值将被当做是 0 。 使用 SET 子命令或者 INCRBY 子命令对超出字符串当前范围的二进制位进行访问将导致字符串被扩大， 被扩大的部分会使用值为 0 的二进制位进行填充。 在对字符串进行扩展时， 命令会根据字符串目前已有的最远端二进制位， 计算出执行操作所需的最小长度。 支持的子命令以及数字类型以下是 BITFIELD 命令支持的子命令： GET &lt;type&gt; &lt;offset&gt; —— 返回指定的二进制位范围。 SET &lt;type&gt; &lt;offset&gt; &lt;value&gt; —— 对指定的二进制位范围进行设置，并返回它的旧值。 INCRBY &lt;type&gt; &lt;offset&gt; &lt;increment&gt; —— 对指定的二进制位范围执行加法操作，并返回它的旧值。用户可以通过向 increment 参数传入负值来实现相应的减法操作。 除了以上三个子命令之外， 还有一个子命令， 它可以改变之后执行的 INCRBY 子命令在发生溢出情况时的行为： OVERFLOW [WRAP|SAT|FAIL] 当被设置的二进制位范围值为整数时， 用户可以在类型参数的前面添加 i 来表示有符号整数， 或者使用 u 来表示无符号整数。 比如说， 我们可以使用 u8 来表示 8 位长的无符号整数， 也可以使用 i16 来表示 16 位长的有符号整数。 BITFIELD 命令最大支持 64 位长的有符号整数以及 63 位长的无符号整数， 其中无符号整数的 63 位长度限制是由于 Redis 协议目前还无法返回 64 位长的无符号整数而导致的。 二进制位和位置偏移量在二进制位范围命令中， 用户有两种方法来设置偏移量： 如果用户给定的是一个没有任何前缀的数字， 那么这个数字指示的就是字符串以零为开始（zero-base）的偏移量。 另一方面， 如果用户给定的是一个带有 # 前缀的偏移量， 那么命令将使用这个偏移量与被设置的数字类型的位长度相乘， 从而计算出真正的偏移量。 比如说， 对于以下这个命令来说： BITFIELD mystring SET i8 #0 100 i8 #1 200 命令会把 mystring 键里面， 第一个 i8 长度的二进制位的值设置为 100 ， 并把第二个 i8 长度的二进制位的值设置为 200 。 当我们把一个字符串键当成数组来使用， 并且数组中储存的都是同等长度的整数时， 使用 # 前缀可以让我们免去手动计算被设置二进制位所在位置的麻烦。 溢出控制用户可以通过 OVERFLOW 命令以及以下展示的三个参数， 指定 BITFIELD 命令在执行自增或者自减操作时， 碰上向上溢出（overflow）或者向下溢出（underflow）情况时的行为： WRAP ： 使用回绕（wrap around）方法处理有符号整数和无符号整数的溢出情况。 对于无符号整数来说， 回绕就像使用数值本身与能够被储存的最大无符号整数执行取模计算， 这也是 C 语言的标准行为。 对于有符号整数来说， 上溢将导致数字重新从最小的负数开始计算， 而下溢将导致数字重新从最大的正数开始计算。 比如说， 如果我们对一个值为 127 的 i8 整数执行加一操作， 那么将得到结果 -128 。 SAT ： 使用饱和计算（saturation arithmetic）方法处理溢出， 也即是说， 下溢计算的结果为最小的整数值， 而上溢计算的结果为最大的整数值。 举个例子， 如果我们对一个值为 120 的 i8 整数执行加 10 计算， 那么命令的结果将为 i8 类型所能储存的最大整数值 127 。 与此相反， 如果一个针对 i8 值的计算造成了下溢， 那么这个 i8 值将被设置为 -127 。 FAIL ： 在这一模式下， 命令将拒绝执行那些会导致上溢或者下溢情况出现的计算， 并向用户返回空值表示计算未被执行。 需要注意的是， OVERFLOW 子命令只会对紧随着它之后被执行的 INCRBY 命令产生效果， 这一效果将一直持续到与它一同被执行的下一个 OVERFLOW 命令为止。 在默认情况下， INCRBY 命令使用 WRAP 方式来处理溢出计算。 以下是一个使用 OVERFLOW 子命令来控制溢出行为的例子：123456789101112131415&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 12) (integer) 1&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 22) (integer) 2&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 32) (integer) 3&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 0 -- 使用默认的 WRAP 方式处理溢出2) (integer) 3 -- 使用 SAT 方式处理溢出 而以下则是一个因为 OVERFLOW FAIL 行为而导致子命令返回空值的例子：12&gt; BITFIELD mykey OVERFLOW FAIL incrby u2 102 11) (nil) 作用BITFIELD 命令的作用在于它能够将很多小的整数储存到一个长度较大的位图中， 又或者将一个非常庞大的键分割为多个较小的键来进行储存， 从而非常高效地使用内存， 使得 Redis 能够得到更多不同的应用 —— 特别是在实时分析领域： BITFIELD 能够以指定的方式对计算溢出进行控制的能力， 使得它可以被应用于这一领域。 性能注意事项BITFIELD 在一般情况下都是一个快速的命令， 需要注意的是， 访问一个长度较短的字符串的远端二进制位将引发一次内存分配操作， 这一操作花费的时间可能会比命令访问已有的字符串花费的时间要长。 二进制位的排列BITFIELD 把位图第一个字节偏移量 0 上的二进制位看作是 most significant 位， 以此类推。 举个例子， 如果我们对一个已经预先被全部设置为 0 的位图进行设置， 将它在偏移量 7 的值设置为 5 位无符号整数值 23 （二进制位为 10111 ）， 那么命令将生产出以下这个位图表示：123+--------+--------+|00000001|01110000|+--------+--------+ 当偏移量和整数长度与字节边界进行对齐时， BITFIELD 表示二进制位的方式跟大端表示法（big endian）一致， 但是在没有对齐的情况下， 理解这些二进制位是如何进行排列也是非常重要的。 返回值BITFIELD 命令的返回值是一个数组， 数组中的每个元素对应一个被执行的子命令。 需要注意的是， OVERFLOW 子命令本身并不产生任何回复。 数据库EXISTSEXISTS key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 检查给定 key 是否存在。 返回值若 key 存在，返回 1 ，否则返回 0 。 代码示例1234567891011redis&gt; SET db "redis"OKredis&gt; EXISTS db(integer) 1redis&gt; DEL db(integer) 1redis&gt; EXISTS db(integer) 0 TYPETYPE key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回 key 所储存的值的类型。 返回值none (key不存在) string (字符串) list (列表) set (集合) zset (有序集) hash (哈希表) stream （流） 代码示例1234567891011121314151617181920# 字符串redis&gt; SET weather "sunny"OKredis&gt; TYPE weatherstring# 列表redis&gt; LPUSH book_list "programming in scala"(integer) 1redis&gt; TYPE book_listlist# 集合redis&gt; SADD pat "dog"(integer) 1redis&gt; TYPE patset RENAMERENAME key newkey可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将 key 改名为 newkey 。 当 key 和 newkey 相同，或者 key 不存在时，返回一个错误。 当 newkey 已经存在时， RENAME 命令将覆盖旧值。 返回值改名成功时提示 OK ，失败时候返回一个错误。 代码示例1234567891011121314151617181920212223242526272829303132# key 存在且 newkey 不存在redis&gt; SET message "hello world"OKredis&gt; RENAME message greetingOKredis&gt; EXISTS message # message 不复存在(integer) 0redis&gt; EXISTS greeting # greeting 取而代之(integer) 1# 当 key 不存在时，返回错误redis&gt; RENAME fake_key never_exists(error) ERR no such key# newkey 已存在时， RENAME 会覆盖旧 newkeyredis&gt; SET pc "lenovo"OKredis&gt; SET personal_computer "dell"OKredis&gt; RENAME pc personal_computerOKredis&gt; GET pc(nil)redis:1&gt; GET personal_computer # 原来的值 dell 被覆盖了"lenovo" RENAMENXRENAMENX key newkey可用版本： &gt;= 1.0.0 时间复杂度： O(1) 当且仅当 newkey 不存在时，将 key 改名为 newkey 。 当 key 不存在时，返回一个错误。 返回值修改成功时，返回 1 ； 如果 newkey 已经存在，返回 0 。 代码示例12345678910111213141516171819202122232425# newkey 不存在，改名成功redis&gt; SET player "MPlyaer"OKredis&gt; EXISTS best_player(integer) 0redis&gt; RENAMENX player best_player(integer) 1# newkey存在时，失败redis&gt; SET animal "bear"OKredis&gt; SET favorite_animal "butterfly"OKredis&gt; RENAMENX animal favorite_animal(integer) 0redis&gt; get animal"bear"redis&gt; get favorite_animal"butterfly" MOVEMOVE key db可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将当前数据库的 key 移动到给定的数据库 db 当中。 如果当前数据库(源数据库)和给定数据库(目标数据库)有相同名字的给定 key ，或者 key 不存在于当前数据库，那么 MOVE 没有任何效果。 因此，也可以利用这一特性，将 MOVE 当作锁(locking)原语(primitive)。 返回值移动成功返回 1 ，失败则返回 0 。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# key 存在于当前数据库redis&gt; SELECT 0 # redis默认使用数据库 0，为了清晰起见，这里再显式指定一次。OKredis&gt; SET song "secret base - Zone"OKredis&gt; MOVE song 1 # 将 song 移动到数据库 1(integer) 1redis&gt; EXISTS song # song 已经被移走(integer) 0redis&gt; SELECT 1 # 使用数据库 1OKredis:1&gt; EXISTS song # 证实 song 被移到了数据库 1 (注意命令提示符变成了"redis:1"，表明正在使用数据库 1)(integer) 1# 当 key 不存在的时候redis:1&gt; EXISTS fake_key(integer) 0redis:1&gt; MOVE fake_key 0 # 试图从数据库 1 移动一个不存在的 key 到数据库 0，失败(integer) 0redis:1&gt; select 0 # 使用数据库0OKredis&gt; EXISTS fake_key # 证实 fake_key 不存在(integer) 0# 当源数据库和目标数据库有相同的 key 时redis&gt; SELECT 0 # 使用数据库0OKredis&gt; SET favorite_fruit "banana"OKredis&gt; SELECT 1 # 使用数据库1OKredis:1&gt; SET favorite_fruit "apple"OKredis:1&gt; SELECT 0 # 使用数据库0，并试图将 favorite_fruit 移动到数据库 1OKredis&gt; MOVE favorite_fruit 1 # 因为两个数据库有相同的 key，MOVE 失败(integer) 0redis&gt; GET favorite_fruit # 数据库 0 的 favorite_fruit 没变"banana"redis&gt; SELECT 1OKredis:1&gt; GET favorite_fruit # 数据库 1 的 favorite_fruit 也是"apple" DELDEL key [key …]可用版本： &gt;= 1.0.0 时间复杂度：O(N)， N 为被删除的 key 的数量，其中删除单个字符串类型的 key ，时间复杂度为O(1)；删除单个列表、集合、有序集合或哈希表类型的 key ，时间复杂度为O(M)， M 为以上数据结构内的元素数量。 删除给定的一个或多个 key 。 不存在的 key 会被忽略。 返回值被删除 key 的数量。 代码示例1234567891011121314151617181920212223242526# 删除单个 keyredis&gt; SET name huangzOKredis&gt; DEL name(integer) 1# 删除一个不存在的 keyredis&gt; EXISTS phone(integer) 0redis&gt; DEL phone # 失败，没有 key 被删除(integer) 0# 同时删除多个 keyredis&gt; SET name "redis"OKredis&gt; SET type "key-value store"OKredis&gt; SET website "redis.com"OKredis&gt; DEL name type website(integer) 3 RANDOMKEYRANDOMKEY可用版本： &gt;= 1.0.0 时间复杂度： O(1) 从当前数据库中随机返回(不删除)一个 key 。 返回值当数据库不为空时，返回一个 key 。 当数据库为空时，返回 nil 。 代码示例123456789101112131415161718192021# 数据库不为空redis&gt; MSET fruit "apple" drink "beer" food "cookies" # 设置多个 keyOKredis&gt; RANDOMKEY"fruit"redis&gt; RANDOMKEY"food"redis&gt; KEYS * # 查看数据库内所有key，证明 RANDOMKEY 并不删除 key1) "food"2) "drink"3) "fruit"# 数据库为空redis&gt; FLUSHDB # 删除当前数据库所有 keyOKredis&gt; RANDOMKEY(nil) DBSIZEDBSIZE可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回当前数据库的 key 的数量。 返回值当前数据库的 key 的数量。 代码示例12345678redis&gt; DBSIZE(integer) 5redis&gt; SET new_key "hello_moto" # 增加一个 key 试试OKredis&gt; DBSIZE(integer) 6 KEYSKEYS pattern可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为数据库中 key 的数量。 查找所有符合给定模式 pattern 的 key ， 比如说： KEYS * 匹配数据库中所有 key 。 KEYS h?llo 匹配 hello ， hallo 和 hxllo 等。 KEYS h*llo 匹配 hllo 和 heeeeello 等。 KEYS h[ae]llo 匹配 hello 和 hallo ，但不匹配 hillo 。 特殊符号用 \ 隔开。 WarningKEYS 的速度非常快，但在一个大的数据库中使用它仍然可能造成性能问题，如果你需要从一个数据集中查找特定的 key ，你最好还是用 Redis 的集合结构(set)来代替。 返回值符合给定模式的 key 列表。 代码示例12345678910111213141516171819redis&gt; MSET one 1 two 2 three 3 four 4 # 一次设置 4 个 keyOKredis&gt; KEYS *o*1) "four"2) "two"3) "one"redis&gt; KEYS t??1) "two"redis&gt; KEYS t[w]*1) "two"redis&gt; KEYS * # 匹配数据库内所有 key1) "four"2) "three"3) "two"4) "one" SCANSCAN cursor [MATCH pattern] [COUNT count]可用版本： &gt;= 2.8.0 时间复杂度：增量式迭代命令每次执行的复杂度为 O(1) ， 对数据集进行一次完整迭代的复杂度为 O(N) ， 其中 N 为数据集中的元素数量。 SCAN 命令及其相关的 SSCAN 命令、 HSCAN 命令和 ZSCAN 命令都用于增量地迭代（incrementally iterate）一集元素（a collection of elements）： SCAN 命令用于迭代当前数据库中的数据库键。 SSCAN 命令用于迭代集合键中的元素。 HSCAN 命令用于迭代哈希键中的键值对。 ZSCAN 命令用于迭代有序集合中的元素（包括元素成员和元素分值）。 以上列出的四个命令都支持增量式迭代， 它们每次执行都只会返回少量元素， 所以这些命令可以用于生产环境， 而不会出现像 KEYS 命令、 SMEMBERS 命令带来的问题 —— 当 KEYS 命令被用于处理一个大的数据库时， 又或者 SMEMBERS 命令被用于处理一个大的集合键时， 它们可能会阻塞服务器达数秒之久。 不过， 增量式迭代命令也不是没有缺点的： 举个例子， 使用 SMEMBERS 命令可以返回集合键当前包含的所有元素， 但是对于 SCAN 这类增量式迭代命令来说， 因为在对键进行增量式迭代的过程中， 键可能会被修改， 所以增量式迭代命令只能对被返回的元素提供有限的保证 （offer limited guarantees about the returned elements）。 因为 SCAN 、 SSCAN 、 HSCAN 和 ZSCAN 四个命令的工作方式都非常相似， 所以这个文档会一并介绍这四个命令， 但是要记住： SSCAN 命令、 HSCAN 命令和 ZSCAN 命令的第一个参数总是一个数据库键。 而 SCAN 命令则不需要在第一个参数提供任何数据库键 —— 因为它迭代的是当前数据库中的所有数据库键。SCAN 命令的基本用法SCAN 命令是一个基于游标的迭代器（cursor based iterator）： SCAN 命令每次被调用之后， 都会向用户返回一个新的游标， 用户在下次迭代时需要使用这个新游标作为 SCAN 命令的游标参数， 以此来延续之前的迭代过程。 当 SCAN 命令的游标参数被设置为 0 时， 服务器将开始一次新的迭代， 而当服务器向用户返回值为 0 的游标时， 表示迭代已结束。 以下是一个 SCAN 命令的迭代过程示例：12345678910111213141516171819202122232425redis 127.0.0.1:6379&gt; scan 01) "17"2) 1) "key:12" 2) "key:8" 3) "key:4" 4) "key:14" 5) "key:16" 6) "key:17" 7) "key:15" 8) "key:10" 9) "key:3" 10) "key:7" 11) "key:1"redis 127.0.0.1:6379&gt; scan 171) "0"2) 1) "key:5" 2) "key:18" 3) "key:0" 4) "key:2" 5) "key:19" 6) "key:13" 7) "key:6" 8) "key:9" 9) "key:11" 在上面这个例子中， 第一次迭代使用 0 作为游标， 表示开始一次新的迭代。 第二次迭代使用的是第一次迭代时返回的游标， 也即是命令回复第一个元素的值 —— 17 。 从上面的示例可以看到， SCAN 命令的回复是一个包含两个元素的数组， 第一个数组元素是用于进行下一次迭代的新游标， 而第二个数组元素则是一个数组， 这个数组中包含了所有被迭代的元素。 在第二次调用 SCAN 命令时， 命令返回了游标 0 ， 这表示迭代已经结束， 整个数据集（collection）已经被完整遍历过了。 以 0 作为游标开始一次新的迭代， 一直调用 SCAN 命令， 直到命令返回游标 0 ， 我们称这个过程为一次完整遍历（full iteration）。 SCAN 命令的保证（guarantees）SCAN 命令， 以及其他增量式迭代命令， 在进行完整遍历的情况下可以为用户带来以下保证： 从完整遍历开始直到完整遍历结束期间， 一直存在于数据集内的所有元素都会被完整遍历返回； 这意味着， 如果有一个元素， 它从遍历开始直到遍历结束期间都存在于被遍历的数据集当中， 那么 SCAN 命令总会在某次迭代中将这个元素返回给用户。 然而因为增量式命令仅仅使用游标来记录迭代状态， 所以这些命令带有以下缺点： 同一个元素可能会被返回多次。 处理重复元素的工作交由应用程序负责， 比如说， 可以考虑将迭代返回的元素仅仅用于可以安全地重复执行多次的操作上。 如果一个元素是在迭代过程中被添加到数据集的， 又或者是在迭代过程中从数据集中被删除的， 那么这个元素可能会被返回， 也可能不会， 这是未定义的（undefined）。SCAN 命令每次执行返回的元素数量增量式迭代命令并不保证每次执行都返回某个给定数量的元素。 增量式命令甚至可能会返回零个元素， 但只要命令返回的游标不是 0 ， 应用程序就不应该将迭代视作结束。 不过命令返回的元素数量总是符合一定规则的， 在实际中： 对于一个大数据集来说， 增量式迭代命令每次最多可能会返回数十个元素； 而对于一个足够小的数据集来说， 如果这个数据集的底层表示为编码数据结构（encoded data structure，适用于是小集合键、小哈希键和小有序集合键）， 那么增量迭代命令将在一次调用中返回数据集中的所有元素。最后， 用户可以通过增量式迭代命令提供的 COUNT 选项来指定每次迭代返回元素的最大值。 COUNT 选项虽然增量式迭代命令不保证每次迭代所返回的元素数量， 但我们可以使用 COUNT 选项， 对命令的行为进行一定程度上的调整。 基本上， COUNT 选项的作用就是让用户告知迭代命令， 在每次迭代中应该从数据集里返回多少元素。 虽然 COUNT 选项只是对增量式迭代命令的一种提示（hint）， 但是在大多数情况下， 这种提示都是有效的。 COUNT 参数的默认值为 10 。 在迭代一个足够大的、由哈希表实现的数据库、集合键、哈希键或者有序集合键时， 如果用户没有使用 MATCH 选项， 那么命令返回的元素数量通常和 COUNT 选项指定的一样， 或者比 COUNT 选项指定的数量稍多一些。 在迭代一个编码为整数集合（intset，一个只由整数值构成的小集合）、 或者编码为压缩列表（ziplist，由不同值构成的一个小哈希或者一个小有序集合）时， 增量式迭代命令通常会无视 COUNT 选项指定的值， 在第一次迭代就将数据集包含的所有元素都返回给用户。Note 并非每次迭代都要使用相同的 COUNT 值。 用户可以在每次迭代中按自己的需要随意改变 COUNT 值， 只要记得将上次迭代返回的游标用到下次迭代里面就可以了。 MATCH 选项和 KEYS 命令一样， 增量式迭代命令也可以通过提供一个 glob 风格的模式参数， 让命令只返回和给定模式相匹配的元素， 这一点可以通过在执行增量式迭代命令时， 通过给定 MATCH \&lt;pattern> 参数来实现。 以下是一个使用 MATCH 选项进行迭代的示例：12345678redis 127.0.0.1:6379&gt; sadd myset 1 2 3 foo foobar feelsgood(integer) 6redis 127.0.0.1:6379&gt; sscan myset 0 match f*1) "0"2) 1) "foo" 2) "feelsgood" 3) "foobar" 需要注意的是， 对元素的模式匹配工作是在命令从数据集中取出元素之后， 向客户端返回元素之前的这段时间内进行的， 所以如果被迭代的数据集中只有少量元素和模式相匹配， 那么迭代命令或许会在多次执行中都不返回任何元素。 以下是这种情况的一个例子：123456789101112131415161718192021222324252627282930313233343536redis 127.0.0.1:6379&gt; scan 0 MATCH *11*1) "288"2) 1) "key:911"redis 127.0.0.1:6379&gt; scan 288 MATCH *11*1) "224"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 224 MATCH *11*1) "80"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 80 MATCH *11*1) "176"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 176 MATCH *11* COUNT 10001) "0"2) 1) "key:611" 2) "key:711" 3) "key:118" 4) "key:117" 5) "key:311" 6) "key:112" 7) "key:111" 8) "key:110" 9) "key:113" 10) "key:211" 11) "key:411" 12) "key:115" 13) "key:116" 14) "key:114" 15) "key:119" 16) "key:811" 17) "key:511" 18) "key:11" 如你所见， 以上的大部分迭代都不返回任何元素。 在最后一次迭代， 我们通过将 COUNT 选项的参数设置为 1000 ， 强制命令为本次迭代扫描更多元素， 从而使得命令返回的元素也变多了。 并发执行多个迭代在同一时间， 可以有任意多个客户端对同一数据集进行迭代， 客户端每次执行迭代都需要传入一个游标， 并在迭代执行之后获得一个新的游标， 而这个游标就包含了迭代的所有状态， 因此， 服务器无须为迭代记录任何状态。 中途停止迭代因为迭代的所有状态都保存在游标里面， 而服务器无须为迭代保存任何状态， 所以客户端可以在中途停止一个迭代， 而无须对服务器进行任何通知。 即使有任意数量的迭代在中途停止， 也不会产生任何问题。 使用错误的游标进行增量式迭代使用间断的（broken）、负数、超出范围或者其他非正常的游标来执行增量式迭代并不会造成服务器崩溃， 但可能会让命令产生未定义的行为。 未定义行为指的是， 增量式命令对返回值所做的保证可能会不再为真。 只有两种游标是合法的： 在开始一个新的迭代时， 游标必须为 0 。 增量式迭代命令在执行之后返回的， 用于延续（continue）迭代过程的游标。迭代终结的保证增量式迭代命令所使用的算法只保证在数据集的大小有界（bounded）的情况下， 迭代才会停止， 换句话说， 如果被迭代数据集的大小不断地增长的话， 增量式迭代命令可能永远也无法完成一次完整迭代。 从直觉上可以看出， 当一个数据集不断地变大时， 想要访问这个数据集中的所有元素就需要做越来越多的工作， 能否结束一个迭代取决于用户执行迭代的速度是否比数据集增长的速度更快。 返回值SCAN 命令、 SSCAN 命令、 HSCAN 命令和 ZSCAN 命令都返回一个包含两个元素的 multi-bulk 回复： 回复的第一个元素是字符串表示的无符号 64 位整数（游标）， 回复的第二个元素是另一个 multi-bulk 回复， 这个 multi-bulk 回复包含了本次被迭代的元素。 SCAN 命令返回的每个元素都是一个数据库键。 SSCAN 命令返回的每个元素都是一个集合成员。 HSCAN 命令返回的每个元素都是一个键值对，一个键值对由一个键和一个值组成。 ZSCAN 命令返回的每个元素都是一个有序集合元素，一个有序集合元素由一个成员（member）和一个分值（score）组成。 SORTSORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination]可用版本： &gt;= 1.0.0 时间复杂度： O(N+M*log(M))， N 为要排序的列表或集合内的元素数量， M 为要返回的元素数量。如果只是使用 SORT 命令的 GET 选项获取数据而没有进行排序，时间复杂度 O(N)。 返回或保存给定列表、集合、有序集合 key 中经过排序的元素。 排序默认以数字作为对象，值被解释为双精度浮点数，然后进行比较。 一般 SORT 用法最简单的 SORT 使用方法是 SORT key 和 SORT key DESC ： SORT key 返回键值从小到大排序的结果。 SORT key DESC 返回键值从大到小排序的结果。假设 today_cost 列表保存了今日的开销金额， 那么可以用 SORT 命令对它进行排序： 1234567891011121314151617# 开销金额列表redis&gt; LPUSH today_cost 30 1.5 10 8(integer) 4# 排序redis&gt; SORT today_cost1) "1.5"2) "8"3) "10"4) "30"# 逆序排序redis 127.0.0.1:6379&gt; SORT today_cost DESC1) "30"2) "10"3) "8"4) "1.5" 使用 ALPHA 修饰符对字符串进行排序因为 SORT 命令默认排序对象为数字， 当需要对字符串进行排序时， 需要显式地在 SORT 命令之后添加 ALPHA 修饰符：123456789101112131415161718192021# 网址redis&gt; LPUSH website "www.reddit.com"(integer) 1redis&gt; LPUSH website "www.slashdot.com"(integer) 2redis&gt; LPUSH website "www.infoq.com"(integer) 3# 默认（按数字）排序redis&gt; SORT website1) "www.infoq.com"2) "www.slashdot.com"3) "www.reddit.com"# 按字符排序redis&gt; SORT website ALPHA1) "www.infoq.com"2) "www.reddit.com"3) "www.slashdot.com" 如果系统正确地设置了 LC_COLLATE 环境变量的话，Redis能识别 UTF-8 编码。 使用 LIMIT 修饰符限制返回结果排序之后返回元素的数量可以通过 LIMIT 修饰符进行限制， 修饰符接受 offset 和 count 两个参数： offset 指定要跳过的元素数量。 count 指定跳过 offset 个指定的元素之后，要返回多少个对象。以下例子返回排序结果的前 5 个对象( offset 为 0 表示没有元素被跳过)。1234567891011121314# 添加测试数据，列表值为 1 指 10redis 127.0.0.1:6379&gt; RPUSH rank 1 3 5 7 9(integer) 5redis 127.0.0.1:6379&gt; RPUSH rank 2 4 6 8 10(integer) 10# 返回列表中最小的 5 个值redis 127.0.0.1:6379&gt; SORT rank LIMIT 0 51) "1"2) "2"3) "3"4) "4"5) "5" 可以组合使用多个修饰符。以下例子返回从大到小排序的前 5 个对象。123456redis 127.0.0.1:6379&gt; SORT rank LIMIT 0 5 DESC1) "10"2) "9"3) "8"4) "7"5) "6" 使用外部 key 进行排序可以使用外部 key 的数据作为权重，代替默认的直接对比键值的方式来进行排序。 假设现在有用户数据如下： 以下代码将数据输入到 Redis 中：123456789101112131415161718192021222324252627282930313233343536373839# adminredis 127.0.0.1:6379&gt; LPUSH uid 1(integer) 1redis 127.0.0.1:6379&gt; SET user_name_1 adminOKredis 127.0.0.1:6379&gt; SET user_level_1 9999OK# jackredis 127.0.0.1:6379&gt; LPUSH uid 2(integer) 2redis 127.0.0.1:6379&gt; SET user_name_2 jackOKredis 127.0.0.1:6379&gt; SET user_level_2 10OK# peterredis 127.0.0.1:6379&gt; LPUSH uid 3(integer) 3redis 127.0.0.1:6379&gt; SET user_name_3 peterOKredis 127.0.0.1:6379&gt; SET user_level_3 25OK# maryredis 127.0.0.1:6379&gt; LPUSH uid 4(integer) 4redis 127.0.0.1:6379&gt; SET user_name_4 maryOKredis 127.0.0.1:6379&gt; SET user_level_4 70OK BY 选项默认情况下， SORT uid 直接按 uid 中的值排序：12345redis 127.0.0.1:6379&gt; SORT uid1) "1" # admin2) "2" # jack3) "3" # peter4) "4" # mary 通过使用 BY 选项，可以让 uid 按其他键的元素来排序。 比如说， 以下代码让 uid 键按照 user_level_{uid} 的大小来排序：12345redis 127.0.0.1:6379&gt; SORT uid BY user_level_*1) "2" # jack , level = 102) "3" # peter, level = 253) "4" # mary, level = 704) "1" # admin, level = 9999 user_level_* 是一个占位符， 它先取出 uid 中的值， 然后再用这个值来查找相应的键。 比如在对 uid 列表进行排序时， 程序就会先取出 uid 的值 1 、 2 、 3 、 4 ， 然后使用 user_level_1 、 user_level_2 、 user_level_3 和 user_level_4 的值作为排序 uid 的权重。 GET 选项使用 GET 选项， 可以根据排序的结果来取出相应的键值。 比如说， 以下代码先排序 uid ， 再取出键 user_name_{uid} 的值：12345redis 127.0.0.1:6379&gt; SORT uid GET user_name_*1) "admin"2) "jack"3) "peter"4) "mary" 组合使用 BY 和 GET通过组合使用 BY 和 GET ， 可以让排序结果以更直观的方式显示出来。 比如说， 以下代码先按 user_level_{uid} 来排序 uid 列表， 再取出相应的 user_name_{uid} 的值：12345redis 127.0.0.1:6379&gt; SORT uid BY user_level_* GET user_name_*1) "jack" # level = 102) "peter" # level = 253) "mary" # level = 704) "admin" # level = 9999 现在的排序结果要比只使用 SORT uid BY user_level_* 要直观得多。 获取多个外部键可以同时使用多个 GET 选项， 获取多个外部键的值。 以下代码就按 uid 分别获取 user_level_{uid} 和 user_name_{uid} ：123456789redis 127.0.0.1:6379&gt; SORT uid GET user_level_* GET user_name_*1) "9999" # level2) "admin" # name3) "10"4) "jack"5) "25"6) "peter"7) "70"8) "mary" GET 有一个额外的参数规则，那就是 —— 可以用 # 获取被排序键的值。 以下代码就将 uid 的值、及其相应的 user_level_ 和 user_name_ 都返回为结果：12345678910111213redis 127.0.0.1:6379&gt; SORT uid GET # GET user_level_* GET user_name_*1) "1" # uid2) "9999" # level3) "admin" # name4) "2"5) "10"6) "jack"7) "3"8) "25"9) "peter"10) "4"11) "70"12) "mary" 获取外部键，但不进行排序通过将一个不存在的键作为参数传给 BY 选项， 可以让 SORT 跳过排序操作， 直接返回结果：12345redis 127.0.0.1:6379&gt; SORT uid BY not-exists-key1) "4"2) "3"3) "2"4) "1" 这种用法在单独使用时，没什么实际用处。 不过，通过将这种用法和 GET 选项配合， 就可以在不排序的情况下， 获取多个外部键， 相当于执行一个整合的获取操作（类似于 SQL 数据库的 join 关键字）。 以下代码演示了，如何在不引起排序的情况下，使用 SORT 、 BY 和 GET 获取多个外部键：12345678910111213redis 127.0.0.1:6379&gt; SORT uid BY not-exists-key GET # GET user_level_* GET user_name_*1) "4" # id2) "70" # level3) "mary" # name4) "3"5) "25"6) "peter"7) "2"8) "10"9) "jack"10) "1"11) "9999"12) "admin" 将哈希表作为 GET 或 BY 的参数除了可以将字符串键之外， 哈希表也可以作为 GET 或 BY 选项的参数来使用。 比如说，对于前面给出的用户信息表： 我们可以不将用户的名字和级别保存在 user_name_{uid} 和 user_level_{uid} 两个字符串键中， 而是用一个带有 name 域和 level 域的哈希表 user_info_{uid} 来保存用户的名字和级别信息：1234567891011redis 127.0.0.1:6379&gt; HMSET user_info_1 name admin level 9999OKredis 127.0.0.1:6379&gt; HMSET user_info_2 name jack level 10OKredis 127.0.0.1:6379&gt; HMSET user_info_3 name peter level 25OKredis 127.0.0.1:6379&gt; HMSET user_info_4 name mary level 70OK 之后， BY 和 GET 选项都可以用 key-&gt;field 的格式来获取哈希表中的域的值， 其中 key 表示哈希表键， 而 field 则表示哈希表的域：1234567891011redis 127.0.0.1:6379&gt; SORT uid BY user_info_*-&gt;level1) "2"2) "3"3) "4"4) "1"redis 127.0.0.1:6379&gt; SORT uid BY user_info_*-&gt;level GET user_info_*-&gt;name1) "jack"2) "peter"3) "mary"4) "admin" 保存排序结果默认情况下， SORT 操作只是简单地返回排序结果，并不进行任何保存操作。 通过给 STORE 选项指定一个 key 参数，可以将排序结果保存到给定的键上。 如果被指定的 key 已存在，那么原有的值将被排序结果覆盖。12345678910111213141516171819202122232425262728293031323334# 测试数据redis 127.0.0.1:6379&gt; RPUSH numbers 1 3 5 7 9(integer) 5redis 127.0.0.1:6379&gt; RPUSH numbers 2 4 6 8 10(integer) 10redis 127.0.0.1:6379&gt; LRANGE numbers 0 -11) "1"2) "3"3) "5"4) "7"5) "9"6) "2"7) "4"8) "6"9) "8"10) "10"redis 127.0.0.1:6379&gt; SORT numbers STORE sorted-numbers(integer) 10# 排序后的结果redis 127.0.0.1:6379&gt; LRANGE sorted-numbers 0 -11) "1"2) "2"3) "3"4) "4"5) "5"6) "6"7) "7"8) "8"9) "9"10) "10" 可以通过将 SORT 命令的执行结果保存，并用 EXPIRE key seconds 为结果设置生存时间，以此来产生一个 SORT 操作的结果缓存。 这样就可以避免对 SORT 操作的频繁调用：只有当结果集过期时，才需要再调用一次 SORT 操作。 另外，为了正确实现这一用法，你可能需要加锁以避免多个客户端同时进行缓存重建(也就是多个客户端，同一时间进行 SORT 操作，并保存为结果集)，具体参见 SETNX key value 命令。 返回值没有使用 STORE 参数，返回列表形式的排序结果。 使用 STORE 参数，返回排序结果的元素数量。 FLUSHDBFLUSHDB可用版本： &gt;= 1.0.0 时间复杂度： O(1) 清空当前数据库中的所有 key。 此命令从不失败。 返回值总是返回 OK 。 代码示例12345678redis&gt; DBSIZE # 清空前的 key 数量(integer) 4redis&gt; FLUSHDBOKredis&gt; DBSIZE # 清空后的 key 数量(integer) 0 FLUSHALLFLUSHALL可用版本： &gt;= 1.0.0 时间复杂度： O(N) 清空整个 Redis 服务器的数据(删除所有数据库的所有 key )。 此命令从不失败。 返回值总是返回 OK 。1234567891011121314151617181920redis&gt; DBSIZE # 0 号数据库的 key 数量(integer) 9redis&gt; SELECT 1 # 切换到 1 号数据库OKredis[1]&gt; DBSIZE # 1 号数据库的 key 数量(integer) 6redis[1]&gt; flushall # 清空所有数据库的所有 keyOKredis[1]&gt; DBSIZE # 不但 1 号数据库被清空了(integer) 0redis[1]&gt; SELECT 0 # 0 号数据库(以及其他所有数据库)也一样OKredis&gt; DBSIZE(integer) 0 SELECTSELECT index可用版本： &gt;= 1.0.0 时间复杂度： O(1) 切换到指定的数据库，数据库索引号 index 用数字值指定，以 0 作为起始索引值。 默认使用 0 号数据库。 返回值OK 代码示例12345678910111213141516171819redis&gt; SET db_number 0 # 默认使用 0 号数据库OKredis&gt; SELECT 1 # 使用 1 号数据库OKredis[1]&gt; GET db_number # 已经切换到 1 号数据库，注意 Redis 现在的命令提示符多了个 [1](nil)redis[1]&gt; SET db_number 1OKredis[1]&gt; GET db_number"1"redis[1]&gt; SELECT 3 # 再切换到 3 号数据库OKredis[3]&gt; # 提示符从 [1] 改变成了 [3] SWAPDBSWAPDB db1 db2版本要求： &gt;= 4.0.0 时间复杂度： O(1) 对换指定的两个数据库， 使得两个数据库的数据立即互换。 返回值OK 代码示例123# 对换数据库 0 和数据库 1redis&gt; SWAPDB 0 1OK 自动过期EXPIREEXPIRE key seconds可用版本： &gt;= 1.0.0 时间复杂度： O(1) 为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 在 Redis 中，带有生存时间的 key 被称为『易失的』(volatile)。 生存时间可以通过使用 DEL 命令来删除整个 key 来移除，或者被 SET 和 GETSET 命令覆写(overwrite)，这意味着，如果一个命令只是修改(alter)一个带生存时间的 key 的值而不是用一个新的 key 值来代替(replace)它的话，那么生存时间不会被改变。 比如说，对一个 key 执行 INCR 命令，对一个列表进行 LPUSH 命令，或者对一个哈希表执行 HSET 命令，这类操作都不会修改 key 本身的生存时间。 另一方面，如果使用 RENAME 对一个 key 进行改名，那么改名后的 key 的生存时间和改名前一样。 RENAME 命令的另一种可能是，尝试将一个带生存时间的 key 改名成另一个带生存时间的 another_key ，这时旧的 another_key (以及它的生存时间)会被删除，然后旧的 key 会改名为 another_key ，因此，新的 another_key 的生存时间也和原本的 key 一样。 使用 PERSIST 命令可以在不删除 key 的情况下，移除 key 的生存时间，让 key 重新成为一个『持久的』(persistent) key 。 更新生存时间可以对一个已经带有生存时间的 key 执行 EXPIRE 命令，新指定的生存时间会取代旧的生存时间。 过期时间的精确度在 Redis 2.4 版本中，过期时间的延迟在 1 秒钟之内 —— 也即是，就算 key 已经过期，但它还是可能在过期之后一秒钟之内被访问到，而在新的 Redis 2.6 版本中，延迟被降低到 1 毫秒之内。 Redis 2.1.3 之前的不同之处在 Redis 2.1.3 之前的版本中，修改一个带有生存时间的 key 会导致整个 key 被删除，这一行为是受当时复制(replication)层的限制而作出的，现在这一限制已经被修复。 返回值设置成功返回 1 。 当 key 不存在或者不能为 key 设置生存时间时(比如在低于 2.1.3 版本的 Redis 中你尝试更新 key 的生存时间)，返回 0 。 代码示例1234567891011121314redis&gt; SET cache_page "www.google.com"OKredis&gt; EXPIRE cache_page 30 # 设置过期时间为 30 秒(integer) 1redis&gt; TTL cache_page # 查看剩余生存时间(integer) 23redis&gt; EXPIRE cache_page 30000 # 更新过期时间(integer) 1redis&gt; TTL cache_page(integer) 29996 模式：导航会话假设你有一项 web 服务，打算根据用户最近访问的 N 个页面来进行物品推荐，并且假设用户停止阅览超过 60 秒，那么就清空阅览记录(为了减少物品推荐的计算量，并且保持推荐物品的新鲜度)。 这些最近访问的页面记录，我们称之为『导航会话』(Navigation session)，可以用 INCR 和 RPUSH 命令在 Redis 中实现它：每当用户阅览一个网页的时候，执行以下代码：1234MULTI RPUSH pagewviews.user:&lt;userid&gt; http://..... EXPIRE pagewviews.user:&lt;userid&gt; 60EXEC 如果用户停止阅览超过 60 秒，那么它的导航会话就会被清空，当用户重新开始阅览的时候，系统又会重新记录导航会话，继续进行物品推荐。 EXPIREATEXPIREAT key timestamp可用版本： &gt;= 1.2.0 时间复杂度： O(1) EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置生存时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 返回值如果生存时间设置成功，返回 1 ； 当 key 不存在或没办法设置生存时间，返回 0 。 代码示例12345678redis&gt; SET cache www.google.comOKredis&gt; EXPIREAT cache 1355292000 # 这个 key 将在 2012.12.12 过期(integer) 1redis&gt; TTL cache(integer) 45081860 TTLTTL key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 返回值当 key 不存在时，返回 -2 。 当 key 存在但没有设置剩余生存时间时，返回 -1 。 否则，以秒为单位，返回 key 的剩余生存时间。 Note在 Redis 2.8 以前，当 key 不存在，或者 key 没有设置剩余生存时间时，命令都返回 -1 。 代码示例1234567891011121314151617181920# 不存在的 keyredis&gt; FLUSHDBOKredis&gt; TTL key(integer) -2# key 存在，但没有设置剩余生存时间redis&gt; SET key valueOKredis&gt; TTL key(integer) -1# 有剩余生存时间的 keyredis&gt; EXPIRE key 10086(integer) 1redis&gt; TTL key(integer) 10084 PERSISTPERSIST key可用版本： &gt;= 2.2.0 时间复杂度： O(1) 移除给定 key 的生存时间，将这个 key 从“易失的”(带生存时间 key )转换成“持久的”(一个不带生存时间、永不过期的 key )。 返回值当生存时间移除成功时，返回 1 . 如果 key 不存在或 key 没有设置生存时间，返回 0 。 代码示例1234567891011121314redis&gt; SET mykey "Hello"OKredis&gt; EXPIRE mykey 10 # 为 key 设置生存时间(integer) 1redis&gt; TTL mykey(integer) 10redis&gt; PERSIST mykey # 移除 key 的生存时间(integer) 1redis&gt; TTL mykey(integer) -1 PEXPIREPEXPIRE key milliseconds可用版本： &gt;= 2.6.0 时间复杂度： O(1) 这个命令和 EXPIRE 命令的作用类似，但是它以毫秒为单位设置 key 的生存时间，而不像 EXPIRE 命令那样，以秒为单位。 返回值设置成功，返回 1 key 不存在或设置失败，返回 0 代码示例1234567891011redis&gt; SET mykey "Hello"OKredis&gt; PEXPIRE mykey 1500(integer) 1redis&gt; TTL mykey # TTL 的返回值以秒为单位(integer) 2redis&gt; PTTL mykey # PTTL 可以给出准确的毫秒数(integer) 1499 PEXPIREATPEXPIREAT key milliseconds-timestamp可用版本： &gt;= 2.6.0 时间复杂度： O(1) 这个命令和 expireat 命令类似，但它以毫秒为单位设置 key 的过期 unix 时间戳，而不是像 expireat 那样，以秒为单位。 返回值如果生存时间设置成功，返回 1 。 当 key 不存在或没办法设置生存时间时，返回 0 。(查看 EXPIRE key seconds 命令获取更多信息) 代码示例1234567891011redis&gt; SET mykey "Hello"OKredis&gt; PEXPIREAT mykey 1555555555005(integer) 1redis&gt; TTL mykey # TTL 返回秒(integer) 223157079redis&gt; PTTL mykey # PTTL 返回毫秒(integer) 223157079318 PTTLPTTL key可用版本： &gt;= 2.6.0 复杂度： O(1) 这个命令类似于 TTL 命令，但它以毫秒为单位返回 key 的剩余生存时间，而不是像 TTL 命令那样，以秒为单位。 返回值 当 key 不存在时，返回 -2 。 当 key 存在但没有设置剩余生存时间时，返回 -1 。 否则，以毫秒为单位，返回 key 的剩余生存时间。Note 在 Redis 2.8 以前，当 key 不存在，或者 key 没有设置剩余生存时间时，命令都返回 -1 。 代码示例1234567891011121314151617181920# 不存在的 keyredis&gt; FLUSHDBOKredis&gt; PTTL key(integer) -2# key 存在，但没有设置剩余生存时间redis&gt; SET key valueOKredis&gt; PTTL key(integer) -1# 有剩余生存时间的 keyredis&gt; PEXPIRE key 10086(integer) 1redis&gt; PTTL key(integer) 6179 事务MULTIMULTI可用版本：&gt;= 1.2.0 时间复杂度：O(1)。 标记一个事务块的开始。 事务块内的多条命令会按照先后顺序被放进一个队列当中，最后由 EXEC 命令原子性(atomic)地执行。 返回值：总是返回 OK 。 代码示例1234567891011121314151617181920redis&gt; MULTI # 标记事务开始OKredis&gt; INCR user_id # 多条命令按顺序入队QUEUEDredis&gt; INCR user_idQUEUEDredis&gt; INCR user_idQUEUEDredis&gt; PINGQUEUEDredis&gt; EXEC # 执行1) (integer) 12) (integer) 23) (integer) 34) PONG EXECEXEC可用版本：&gt;= 1.2.0 时间复杂度：事务块内所有命令的时间复杂度的总和。 执行所有事务块内的命令。 假如某个(或某些) key 正处于 WATCH 命令的监视之下，且事务块中有和这个(或这些) key 相关的命令，那么 EXEC 命令只在这个(或这些) key 没有被其他命令所改动的情况下执行并生效，否则该事务被打断(abort)。 返回值：事务块内所有命令的返回值，按命令执行的先后顺序排列。当操作被打断时，返回空值 nil 。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 事务被成功执行redis&gt; MULTIOKredis&gt; INCR user_idQUEUEDredis&gt; INCR user_idQUEUEDredis&gt; INCR user_idQUEUEDredis&gt; PINGQUEUEDredis&gt; EXEC1) (integer) 12) (integer) 23) (integer) 34) PONG# 监视 key ，且事务成功执行redis&gt; WATCH lock lock_timesOKredis&gt; MULTIOKredis&gt; SET lock "huangz"QUEUEDredis&gt; INCR lock_timesQUEUEDredis&gt; EXEC1) OK2) (integer) 1# 监视 key ，且事务被打断redis&gt; WATCH lock lock_timesOKredis&gt; MULTIOKredis&gt; SET lock "joe" # 就在这时，另一个客户端修改了 lock_times 的值QUEUEDredis&gt; INCR lock_timesQUEUEDredis&gt; EXEC # 因为 lock_times 被修改， joe 的事务执行失败(nil) DISCARDDISCARD可用版本： &gt;= 2.0.0 时间复杂度： O(1)。 取消事务，放弃执行事务块内的所有命令。 如果正在使用 WATCH 命令监视某个(或某些) key，那么取消所有监视，等同于执行命令 UNWATCH 。 返回值总是返回 OK 。 代码示例1234567891011redis&gt; MULTIOKredis&gt; PINGQUEUEDredis&gt; SET greeting "hello"QUEUEDredis&gt; DISCARDOK WATCHWATCH key [key …]可用版本：&gt;= 2.2.0 时间复杂度：O(1)。 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 返回值：总是返回 OK 。 代码示例12redis&gt; WATCH lock lock_timesOK UNWATCHUNWATCH可用版本：&gt;= 2.2.0 时间复杂度：O(1) 取消 WATCH 命令对所有 key 的监视。 如果在执行 WATCH 命令之后， EXEC 命令或 DISCARD 命令先被执行了的话，那么就不需要再执行 UNWATCH 了。 因为 EXEC 命令会执行事务，因此 WATCH 命令的效果已经产生了；而 DISCARD 命令在取消事务的同时也会取消所有对 key 的监视，因此这两个命令执行之后，就没有必要执行 UNWATCH 了。 返回值：总是 OK 。 代码示例12345redis&gt; WATCH lock lock_timesOKredis&gt; UNWATCHOK Lua 脚本EVALEVAL script numkeys key [key …] arg [arg …]可用版本： &gt;= 2.6.0 时间复杂度： EVAL 和 EVALSHA 可以在 O(1) 复杂度内找到要被执行的脚本，其余的复杂度取决于执行的脚本本身。 从 Redis 2.6.0 版本开始，通过内置的 Lua 解释器，可以使用 EVAL 命令对 Lua 脚本进行求值。 script 参数是一段 Lua 5.1 脚本程序，它会被运行在 Redis 服务器上下文中，这段脚本不必(也不应该)定义为一个 Lua 函数。 numkeys 参数用于指定键名参数的个数。 键名参数 key [key …] 从 EVAL 的第三个参数开始算起，表示在脚本中所用到的那些 Redis 键(key)，这些键名参数可以在 Lua 中通过全局变量 KEYS 数组，用 1 为基址的形式访问( KEYS[1] ， KEYS[2] ，以此类推)。 在命令的最后，那些不是键名参数的附加参数 arg [arg …] ，可以在 Lua 中通过全局变量 ARGV 数组访问，访问的形式和 KEYS 变量类似( ARGV[1] 、 ARGV[2] ，诸如此类)。 上面这几段长长的说明可以用一个简单的例子来概括：12345&gt; eval "return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;" 2 key1 key2 first second1) "key1"2) "key2"3) "first"4) "second" 其中 “return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}” 是被求值的 Lua 脚本，数字 2 指定了键名参数的数量， key1 和 key2 是键名参数，分别使用 KEYS[1] 和 KEYS[2] 访问，而最后的 first 和 second 则是附加参数，可以通过 ARGV[1] 和 ARGV[2] 访问它们。 在 Lua 脚本中，可以使用两个不同函数来执行 Redis 命令，它们分别是： redis.call() redis.pcall()这两个函数的唯一区别在于它们使用不同的方式处理执行命令所产生的错误，在后面的『错误处理』部分会讲到这一点。 redis.call() 和 redis.pcall() 两个函数的参数可以是任何格式良好(well formed)的 Redis 命令：12&gt; eval "return redis.call('set','foo','bar')" 0OK 需要注意的是，上面这段脚本的确实现了将键 foo 的值设为 bar 的目的，但是，它违反了 EVAL 命令的语义，因为脚本里使用的所有键都应该由 KEYS 数组来传递，就像这样：12&gt; eval "return redis.call('set',KEYS[1],'bar')" 1 fooOK 要求使用正确的形式来传递键(key)是有原因的，因为不仅仅是 EVAL 这个命令，所有的 Redis 命令，在执行之前都会被分析，籍此来确定命令会对哪些键进行操作。 因此，对于 EVAL 命令来说，必须使用正确的形式来传递键，才能确保分析工作正确地执行。除此之外，使用正确的形式来传递键还有很多其他好处，它的一个特别重要的用途就是确保 Redis 集群可以将你的请求发送到正确的集群节点。(对 Redis 集群的工作还在进行当中，但是脚本功能被设计成可以与集群功能保持兼容。)不过，这条规矩并不是强制性的，从而使得用户有机会滥用(abuse) Redis 单实例配置(single instance configuration)，代价是这样写出的脚本不能被 Redis 集群所兼容。 在 Lua 数据类型和 Redis 数据类型之间转换当 Lua 通过 call() 或 pcall() 函数执行 Redis 命令的时候，命令的返回值会被转换成 Lua 数据结构。同样地，当 Lua 脚本在 Redis 内置的解释器里运行时，Lua 脚本的返回值也会被转换成 Redis 协议(protocol)，然后由 EVAL 将值返回给客户端。 数据类型之间的转换遵循这样一个设计原则：如果将一个 Redis 值转换成 Lua 值，之后再将转换所得的 Lua 值转换回 Redis 值，那么这个转换所得的 Redis 值应该和最初时的 Redis 值一样。 换句话说， Lua 类型和 Redis 类型之间存在着一一对应的转换关系。 以下列出的是详细的转换规则： 从 Redis 转换到 Lua ： Redis integer reply -&gt; Lua number / Redis 整数转换成 Lua 数字 Redis bulk reply -&gt; Lua string / Redis bulk 回复转换成 Lua 字符串 Redis multi bulk reply -&gt; Lua table (may have other Redis data types nested) / Redis 多条 bulk 回复转换成 Lua 表，表内可能有其他别的 Redis 数据类型 Redis status reply -&gt; Lua table with a single ok field containing the status / Redis 状态回复转换成 Lua 表，表内的 ok 域包含了状态信息 Redis error reply -&gt; Lua table with a single err field containing the error / Redis 错误回复转换成 Lua 表，表内的 err 域包含了错误信息 Redis Nil bulk reply and Nil multi bulk reply -&gt; Lua false boolean type / Redis 的 Nil 回复和 Nil 多条回复转换成 Lua 的布尔值 false从 Lua 转换到 Redis： Lua number -&gt; Redis integer reply / Lua 数字转换成 Redis 整数 Lua string -&gt; Redis bulk reply / Lua 字符串转换成 Redis bulk 回复 Lua table (array) -&gt; Redis multi bulk reply / Lua 表(数组)转换成 Redis 多条 bulk 回复 Lua table with a single ok field -&gt; Redis status reply / 一个带单个 ok 域的 Lua 表，转换成 Redis 状态回复 Lua table with a single err field -&gt; Redis error reply / 一个带单个 err 域的 Lua 表，转换成 Redis 错误回复 Lua boolean false -&gt; Redis Nil bulk reply / Lua 的布尔值 false 转换成 Redis 的 Nil bulk 回复从 Lua 转换到 Redis 有一条额外的规则，这条规则没有和它对应的从 Redis 转换到 Lua 的规则： Lua boolean true -&gt; Redis integer reply with value of 1 / Lua 布尔值 true 转换成 Redis 整数回复中的 1以下是几个类型转换的例子： 1234567891011&gt; eval "return 10" 0(integer) 10&gt; eval "return &#123;1,2,&#123;3,'Hello World!'&#125;&#125;" 01) (integer) 12) (integer) 23) 1) (integer) 3 2) "Hello World!"&gt; eval "return redis.call('get','foo')" 0"bar" 在上面的三个代码示例里，前两个演示了如何将 Lua 值转换成 Redis 值，最后一个例子更复杂一些，它演示了一个将 Redis 值转换成 Lua 值，然后再将 Lua 值转换成 Redis 值的类型转过程。 脚本的原子性Redis 使用单个 Lua 解释器去运行所有脚本，并且， Redis 也保证脚本会以原子性(atomic)的方式执行：当某个脚本正在运行的时候，不会有其他脚本或 Redis 命令被执行。这和使用 MULTI / EXEC 包围的事务很类似。在其他别的客户端看来，脚本的效果(effect)要么是不可见的(not visible)，要么就是已完成的(already completed)。 另一方面，这也意味着，执行一个运行缓慢的脚本并不是一个好主意。写一个跑得很快很顺溜的脚本并不难，因为脚本的运行开销(overhead)非常少，但是当你不得不使用一些跑得比较慢的脚本时，请小心，因为当这些蜗牛脚本在慢吞吞地运行的时候，其他客户端会因为服务器正忙而无法执行命令。 错误处理前面的命令介绍部分说过， redis.call() 和 redis.pcall() 的唯一区别在于它们对错误处理的不同。 当 redis.call() 在执行命令的过程中发生错误时，脚本会停止执行，并返回一个脚本错误，错误的输出信息会说明错误造成的原因：12345redis&gt; lpush foo a(integer) 1redis&gt; eval "return redis.call('get', 'foo')" 0(error) ERR Error running script (call to f_282297a0228f48cd3fc6a55de6316f31422f5d17): ERR Operation against a key holding the wrong kind of value 和 redis.call() 不同， redis.pcall() 出错时并不引发(raise)错误，而是返回一个带 err 域的 Lua 表(table)，用于表示错误：12redis 127.0.0.1:6379&gt; EVAL "return redis.pcall('get', 'foo')" 0(error) ERR Operation against a key holding the wrong kind of value 带宽和 EVALSHAEVAL 命令要求你在每次执行脚本的时候都发送一次脚本主体(script body)。Redis 有一个内部的缓存机制，因此它不会每次都重新编译脚本，不过在很多场合，付出无谓的带宽来传送脚本主体并不是最佳选择。 为了减少带宽的消耗， Redis 实现了 EVALSHA 命令，它的作用和 EVAL 一样，都用于对脚本求值，但它接受的第一个参数不是脚本，而是脚本的 SHA1 校验和(sum)。 EVALSHA 命令的表现如下： 如果服务器还记得给定的 SHA1 校验和所指定的脚本，那么执行这个脚本 如果服务器不记得给定的 SHA1 校验和所指定的脚本，那么它返回一个特殊的错误，提醒用户使用 EVAL 代替 EVALSHA以下是示例：1234567891011&gt; set foo barOK&gt; eval "return redis.call('get','foo')" 0"bar"&gt; evalsha 6b1bf486c81ceb7edf3c093f4c48582e38c0e791 0"bar"&gt; evalsha ffffffffffffffffffffffffffffffffffffffff 0(error) `NOSCRIPT` No matching script. Please use [EVAL](/commands/eval). 客户端库的底层实现可以一直乐观地使用 EVALSHA 来代替 EVAL ，并期望着要使用的脚本已经保存在服务器上了，只有当 NOSCRIPT 错误发生时，才使用 EVAL 命令重新发送脚本，这样就可以最大限度地节省带宽。 这也说明了执行 EVAL 命令时，使用正确的格式来传递键名参数和附加参数的重要性：因为如果将参数硬写在脚本中，那么每次当参数改变的时候，都要重新发送脚本，即使脚本的主体并没有改变，相反，通过使用正确的格式来传递键名参数和附加参数，就可以在脚本主体不变的情况下，直接使用 EVALSHA 命令对脚本进行复用，免去了无谓的带宽消耗。 脚本缓存Redis 保证所有被运行过的脚本都会被永久保存在脚本缓存当中，这意味着，当 EVAL 命令在一个 Redis 实例上成功执行某个脚本之后，随后针对这个脚本的所有 EVALSHA 命令都会成功执行。 刷新脚本缓存的唯一办法是显式地调用 SCRIPT FLUSH 命令，这个命令会清空运行过的所有脚本的缓存。通常只有在云计算环境中，Redis 实例被改作其他客户或者别的应用程序的实例时，才会执行这个命令。 缓存可以长时间储存而不产生内存问题的原因是，它们的体积非常小，而且数量也非常少，即使脚本在概念上类似于实现一个新命令，即使在一个大规模的程序里有成百上千的脚本，即使这些脚本会经常修改，即便如此，储存这些脚本的内存仍然是微不足道的。 事实上，用户会发现 Redis 不移除缓存中的脚本实际上是一个好主意。比如说，对于一个和 Redis 保持持久化链接(persistent connection)的程序来说，它可以确信，执行过一次的脚本会一直保留在内存当中，因此它可以在流水线中使用 EVALSHA 命令而不必担心因为找不到所需的脚本而产生错误(稍候我们会看到在流水线中执行脚本的相关问题)。 SCRIPT 命令Redis 提供了以下几个 SCRIPT 命令，用于对脚本子系统(scripting subsystem)进行控制： SCRIPT FLUSH ：清除所有脚本缓存 SCRIPT EXISTS sha1 [sha1 …] ：根据给定的脚本校验和，检查指定的脚本是否存在于脚本缓存 SCRIPT LOAD script ：将一个脚本装入脚本缓存，但并不立即运行它 SCRIPT KILL ：杀死当前正在运行的脚本纯函数脚本在编写脚本方面，一个重要的要求就是，脚本应该被写成纯函数(pure function)。 也就是说，脚本应该具有以下属性： 对于同样的数据集输入，给定相同的参数，脚本执行的 Redis 写命令总是相同的。脚本执行的操作不能依赖于任何隐藏(非显式)数据，不能依赖于脚本在执行过程中、或脚本在不同执行时期之间可能变更的状态，并且它也不能依赖于任何来自 I/O 设备的外部输入。 使用系统时间(system time)，调用像 RANDOMKEY 那样的随机命令，或者使用 Lua 的随机数生成器，类似以上的这些操作，都会造成脚本的求值无法每次都得出同样的结果。 为了确保脚本符合上面所说的属性， Redis 做了以下工作： Lua 没有访问系统时间或者其他内部状态的命令 Redis 会返回一个错误，阻止这样的脚本运行： 这些脚本在执行随机命令之后(比如 RANDOMKEY 、 SRANDMEMBER key [count] 或 TIME 等)，还会执行可以修改数据集的 Redis 命令。如果脚本只是执行只读操作，那么就没有这一限制。注意，随机命令并不一定就指那些带 RAND 字眼的命令，任何带有非确定性的命令都会被认为是随机命令，比如 TIME 命令就是这方面的一个很好的例子。 每当从 Lua 脚本中调用那些返回无序元素的命令时，执行命令所得的数据在返回给 Lua 之前会先执行一个静默(slient)的字典序排序(lexicographical sorting)。举个例子，因为 Redis 的 Set 保存的是无序的元素，所以在 Redis 命令行客户端中直接执行 SMEMBERS key ，返回的元素是无序的，但是，假如在脚本中执行 redis.call(“smembers”, KEYS[1]) ，那么返回的总是排过序的元素。 对 Lua 的伪随机数生成函数 math.random 和 math.randomseed 进行修改，使得每次在运行新脚本的时候，总是拥有同样的 seed 值。这意味着，每次运行脚本时，只要不使用 math.randomseed ，那么 math.random 产生的随机数序列总是相同的。 尽管有那么多的限制，但用户还是可以用一个简单的技巧写出带随机行为的脚本(如果他们需要的话)。 假设现在我们要编写一个 Redis 脚本，这个脚本从列表中弹出 N 个随机数。一个 Ruby 写的例子如下：1234567891011121314151617require 'rubygems'require 'redis'r = Redis.newRandomPushScript = &lt;&lt;EOF local i = tonumber(ARGV[1]) local res while (i &gt; 0) do res = redis.call('lpush',KEYS[1],math.random()) i = i-1 end return resEOFr.del(:mylist)puts r.eval(RandomPushScript,[:mylist],[10,rand(2**32)]) 这个程序每次运行都会生成带有以下元素的列表：1234567891011&gt; lrange mylist 0 -11) "0.74509509873814"2) "0.87390407681181"3) "0.36876626981831"4) "0.6921941534114"5) "0.7857992587545"6) "0.57730350670279"7) "0.87046522734243"8) "0.09637165539729"9) "0.74990198051087"10) "0.17082803611217" 上面的 Ruby 程序每次都只生成同样的列表，用途并不是太大。那么，该怎样修改这个脚本，使得它仍然是一个纯函数(符合 Redis 的要求)，但是每次调用都可以产生不同的随机元素呢？ 一个简单的办法是，为脚本添加一个额外的参数，让这个参数作为 Lua 的随机数生成器的 seed 值，这样的话，只要给脚本传入不同的 seed ，脚本就会生成不同的列表元素。 以下是修改后的脚本：12345678910111213RandomPushScript = &lt;&lt;EOF local i = tonumber(ARGV[1]) local res math.randomseed(tonumber(ARGV[2])) while (i &gt; 0) do res = redis.call('lpush',KEYS[1],math.random()) i = i-1 end return resEOFr.del(:mylist)puts r.eval(RandomPushScript,1,:mylist,10,rand(2**32)) 尽管对于同样的 seed ，上面的脚本产生的列表元素是一样的(因为它是一个纯函数)，但是只要每次在执行脚本的时候传入不同的 seed ，我们就可以得到带有不同随机元素的列表。 Seed 会在复制(replication link)和写 AOF 文件时作为一个参数来传播，保证在载入 AOF 文件或附属节点(slave)处理脚本时， seed 仍然可以及时得到更新。 注意，Redis 实现保证 math.random 和 math.randomseed 的输出和运行 Redis 的系统架构无关，无论是 32 位还是 64 位系统，无论是小端(little endian)还是大端(big endian)系统，这两个函数的输出总是相同的。 全局变量保护为了防止不必要的数据泄漏进 Lua 环境， Redis 脚本不允许创建全局变量。如果一个脚本需要在多次执行之间维持某种状态，它应该使用 Redis key 来进行状态保存。 企图在脚本中访问一个全局变量(不论这个变量是否存在)将引起脚本停止， EVAL 命令会返回一个错误：12redis 127.0.0.1:6379&gt; eval 'a=10' 0(error) ERR Error running script (call to f_933044db579a2f8fd45d8065f04a8d0249383e57): user_script:1: Script attempted to create global variable 'a' Lua 的 debug 工具，或者其他设施，比如打印（alter）用于实现全局保护的 meta table ，都可以用于实现全局变量保护。 实现全局变量保护并不难，不过有时候还是会不小心而为之。一旦用户在脚本中混入了 Lua 全局状态，那么 AOF 持久化和复制（replication）都会无法保证，所以，请不要使用全局变量。 避免引入全局变量的一个诀窍是：将脚本中用到的所有变量都使用 local 关键字定义为局部变量。 库Redis 内置的 Lua 解释器加载了以下 Lua 库： base table string math debug cjson cmsgpack其中 cjson 库可以让 Lua 以非常快的速度处理 JSON 数据，除此之外，其他别的都是 Lua 的标准库。 每个 Redis 实例都保证会加载上面列举的库，从而确保每个 Redis 脚本的运行环境都是相同的。 使用脚本散发 Redis 日志在 Lua 脚本中，可以通过调用 redis.log 函数来写 Redis 日志(log)：1redis.log(loglevel, message) 其中， message 参数是一个字符串，而 loglevel 参数可以是以下任意一个值： redis.LOG_DEBUG redis.LOG_VERBOSE redis.LOG_NOTICE redis.LOG_WARNING上面的这些等级(level)和标准 Redis 日志的等级相对应。 对于脚本散发(emit)的日志，只有那些和当前 Redis 实例所设置的日志等级相同或更高级的日志才会被散发。 以下是一个日志示例：1redis.log(redis.LOG_WARNING, "Something is wrong with this script.") 执行上面的函数会产生这样的信息：1[32343] 22 Mar 15:21:39 # Something is wrong with this script. 沙箱(sandbox)和最大执行时间脚本应该仅仅用于传递参数和对 Redis 数据进行处理，它不应该尝试去访问外部系统(比如文件系统)，或者执行任何系统调用。 除此之外，脚本还有一个最大执行时间限制，它的默认值是 5 秒钟，一般正常运作的脚本通常可以在几分之几毫秒之内完成，花不了那么多时间，这个限制主要是为了防止因编程错误而造成的无限循环而设置的。 最大执行时间的长短由 lua-time-limit 选项来控制(以毫秒为单位)，可以通过编辑 redis.conf 文件或者使用 CONFIG GET parameter 和 CONFIG SET parameter value 命令来修改它。 当一个脚本达到最大执行时间的时候，它并不会自动被 Redis 结束，因为 Redis 必须保证脚本执行的原子性，而中途停止脚本的运行意味着可能会留下未处理完的数据在数据集(data set)里面。 因此，当脚本运行的时间超过最大执行时间后，以下动作会被执行： Redis 记录一个脚本正在超时运行 Redis 开始重新接受其他客户端的命令请求，但是只有 SCRIPT KILL 和 SHUTDOWN NOSAVE 两个命令会被处理，对于其他命令请求， Redis 服务器只是简单地返回 BUSY 错误。 可以使用 SCRIPT KILL 命令将一个仅执行只读命令的脚本杀死，因为只读命令并不修改数据，因此杀死这个脚本并不破坏数据的完整性 如果脚本已经执行过写命令，那么唯一允许执行的操作就是 SHUTDOWN NOSAVE ，它通过停止服务器来阻止当前数据集写入磁盘流水线(pipeline)上下文(context)中的 EVALSHA在流水线请求的上下文中使用 EVALSHA 命令时，要特别小心，因为在流水线中，必须保证命令的执行顺序。 一旦在流水线中因为 EVALSHA 命令而发生 NOSCRIPT 错误，那么这个流水线就再也没有办法重新执行了，否则的话，命令的执行顺序就会被打乱。 为了防止出现以上所说的问题，客户端库实现应该实施以下的其中一项措施： 总是在流水线中使用 EVAL 命令 检查流水线中要用到的所有命令，找到其中的 EVAL 命令，并使用 SCRIPT EXISTS sha1 [sha1 …] 命令检查要用到的脚本是不是全都已经保存在缓存里面了。如果所需的全部脚本都可以在缓存里找到，那么就可以放心地将所有 EVAL 命令改成 EVALSHA 命令，否则的话，就要在流水线的顶端(top)将缺少的脚本用 SCRIPT LOAD script 命令加上去。 EVALSHAEVALSHA sha1 numkeys key [key …] arg [arg …]可用版本： &gt;= 2.6.0 时间复杂度： 根据脚本的复杂度而定。 根据给定的 sha1 校验码，对缓存在服务器中的脚本进行求值。 将脚本缓存到服务器的操作可以通过 SCRIPT LOAD script 命令进行。 这个命令的其他地方，比如参数的传入方式，都和 EVAL script numkeys key [key …] arg [arg …] 命令一样。12345redis&gt; SCRIPT LOAD "return 'hello moto'""232fd51614574cf0867b83d384a5e898cfd24e5a"redis&gt; EVALSHA "232fd51614574cf0867b83d384a5e898cfd24e5a" 0"hello moto" SCRIPT LOADSCRIPT LOAD script可用版本： &gt;= 2.6.0 时间复杂度： O(N) , N 为脚本的长度(以字节为单位)。 将脚本 script 添加到脚本缓存中，但并不立即执行这个脚本。 EVAL script numkeys key [key …] arg [arg …] 命令也会将脚本添加到脚本缓存中，但是它会立即对输入的脚本进行求值。 如果给定的脚本已经在缓存里面了，那么不做动作。 在脚本被加入到缓存之后，通过 EVALSHA 命令，可以使用脚本的 SHA1 校验和来调用这个脚本。 脚本可以在缓存中保留无限长的时间，直到执行 SCRIPT FLUSH 为止。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值给定 script 的 SHA1 校验和。 代码示例12345redis&gt; SCRIPT LOAD "return 'hello moto'""232fd51614574cf0867b83d384a5e898cfd24e5a"redis&gt; EVALSHA 232fd51614574cf0867b83d384a5e898cfd24e5a 0"hello moto" SCRIPT EXISTSSCRIPT EXISTS sha1 [sha1 …]可用版本： &gt;= 2.6.0 时间复杂度： O(N) , N 为给定的 SHA1 校验和的数量。 给定一个或多个脚本的 SHA1 校验和，返回一个包含 0 和 1 的列表，表示校验和所指定的脚本是否已经被保存在缓存当中。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值一个列表，包含 0 和 1 ，前者表示脚本不存在于缓存，后者表示脚本已经在缓存里面了。 列表中的元素和给定的 SHA1 校验和保持对应关系，比如列表的第三个元素的值就表示第三个 SHA1 校验和所指定的脚本在缓存中的状态。 代码示例1234567891011redis&gt; SCRIPT LOAD "return 'hello moto'" # 载入一个脚本"232fd51614574cf0867b83d384a5e898cfd24e5a"redis&gt; SCRIPT EXISTS 232fd51614574cf0867b83d384a5e898cfd24e5a1) (integer) 1redis&gt; SCRIPT FLUSH # 清空缓存OKredis&gt; SCRIPT EXISTS 232fd51614574cf0867b83d384a5e898cfd24e5a1) (integer) 0 SCRIPT FLUSHSCRIPT FLUSH可用版本： &gt;= 2.6.0 复杂度： O(N) ， N 为缓存中脚本的数量。 清除所有 Lua 脚本缓存。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值总是返回 OK 代码示例12redis&gt; SCRIPT FLUSHOK SCRIPT KILLSCRIPT KILL可用版本： &gt;= 2.6.0 时间复杂度： O(1) 杀死当前正在运行的 Lua 脚本，当且仅当这个脚本没有执行过任何写操作时，这个命令才生效。 这个命令主要用于终止运行时间过长的脚本，比如一个因为 BUG 而发生无限 loop 的脚本，诸如此类。 SCRIPT KILL 执行之后，当前正在运行的脚本会被杀死，执行这个脚本的客户端会从 EVAL script numkeys key [key …] arg [arg …] 命令的阻塞当中退出，并收到一个错误作为返回值。 另一方面，假如当前正在运行的脚本已经执行过写操作，那么即使执行 SCRIPT KILL ，也无法将它杀死，因为这是违反 Lua 脚本的原子性执行原则的。在这种情况下，唯一可行的办法是使用 SHUTDOWN NOSAVE 命令，通过停止整个 Redis 进程来停止脚本的运行，并防止不完整(half-written)的信息被写入数据库中。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值执行成功返回 OK ，否则返回一个错误。 代码示例12345678910111213# 没有脚本在执行时redis&gt; SCRIPT KILL(error) ERR No scripts in execution right now.# 成功杀死脚本时redis&gt; SCRIPT KILLOK(1.30s)# 尝试杀死一个已经执行过写操作的脚本，失败redis&gt; SCRIPT KILL(error) ERR Sorry the script already executed write commands against the dataset. You can either wait the script termination or kill the server in an hard way using the SHUTDOWN NOSAVE command.(1.69s) 以下是脚本被杀死之后，返回给执行脚本的客户端的错误：123redis&gt; EVAL "while true do end" 0(error) ERR Error running script (call to f_694a5fe1ddb97a4c6a1bf299d9537c7d3d0f84e7): Script killed by user with SCRIPT KILL...(5.00s) 持久化SAVESAVE可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为要保存到数据库中的 key 的数量。 SAVE 命令执行一个同步保存操作，将当前 Redis 实例的所有数据快照(snapshot)以 RDB 文件的形式保存到硬盘。 一般来说，在生产环境很少执行 SAVE 操作，因为它会阻塞所有客户端，保存数据库的任务通常由 BGSAVE 命令异步地执行。然而，如果负责保存数据的后台子进程不幸出现问题时， SAVE 可以作为保存数据的最后手段来使用。 返回值保存成功时返回 OK 。 代码示例12redis&gt; SAVEOK BGSAVEBGSAVE可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为要保存到数据库中的 key 的数量。 在后台异步(Asynchronously)保存当前数据库的数据到磁盘。 BGSAVE 命令执行之后立即返回 OK ，然后 Redis fork 出一个新子进程，原来的 Redis 进程(父进程)继续处理客户端请求，而子进程则负责将数据保存到磁盘，然后退出。 客户端可以通过 LASTSAVE 命令查看相关信息，判断 BGSAVE 命令是否执行成功。 请移步 持久化文档 查看更多相关细节。 返回值反馈信息。 代码示例12redis&gt; BGSAVEBackground saving started BGREWRITEAOFBGREWRITEAOF可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为要追加到 AOF 文件中的数据数量。 执行一个 AOF文件 重写操作。重写会创建一个当前 AOF 文件的体积优化版本。 即使 BGREWRITEAOF 执行失败，也不会有任何数据丢失，因为旧的 AOF 文件在 BGREWRITEAOF 成功之前不会被修改。 重写操作只会在没有其他持久化工作在后台执行时被触发，也就是说： 如果 Redis 的子进程正在执行快照的保存工作，那么 AOF 重写的操作会被预定(scheduled)，等到保存工作完成之后再执行 AOF 重写。在这种情况下， BGREWRITEAOF 的返回值仍然是 OK ，但还会加上一条额外的信息，说明 BGREWRITEAOF 要等到保存操作完成之后才能执行。在 Redis 2.6 或以上的版本，可以使用 INFO [section] 命令查看 BGREWRITEAOF 是否被预定。如果已经有别的 AOF 文件重写在执行，那么 BGREWRITEAOF 返回一个错误，并且这个新的 BGREWRITEAOF 请求也不会被预定到下次执行。从 Redis 2.4 开始， AOF 重写由 Redis 自行触发， BGREWRITEAOF 仅仅用于手动触发重写操作。 请移步 持久化文档(英文) 查看更多相关细节。 返回值反馈信息。 代码示例12redis&gt; BGREWRITEAOFBackground append only file rewriting started LASTSAVELASTSAVE可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回最近一次 Redis 成功将数据保存到磁盘上的时间，以 UNIX 时间戳格式表示。 返回值一个 UNIX 时间戳。 代码示例12redis&gt; LASTSAVE(integer) 1324043588 发布与订阅PUBLISHPUBLISH channel message可用版本： &gt;= 2.0.0 时间复杂度： O(N+M)，其中 N 是频道 channel 的订阅者数量，而 M 则是使用模式订阅(subscribed patterns)的客户端的数量。 将信息 message 发送到指定的频道 channel 。 返回值接收到信息 message 的订阅者数量。 代码示例1234567891011# 对没有订阅者的频道发送信息redis&gt; publish bad_channel "can any body hear me?"(integer) 0# 向有一个订阅者的频道发送信息redis&gt; publish msg "good morning"(integer) 1# 向有多个订阅者的频道发送信息redis&gt; publish chat_room "hello~ everyone"(integer) 3 SUBSCRIBESUBSCRIBE channel [channel …]可用版本： &gt;= 2.0.0 时间复杂度： O(N)，其中 N 是订阅的频道的数量。 订阅给定的一个或多个频道的信息。 返回值接收到的信息(请参见下面的代码说明)。 代码示例123456789101112131415161718192021# 订阅 msg 和 chat_room 两个频道# 1 - 6 行是执行 subscribe 之后的反馈信息# 第 7 - 9 行才是接收到的第一条信息# 第 10 - 12 行是第二条redis&gt; subscribe msg chat_roomReading messages... (press Ctrl-C to quit)1) "subscribe" # 返回值的类型：显示订阅成功2) "msg" # 订阅的频道名字3) (integer) 1 # 目前已订阅的频道数量1) "subscribe"2) "chat_room"3) (integer) 21) "message" # 返回值的类型：信息2) "msg" # 来源(从那个频道发送过来)3) "hello moto" # 信息内容1) "message"2) "chat_room"3) "testing...haha" PSUBSCRIBEPSUBSCRIBE pattern [pattern …]可用版本： &gt;= 2.0.0 时间复杂度： O(N)， N 是订阅的模式的数量。 订阅一个或多个符合给定模式的频道。 每个模式以 作为匹配符，比如 it 匹配所有以 it 开头的频道( it.news 、 it.blog 、 it.tweets 等等)， news.* 匹配所有以 news. 开头的频道( news.it 、 news.global.today 等等)，诸如此类。 返回值接收到的信息(请参见下面的代码说明)。 代码示例1234567891011121314151617181920212223242526272829303132333435# 订阅 news.* 和 tweet.* 两个模式# 第 1 - 6 行是执行 psubscribe 之后的反馈信息# 第 7 - 10 才是接收到的第一条信息# 第 11 - 14 是第二条# 以此类推。。。redis&gt; psubscribe news.* tweet.*Reading messages... (press Ctrl-C to quit)1) "psubscribe" # 返回值的类型：显示订阅成功2) "news.*" # 订阅的模式3) (integer) 1 # 目前已订阅的模式的数量1) "psubscribe"2) "tweet.*"3) (integer) 21) "pmessage" # 返回值的类型：信息2) "news.*" # 信息匹配的模式3) "news.it" # 信息本身的目标频道4) "Google buy Motorola" # 信息的内容1) "pmessage"2) "tweet.*"3) "tweet.huangz"4) "hello"1) "pmessage"2) "tweet.*"3) "tweet.joe"4) "@huangz morning"1) "pmessage"2) "news.*"3) "news.life"4) "An apple a day, keep doctors away" UNSUBSCRIBEUNSUBSCRIBE [channel [channel …]]可用版本： &gt;= 2.0.0 时间复杂度： O(N) ， N 是客户端已订阅的频道的数量。 指示客户端退订给定的频道。 如果没有频道被指定，也即是，一个无参数的 UNSUBSCRIBE 调用被执行，那么客户端使用 SUBSCRIBE 命令订阅的所有频道都会被退订。在这种情况下，命令会返回一个信息，告知客户端所有被退订的频道。 返回值这个命令在不同的客户端中有不同的表现。 PUNSUBSCRIBEPUNSUBSCRIBE [pattern [pattern …]]可用版本： &gt;= 2.0.0 时间复杂度： O(N+M) ，其中 N 是客户端已订阅的模式的数量， M 则是系统中所有客户端订阅的模式的数量。 指示客户端退订所有给定模式。 如果没有模式被指定，也即是，一个无参数的 PUNSUBSCRIBE 调用被执行，那么客户端使用 PSUBSCRIBE pattern [pattern …] 命令订阅的所有模式都会被退订。在这种情况下，命令会返回一个信息，告知客户端所有被退订的模式。 返回值这个命令在不同的客户端中有不同的表现。 PUBSUBPUBSUB [argument [argument …]]可用版本：&gt;= 2.8.0 PUBSUB 是一个查看订阅与发布系统状态的内省命令， 它由数个不同格式的子命令组成， 以下将分别对这些子命令进行介绍。 PUBSUB CHANNELS [pattern]复杂度： O(N) ， N 为活跃频道的数量（对于长度较短的频道和模式来说，将进行模式匹配的复杂度视为常数）。列出当前的活跃频道。 活跃频道指的是那些至少有一个订阅者的频道， 订阅模式的客户端不计算在内。 pattern 参数是可选的： 如果不给出 pattern 参数，那么列出订阅与发布系统中的所有活跃频道。 如果给出 pattern 参数，那么只列出和给定模式 pattern 相匹配的那些活跃频道。返回值一个由活跃频道组成的列表。 代码示例1234567891011121314151617181920212223242526272829303132# client-1 订阅 news.it 和 news.sport 两个频道client-1&gt; SUBSCRIBE news.it news.sportReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.sport"3) (integer) 2# client-2 订阅 news.it 和 news.internet 两个频道client-2&gt; SUBSCRIBE news.it news.internetReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.internet"3) (integer) 2# 首先， client-3 打印所有活跃频道# 注意，即使一个频道有多个订阅者，它也只输出一次，比如 news.itclient-3&gt; PUBSUB CHANNELS1) "news.sport"2) "news.internet"3) "news.it"# 接下来， client-3 打印那些与模式 news.i* 相匹配的活跃频道# 因为 news.sport 不匹配 news.i* ，所以它没有被打印redis&gt; PUBSUB CHANNELS news.i*1) "news.internet"2) "news.it" PUBSUB NUMSUB [channel-1 … channel-N]复杂度： O(N) ， N 为给定频道的数量。 返回给定频道的订阅者数量， 订阅模式的客户端不计算在内。 返回值一个多条批量回复（Multi-bulk reply），回复中包含给定的频道，以及频道的订阅者数量。 格式为：频道 channel-1 ， channel-1 的订阅者数量，频道 channel-2 ， channel-2 的订阅者数量，诸如此类。 回复中频道的排列顺序和执行命令时给定频道的排列顺序一致。 不给定任何频道而直接调用这个命令也是可以的， 在这种情况下， 命令只返回一个空列表。 代码示例123456789101112131415161718192021222324252627282930# client-1 订阅 news.it 和 news.sport 两个频道client-1&gt; SUBSCRIBE news.it news.sportReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.sport"3) (integer) 2# client-2 订阅 news.it 和 news.internet 两个频道client-2&gt; SUBSCRIBE news.it news.internetReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.internet"3) (integer) 2# client-3 打印各个频道的订阅者数量client-3&gt; PUBSUB NUMSUB news.it news.internet news.sport news.music1) "news.it" # 频道2) "2" # 订阅该频道的客户端数量3) "news.internet"4) "1"5) "news.sport"6) "1"7) "news.music" # 没有任何订阅者8) "0" PUBSUB NUMPAT复杂度： O(1) 。 返回订阅模式的数量。 注意， 这个命令返回的不是订阅模式的客户端的数量， 而是客户端订阅的所有模式的数量总和。 返回值一个整数回复（Integer reply）。 代码示例1234567891011121314151617181920212223242526272829303132# client-1 订阅 news.* 和 discount.* 两个模式client-1&gt; PSUBSCRIBE news.* discount.*Reading messages... (press Ctrl-C to quit)1) "psubscribe"2) "news.*"3) (integer) 11) "psubscribe"2) "discount.*"3) (integer) 2# client-2 订阅 tweet.* 一个模式client-2&gt; PSUBSCRIBE tweet.*Reading messages... (press Ctrl-C to quit)1) "psubscribe"2) "tweet.*"3) (integer) 1# client-3 返回当前订阅模式的数量为 3client-3&gt; PUBSUB NUMPAT(integer) 3# 注意，当有多个客户端订阅相同的模式时，相同的订阅也被计算在 PUBSUB NUMPAT 之内# 比如说，再新建一个客户端 client-4 ，让它也订阅 news.* 频道client-4&gt; PSUBSCRIBE news.*Reading messages... (press Ctrl-C to quit)1) "psubscribe"2) "news.*"3) (integer) 1# 这时再计算被订阅模式的数量，就会得到数量为 4client-3&gt; PUBSUB NUMPAT(integer) 4 复制SLAVEOFSLAVEOF host port可用版本： &gt;= 1.0.0 时间复杂度： SLAVEOF host port 的复杂度为 O(N)，其中 N 为要同步的数据数量； SLAVEOF NO ONE 命令的复杂度为 O(1) 。 SLAVEOF 命令用于在 Redis 运行时动态地修改复制(replication)功能的行为。 通过执行 SLAVEOF host port 命令，可以将当前服务器转变为指定服务器的从属服务器(slave server)。 如果当前服务器已经是某个主服务器(master server)的从属服务器，那么执行 SLAVEOF host port 将使当前服务器停止对旧主服务器的同步，丢弃旧数据集，转而开始对新主服务器进行同步。 另外，对一个从属服务器执行命令 SLAVEOF NO ONE 将使得这个从属服务器关闭复制功能，并从从属服务器转变回主服务器，原来同步所得的数据集不会被丢弃。 利用“SLAVEOF NO ONE 不会丢弃同步所得数据集”这个特性，可以在主服务器失败的时候，将从属服务器用作新的主服务器，从而实现无间断运行。 返回值总是返回 OK 。 代码示例12345redis&gt; SLAVEOF 127.0.0.1 6379OKredis&gt; SLAVEOF NO ONEOK ROLEROLE可用版本： &gt;= 2.8.12 时间复杂度： O(1) 返回实例在复制中担任的角色， 这个角色可以是 master 、 slave 或者 sentinel 。 除了角色之外， 命令还会返回与该角色相关的其他信息， 其中： 主服务器将返回属下从服务器的 IP 地址和端口。 从服务器将返回自己正在复制的主服务器的 IP 地址、端口、连接状态以及复制偏移量。 Sentinel 将返回自己正在监视的主服务器列表。返回值ROLE 命令将返回一个数组。 代码示例主服务器123456781) "master"2) (integer) 31296593) 1) 1) "127.0.0.1" 2) "9001" 3) "3129242" 2) 1) "127.0.0.1" 2) "9002" 3) "3129543" 从服务器123451) "slave"2) "127.0.0.1"3) (integer) 90004) "connected"5) (integer) 3167038 Sentinel123451) "sentinel"2) 1) "resque-master" 2) "html-fragments-master" 3) "stats-master" 4) "metadata-master" 客户端与服务器AUTHAUTH password可用版本： &gt;= 1.0.0 时间复杂度： O(1) 通过设置配置文件中 requirepass 项的值(使用命令 CONFIG SET requirepass password )，可以使用密码来保护 Redis 服务器。 如果开启了密码保护的话，在每次连接 Redis 服务器之后，就要使用 AUTH 命令解锁，解锁之后才能使用其他 Redis 命令。 如果 AUTH 命令给定的密码 password 和配置文件中的密码相符的话，服务器会返回 OK 并开始接受命令输入。 另一方面，假如密码不匹配的话，服务器将返回一个错误，并要求客户端需重新输入密码。 Warning因为 Redis 高性能的特点，在很短时间内尝试猜测非常多个密码是有可能的，因此请确保使用的密码足够复杂和足够长，以免遭受密码猜测攻击。 返回值密码匹配时返回 OK ，否则返回一个错误。 代码示例12345678910111213141516171819202122232425262728# 设置密码redis&gt; CONFIG SET requirepass secret_password # 将密码设置为 secret_passwordOKredis&gt; QUIT # 退出再连接，让新密码对客户端生效[huangz@mypad]$ redisredis&gt; PING # 未验证密码，操作被拒绝(error) ERR operation not permittedredis&gt; AUTH wrong_password_testing # 尝试输入错误的密码(error) ERR invalid passwordredis&gt; AUTH secret_password # 输入正确的密码OKredis&gt; PING # 密码验证成功，可以正常操作命令了PONG# 清空密码redis&gt; CONFIG SET requirepass "" # 通过将密码设为空字符来清空密码OKredis&gt; QUIT$ redis # 重新进入客户端redis&gt; PING # 执行命令不再需要密码，清空密码操作成功PONG QUITQUIT可用版本： &gt;= 1.0.0 时间复杂度： O(1) 请求服务器关闭与当前客户端的连接。 一旦所有等待中的回复(如果有的话)顺利写入到客户端，连接就会被关闭。 返回值总是返回 OK (但是不会被打印显示，因为当时 Redis-cli 已经退出)。 代码示例123$ redisredis&gt; QUIT$ INFOINFO [section]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 以一种易于解释（parse）且易于阅读的格式，返回关于 Redis 服务器的各种信息和统计数值。 通过给定可选的参数 section ，可以让命令只返回某一部分的信息： server 部分记录了 Redis 服务器的信息，它包含以下域： redis_version : Redis 服务器版本 redis_git_sha1 : Git SHA1 redis_git_dirty : Git dirty flag os : Redis 服务器的宿主操作系统 arch_bits : 架构（32 或 64 位） multiplexing_api : Redis 所使用的事件处理机制 gcc_version : 编译 Redis 时所使用的 GCC 版本 process_id : 服务器进程的 PID run_id : Redis 服务器的随机标识符（用于 Sentinel 和集群） tcp_port : TCP/IP 监听端口 uptime_in_seconds : 自 Redis 服务器启动以来，经过的秒数 uptime_in_days : 自 Redis 服务器启动以来，经过的天数 lru_clock : 以分钟为单位进行自增的时钟，用于 LRU 管理 clients 部分记录了已连接客户端的信息，它包含以下域： connected_clients : 已连接客户端的数量（不包括通过从属服务器连接的客户端） client_longest_output_list : 当前连接的客户端当中，最长的输出列表 client_longest_input_buf : 当前连接的客户端当中，最大输入缓存 blocked_clients : 正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 memory 部分记录了服务器的内存信息，它包含以下域： used_memory : 由 Redis 分配器分配的内存总量，以字节（byte）为单位 used_memory_human : 以人类可读的格式返回 Redis 分配的内存总量 used_memory_rss : 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致。 used_memory_peak : Redis 的内存消耗峰值（以字节为单位） used_memory_peak_human : 以人类可读的格式返回 Redis 的内存消耗峰值 used_memory_lua : Lua 引擎所使用的内存大小（以字节为单位） mem_fragmentation_ratio : used_memory_rss 和 used_memory 之间的比率 mem_allocator : 在编译时指定的， Redis 所使用的内存分配器。可以是 libc 、 jemalloc 或者 tcmalloc 。 在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。 当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。 内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。 当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。 1Because Redis does not have control over how its allocations are mapped to memory pages, high used_memory_rss is often the result of a spike in memory usage. 当 Redis 释放内存时，分配器可能会，也可能不会，将内存返还给操作系统。 如果 Redis 释放了内存，却没有将内存返还给操作系统，那么 used_memory 的值可能和操作系统显示的 Redis 内存占用并不一致。 查看 used_memory_peak 的值可以验证这种情况是否发生。 persistence 部分记录了跟 RDB 持久化和 AOF 持久化有关的信息，它包含以下域： loading : 一个标志值，记录了服务器是否正在载入持久化文件。 rdb_changes_since_last_save : 距离最近一次成功创建持久化文件之后，经过了多少秒。 rdb_bgsave_in_progress : 一个标志值，记录了服务器是否正在创建 RDB 文件。 rdb_last_save_time : 最近一次成功创建 RDB 文件的 UNIX 时间戳。 rdb_last_bgsave_status : 一个标志值，记录了最近一次创建 RDB 文件的结果是成功还是失败。 rdb_last_bgsave_time_sec : 记录了最近一次创建 RDB 文件耗费的秒数。 rdb_current_bgsave_time_sec : 如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数。 aof_enabled : 一个标志值，记录了 AOF 是否处于打开状态。 aof_rewrite_in_progress : 一个标志值，记录了服务器是否正在创建 AOF 文件。 aof_rewrite_scheduled : 一个标志值，记录了在 RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作。 aof_last_rewrite_time_sec : 最近一次创建 AOF 文件耗费的时长。 aof_current_rewrite_time_sec : 如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数。 aof_last_bgrewrite_status : 一个标志值，记录了最近一次创建 AOF 文件的结果是成功还是失败。 如果 AOF 持久化功能处于开启状态，那么这个部分还会加上以下域： aof_current_size : AOF 文件目前的大小。 aof_base_size : 服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小。 aof_pending_rewrite : 一个标志值，记录了是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行。 aof_buffer_length : AOF 缓冲区的大小。 aof_rewrite_buffer_length : AOF 重写缓冲区的大小。 aof_pending_bio_fsync : 后台 I/O 队列里面，等待执行的 fsync 调用数量。 aof_delayed_fsync : 被延迟的 fsync 调用数量。 stats 部分记录了一般统计信息，它包含以下域： total_connections_received : 服务器已接受的连接请求数量。 total_commands_processed : 服务器已执行的命令数量。 instantaneous_ops_per_sec : 服务器每秒钟执行的命令数量。 rejected_connections : 因为最大客户端数量限制而被拒绝的连接请求数量。 expired_keys : 因为过期而被自动删除的数据库键数量。 evicted_keys : 因为最大内存容量限制而被驱逐（evict）的键数量。 keyspace_hits : 查找数据库键成功的次数。 keyspace_misses : 查找数据库键失败的次数。 pubsub_channels : 目前被订阅的频道数量。 pubsub_patterns : 目前被订阅的模式数量。 latest_fork_usec : 最近一次 fork() 操作耗费的毫秒数。 replication : 主/从复制信息 role : 如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器。 如果当前服务器是一个从服务器的话，那么这个部分还会加上以下域： master_host : 主服务器的 IP 地址。 master_port : 主服务器的 TCP 监听端口号。 master_link_status : 复制连接当前的状态， up 表示连接正常， down 表示连接断开。 master_last_io_seconds_ago : 距离最近一次与主服务器进行通信已经过去了多少秒钟。 master_sync_in_progress : 一个标志值，记录了主服务器是否正在与这个从服务器进行同步。 如果同步操作正在进行，那么这个部分还会加上以下域： master_sync_left_bytes : 距离同步完成还缺少多少字节数据。 master_sync_last_io_seconds_ago : 距离最近一次因为 SYNC 操作而进行 I/O 已经过去了多少秒。 如果主从服务器之间的连接处于断线状态，那么这个部分还会加上以下域： master_link_down_since_seconds : 主从服务器连接断开了多少秒。 以下是一些总会出现的域： connected_slaves : 已连接的从服务器数量。 对于每个从服务器，都会添加以下一行信息： slaveXXX : ID、IP 地址、端口号、连接状态 cpu 部分记录了 CPU 的计算量统计信息，它包含以下域： used_cpu_sys : Redis 服务器耗费的系统 CPU 。 used_cpu_user : Redis 服务器耗费的用户 CPU 。 used_cpu_sys_children : 后台进程耗费的系统 CPU 。 used_cpu_user_children : 后台进程耗费的用户 CPU 。 commandstats 部分记录了各种不同类型的命令的执行统计信息，比如命令执行的次数、命令耗费的 CPU 时间、执行每个命令耗费的平均 CPU 时间等等。对于每种类型的命令，这个部分都会添加一行以下格式的信息： cmdstat_XXX:calls=XXX,usec=XXX,usecpercall=XXX cluster 部分记录了和集群有关的信息，它包含以下域： cluster_enabled : 一个标志值，记录集群功能是否已经开启。 keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库已经被删除的过期键数量等。对于每个数据库，这个部分都会添加一行以下格式的信息： dbXXX:keys=XXX,expires=XXX 除上面给出的这些值以外， section 参数的值还可以是下面这两个： all : 返回所有信息 default : 返回默认选择的信息 当不带参数直接调用 INFO 命令时，使用 default 作为默认参数。 Note不同版本的 Redis 可能对返回的一些域进行了增加或删减。 因此，一个健壮的客户端程序在对 INFO [section] 命令的输出进行分析时，应该能够跳过不认识的域，并且妥善地处理丢失不见的域。 返回值具体请参见下面的测试代码。 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889redis&gt; INFO# Serverredis_version:2.9.11redis_git_sha1:937384d0redis_git_dirty:0redis_build_id:8e9509442863f22redis_mode:standaloneos:Linux 3.13.0-35-generic x86_64arch_bits:64multiplexing_api:epollgcc_version:4.8.2process_id:4716run_id:26186aac3f2380aaee9eef21cc50aecd542d97dctcp_port:6379uptime_in_seconds:362uptime_in_days:0hz:10lru_clock:1725349config_file:# Clientsconnected_clients:1client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0# Memoryused_memory:508536used_memory_human:496.62Kused_memory_rss:7974912used_memory_peak:508536used_memory_peak_human:496.62Kused_memory_lua:33792mem_fragmentation_ratio:15.68mem_allocator:jemalloc-3.2.0# Persistenceloading:0rdb_changes_since_last_save:6rdb_bgsave_in_progress:0rdb_last_save_time:1411011131rdb_last_bgsave_status:okrdb_last_bgsave_time_sec:-1rdb_current_bgsave_time_sec:-1aof_enabled:0aof_rewrite_in_progress:0aof_rewrite_scheduled:0aof_last_rewrite_time_sec:-1aof_current_rewrite_time_sec:-1aof_last_bgrewrite_status:okaof_last_write_status:ok# Statstotal_connections_received:2total_commands_processed:4instantaneous_ops_per_sec:0rejected_connections:0sync_full:0sync_partial_ok:0sync_partial_err:0expired_keys:0evicted_keys:0keyspace_hits:0keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:0migrate_cached_sockets:0# Replicationrole:masterconnected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0# CPUused_cpu_sys:0.21used_cpu_user:0.17used_cpu_sys_children:0.00used_cpu_user_children:0.00# Clustercluster_enabled:0# Keyspacedb0:keys=2,expires=0,avg_ttl=0 SHUTDOWNSHUTDOWN [SAVE|NOSAVE]可用版本： &gt;= 1.0.0 时间复杂度： O(N)，其中 N 为关机时需要保存的数据库键数量。 SHUTDOWN 命令执行以下操作： 停止所有客户端 如果有至少一个保存点在等待，执行 SAVE 命令 如果 AOF 选项被打开，更新 AOF 文件 关闭 redis 服务器(server) 如果持久化被打开的话， SHUTDOWN 命令会保证服务器正常关闭而不丢失任何数据。 另一方面，假如只是单纯地执行 SAVE 命令，然后再执行 QUIT 命令，则没有这一保证 —— 因为在执行 SAVE 之后、执行 QUIT 之前的这段时间中间，其他客户端可能正在和服务器进行通讯，这时如果执行 QUIT 就会造成数据丢失。 SAVE 和 NOSAVE 修饰符通过使用可选的修饰符，可以修改 SHUTDOWN 命令的表现。比如说： 执行 SHUTDOWN SAVE 会强制让数据库执行保存操作，即使没有设定(configure)保存点 执行 SHUTDOWN NOSAVE 会阻止数据库执行保存操作，即使已经设定有一个或多个保存点(你可以将这一用法看作是强制停止服务器的一个假想的 ABORT 命令)返回值执行失败时返回错误。 执行成功时不返回任何信息，服务器和客户端的连接断开，客户端自动退出。 代码示例123456789redis&gt; PINGPONGredis&gt; SHUTDOWN$$ redisCould not connect to Redis at: Connection refusednot connected&gt; TIMETIME可用版本： &gt;= 2.6.0 时间复杂度： O(1) 返回当前服务器时间。 返回值一个包含两个字符串的列表： 第一个字符串是当前时间(以 UNIX 时间戳格式表示)，而第二个字符串是当前这一秒钟已经逝去的微秒数。 代码示例1234567redis&gt; TIME1) "1332395997"2) "952581"redis&gt; TIME1) "1332395997"2) "953148" CLIENTCLIENT GETNAME可用版本： &gt;= 2.6.9 时间复杂度： O(1) 返回 CLIENT SETNAME 命令为连接设置的名字。 因为新创建的连接默认是没有名字的， 对于没有名字的连接， CLIENT GETNAME 返回空白回复。 返回值如果连接没有设置名字，那么返回空白回复； 如果有设置名字，那么返回名字。 代码示例1234567891011# 新连接默认没有名字redis 127.0.0.1:6379&gt; CLIENT GETNAME(nil)# 设置名字redis 127.0.0.1:6379&gt; CLIENT SETNAME hello-world-connectionOK# 返回名字redis 127.0.0.1:6379&gt; CLIENT GETNAME"hello-world-connection" CLIENT KILL ip:port可用版本： &gt;= 2.4.0 时间复杂度： O(N) ， N 为已连接的客户端数量。 关闭地址为 ip:port 的客户端。 ip:port 应该和 CLIENT LIST 命令输出的其中一行匹配。 因为 Redis 使用单线程设计，所以当 Redis 正在执行命令的时候，不会有客户端被断开连接。 如果要被断开连接的客户端正在执行命令，那么当这个命令执行之后，在发送下一个命令的时候，它就会收到一个网络错误，告知它自身的连接已被关闭。 返回值当指定的客户端存在，且被成功关闭时，返回 OK 。 代码示例123456789101112# 列出所有已连接客户端redis 127.0.0.1:6379&gt; CLIENT LISTaddr=127.0.0.1:43501 fd=5 age=10 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client# 杀死当前客户端的连接redis 127.0.0.1:6379&gt; CLIENT KILL 127.0.0.1:43501OK# 之前的连接已经被关闭，CLI 客户端又重新建立了连接# 之前的端口是 43501 ，现在是 43504redis 127.0.0.1:6379&gt; CLIENT LISTaddr=127.0.0.1:43504 fd=5 age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client CLIENT LIST可用版本： &gt;= 2.4.0 时间复杂度： O(N) ， N 为连接到服务器的客户端数量。 以人类可读的格式，返回所有连接到服务器的客户端信息和统计数据。1234redis&gt; CLIENT LISTaddr=127.0.0.1:43143 fd=6 age=183 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=clientaddr=127.0.0.1:43163 fd=5 age=35 idle=15 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=pingaddr=127.0.0.1:43167 fd=7 age=24 idle=6 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=get 返回值命令返回多行字符串，这些字符串按以下形式被格式化： 每个已连接客户端对应一行（以 LF 分割） 每行字符串由一系列 属性=值 形式的域组成，每个域之间以空格分开 以下是域的含义： addr ： 客户端的地址和端口 fd ： 套接字所使用的文件描述符 age ： 以秒计算的已连接时长 idle ： 以秒计算的空闲时长 flags ： 客户端 flag （见下文） db ： 该客户端正在使用的数据库 ID sub ： 已订阅频道的数量 psub ： 已订阅模式的数量 multi ： 在事务中被执行的命令数量 qbuf ： 查询缓冲区的长度（字节为单位， 0 表示没有分配查询缓冲区） qbuf-free ： 查询缓冲区剩余空间的长度（字节为单位， 0 表示没有剩余空间） obl ： 输出缓冲区的长度（字节为单位， 0 表示没有分配输出缓冲区） oll ： 输出列表包含的对象数量（当输出缓冲区没有剩余空间时，命令回复会以字符串对象的形式被入队到这个队列里） omem ： 输出缓冲区和输出列表占用的内存总量 events ： 文件描述符事件（见下文） cmd ： 最近一次执行的命令 客户端 flag 可以由以下部分组成： O ： 客户端是 MONITOR 模式下的附属节点（slave） S ： 客户端是一般模式下（normal）的附属节点 M ： 客户端是主节点（master） x ： 客户端正在执行事务 b ： 客户端正在等待阻塞事件 i ： 客户端正在等待 VM I/O 操作（已废弃） d ： 一个受监视（watched）的键已被修改， EXEC 命令将失败 c : 在将回复完整地写出之后，关闭链接 u : 客户端未被阻塞（unblocked） A : 尽可能快地关闭连接 N : 未设置任何 flag 文件描述符事件可以是： r : 客户端套接字（在事件 loop 中）是可读的（readable） w : 客户端套接字（在事件 loop 中）是可写的（writeable）Note 为了 debug 的需要，经常会对域进行添加和删除，一个安全的 Redis 客户端应该可以对 CLIENT LIST 的输出进行相应的处理（parse），比如忽略不存在的域，跳过未知域，诸如此类。 CLIENT SETNAME connection-name可用版本： &gt;= 2.6.9 时间复杂度： O(1) 为当前连接分配一个名字。 这个名字会显示在 CLIENT LIST 命令的结果中， 用于识别当前正在与服务器进行连接的客户端。 举个例子， 在使用 Redis 构建队列（queue）时， 可以根据连接负责的任务（role）， 为信息生产者（producer）和信息消费者（consumer）分别设置不同的名字。 名字使用 Redis 的字符串类型来保存， 最大可以占用 512 MB 。 另外， 为了避免和 CLIENT LIST 命令的输出格式发生冲突， 名字里不允许使用空格。 要移除一个连接的名字， 可以将连接的名字设为空字符串 “” 。 使用 CLIENT GETNAME 命令可以取出连接的名字。 新创建的连接默认是没有名字的。 Tip在 Redis 应用程序发生连接泄漏时，为连接设置名字是一种很好的 debug 手段。 返回值设置成功时返回 OK 。 代码示例1234567891011121314151617181920212223242526272829# 新连接默认没有名字redis 127.0.0.1:6379&gt; CLIENT GETNAME(nil)# 设置名字redis 127.0.0.1:6379&gt; CLIENT SETNAME hello-world-connectionOK# 返回名字redis 127.0.0.1:6379&gt; CLIENT GETNAME"hello-world-connection"# 在客户端列表中查看redis 127.0.0.1:6379&gt; CLIENT LISTaddr=127.0.0.1:36851fd=5name=hello-world-connection # &lt;- 名字age=51...# 清除名字redis 127.0.0.1:6379&gt; CLIENT SETNAME # 只用空格是不行的！(error) ERR Syntax error, try CLIENT (LIST | KILL ip:port)redis 127.0.0.1:6379&gt; CLIENT SETNAME "" # 必须双引号显示包围OKredis 127.0.0.1:6379&gt; CLIENT GETNAME # 清除完毕(nil) 配置选项CONFIG SETCONFIG SET parameter value可用版本： &gt;= 2.0.0 时间复杂度：O(1) CONFIG SET 命令可以动态地调整 Redis 服务器的配置(configuration)而无须重启。 你可以使用它修改配置参数，或者改变 Redis 的持久化(Persistence)方式。 CONFIG SET 可以修改的配置参数可以使用命令 CONFIG GET * 来列出，所有被 CONFIG SET 修改的配置参数都会立即生效。 关于 CONFIG SET 命令的更多消息，请参见命令 CONFIG GET 的说明。 关于如何使用 CONFIG SET 命令修改 Redis 持久化方式，请参见 Redis Persistence 。 返回值当设置成功时返回 OK ，否则返回一个错误。 代码示例12345678910redis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "1024"redis&gt; CONFIG SET slowlog-max-len 10086OKredis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "10086" CONFIG GETCONFIG GET parameter可用版本： &gt;= 2.0.0 时间复杂度： O(N)，其中 N 为命令返回的配置选项数量。 CONFIG GET 命令用于取得运行中的 Redis 服务器的配置参数(configuration parameters)，在 Redis 2.4 版本中， 有部分参数没有办法用 CONFIG GET 访问，但是在最新的 Redis 2.6 版本中，所有配置参数都已经可以用 CONFIG GET 访问了。 CONFIG GET 接受单个参数 parameter 作为搜索关键字，查找所有匹配的配置参数，其中参数和值以“键-值对”(key-value pairs)的方式排列。 比如执行 CONFIG GET s* 命令，服务器就会返回所有以 s 开头的配置参数及参数的值： 1234567891011redis&gt; CONFIG GET s*1) "save" # 参数名：save2) "900 1 300 10 60 10000" # save 参数的值3) "slave-serve-stale-data" # 参数名： slave-serve-stale-data4) "yes" # slave-serve-stale-data 参数的值5) "set-max-intset-entries" # ...6) "512"7) "slowlog-log-slower-than"8) "1000"9) "slowlog-max-len"10) "1000" 如果你只是寻找特定的某个参数的话，你当然也可以直接指定参数的名字： 123redis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "1000" 使用命令 CONFIG GET * ，可以列出 CONFIG GET 命令支持的所有参数：12345678910111213141516171819202122redis&gt; CONFIG GET *1) "dir"2) "/var/lib/redis"3) "dbfilename"4) "dump.rdb"5) "requirepass"6) (nil)7) "masterauth"8) (nil)9) "maxmemory"10) "0"11) "maxmemory-policy"12) "volatile-lru"13) "maxmemory-samples"14) "3"15) "timeout"16) "0"17) "appendonly"18) "no"# ...49) "loglevel"50) "verbose" 所有被 CONFIG SET 所支持的配置参数都可以在配置文件 redis.conf 中找到，不过 CONFIG GET 和 CONFIG SET 使用的格式和 redis.conf 文件所使用的格式有以下两点不同： 10kb 、 2gb 这些在配置文件中所使用的储存单位缩写，不可以用在 CONFIG 命令中， CONFIG SET 的值只能通过数字值显式地设定。 像 CONFIG SET xxx 1k 这样的命令是错误的，正确的格式是 CONFIG SET xxx 1000 。save 选项在 redis.conf 中是用多行文字储存的，但在 CONFIG GET 命令中，它只打印一行文字。 以下是 save 选项在 redis.conf 文件中的表示：123save 900 1save 300 10save 60 10000 但是 CONFIG GET 命令的输出只有一行：123redis&gt; CONFIG GET save1) "save"2) "900 1 300 10 60 10000" 上面 save 参数的三个值表示：在 900 秒内最少有 1 个 key 被改动，或者 300 秒内最少有 10 个 key 被改动，又或者 60 秒内最少有 1000 个 key 被改动，以上三个条件随便满足一个，就触发一次保存操作。 返回值给定配置参数的值。 CONFIG RESETSTATCONFIG RESETSTAT可用版本： &gt;= 2.0.0 时间复杂度： O(1) 重置 INFO 命令中的某些统计数据，包括： Keyspace hits (键空间命中次数) Keyspace misses (键空间不命中次数) Number of commands processed (执行命令的次数) Number of connections received (连接服务器的次数) Number of expired keys (过期key的数量) Number of rejected connections (被拒绝的连接数量) Latest fork(2) time(最后执行 fork(2) 的时间) The aof_delayed_fsync counter(aof_delayed_fsync 计数器的值)返回值总是返回 OK 。 代码示例重置前1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465redis 127.0.0.1:6379&gt; INFO# Serverredis_version:2.5.3redis_git_sha1:d0407c2dredis_git_dirty:0arch_bits:32multiplexing_api:epollgcc_version:4.6.3process_id:11095run_id:ef1f6b6c7392e52d6001eaf777acbe547d1192e2tcp_port:6379uptime_in_seconds:6uptime_in_days:0lru_clock:1205426# Clientsconnected_clients:1client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0# Memoryused_memory:331076used_memory_human:323.32Kused_memory_rss:1568768used_memory_peak:293424used_memory_peak_human:286.55Kused_memory_lua:16384mem_fragmentation_ratio:4.74mem_allocator:jemalloc-2.2.5# Persistenceloading:0aof_enabled:0changes_since_last_save:0bgsave_in_progress:0last_save_time:1333260015last_bgsave_status:okbgrewriteaof_in_progress:0# Statstotal_connections_received:1total_commands_processed:0instantaneous_ops_per_sec:0rejected_connections:0expired_keys:0evicted_keys:0keyspace_hits:0keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:0# Replicationrole:masterconnected_slaves:0# CPUused_cpu_sys:0.01used_cpu_user:0.00used_cpu_sys_children:0.00used_cpu_user_children:0.00# Keyspacedb0:keys=20,expires=0 重置12redis 127.0.0.1:6379&gt; CONFIG RESETSTATOK 重置后1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465redis 127.0.0.1:6379&gt; INFO# Serverredis_version:2.5.3redis_git_sha1:d0407c2dredis_git_dirty:0arch_bits:32multiplexing_api:epollgcc_version:4.6.3process_id:11095run_id:ef1f6b6c7392e52d6001eaf777acbe547d1192e2tcp_port:6379uptime_in_seconds:134uptime_in_days:0lru_clock:1205438# Clientsconnected_clients:1client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0# Memoryused_memory:331076used_memory_human:323.32Kused_memory_rss:1568768used_memory_peak:330280used_memory_peak_human:322.54Kused_memory_lua:16384mem_fragmentation_ratio:4.74mem_allocator:jemalloc-2.2.5# Persistenceloading:0aof_enabled:0changes_since_last_save:0bgsave_in_progress:0last_save_time:1333260015last_bgsave_status:okbgrewriteaof_in_progress:0# Statstotal_connections_received:0total_commands_processed:1instantaneous_ops_per_sec:0rejected_connections:0expired_keys:0evicted_keys:0keyspace_hits:0keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:0# Replicationrole:masterconnected_slaves:0# CPUused_cpu_sys:0.05used_cpu_user:0.02used_cpu_sys_children:0.00used_cpu_user_children:0.00# Keyspacedb0:keys=20,expires=0 CONFIG REWRITECONFIG REWRITE可用版本： &gt;= 2.8.0 时间复杂度：O(N)，其中 N 为被重写的配置选项数量。 CONFIG REWRITE 命令对启动 Redis 服务器时所指定的 redis.conf 文件进行改写： 因为 CONFIG_SET 命令可以对服务器的当前配置进行修改， 而修改后的配置可能和 redis.conf 文件中所描述的配置不一样， CONFIG REWRITE 的作用就是通过尽可能少的修改， 将服务器当前所使用的配置记录到 redis.conf 文件中。 重写会以非常保守的方式进行： 原有 redis.conf 文件的整体结构和注释会被尽可能地保留。 如果一个选项已经存在于原有 redis.conf 文件中 ， 那么对该选项的重写会在选项原本所在的位置（行号）上进行。 如果一个选项不存在于原有 redis.conf 文件中， 并且该选项被设置为默认值， 那么重写程序不会将这个选项添加到重写后的 redis.conf 文件中。 如果一个选项不存在于原有 redis.conf 文件中， 并且该选项被设置为非默认值， 那么这个选项将被添加到重写后的 redis.conf 文件的末尾。 未使用的行会被留白。 比如说， 如果你在原有 redis.conf 文件上设置了数个关于 save 选项的参数， 但现在你将这些 save 参数的一个或全部都关闭了， 那么这些不再使用的参数原本所在的行就会变成空白的。 即使启动服务器时所指定的 redis.conf 文件已经不再存在， CONFIG REWRITE 命令也可以重新构建并生成出一个新的 redis.conf 文件。 另一方面， 如果启动服务器时没有载入 redis.conf 文件， 那么执行 CONFIG REWRITE 命令将引发一个错误。 原子性重写对 redis.conf 文件的重写是原子性的， 并且是一致的： 如果重写出错或重写期间服务器崩溃， 那么重写失败， 原有 redis.conf 文件不会被修改。 如果重写成功， 那么 redis.conf 文件为重写后的新文件。 返回值一个状态值：如果配置重写成功则返回 OK ，失败则返回一个错误。 代码示例以下是执行 CONFIG REWRITE 前， 被载入到 Redis 服务器的 redis.conf 文件中关于 appendonly 选项的设置：1234567891011121314151617181920212223# ... 其他选项appendonly no# ... 其他选项在执行以下命令之后：redis&gt; CONFIG GET appendonly # appendonly 处于关闭状态1) "appendonly"2) "no"redis&gt; CONFIG SET appendonly yes # 打开 appendonlyOKredis&gt; CONFIG GET appendonly1) "appendonly"2) "yes"redis&gt; CONFIG REWRITE # 将 appendonly 的修改写入到 redis.conf 中OK重写后的 redis.conf 文件中的 appendonly 选项将被改写：# ... 其他选项appendonly yes# ... 其他选项 调试PINGPING可用版本： &gt;= 1.0.0 时间复杂度： O(1) 使用客户端向 Redis 服务器发送一个 PING ，如果服务器运作正常的话，会返回一个 PONG 。 通常用于测试与服务器的连接是否仍然生效，或者用于测量延迟值。 返回值如果连接正常就返回一个 PONG ，否则返回一个连接错误。 代码示例1234567# 客户端和服务器连接正常redis&gt; PINGPONG# 客户端和服务器连接不正常(网络不正常或服务器未能正常运行)redis 127.0.0.1:6379&gt; PINGCould not connect to Redis at 127.0.0.1:6379: Connection refused ECHOECHO message可用版本： &gt;= 1.0.0 时间复杂度： O(1) 打印一个特定的信息 message ，测试时使用。 返回值message 自身。 代码示例12345redis&gt; ECHO "Hello Moto""Hello Moto"redis&gt; ECHO "Goodbye Moto""Goodbye Moto" OBJECTOBJECT subcommand [arguments [arguments]]可用版本： &gt;= 2.2.3 时间复杂度： O(1) OBJECT 命令允许从内部察看给定 key 的 Redis 对象， 它通常用在除错(debugging)或者了解为了节省空间而对 key 使用特殊编码的情况。 当将Redis用作缓存程序时，你也可以通过 OBJECT 命令中的信息，决定 key 的驱逐策略(eviction policies)。 OBJECT 命令有多个子命令： OBJECT REFCOUNT \&lt;key> 返回给定 key 引用所储存的值的次数。此命令主要用于除错。 OBJECT ENCODING \&lt;key> 返回给定 key 锁储存的值所使用的内部表示(representation)。 OBJECT IDLETIME \&lt;key> 返回给定 key 自储存以来的空闲时间(idle， 没有被读取也没有被写入)，以秒为单位。对象可以以多种方式编码： 字符串可以被编码为 raw (一般字符串)或 int (为了节约内存，Redis 会将字符串表示的 64 位有符号整数编码为整数来进行储存）。 列表可以被编码为 ziplist 或 linkedlist 。 ziplist 是为节约大小较小的列表空间而作的特殊表示。 集合可以被编码为 intset 或者 hashtable 。 intset 是只储存数字的小集合的特殊表示。 哈希表可以编码为 zipmap 或者 hashtable 。 zipmap 是小哈希表的特殊表示。 有序集合可以被编码为 ziplist 或者 skiplist 格式。 ziplist 用于表示小的有序集合，而 skiplist 则用于表示任何大小的有序集合。 假如你做了什么让 Redis 没办法再使用节省空间的编码时(比如将一个只有 1 个元素的集合扩展为一个有 100 万个元素的集合)，特殊编码类型(specially encoded types)会自动转换成通用类型(general type)。 返回值REFCOUNT 和 IDLETIME 返回数字。 ENCODING 返回相应的编码类型。 代码示例1234567891011121314151617181920212223242526272829redis&gt; SET game "COD" # 设置一个字符串OKredis&gt; OBJECT REFCOUNT game # 只有一个引用(integer) 1redis&gt; OBJECT IDLETIME game # 等待一阵。。。然后查看空闲时间(integer) 90redis&gt; GET game # 提取game， 让它处于活跃(active)状态"COD"redis&gt; OBJECT IDLETIME game # 不再处于空闲状态(integer) 0redis&gt; OBJECT ENCODING game # 字符串的编码方式"raw"redis&gt; SET big-number 23102930128301091820391092019203810281029831092 # 非常长的数字会被编码为字符串OKredis&gt; OBJECT ENCODING big-number"raw"redis&gt; SET small-number 12345 # 而短的数字则会被编码为整数OKredis&gt; OBJECT ENCODING small-number"int" SLOWLOGSLOWLOG subcommand [argument]可用版本： &gt;= 2.2.12 时间复杂度： O(1) 什么是 SLOWLOGSlow log 是 Redis 用来记录查询执行时间的日志系统。 查询执行时间指的是不包括像客户端响应(talking)、发送回复等 IO 操作，而单单是执行一个查询命令所耗费的时间。 另外，slow log 保存在内存里面，读写速度非常快，因此你可以放心地使用它，不必担心因为开启 slow log 而损害 Redis 的速度。 设置 SLOWLOGSlow log 的行为由两个配置参数(configuration parameter)指定，可以通过改写 redis.conf 文件或者用 CONFIG GET 和 CONFIG SET 命令对它们动态地进行修改。 第一个选项是 slowlog-log-slower-than ，它决定要对执行时间大于多少微秒(microsecond，1秒 = 1,000,000 微秒)的查询进行记录。 比如执行以下命令将让 slow log 记录所有查询时间大于等于 100 微秒的查询：1CONFIG SET slowlog-log-slower-than 100 而以下命令记录所有查询时间大于 1000 微秒的查询：1CONFIG SET slowlog-log-slower-than 1000 另一个选项是 slowlog-max-len ，它决定 slow log 最多能保存多少条日志， slow log 本身是一个 FIFO 队列，当队列大小超过 slowlog-max-len 时，最旧的一条日志将被删除，而最新的一条日志加入到 slow log ，以此类推。 以下命令让 slow log 最多保存 1000 条日志：1CONFIG SET slowlog-max-len 1000 使用 CONFIG GET 命令可以查询两个选项的当前值：1234567redis&gt; CONFIG GET slowlog-log-slower-than1) "slowlog-log-slower-than"2) "1000"redis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "1000" 查看 slow log要查看 slow log ，可以使用 SLOWLOG GET 或者 SLOWLOG GET number 命令，前者打印所有 slow log ，最大长度取决于 slowlog-max-len 选项的值，而 SLOWLOG GET number 则只打印指定数量的日志。 最新的日志会最先被打印：12345678910111213141516171819202122232425# 为测试需要，将 slowlog-log-slower-than 设成了 10 微秒redis&gt; SLOWLOG GET1) 1) (integer) 12 # 唯一性(unique)的日志标识符 2) (integer) 1324097834 # 被记录命令的执行时间点，以 UNIX 时间戳格式表示 3) (integer) 16 # 查询执行时间，以微秒为单位 4) 1) "CONFIG" # 执行的命令，以数组的形式排列 2) "GET" # 这里完整的命令是 CONFIG GET slowlog-log-slower-than 3) "slowlog-log-slower-than"2) 1) (integer) 11 2) (integer) 1324097825 3) (integer) 42 4) 1) "CONFIG" 2) "GET" 3) "*"3) 1) (integer) 10 2) (integer) 1324097820 3) (integer) 11 4) 1) "CONFIG" 2) "GET" 3) "slowlog-log-slower-than"# ... 日志的唯一 id 只有在 Redis 服务器重启的时候才会重置，这样可以避免对日志的重复处理(比如你可能会想在每次发现新的慢查询时发邮件通知你)。 查看当前日志的数量使用命令 SLOWLOG LEN 可以查看当前日志的数量。 请注意这个值和 slower-max-len 的区别，它们一个是当前日志的数量，一个是允许记录的最大日志的数量。12redis&gt; SLOWLOG LEN(integer) 14 清空日志使用命令 SLOWLOG RESET 可以清空 slow log 。12345678redis&gt; SLOWLOG LEN(integer) 14redis&gt; SLOWLOG RESETOKredis&gt; SLOWLOG LEN(integer) 0 返回值取决于不同命令，返回不同的值。 MONITORMONITOR可用版本： &gt;= 1.0.0 时间复杂度： O(N) 实时打印出 Redis 服务器接收到的命令，调试用。 返回值总是返回 OK 。 代码示例12345678910111213127.0.0.1:6379&gt; MONITOROK# 以第一个打印值为例# 1378822099.421623 是时间戳# [0 127.0.0.1:56604] 中的 0 是数据库号码， 127... 是 IP 地址和端口# "PING" 是被执行的命令1378822099.421623 [0 127.0.0.1:56604] "PING"1378822105.089572 [0 127.0.0.1:56604] "SET" "msg" "hello world"1378822109.036925 [0 127.0.0.1:56604] "SET" "number" "123"1378822140.649496 [0 127.0.0.1:56604] "SADD" "fruits" "Apple" "Banana" "Cherry"1378822154.117160 [0 127.0.0.1:56604] "EXPIRE" "msg" "10086"1378822257.329412 [0 127.0.0.1:56604] "KEYS" "*"1378822258.690131 [0 127.0.0.1:56604] "DBSIZE" DEBUG OBJECTDEBUG OBJECT key可用版本： &gt;= 1.0.0 时间复杂度： O(1) DEBUG OBJECT 是一个调试命令，它不应被客户端所使用。 查看 OBJECT 命令获取更多信息。 返回值当 key 存在时，返回有关信息。 当 key 不存在时，返回一个错误。 代码示例12345redis&gt; DEBUG OBJECT my_pcValue at:0xb6838d20 refcount:1 encoding:raw serializedlength:9 lru:283790 lru_seconds_idle:150redis&gt; DEBUG OBJECT your_mac(error) ERR no such key DEBUG SEGFAULTDEBUG SEGFAULT可用版本： &gt;= 1.0.0 时间复杂度： O(1) 执行一个不合法的内存访问从而让 Redis 崩溃，仅在开发时用于 BUG 模拟。 返回值无 代码示例1234redis&gt; DEBUG SEGFAULTCould not connect to Redis at: Connection refusednot connected&gt; 内部命令MIGRATEMIGRATE host port key destination-db timeout [COPY] [REPLACE]可用版本： &gt;= 2.6.0 时间复杂度： 这个命令在源实例上实际执行 DUMP 命令和 DEL 命令，在目标实例执行 RESTORE 命令，查看以上命令的文档可以看到详细的复杂度说明。 key 数据在两个实例之间传输的复杂度为 O(N) 。 将 key 原子性地从当前实例传送到目标实例的指定数据库上，一旦传送成功， key 保证会出现在目标实例上，而当前实例上的 key 会被删除。 这个命令是一个原子操作，它在执行的时候会阻塞进行迁移的两个实例，直到以下任意结果发生：迁移成功，迁移失败，等待超时。 命令的内部实现是这样的：它在当前实例对给定 key 执行 DUMP 命令 ，将它序列化，然后传送到目标实例，目标实例再使用 RESTORE 对数据进行反序列化，并将反序列化所得的数据添加到数据库中；当前实例就像目标实例的客户端那样，只要看到 RESTORE 命令返回 OK ，它就会调用 DEL 删除自己数据库上的 key 。 timeout 参数以毫秒为格式，指定当前实例和目标实例进行沟通的最大间隔时间。这说明操作并不一定要在 timeout 毫秒内完成，只是说数据传送的时间不能超过这个 timeout 数。 MIGRATE 命令需要在给定的时间规定内完成 IO 操作。如果在传送数据时发生 IO 错误，或者达到了超时时间，那么命令会停止执行，并返回一个特殊的错误： IOERR 。 当 IOERR 出现时，有以下两种可能： key 可能存在于两个实例 key 可能只存在于当前实例 唯一不可能发生的情况就是丢失 key ，因此，如果一个客户端执行 MIGRATE 命令，并且不幸遇上 IOERR 错误，那么这个客户端唯一要做的就是检查自己数据库上的 key 是否已经被正确地删除。 如果有其他错误发生，那么 MIGRATE 保证 key 只会出现在当前实例中。（当然，目标实例的给定数据库上可能有和 key 同名的键，不过这和 MIGRATE 命令没有关系）。 可选项COPY ：不移除源实例上的 key 。 REPLACE ：替换目标实例上已存在的 key 。 返回值迁移成功时返回 OK ，否则返回相应的错误。 代码示例先启动两个 Redis 实例，一个使用默认的 6379 端口，一个使用 7777 端口。123456789$ ./redis-server &amp;[1] 3557...$ ./redis-server --port 7777 &amp;[2] 3560... 然后用客户端连上 6379 端口的实例，设置一个键，然后将它迁移到 7777 端口的实例上：12345678910111213$ ./redis-cliredis 127.0.0.1:6379&gt; flushdbOKredis 127.0.0.1:6379&gt; SET greeting "Hello from 6379 instance"OKredis 127.0.0.1:6379&gt; MIGRATE 127.0.0.1 7777 greeting 0 1000OKredis 127.0.0.1:6379&gt; EXISTS greeting # 迁移成功后 key 被删除(integer) 0 使用另一个客户端，查看 7777 端口上的实例：1234$ ./redis-cli -p 7777redis 127.0.0.1:7777&gt; GET greeting"Hello from 6379 instance" DUMPDUMP key可用版本： &gt;= 2.6.0 时间复杂度：查找给定键的复杂度为 O(1) ，对键进行序列化的复杂度为 O(N*M) ，其中 N 是构成 key 的 Redis 对象的数量，而 M 则是这些对象的平均大小。如果序列化的对象是比较小的字符串，那么复杂度为 O(1) 。 序列化给定 key ，并返回被序列化的值，使用 RESTORE 命令可以将这个值反序列化为 Redis 键。 序列化生成的值有以下几个特点： 它带有 64 位的校验和，用于检测错误， RESTORE 在进行反序列化之前会先检查校验和。 值的编码格式和 RDB 文件保持一致。 RDB 版本会被编码在序列化值当中，如果因为 Redis 的版本不同造成 RDB 格式不兼容，那么 Redis 会拒绝对这个值进行反序列化操作。 序列化的值不包括任何生存时间信息。 返回值如果 key 不存在，那么返回 nil 。 否则，返回序列化之后的值。 代码示例12345678redis&gt; SET greeting "hello, dumping world!"OKredis&gt; DUMP greeting"\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"redis&gt; DUMP not-exists-key(nil) RESTORERESTORE key ttl serialized-value [REPLACE]可用版本： &gt;= 2.6.0 时间复杂度： 查找给定键的复杂度为 O(1) ，对键进行反序列化的复杂度为 O(N*M) ，其中 N 是构成 key 的 Redis 对象的数量，而 M 则是这些对象的平均大小。 有序集合(sorted set)的反序列化复杂度为 O(N*M*log(N)) ，因为有序集合每次插入的复杂度为 O(log(N)) 。 如果反序列化的对象是比较小的字符串，那么复杂度为 O(1) 。 反序列化给定的序列化值，并将它和给定的 key 关联。 参数 ttl 以毫秒为单位为 key 设置生存时间；如果 ttl 为 0 ，那么不设置生存时间。 RESTORE 在执行反序列化之前会先对序列化值的 RDB 版本和数据校验和进行检查，如果 RDB 版本不相同或者数据不完整的话，那么 RESTORE 会拒绝进行反序列化，并返回一个错误。 如果键 key 已经存在， 并且给定了 REPLACE 选项， 那么使用反序列化得出的值来代替键 key 原有的值； 相反地， 如果键 key 已经存在， 但是没有给定 REPLACE 选项， 那么命令返回一个错误。 更多信息可以参考 DUMP 命令。 返回值如果反序列化成功那么返回 OK ，否则返回一个错误。 代码示例12345678910111213141516171819202122232425# 创建一个键，作为 DUMP 命令的输入redis&gt; SET greeting "hello, dumping world!"OKredis&gt; DUMP greeting"\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"# 将序列化数据 RESTORE 到另一个键上面redis&gt; RESTORE greeting-again 0 "\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"OKredis&gt; GET greeting-again"hello, dumping world!"# 在没有给定 REPLACE 选项的情况下，再次尝试反序列化到同一个键，失败redis&gt; RESTORE greeting-again 0 "\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"(error) ERR Target key name is busy.# 给定 REPLACE 选项，对同一个键进行反序列化成功redis&gt; RESTORE greeting-again 0 "\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde" REPLACEOK# 尝试使用无效的值进行反序列化，出错redis&gt; RESTORE fake-message 0 "hello moto moto blah blah"(error) ERR DUMP payload version or checksum are wrong SYNCSYNC可用版本： &gt;= 1.0.0 时间复杂度： O(N) 用于复制功能(replication)的内部命令。 更多信息请参考 Redis 官网的 Replication 章节 。 返回值序列化数据。 代码示例123redis&gt; SYNC"REDIS0002\xfe\x00\x00\auser_id\xc0\x03\x00\anumbers\xc2\xf3\xe0\x01\x00\x00\tdb_number\xc0\x00\x00\x04name\x06huangz\x00\anew_key\nhello_moto\x00\bgreeting\nhello moto\x00\x05my_pc\bthinkpad\x00\x04lock\xc0\x01\x00\nlock_times\xc0\x04\xfe\x01\t\x04info\x19\x02\x04name\b\x00zhangyue\x03age\x02\x0022\xff\t\aooredis,\x03\x04name\a\x00ooredis\aversion\x03\x001.0\x06author\x06\x00huangz\xff\x00\tdb_number\xc0\x01\x00\x05greet\x0bhello world\x02\nmy_friends\x02\x05marry\x04jack\x00\x04name\x05value\xfe\x02\x0c\x01s\x12\x12\x00\x00\x00\r\x00\x00\x00\x02\x00\x00\x01a\x03\xc0f'\xff\xff"(1.90s) PSYNCPSYNC master_run_id offset可用版本： &gt;= 2.8.0 时间复杂度： 不明确 用于复制功能(replication)的内部命令。 更多信息请参考 复制（Replication） 文档。 返回值序列化数据。 代码示例12127.0.0.1:6379&gt; PSYNC ? -1"REDIS0006\xfe\x00\x00\x02kk\x02vv\x00\x03msg\x05hello\xff\xc3\x96P\x12h\bK\xef" 转载出处Redis命令参考]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Java发送邮件]]></title>
    <url>%2Fblog%2F%E4%BD%BF%E7%94%A8Java%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6.html</url>
    <content type="text"><![CDATA[前言在使用Java开发项目过程中我们通常会遇到发送邮件的需求，那么系统中是如何使用Java发送邮件的呢？今天我们来了解下使用Java发送邮件的两种方式。 它们分别是使用 apache-commons-email 包来发送邮件、使用javax.mail 包来发送邮件。 我们分别对它们进行了解。 正文JavaMail我们来看下使用JavaMail发送一封邮件的Java代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * @author zwt * @detail 使用JavaxMail 发送邮件 * @date 2019/1/23 * @since 1.0 */public class SendMail&#123; //SMTP主机设置 private static final String MAIL_HOST_NAME = "smtp.qq.com"; //发送邮件的邮箱用户名 private static final String MAIL_SYSTEM_USER = "test@qq.com"; //发送邮件的邮箱密码 private static final String MAIL_SYSTEM_PASSWORD = "SMTP授权码"; /** * 发送邮件 * @param subject * @param receivers * @param copys * @param msg * @param attachName * @return */ public static boolean sendMail(String subject, List&lt;String&gt; receivers, List&lt;String&gt; copys, String msg, String attachName)&#123; if (CollectionUtils.isEmpty(receivers) || (subject == null) || (msg == null)) &#123; throw new SendMailException("Email receivers or subject or content must be not null"); &#125; boolean isSuccess = false; try &#123; // 获取系统属性 Properties properties = System.getProperties(); // 设置邮件服务器 properties.setProperty("mail.smtp.host", MAIL_HOST_NAME); //传输协议SMTP properties.setProperty("mail.transport.protocol", "smtp"); //开启SMTP认证 properties.setProperty("mail.smtp.auth", "true"); //SMTP端口号 properties.setProperty("mail.smtp.port","465"); //开启ssl properties.setProperty("mail.smtp.ssl.enable","true"); //用户名密码认证 properties.setProperty("mail.user",MAIL_SYSTEM_USER ); properties.setProperty("mail.password", MAIL_SYSTEM_PASSWORD); // 获取默认的 Session 对象。 Session session = Session.getDefaultInstance(properties); // 创建默认的 MimeMessage 对象。 MimeMessage message = new MimeMessage(session); // 发信人 message.setFrom(new InternetAddress(MAIL_SYSTEM_USER)); // 收信人 for(String receiver:receivers)&#123; message.addRecipient(Message.RecipientType.TO, new InternetAddress(receiver)); &#125; //抄送 if(!CollectionUtils.isEmpty(copys))&#123; for(String copy:copys)&#123; message.addRecipient(Message.RecipientType.CC, new InternetAddress(copy)); &#125; &#125; //邮件主题 message.setSubject(subject); //有附件创建附件发送 if(!StringUtils.isEmpty(attachName))&#123; // 创建消息部分 BodyPart messageBodyPart = new MimeBodyPart(); // 消息 messageBodyPart.setText(msg); // 创建多重消息 Multipart multipart = new MimeMultipart(); // 设置文本消息部分 multipart.addBodyPart(messageBodyPart); // 附件部分 messageBodyPart = new MimeBodyPart(); DataSource source = new FileDataSource(attachName); messageBodyPart.setDataHandler(new DataHandler(source)); messageBodyPart.setFileName("附件"); multipart.addBodyPart(messageBodyPart); // 发送完整消息 message.setContent(multipart); //setContent方法可以插入html标签生成Html邮件 &#125;else&#123; message.setText(msg); &#125; // 发送消息 Transport.send(message); isSuccess = true; &#125;catch (MessagingException e)&#123; e.printStackTrace(); throw new SendMailException(e.getMessage()); &#125; return isSuccess; &#125; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("test1@qq.com"); sendMail("test",list,null,"测试一下",null); &#125;&#125; 可以看到，要发送一封邮件，Java的处理主要有以下几步： ①系统参数及属性设置，包括使用的协议（Pop3、SMTP等），是否开启ssl加密、发送邮件邮箱的用户名和密码等 ②设置邮件属性，比如主题、内容、附件、接收人、抄送人等等 ③发送邮件 我这儿使用的是QQ邮箱，要注意使用SMTP传输协议时，要开启SMTP，使用SSL，邮箱的认证密码不是邮箱密码，而是授权码，需要在QQ邮箱进行设置，如下： PS：在发送邮件时，如果想插入图片，可以设置成Html的内联图片，然后通过message.setContent添加这个Html文档。还有一些其它的功能（比如给某个人发送回执等）都可以通过设置实现，有兴趣的可以看下。 整个发送邮件的代码基本如上，还是比较易于理解的，我们不再对此做过多介绍。 apache-commons-email再来看看Apache提供的apache-commons-email这个包。 这个包就是对JavaMail进行了一层封装，我们可以看到这个工具包本质还是使用了JavaMail去发送邮件。 我们来看一个使用apache-commons-email包去发送邮件的例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class SendMail &#123; //SMTP主机设置 private static final String MAIL_HOST_NAME = "smtp.qq.com"; //发送邮件的邮箱用户名 private static final String MAIL_SYSTEM_USER = "test@qq.com"; //发送邮件的邮箱密码 private static final String MAIL_SYSTEM_PASSWORD = "SMTP授权码"; /** * 发送邮件 * @param subject 邮件主题 * @param receivers 接受者邮件地址 * @param copys 要抄送的邮件地址 * @param msg 内容 * @param attachName 附件地址 * @return * @throws Exception */ public static boolean sendMail(String subject, List&lt;String&gt; receivers,List&lt;String&gt; copys, String msg, String attachName)&#123; if (CollectionUtils.isEmpty(receivers) || (subject == null) || (msg == null)) &#123; throw new SendMailException("Email receivers or subject or content must be not null"); &#125; boolean isSuccess = false; try &#123; //SimpleEmail email = new SimpleEmail();//创建简单邮件,不可添加附件、HTML文本等 //MultiPartEmail email = new MultiPartEmail();//创建能加附件的邮件,可多个、网络附件亦可 //HtmlEmail email = new HtmlEmail();//创建能加附件内容为HTML文本的邮件、HTML直接内联图片但必须用setHtmlMsg()传邮件内容 MultiPartEmail email = new MultiPartEmail(); //设置smtp 主机 email.setHostName(MAIL_HOST_NAME); email.setSmtpPort(465); email.setSSL(true); //邮箱登录认证（保证邮件服务器POP3/SMTP服务开启） email.setAuthentication(MAIL_SYSTEM_USER,MAIL_SYSTEM_PASSWORD); //收信人列表 for(String receiver:receivers)&#123; email.addTo(receiver, receiver); &#125; //抄送人列表 if(!CollectionUtils.isEmpty(copys))&#123; for(String copy:copys)&#123; email.addCc(copy,copy); &#125; &#125; //发信人 email.setFrom(MAIL_SYSTEM_USER, MAIL_SYSTEM_USER); //邮件主题 email.setSubject(MimeUtility.encodeText(subject)); //邮件内容 email.setMsg(msg); //邮件附件 if ((attachName != null) &amp;&amp; (attachName.trim().length() &gt; 0)) &#123; EmailAttachment attachment = new EmailAttachment(); attachment.setPath(attachName); attachment.setDisposition("attachment"); email.attach(attachment); &#125; //发送邮件 email.send(); isSuccess = true; &#125; catch (EmailException e) &#123; throw new SendMailException(e.getMessage()); &#125; catch (UnsupportedEncodingException e) &#123; throw new SendMailException(e.getMessage()); &#125; return isSuccess; &#125; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("test1@qq.com"); sendMail("test",list,null,"测试一下",null); &#125;&#125; 对比两个程序，可以了解apache-commons-email这个包把JavaMail的Session给我们隐藏了，使得我们更专注于API调用，同时它分出了三种邮件类型，供我们使用。 使用apache-commons-email，要发送不同类型的邮件，可以创建不同的mail class，如下： //SimpleEmail email = new SimpleEmail();//创建简单邮件,不可添加附件、HTML文本等 //MultiPartEmail email = new MultiPartEmail();//创建能加附件的邮件,可多个、网络附件亦可 //HtmlEmail email = new HtmlEmail();//创建能加附件内容为HTML文本的邮件、HTML直接内联图片 上面基本上是使用Java发送邮件的两种方式。 我测试了一下发邮件的代码，可以成功收到邮件。如下图： 总结在后台应用系统中邮件系统还是比较常用的，可以用来提供系统邮件预警、发送客户邮件进行活动推广等等一系列的事情。SpringBoot里整合了JavaMail，创建SpringBoot项目时可以直接引入，是十分方便的。 在application.properties里的配置均以spring.mail开头。 1234567891011# JavaMailSender 邮件发送的配置spring.mail.host=smtp.qq.comspring.mail.username=qq邮箱#QQ邮箱的授权码spring.mail.password=授权码spring.mail.properties.mail.smtp.auth=truespring.mail.properties.mail.smtp.starttls.enable=truespring.mail.properties.mail.smtp.starttls.required=truespring.mail.default-encoding=UTF-8spring.mail.protocol=smtpspring.mail.port=465]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaMail</tag>
        <tag>Java邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JWT简介及使用]]></title>
    <url>%2Fblog%2FJWT%E7%AE%80%E4%BB%8B%E5%8F%8A%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言JWT（JSON Web Tokens）是目前比较流行的跨域认证解决方案，遵循RFC 7519 标准，我们可以使用JWT在用户和服务器之间传递安全可靠的信息。 我们可以在 JWT的官网 了解到更多关于JWT的信息。 正文构成JWT主要由头部、载荷与签名三部分构成，三部分的信息用英文逗号“.”分割。 一个完整的JWT如下： 它的头部部分（Header）为红色标注部分，载荷（Payload）为蓝色部分，签名（Signature）为橘色部分。 它们均使用Base64进行编码，我们将上述Base64解码后可以看到如下： Header部分Json： 1234&#123; "typ": "JWT", "alg": "HS256"&#125; 这儿Header声明了使用的加密算法（HS256）及token类型（JWT）。 Payload部分Json如下： 1234567&#123; "phone": "1888888888", "sessionId": "111111111111", "exp": 1548052800, "userId": "1433223", "platform": "APP"&#125; 其中除exp字段其它都是我自定义的字段。JWT 规定了7个官方字段，供我们选用，如下： iss (issuer)：签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 PS：可以看到信息仅仅使用了Base64进行了一下编码，非加密，如果我们想是信息更安全可以对JWT生成的token进行可逆加密。 Signature部分: 会对上面两部分进行签名，通常使用RSA，Hmac或者ECDSA等签名方式。用于防止上面两部分的数据遭到篡改。 源码要想在Java项目里使用JWT，需要引入以下依赖。 123456&lt;!--JSON Web Tokens--&gt;&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 我们来简单的看一下它的源码部分。 先看一下包的结构： algorithm：各种签名的包。 exceptions：自定义异常类的包。 interfaces和impl：JWT的接口和实现类包。 在algorithm包里我们可以看到我们刚才描述的几种签名算法（RSA，HMAC，ECDSA）。 在JWTCreator类中，我们可以看到Signature部分是通过Header，Payload经过签名算法得来的。 同时载荷Payload里JWT规定的几个可使用字段也能看到。 生成签名，放入Header信息及Payload信息，使用指定签名算法生成JWT token。 JWTDecoder类为解密token的类，可以看到它的处理方法，获取headerJson和payloadJson，还是比较好理解的。 应用我们使用JWT生成token并使用。 我们知道，前后端使用token进行交互，服务器端可以不用保存session状态，减轻压力。 我们定义一个Vo，用于存放一些用户数据，如下： 123456789101112131415161718192021222324@Data@Accessors(chain = true)public class UserVo &#123; /** * 用户手机号 */ private String phone; /** * 用户唯一Id */ private String userId; /** * sessionId */ private String sessionId; /** * 过期时间 */ private long expiresAt; /** * 用户所属平台 */ private String platform;&#125; 可以认为这些为公共部分，用户登录后应该携带这些信息。 这样我们就可以使用JWT在该用户登录后生成一个有效token，为保证信息安全，我们可以对生成的token进行加密，如下： 我们使用AES算法对token进行加解密。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394@Slf4jpublic class AESUtils &#123; private final static String ENCODING_UTF8 = "utf-8"; private static final String KEY_ALGORITHM = "AES"; /**默认的加密算法*/ private static final String CIPHER_ALGORITHM = "AES/CBC/PKCS5Padding"; private String secretKeySeed = null; private String ivParameterSeed = null; public AESUtils(String secretKeySeed,String ivParameterSeed) &#123; this.secretKeySeed = secretKeySeed; if (ivParameterSeed.length() != 16) &#123; throw new RuntimeException("iv向量长度必须为16"); &#125; this.ivParameterSeed=ivParameterSeed; &#125; public AESUtils(String secretKeySeed) &#123; if (secretKeySeed.length() != 16) &#123; throw new RuntimeException("iv向量长度必须为16"); &#125; this.secretKeySeed = secretKeySeed; this.ivParameterSeed=secretKeySeed; &#125; /** * AES加密 * @param content * @return String */ public String aesEncrypt(String content) throws Exception &#123; // AES加密 byte[] encryptStr = encrypt(content, secretKeySeed,ivParameterSeed); // BASE64位加密 return Base64.encodeBase64String(encryptStr); &#125; /** * AES解密 * @param encryptStr * @return String */ public String aesDecrypt(String encryptStr) throws Exception &#123; // BASE64位解密 byte[] decodeBase64 = Base64.decodeBase64(encryptStr); // AES解密 return new String(decrypt(decodeBase64, secretKeySeed,ivParameterSeed),ENCODING_UTF8); &#125; /** * 生成加密秘钥 * @return */ private static SecretKeySpec getSecretKeySpec(final String secretKeySeed) &#123; try &#123; return new SecretKeySpec(secretKeySeed.getBytes(), "AES"); &#125; catch (Exception ex) &#123; log.error("生成加密密钥异常",ex); &#125; return null; &#125; /** * 生成向量秘钥 * @return */ private static IvParameterSpec getIvParameterSpec(final String ivParameterSeed) &#123; //使用CBC模式，需要一个向量iv，可增加加密算法的强度 return new IvParameterSpec(ivParameterSeed.getBytes()); &#125; /** * 加密 * @param content * @return byte[] */ private static byte[] encrypt(String content, String secretKeySeed,String ivParameterSeed) throws Exception &#123; // 创建密码器 Cipher cipher = Cipher.getInstance(CIPHER_ALGORITHM); // 初始化为加密模式的密码器 cipher.init(Cipher.ENCRYPT_MODE, getSecretKeySpec(secretKeySeed),getIvParameterSpec(ivParameterSeed)); // 加密 return cipher.doFinal(content.getBytes(ENCODING_UTF8)); &#125; /** * 解密 * @param content * @return byte[] */ private static byte[] decrypt(byte[] content, String secretKeySeed,String ivParameterSeed) throws Exception &#123; // 创建密码器 Cipher cipher = Cipher.getInstance(CIPHER_ALGORITHM); // 初始化为加密模式的密码器 cipher.init(Cipher.DECRYPT_MODE, getSecretKeySpec(secretKeySeed),getIvParameterSpec(ivParameterSeed)); // 解密 return cipher.doFinal(content); &#125;&#125; 同时，根据刚才我们的说明创建一个JWT帮助类用于生成token，大致如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105@Slf4jpublic class JWTUtils &#123; //JWT header private String JWT_HEADER = ""; //签名算法 private Algorithm algorithm; //默认token过期时间 private long expireTimeMillis=2*60*60*1000; private AtomicBoolean initState = new AtomicBoolean(false); private static AESUtils aesUtils=null; private static class SingletonHolder &#123; static final JWTUtils instance = new JWTUtils(); &#125; public static JWTUtils getInstance() &#123; return JWTUtils.SingletonHolder.instance; &#125; /** * 初始化jwt * @param jwtSecretKey * @param jwtExpireTimeSeconds * @param aesSecretKeySeed * @param aesIvParameterSeed * @throws UnsupportedEncodingException */ public void init(String jwtSecretKey,long jwtExpireTimeSeconds,String aesSecretKeySeed,String aesIvParameterSeed) throws UnsupportedEncodingException &#123; if (initState.compareAndSet(false, true)) &#123; this.algorithm = Algorithm.HMAC256(jwtSecretKey); if(jwtExpireTimeSeconds&gt;0) &#123; this.expireTimeMillis = jwtExpireTimeSeconds * 1000; &#125; this.JWT_HEADER = StringUtils.substringBefore(JWT.create().sign(Algorithm.HMAC256(jwtSecretKey)), ".")+"."; aesUtils = new AESUtils(aesSecretKeySeed, aesIvParameterSeed); &#125;else&#123; log.error("重复初始化jwt"); &#125; &#125; /** * 使用jwt生成token * @param userVo * @return */ public String encodeJWT(UserVo userVo) &#123; return encodeJWT(userVo,this.expireTimeMillis); &#125; /** * 生成token * @param userVo * @param expireTimeMillis * @return */ public String encodeJWT(UserVo userVo,long expireTimeMillis)&#123; String token = JWT.create() .withExpiresAt(new Date(System.currentTimeMillis()+expireTimeMillis)) .withClaim("phone",userVo.getPhone()) .withClaim("userId",userVo.getUserId()) .withClaim("sessionId",userVo.getSessionId()) .withClaim("platform",userVo.getPlatform()) .sign(algorithm); System.out.println("token----&gt; "+token); try &#123; token = aesUtils.aesEncrypt(StringUtils.removeStart(token, JWT_HEADER)); &#125; catch (Exception ex) &#123; log.error("加密异常",ex); token = ""; &#125; return token; &#125; /** * 解密token * @param token * @return */ public UserVo decodeJWT(String token) &#123; UserVo userVo =new UserVo(); try &#123; if (StringUtils.isBlank(token)) &#123; throw new RuntimeException("无效token"); &#125; String decryptJwtToken = aesUtils.aesDecrypt(token); JWTVerifier verifier = JWT.require(algorithm).build(); DecodedJWT jwt = verifier.verify(StringUtils.join(JWT_HEADER,decryptJwtToken)); long expiresAt = jwt.getExpiresAt()==null?0:jwt.getExpiresAt().getTime(); if(System.currentTimeMillis()&gt;expiresAt)&#123; throw new RuntimeException("token有效期超期"); &#125; Map&lt;String, Claim&gt; claims = jwt.getClaims(); userVo.setPhone(claims.get("phone")==null?"":claims.get("phone").asString()); userVo.setUserId(claims.get("userId")==null?"":claims.get("userId").asString()); userVo.setSessionId(claims.get("sessionId")==null?"":claims.get("sessionId").asString()); userVo.setPlatform(claims.get("platform")==null?"":claims.get("platform").asString()); userVo.setExpiresAt(expiresAt); &#125; catch (Exception exception)&#123; throw new RuntimeException("无效token"); &#125; return userVo; &#125;&#125; 这样，一个简单的JWT工具类就搞定了，可以用于生成token。 测试我们测试一下效果，新建方法如下： 1234567891011public static void main(String[] args) throws Exception&#123; JWTUtils jwtUtils = JWTUtils.getInstance(); jwtUtils.init("sakuratears",1000,"1234567891111111","test111111111111"); UserVo userVo = new UserVo(); userVo.setUserId("1433223").setPhone("1888888888").setPlatform("APP").setSessionId("111111111111"); String token = jwtUtils.encodeJWT(userVo); System.out.println("加密token----&gt; "+token); Thread.sleep(1); UserVo vo = jwtUtils.decodeJWT(token); System.out.println("解密token得到结果----&gt; "+vo.toString());&#125; 运行结果如下： 我们尝试缩短token失效时间，增加线程等待时间。如下： 123jwtUtils.init("sakuratears",1,"1234567891111111","test111111111111");......Thread.sleep(10000); 再次运行，可以看到token已失效。 所以我们在为客户端颁发token后，应该设置合理的token失效时间，当token失效后，再次请求，应告诉用户需要重新登录了。 总结经过上面的一些描述，我们可以知道JWT的一些特点。 JWT默认是不加密的，为了保证安全，可以对JWT（token）进行可逆加密处理。 JWT可以用于信息交换，比如上面UserVo里面的手机号，这样我们不用在使用userId在对用户进行数据库查询，提高系统性能。 可以看到，JWT一旦签发生成token，如果不设置超时时间或者设置不合理（过长），在有效期内，token始终是有效的，除非服务器进行额外的处理。所以如果token泄露或被盗用，将是十分危险的，故应当设置合理的过期时间。 为了减少泄露或者盗用风险，JWT一般使用HTTPS协议进行传输。若使用HTTP协议，务必对token进行可逆加密处理后在进行传输。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JWT</tag>
        <tag>Java Web Token</tag>
        <tag>Token</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot多数据源配置]]></title>
    <url>%2Fblog%2FSpringBoot%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E9%85%8D%E7%BD%AE.html</url>
    <content type="text"><![CDATA[前言之前研究了下SpringBoot下多数据源的配置处理，感觉蛮有意思的，今天特地总结分享下。 我们知道，当一些项目较大时，有可能出现分库等技术操作，这时候就有可能需要使用多数据源了，为保证程序SQL能在多个数据源之间来回切换并正常执行，就需要对数据源代码上进行一些处理，这里我已SpringBoot下的配置为例。 正文如何配置是简单的，但是要了解如何配置，我们应该下些功夫。 在spring-jdbc这个jar包里，我们可以找到这个抽象类AbstractRoutingDataSource.class，这个类是处理多数据源配置的关键类，我们来看一下。 这个类的主要作用是是的我们可以切换数据源key以切换数据源，key值的切换一般通过线程Context实现。 targetDataSources： 是一个Map集合，用于存放多个数据源。 resolvedDataSources：dataSource的Map集合，用于确定多个数据源。 defaultTargetDataSource：默认使用的数据源。 lenientFallback：可以用来回滚到原来的数据源的设置。 dataSourceLookup：数据源循环查看实现，有多个实现。 resolvedDefaultDataSource：确定的要使用哪一个数据源。 我们来看一些相关代码。 先看属性设置完成后执行的这个方法。 可以看到，resolvedDataSources从targetDataSources拿到值并转换为DataSource集合，resolvedDefaultDataSource会对defaultTargetDataSource进行转化处理。 再看这个决定使用哪个数据源的方法。 可以看到会先拿到当前的LookupKey，即当前要使用的数据源的key，拿到key后在尝试拿到当前key对应的数据源，如果没有数据源并且设置了回滚属性，会继续使用默认的数据源而不切换，如果没有设置回滚又拿不到数据源，dataSource==null，那么就会抛出异常，如果拿到数据源了，dataSource!=null，那么就会进行数据源的切换，返回一个切换后的数据源。 这儿lenientFallback（仁慈回滚）的作用，如果设置为false，拿不到数据源会出错，便于我们分析问题。 再看下 determineCurrentLookupKey 这个方法，就是获取要使用的数据源的key，是个抽象方法，需要我们进行实现。（具体什么时候切换数据源，就改变key值，我们一般使用线程绑定的上下文来对key值进行控制） 我们再来看下把把我们的对象转为数据源对象的这个方法。 可以看到如果传入的是String，会使用DataSourceLookup去获取数据源，默认使用JndiDataSourceLookup。 DataSourceLookup有4种实现，可以看下。 BeanFactoryDataSourceLookup是从SpringBean里获取DataSource。 JndiDataSourceLookup是通过JndiTemplate获取DataSource。 MapDataSourceLookup可以自定义DataSourceMap，然后根据Key值获取。下图中的dataSources就是我们可以提前主动set进去的数据。 SingleDataSourceLookup指的单一数据源，一般很少使用。这个dataSource我们也是可以主动设置的。 可以看到，我们有多种方式去放置数据源，targetDataSources这个Map集合可以直接放置数据源集合，也可以放置数据源bean名字（但需要指定DataSourceLookup为BeanFactoryDataSourceLookup去解析），也可以放置自定义数据源集合的key（但需要指定DataSourceLookup为MapDataSourceLookup去解析）等等。 因此我们要实现多数据源，需要对AbstractRoutingDataSource进行实现。 多数据源配置新建DynamicDataSource类，内容如下： 123456public class DynamicDataSource extends AbstractRoutingDataSource &#123; @Override protected Object determineCurrentLookupKey() &#123; return DatabaseContextHolder.getDatabaseType(); &#125;&#125; 看一下lookupKey的实现，我们新创建一个DatabaseContextHolder类，用一个ThreadLocal来对当前正在使用的数据源key进行管理。如下： 123456789101112131415161718192021222324252627282930public class DatabaseContextHolder &#123; /** * 使用ThreadLocal管理数据源 */ private static final ThreadLocal&lt;DatabaseType&gt; contextHolder = new ThreadLocal&lt;&gt;(); /** * 当前可以使用的所有数据源key值 */ public static Set&lt;DatabaseType&gt; databaseTypes = new HashSet&lt;&gt;(); /** * 设置数据源类型 * @param type */ public static void setDatabaseType(DatabaseType type) &#123; contextHolder.set(type); &#125; /** * 获取数据源类型 * @return */ public static DatabaseType getDatabaseType() &#123; return contextHolder.get(); &#125; /** * 清除数据源类型 */ public static void clearDatabaseType()&#123; contextHolder.remove(); &#125;&#125; databaseTypes 这个set我们后面可以用来判断是否有此数据源。 DatabaseType枚举如下（用静态String等也可以代替）： 123public enum DatabaseType &#123; MASTER,SLAVE&#125; 可以看到有两个数据源key值，MASTER和SLAVE。 然后我们要使多数据源生效，需要配置多数据源，如下： 1234567891011121314151617181920212223242526@Configuration@MapperScan(basePackages = "com.zwt.frameworkdatasource.mapper")public class DynamicDataSourceConfig &#123; /** * 主数据源 * @return */ @Bean(name = "masterDataSource") @Primary @ConfigurationProperties(prefix = "spring.datasource.multiple.master-datasource") public DataSource masterDataSource() &#123; return DataSourceBuilder.create().build(); &#125; /** * 从数据源 * @return */ @Bean(name = "slaveDataSource") @ConfigurationProperties(prefix = "spring.datasource.multiple.slave-datasource") public DataSource slaveDataSource() &#123; return DataSourceBuilder.create().build(); &#125; ......&#125; 配置文件内容如下： 123456789101112131415spring.datasource.multiple.master-datasource.driverClassName=com.mysql.cj.jdbc.Driverspring.datasource.multiple.master-datasource.jdbcUrl=jdbc:mysql://localhost:3306/test1?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8spring.datasource.multiple.master-datasource.username=rootspring.datasource.multiple.master-datasource.password=rootspring.datasource.multiple.master-datasource.initialSize=5spring.datasource.multiple.master-datasource.minIdle=1spring.datasource.multiple.master-datasource.maxActive=50spring.datasource.multiple.slave-datasource.driverClassName=com.mysql.cj.jdbc.Driverspring.datasource.multiple.slave-datasource.jdbcUrl=jdbc:mysql://localhost:3306/test2?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8spring.datasource.multiple.slave-datasource.username=rootspring.datasource.multiple.slave-datasource.password=rootspring.datasource.multiple.slave-datasource.initialSize=5spring.datasource.multiple.slave-datasource.minIdle=1spring.datasource.multiple.slave-datasource.maxActive=50 我们说下这个Config类。masterDataSource和slaveDataSource很好理解，就是获取配置文件属性生成相应的Bean，这儿要注意@Primary注解，这个注解指定默认使用哪个Bean，因为DataSource有两个实现，master和slave，如果不指定，Spring会不知道选用哪一个。 要使用多数据源，就需要对两个数据源进行管理，我们在Config这个类里创建一个返回DynamicDataSource的方法，如下: 1234567891011121314151617181920/** * 动态数据源 * @param masterDataSource * @param slaveDataSource * @return */@Beanpublic DynamicDataSource dataSource(@Qualifier("masterDataSource") DataSource masterDataSource,@Qualifier("slaveDataSource") DataSource slaveDataSource) &#123; Map&lt;Object, Object&gt; targetDataSources = new HashMap&lt;&gt;(); targetDataSources.put(DatabaseType.MASTER, masterDataSource); targetDataSources.put(DatabaseType.SLAVE, slaveDataSource); DatabaseContextHolder.databaseTypes.add(DatabaseType.MASTER); DatabaseContextHolder.databaseTypes.add(DatabaseType.SLAVE); DynamicDataSource dataSource = new DynamicDataSource(); dataSource.setTargetDataSources(targetDataSources); dataSource.setDefaultTargetDataSource(masterDataSource); return dataSource;&#125; 可以看到我们把MASTER数据源和SLAVE数据源放到了targetDataSources 里面，同时在DatabaseContextHolder的databaseTypes里面保存一份key值，用于一些处理。整体上这个类还是比较好理解的。 如何让程序执行SQL使用使用动态数据源呢？这时候就要处理SqlSessionFactory了，我们创建获取SqlSessionFactory的Bean的方法，传入动态数据源获取SqlSessionFactory，如下： 1234567891011121314/** * 多数据源切换关键类 * @param dynamicDataSource * @return * @throws Exception */@Beanpublic SqlSessionFactory sqlSessionFactory(DynamicDataSource dynamicDataSource) throws Exception&#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dynamicDataSource); sqlSessionFactoryBean.setTypeAliasesPackage("com.zwt.frameworkdatasource.model"); sqlSessionFactoryBean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources("classpath:sqlmap/mapper/*.xml")); return sqlSessionFactoryBean.getObject();&#125; 这个类是是数据源切换的关键类。 为了管理支持多数据源事务，我们的事务Bean也是要传入动态数据源的，如下： 123456789/** * 事务管理 * @param dynamicDataSource * @return */ @Bean public PlatformTransactionManager transactionManager(DynamicDataSource dynamicDataSource)&#123; return new DataSourceTransactionManager(dynamicDataSource); &#125; 这样随着数据源的切换，事务也是切换后的数据源的。 如何切换数据源？其实只要控制DatabaseContextHolder里面ThreadLocal的值就可以控制数据源切换，但是我们想切换数据源时，总不能每个方法都在方法开始前设置，在结束前清除吧，那样太麻烦了。 我们可以借助Aspect切面来处理这个问题。我们定义一个TargetDataSource注解，只要标注这个注解，并指定数据源key便可以进行数据源切换，这个注解一定是作用在方法上的，如下： 12345678910@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface TargetDataSource &#123; /** * 数据源key * @return */ DatabaseType database() default DatabaseType.MASTER;&#125; 切面的话，只要检测到这个方法上有这个注解，那就拿到注解里的数据源key进行切换，如下： 1234567891011121314151617181920212223242526272829303132333435@Aspect@Order(-1)//保证在@Transactional之前执行@Componentpublic class DynamicDataSourceAspect &#123; private static final Logger logger = LoggerFactory.getLogger(DynamicDataSourceAspect.class); /** * 改变数据源 * @param joinPoint * @param targetDataSource */ @Before("@annotation(targetDataSource)") public void changeDataSource(JoinPoint joinPoint, TargetDataSource targetDataSource) &#123; DatabaseType type = targetDataSource.database(); if(DatabaseContextHolder.databaseTypes.contains(type))&#123; logger.info("使用数据源："+ type); DatabaseContextHolder.setDatabaseType(type); &#125;else&#123; logger.error("未配置数据源，使用默认数据源：MASTER"); DatabaseContextHolder.setDatabaseType(DatabaseType.MASTER); &#125; &#125; /** * 清除数据源 * @param joinPoint * @param targetDataSource */ @After("@annotation(targetDataSource)") public void clearDataSource(JoinPoint joinPoint, TargetDataSource targetDataSource) &#123; logger.info("清除数据源 " + targetDataSource.database()); DatabaseContextHolder.clearDatabaseType(); &#125;&#125; @Order注解设置为-1保证此过程在事务之前执行，然后在执行事务。@Before注解表示在在动作之前执行，我们设置为在有此注解的方法之前执行changeDataSource方法，这个方法对数据源进行切换，@After注解表示方法执行完之后要处理的事情，这儿我们用clearDataSource方法来清除使用过的数据源信息。 讲到这儿，SpringBoot的多数据源配置也基本完成了，我们进行必要的测试。 测试创建一个Service类，代码如下： 1234567891011121314151617181920212223242526272829@Servicepublic class TestServiceImpl implements TestService &#123; @Autowired private ScoreMapper scoreMapper; boolean flag = false; @Override @TargetDataSource(database = DatabaseType.MASTER) @Transactional(rollbackFor = Exception.class,propagation = Propagation.REQUIRED) public void doSomething1() &#123; ScoreModel scoreModel = new ScoreModel(); scoreModel.setId(1234); scoreMapper.insert(scoreModel); if(!flag)&#123; throw new RuntimeException("出现异常"); &#125; &#125; @Override @TargetDataSource(database = DatabaseType.SLAVE) @Transactional(rollbackFor = Exception.class,propagation = Propagation.REQUIRED) public void doSomething2() &#123; ScoreModel scoreModel = new ScoreModel(); scoreModel.setId(12345); scoreMapper.insert(scoreModel); if(flag)&#123; throw new RuntimeException("出现异常"); &#125; &#125;&#125; 注：这儿略掉了Score表和Mapper文件的创建。 上面的TestServiceImpl，我们应该期望的结果是 doSomething1执行成功入库test1，doSomething2执行失败不能入库test2（因为抛出异常事务要回滚），我们看下执行结果: 新建Test，执行我们的方法。 1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTestpublic class FrameworkDatasourceApplicationTests &#123; @Autowired TestService testService; @Test public void contextLoads() &#123; try&#123; testService.doSomething1(); &#125;catch(Exception e)&#123; System.out.println(e); &#125; try&#123; testService.doSomething2(); &#125;catch(Exception e)&#123; System.out.println(e); &#125; &#125;&#125; 可以看到如下输出： 查看数据库。 符合我们期望值。我们把数据库数据清除后，把TestServiceImpl里的flag设置为false。执行刚才的测试方法，结果如下： 可以看到也符合我们的期望结果，成功完成了数据源切换及事务支持。 总结通过对SpringBoot多数据源的配置和理解，了解了多数据源的配置和使用，及多数据源实现的一些简单原理，也是蛮不错的一次学习过程。 本节源代码详见： framework-datasource]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>多数据源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Java自制一张雷达图]]></title>
    <url>%2Fblog%2F%E7%94%A8Java%E8%87%AA%E5%88%B6%E4%B8%80%E5%BC%A0%E9%9B%B7%E8%BE%BE%E5%9B%BE.html</url>
    <content type="text"><![CDATA[前言今天整理资料，看到了之前自己用Java画了张雷达图，遂又拿来研究了下，感觉比较好玩，特地分享一下。 众所周知，Java在绘制图像方面的能力是比较差（la）劲（ji）的。自带的主要的一些图像处理类有Java2D中绘图类Graphics，图像流处理类ImageIO等。 有一些基于Java的图像处理开源包也仅仅是对原图像进行缩放、变化、水印等操作，使用Java进行绘图的少之又少。 今天我们用Java自带的图像处理类（Graphics、ImageIO等）来绘制一张雷达图吧。 正文为什么要画雷达图？而不是用Java绘制动漫人物？ 咳咳……因为雷达图应用广泛（就不要为难Java画动漫人物了TAT）。 我们开始吧，先随便看一张雷达图。 可知其主要内容： 外层环、圆环数量、圆环半径、分类名称、各个部分的数值、各种颜色等等很多很多属性。 这里，我们不妨绘制一个考试分数雷达图，这样更结合实际。 我们需要一个Java Bean，里面存放雷达图的一些属性参数。如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 雷达图数据bean * @author zwt * ------------------关于雷达图，以下的说明--------------------- * 默认圆环最大半径为200 * 生成的图片为 宽600*高700 默认 * 雷达图中心点的位置为 300,300 默认 * 线及文字大概占 600*100 默认 * 这些参数都可以调整，但调整后应尽量保证生成图的美观性 * ------------------------------------------------------------- */public class RadarMapInit &#123; // 圆环数量 private int cirNum=2; // 圆的最大半径 private int r=200; // 圆心坐标 private Point point=new Point(300,300); // 圆环颜色 private Color cirColor=new Color(24,165,255); // 射线起始角度 默认顺时针旋转 private float startAngle=-90; // 射线颜色 private Color spoColor=new Color(0,0,255,100); // 绘图参数g 画图时该参数不需要设置，设置了也没用 private Graphics2D g; //填充色 private Color fillColor=new Color(146,199,234,200); //中部文字颜色 private Color cenColor=Color.WHITE; //中部文字 private String cenText; //中部文字字体样式 private Font cenFont=new Font("宋体", Font.TYPE1_FONT, 50); // 外围文字颜色 private Color outColor=Color.BLUE; //外围文字样式 有默认值 private Font outFont=new Font("宋体", Font.TYPE1_FONT,12); //生成图片路径 无默认值，必填 private String imgPath; //生成图片格式 默认jpg 支持 jpg和png 其他格式没测 private String imgType="jpg"; //图片大小设置 宽 默认600*700 像素 private int picWidth=600; //图片大小设置 高 private int picHeight=700; //文字及线段的x方向距图片边的距离 private int shiftX=100; //文字距雷达图y方向的距离 private int shiftY=100; //生成的图片的背景颜色 默认白色 private Color picBackColor=Color.WHITE; //又要开始添加参数了(⊙o⊙).....下面的主要是那一道线的参数 //线的最大值 private int lineMaxVal=1000; //线的当前值 private int lineCurVal=-1; //线的起始值 private int lineStaVal=0; //线的已达到的颜色 private Color lineArrColor=new Color(24,165,255); //线的未达到的颜色 private Color lineNotArrColor=Color.GRAY; //线的粗细 private BasicStroke lineStroke=new BasicStroke(5.0f,BasicStroke.CAP_ROUND,BasicStroke.JOIN_MITER); //下面的一般说明字体 一般说明文字写死吧，参数太多了，不想写了.... private Font genLineFont=new Font("宋体", Font.TYPE1_FONT,12); //下面的特殊说明字体 private Font speLineFont=new Font("宋体", Font.TYPE1_FONT,25); //特殊说明字体百分数 private String speLineValue; //特殊说明自己文字 private String speLineText; //一般说明字体颜色 private Color genTextColor=Color.BLACK; //特殊说明字体颜色 private Color speTextColor=Color.BLUE; ...... //Get Set 及构造方法略&#125; 上面省略掉了get set和构造方法。 另外我里面还有一些参数设置没提到的，主要是用来展示效果让其更人性化（应付客户使其更加满意）的，比如底部的分数击败直线，雷达图中央的分数显示等，一会儿大家可以看到效果。 参数很多，没办法…… 对于每个分类，我们也新建一个属于它们的Java Bean，用来存储它们的属性。（PS：因为到底有多少分类是不确定的，故应该在画图时传入一个Bean List）。 如下： 1234567891011121314public class RadarMapData &#123; //数值 private String value; //该维度数值最大值 private String maxValue; //分组 private String group; //文字 private String textValue; //图片路径 private String picPath; ...... //Get Set 及构造方法略&#125; 同样省略了Get Set 和构造方法，我又增加了图片路径可以在分类的文字部分添加小图标，还可以对分类进行分组（比如数学是理科分组，语文英语是文科分组），可以说很Nice了（以应对莫名其妙的需求）。 好了，开始使用我们的Graphics类进行绘图等操作了。 首先，这个关键类要使用我们刚才的那两个Bean，应该如下代码，同样Get，Set和构造方法略。 123456789101112public class RadarMap &#123; // 日志记录 private Logger logger = LoggerFactory.getLogger(RadarMap.class); // 数据集合 private List&lt;RadarMapData&gt; dataList; // 初始化参数 private RadarMapInit init; ...... //Get Set 及构造方法略 &#125; 开始绘制逻辑，首先有个画圆环方法，可以画出数个同心圆，如下： 1234567891011121314151617/** * 画圆环 * */private void drawCircles() &#123; Graphics2D g = init.getG(); // 根据圆心，半径，画出同心圆 g.setColor(init.getCirColor()); // 获取圆心 int x = init.getPoint().x; int y = init.getPoint().y; // 获取各个圆的直径并画圆环 for (int i = 1; i &lt;= init.getCirNum(); i++) &#123; int d = 2 * i * init.getR() / init.getCirNum(); g.drawOval(x - d / 2, y - d / 2, d, d); &#125;&#125; 主要方法就是g.drawOval画圆，不在详细解释。 然后我们以圆心绘制分类的每条射线，同时拿到每个分类的最大数值和这个人的数值并标记，然后连接这个人的各个数值，并将这个多边形内部填充起来。 如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 以圆心画射线 * */private void drawSpokes() &#123; Graphics2D g = init.getG(); Point point = init.getPoint(); // 解析数组 int num = dataList.size(); // 设置颜色 g.setColor(init.getSpoColor()); // 每个角度 float angle = init.getStartAngle(); float angleStep = 360 / num; for (int i = 1; i &lt;= num; i++) &#123; Point pt = getMappedPoint(init.getR(), angle, point); g.drawLine(point.x, point.y, pt.x, pt.y); addCirWordPic(dataList.get(i - 1).getTextValue(), pt, angle, dataList.get(i - 1).getPicPath()); g.setColor(init.getSpoColor()); angle += angleStep; &#125; // 获取各个点的坐标 Polygon p = new Polygon(); g.setColor(init.getFillColor()); float angle1 = init.getStartAngle(); Point ptNext = new Point(); for (int i = 1; i &lt;= num; i++) &#123; int myR = (int)(Double.parseDouble(dataList.get(i - 1).getValue())*(double)init.getR()/Double.parseDouble(dataList.get(i - 1).getMaxValue())); Point pt = getMappedPoint(myR, angle1, point); p.addPoint(pt.x, pt.y); if (i &lt; num) &#123; angle1 += angleStep; int myRNext = (int)(Double.parseDouble(dataList.get(i).getValue())*(double)init.getR()/Double.parseDouble(dataList.get(i).getMaxValue())); ptNext = getMappedPoint(myRNext, angle1, point); &#125; else &#123; angle1 += angleStep; int myRNext = (int)(Double.parseDouble(dataList.get(0).getValue())*(double)init.getR()/Double.parseDouble(dataList.get(0).getMaxValue())); ptNext = getMappedPoint(myRNext, angle1, point); &#125; //g.drawLine(pt.x, pt.y, ptNext.x, ptNext.y); &#125; g.drawPolygon(p); g.fillPolygon(p);&#125; 这个方法首先解析分类数组，有几个就画几条射线（根据角度，这里逆时针旋转，起始角度-90，也就是从正上方开始旋转），从圆心与角度点之间画直线，并为这条直线的终点（角度点）添加文字和图片（分类文字及图片，方法addCirWordPic）。 完成后对于各个实际值点（实际分，比如语文100分，实际78分，78就是这儿的实际值），绘制成多边形。（PS： Polygon p = new Polygon();绘制多边形，p.addPoint为多边形添加指定点，g.drawPolygon(p)为绘制多边形，g.fillPolygon(p)为多边形填充颜色） 上面方法调用了两个方法getMappedPoint和addCirWordPic，大家可以看下，分别为寻找绘图点方法和添加分类图片文字方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 根据点，角度，半径获取另一个点坐标 * * @param r * @param angle * @param point * @return */private Point getMappedPoint(int r, float angle, Point point) &#123; // 计算坐标 Point pt = new Point(); pt.x = (int) (r * Math.cos(angle * Math.PI / 180) + point.x); pt.y = (int) (r * Math.sin(angle * Math.PI / 180) + point.y); return pt;&#125; /** * 添加圆外文字图片 * * @param point * 绘制起始点 * @param picValue * 图片路径 */private void addCirWordPic(String wordValue, Point point, float angle, String picValue) &#123; Graphics2D g = init.getG(); double x = 0; double y = 0; double dx = 0; double dy = 0; if (wordValue == null || "".equals(wordValue)) &#123; logger.info(angle + "角度的点没有配置圆外文字"); Map&lt;String, Double&gt; map = setDxDy(angle, x, y, dx, dy); dx = map.get("dx"); dy = map.get("dy"); &#125; else &#123; FontRenderContext context = g.getFontRenderContext(); Font font = init.getOutFont(); g.setColor(init.getOutColor()); g.setFont(font); Rectangle2D bounds = font.getStringBounds(wordValue, context); x = (bounds.getWidth()) / 2; y = (bounds.getHeight()) / 2; Map&lt;String, Double&gt; map = setDxDy(angle, x, y, dx, dy); dx = map.get("dx"); dy = map.get("dy"); g.drawString(wordValue, (int) (point.x + dx), (int) (point.y + dy)); &#125; if (picValue == null || "".equals(picValue)) &#123; logger.info(angle + "角度的点没有配置圆外图片"); return; &#125; // 图片放到文字上面 ImageIcon imgIcon = new ImageIcon(picValue); Image img = imgIcon.getImage(); g.drawImage(img, (int) (point.x + dx), (int) (point.y + dy - 5 * y), null);&#125; /** * 设置偏移量 圈外文字及图片使用 * * @param dx * @param dy */private Map&lt;String, Double&gt; setDxDy(float angle, double x, double y, double dx, double dy) &#123; double cosVal = Math.cos(angle * Math.PI / 180); double sinVal = Math.sin(angle * Math.PI / 180); if (cosVal &gt; 0 &amp;&amp; sinVal &gt; 0) &#123; dx = 5; dy = 2 * y + 20; // 第一象限额外偏移15用于放置图片 &#125; else if (cosVal &gt; 0 &amp;&amp; sinVal &lt; 0) &#123; dx = 5; dy = -5; // 第三象限 &#125; else if (cosVal &lt; 0 &amp;&amp; sinVal &gt; 0) &#123; dx = -2 * x - 5; dy = 2 * y + 20; // 第二象限额外偏移15用于放置图片 &#125; else if (cosVal &lt; 0 &amp;&amp; sinVal &lt; 0) &#123; dx = -2 * x - 5; dy = 5; // 第四象限 &#125; Map&lt;String, Double&gt; map = new HashMap&lt;String, Double&gt;(); map.put("dx", dx); map.put("dy", dy); return map;&#125; 绘制分类图片及文字时，由于要考虑文字大小，图片宽度等要求，故对图片及文字位置做了微调，会显得代码多些。 然后我们再对圆心加上一些文字，比如总分多少分什么的，如下： 1234567891011121314151617181920/** * 添加圆心文字 * */private void addWord() &#123; Graphics2D g = init.getG(); String value = init.getCenText(); if (value == null || "".equals(value)) &#123; logger.info("雷达图没有配置圆心文字，不添加圆心文字"); return; &#125; FontRenderContext context = g.getFontRenderContext(); Font font = init.getCenFont(); g.setColor(init.getCenColor()); g.setFont(font); Rectangle2D bounds = font.getStringBounds(value, context); double x = (bounds.getWidth()) / 2; double y = (bounds.getHeight()) / 2; g.drawString(value, (int) (init.getPoint().x - x), (int) (init.getPoint().y + y - 10));&#125; 总的来说就是找到圆心位置，根据字体大小，对字体位置进行调整。 然后我们再来绘制下面的跑分直线部分，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 添加雷达图下面的线 */private void drawLineAndWord() &#123; int curVal=init.getLineCurVal(); //-1为开始设置的值，默认为没有填写，直接跳过线条打印 if(curVal==-1) &#123; return; &#125; int staVal=init.getLineStaVal(); int maxVal=init.getLineMaxVal(); Graphics2D g = init.getG(); int width=init.getPicWidth(); //图片定的为 600*700 目前 雷达图占地 500 * 500 这边从y=600开始画 r+pointy+100 //可以将这些参数改为变量，目前不想改 //x方向前后都要空100单位 int shiftX=init.getShiftX(); //y方向距离雷达图100单位 int shiftY=init.getR()+init.getPoint().y+init.getShiftY(); //起点 Point point=new Point(shiftX,shiftY); //两边空50 实际绘图区域为500 int lineCur=(width-2*shiftX)*curVal/maxVal; int lineMax=(width-2*shiftX)*maxVal/maxVal; Point p1=getMappedPoint(lineCur,0,point); Point p2=getMappedPoint(lineMax,0,point); g.setColor(init.getLineArrColor()); g.setStroke(init.getLineStroke()); g.drawLine(point.x,point.y,p1.x,p1.y); g.setColor(init.getLineNotArrColor()); g.drawLine(p1.x,p1.y,p2.x,p2.y); //画完线后标记起点和结束点 drawWord(g,init.getGenLineFont(),init.getGenTextColor(),point,staVal+"",-10,0); drawWord(g,init.getGenLineFont(),init.getGenTextColor(),p2,maxVal+"",20,0); drawWord(g,init.getGenLineFont(),init.getSpeTextColor(),p1,curVal+"",0,15); //画小箭头 drawArrow(g,init.getLineArrColor(),p1); //写文字 //特殊文字起点 Point speTextP=new Point(shiftX-5,shiftY+60); drawWordNormal(g,init.getSpeLineFont(),init.getGenTextColor(),init.getSpeTextColor(),speTextP,init.getSpeLineText()+"的分数击败了全班",init.getSpeLineValue(),"的童鞋！");&#125; 这个线就是比分用的，比如小明总分500分，击败了全班80%的童鞋，这种直线及文字。也没啥内容，主要就是绘制直线，要求变换直线宽度及颜色，然后在线的两端添加文字，然后在底部添加必要文字。（PS：这儿的底部文字可以设置的，我直接写死了，其实也可以当参数传进来） 这个方法里面也用到了几个小方法，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 写字居中偏移方法 * @param g 绘图参数 * @param font 字体 * @param color 字体颜色 * @param point 绘制点 * @param value 绘制文本内容 * @param dx x偏移量 * @param dy y偏移量 如果x，y偏移量不设置理论上位于绘点中心 */private void drawWord(Graphics2D g,Font font,Color color,Point point,String value,double dx,double dy) &#123; FontRenderContext context = g.getFontRenderContext(); Rectangle2D bounds = font.getStringBounds(value, context); g.setFont(font); g.setColor(color); double x = (bounds.getWidth()) / 2; double y = (bounds.getHeight()) / 2; g.drawString(value, (int) (point.x - x+dx), (int) (point.y + y+dy));&#125;/** * 特殊文字的文字绘制 */private void drawWordNormal(Graphics2D g,Font font,Color genColor,Color speColor,Point point,String firstValue,String cenVal,String endValue) &#123; FontRenderContext context = g.getFontRenderContext(); Rectangle2D bounds1 = font.getStringBounds(firstValue, context); Rectangle2D bounds2 = font.getStringBounds(cenVal, context); //Rectangle2D bounds3 = font.getStringBounds(endValue, context); double x1 = bounds1.getWidth(); double x2 = bounds2.getWidth(); g.setFont(font); g.setColor(genColor); g.drawString(firstValue, (int) (point.x), (int) (point.y)); //画完后 再画特殊文字 再补上后面的内容 g.setColor(speColor); g.drawString(cenVal, (int) (point.x+x1), (int) (point.y)); g.setColor(genColor); g.drawString(endValue, (int) (point.x+x1+x2), (int) (point.y));&#125;/** * 绘制小箭头 * @param g 绘图参数 * @param color 箭头填充色 * @param point 箭头顶点 */private void drawArrow(Graphics2D g,Color color,Point point) &#123; g.setColor(color); Polygon p = new Polygon(); p.addPoint(point.x,point.y); p.addPoint(point.x-10,point.y-20); p.addPoint(point.x,point.y-15); p.addPoint(point.x+10,point.y-20); g.fillPolygon(p);&#125; 相当于对一些细节的操作优化，这儿就不一一介绍了。 然后在程序出图之前，我们先用Java中比较古老的JFrame看看图片的效果，方便调试“修图”。 在RadarMap里添加如下方法，这个方法只有JFrame测试时用到，当图调好了就可以删了，如下： 123456789/** * 画雷达图主方法 这个方法主要是为了显示在jpanel上用，实际中主调 */public void drawRadarMap() &#123; drawCircles(); drawSpokes(); addWord(); drawLineAndWord();&#125; 很简单，依次调用上面的几个方法。 我们新建一个JFrame Test类，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * 测试类，可以把生成的图放到Jpanel可视化界面上，便于调试 * * @author zwt * */public class MyTest extends JFrame &#123; MyPanel mp = null; public static void main(String[] args) &#123; MyTest demo01 = new MyTest(); &#125; public MyTest() &#123; mp = new MyPanel(); this.add(mp); this.setSize(600, 700); this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); this.setVisible(true); &#125;&#125;// 定义一个MyPanel面板，用于绘图区域class MyPanel extends JPanel &#123; // 覆盖JPanel // Graphics 是绘图的重要类，可以理解成一支画笔 public void paint(Graphics g) &#123; // 1. 调用父类函数完成初始化任务 // 这句话不可以少 super.paint(g); List&lt;RadarMapData&gt; dataList = new ArrayList&lt;RadarMapData&gt;(); RadarMapData map1 = new RadarMapData("100","150", "", "数学", ""); RadarMapData map2 = new RadarMapData("120","150", "", "语文", ""); RadarMapData map3 = new RadarMapData("90","150", "", "英语", ""); RadarMapData map4 = new RadarMapData("80","100", "", "物理", ""); RadarMapData map5 = new RadarMapData("95","100", "", "化学", ""); RadarMapData map6 = new RadarMapData("88","100","", "生物", ""); RadarMapData map7 = new RadarMapData("66","100", "", "历史", ""); RadarMapData map8 = new RadarMapData("77","100", "", "政治", ""); RadarMapData map9 = new RadarMapData("45","100", "", "地理", ""); RadarMapData map10 = new RadarMapData("88","100", "", "音乐", ""); RadarMapData map11 = new RadarMapData("80","100", "", "体育", ""); RadarMapData map12 = new RadarMapData("100","100", "", "美术", ""); dataList.add(map1); dataList.add(map2); dataList.add(map3); dataList.add(map4); dataList.add(map5); dataList.add(map6); dataList.add(map7); dataList.add(map8); dataList.add(map9); dataList.add(map10); dataList.add(map11); dataList.add(map12); int lineMaxValue = 0; int currentValue = 0; for(RadarMapData data:dataList)&#123; lineMaxValue+=Integer.valueOf(data.getMaxValue()); currentValue+=Integer.valueOf(data.getValue()); &#125; RadarMapInit init = new RadarMapInit(); init.setCirNum(5);//20分一档，可以自己设置 init.setCenText(currentValue+"分"); init.setSpeLineValue("60%");//击败多少童鞋先瞎传一个数 init.setG((Graphics2D) g); //init.setImgPath("/Users/zhangwentong/Desktop/image.jpg"); init.setLineCurVal(currentValue); init.setLineMaxVal(lineMaxValue); RadarMap test = new RadarMap(dataList, init); test.drawRadarMap(); //test.createImage(); &#125;&#125; 这个Test主要就是给一些参数赋值。然后测试，可以看到结果如下图所所示，哈哈，还是蛮不错的。 这我们只在JFrame上测试了，我们的任务是生成图片，然后开始吧，在RadarMap里添加如下方法： 123456789101112131415161718192021222324252627282930313233/** * 生成图片文件 */public boolean createImage() &#123; boolean flag = false; try &#123; File file = new File(init.getImgPath()); int width=init.getPicWidth(); int height=init.getPicHeight(); BufferedImage bi = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g2 = (Graphics2D) bi.getGraphics(); //设置抗锯齿属性，不然图片锯齿化很模糊 g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING, RenderingHints.VALUE_ANTIALIAS_ON); g2.setRenderingHint(RenderingHints.KEY_TEXT_ANTIALIASING, RenderingHints.VALUE_TEXT_ANTIALIAS_ON); g2.setBackground(init.getPicBackColor()); g2.clearRect(0, 0, width, height); init.setG(g2); //画圆 drawCircles(); //画射线及添加圆边的文字图片 drawSpokes(); //添加中心文字 addWord(); //添加下面那条线及内容 drawLineAndWord(); flag = ImageIO.write(bi, init.getImgType(), file); &#125; catch (Exception e) &#123; flag = false; e.printStackTrace(); &#125; return flag;&#125; 这个方法主要就是利用了BufferedImage来生成一张图片。我们把刚才JFrame里的两行注掉的代码开启。 12//init.setImgPath("/Users/zhangwentong/Desktop/image.jpg");//test.createImage(); 再次启动JFrame类，可以看到生成图片啦。 哈哈，蛮不错的…… 然后我们把你的分数……这句话的你动态传入姓名（比如小红……）。然后新建分数MapTest如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * @Author: zwt * @Description: 总分雷达图 * @Name: ScoreMapTest * @Date: 2019/1/5 11:49 PM * @Version: 1.0 */public class ScoreMapTest &#123; private static final String [] names = &#123;"小一","小二","小三","小四","小五","小六","小七","小八","小九","小十"&#125;; //随机分数使用 private static int randomValue(int maxValue)&#123; Random random = new Random(); return random.nextInt(maxValue); &#125; public static void drawMapPic()&#123; for(String name:names)&#123; List&lt;RadarMapData&gt; dataList = new ArrayList&lt;RadarMapData&gt;(); RadarMapData map1 = new RadarMapData(randomValue(150)+"","150", "", "数学", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map2 = new RadarMapData(randomValue(150)+"","150", "", "语文", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map3 = new RadarMapData(randomValue(150)+"","150", "", "英语", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map4 = new RadarMapData(randomValue(100)+"","100", "", "物理", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map5 = new RadarMapData(randomValue(100)+"","100", "", "化学", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map6 = new RadarMapData(randomValue(100)+"","100","", "生物", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map7 = new RadarMapData(randomValue(100)+"","100", "", "历史", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map8 = new RadarMapData(randomValue(100)+"","100", "", "政治", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map9 = new RadarMapData(randomValue(100)+"","100", "", "地理", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map10 = new RadarMapData(randomValue(100)+"","100", "", "音乐", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map11 = new RadarMapData(randomValue(100)+"","100", "", "体育", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map12 = new RadarMapData(randomValue(100)+"","100", "", "美术", "/Users/zhangwentong/Desktop/book.png"); dataList.add(map1); dataList.add(map2); dataList.add(map3); dataList.add(map4); dataList.add(map5); dataList.add(map6); dataList.add(map7); dataList.add(map8); dataList.add(map9); dataList.add(map10); dataList.add(map11); dataList.add(map12); int lineMaxValue = 0; int currentValue = 0; for(RadarMapData data:dataList)&#123; lineMaxValue+=Integer.valueOf(data.getMaxValue()); currentValue+=Integer.valueOf(data.getValue()); &#125; RadarMapInit init = new RadarMapInit(); init.setCirNum(5);//20分一档，可以自己设置 init.setCenText(currentValue+"分"); init.setSpeLineText(name); init.setSpeLineValue("80%");//这个数可以计算，我这儿就不计算了（因为分数是随机的，计算比较蛋疼） init.setImgPath("/Users/zhangwentong/Desktop/map/image"+name+".jpg"); init.setLineCurVal(currentValue); init.setLineMaxVal(lineMaxValue); RadarMap test = new RadarMap(dataList, init); test.createImage(); &#125; &#125; public static void main(String[] args) &#123; drawMapPic(); &#125;&#125; 同时添加图标（我这里只添加了一个相同的，不同的也是可以的），新建map文件夹（用于存放雷达图）。如下： 运行测试类。打开map文件夹。 随便选一张查看。 总结通过使用Java2D绘制雷达图，学到了Java2D的一些用途吧，虽然Java2D使用的很少，而且以后估计用的概率也不大，但是，我们就当一次对于Java程序的自娱自乐吧！ PS：Java图像处理方面确实很差，因为它的着重点不是这儿，而是大型Web项目，这篇文章的目的也不在于去理解Java2D的一些用途，而是通过一些学习，让我们知道，Java也是可以做一些莫名其妙的事情的，虽然不尽人意，也是，世界上哪有一种编程语言是完美的呢？]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>雷达图</tag>
        <tag>Graphics2D</tag>
        <tag>Java绘图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之注册中心]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83.html</url>
    <content type="text"><![CDATA[前言Zookeeper是可以实现注册中心相关功能的。 何为注册中心？ 我们知道，随着系统业务扩张，为了满足业务规模要求，引入了分布式、微服务等相关技术。引入它们后，相关的进程通信变为了网络通信。进而出现了服务调用方和服务被调用方，由于被调用方提供的服务地址在分布式环境下不是唯一的，因而需要对它们进行统一管理，这个管理的模块称之为注册中心。 可以参照如下图。 Provider代表服务提供方，由于有多个服务地址（分布式情形下，这里简化为一个），均需要向Registry模块注册自己的信息，Invoker代表服务调用方，它通过注册中心的通知或者主动订阅获得服务信息，进而发起请求。其中Registry模块即为注册中心。 注册中心作为服务框架核心模块，它是服务框架唯一核心链路上的一个集中点，所以它的好坏也影响着整个服务框架的可用性以及稳定性。 注册中心的话大部分公司都使用的开源实现，Dubbo体系中使用Zookeeper的居多，SpringCloud体系中使用Eureka的居多。 我们今天来看一下一些好的开源实现吧。 正文我们使用Spring Initializr 初始化一个集成了Zookeeper注册中心的SpringBoot项目。 如图，选择Zookeeper作为注册中心及服务发现。 完成后在pom文件里会有如下依赖。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zookeeper-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 我们来分析下Spring团队是如何使用Zookeeper作为注册中心的。 找到这个jar包，目录结构如下图，可以清楚的明白。 discovery是用于服务发现的包。 serviceregistry是用于服务注册的包。 support包包含一些工具类等信息。 先来看看serviceregistry包下的类。 ServiceInstanceRegistration类是一个服务注册信息bean。其他带AutoConfiguration的是结合properties文件进行自动配置的类，也不做过多介绍。我们来看下ZookeeperServiceRegistry这个服务注册关键类。 它的主要方法如下： 可以看到实现了register（服务注册），getServiceDiscovery（获取服务发现者），deregister（服务解绑）等方法。 继续查看两个关键方法服务注册和解绑。 可以看到它是通过一个叫ServiceDiscoveryImpl类进行实现的。 可以发现这个类在 curator-x-discovery包下。 这个包就是对Zookeeper实现服务注册与发现的一套封装。 这个jar包可以看到internalRegisterService这个方法，如果在缓存里不存在此服务的话就会去创建一个，可以看到它的逻辑，最大重试三次创建节点，可以选择创建节点的类型（服务类型），如果出现异常要删除创建的节点。 也可以看到这个类使用了ConcurrentMap作为数据缓存。 这个类里面的start方法会注册所有服务。 而后最开始的ZookeeperServiceRegistry会调用此方法进而注册服务。 再来看一下discovery（服务发现）包。 这个包里面内容较多，我们只看下比较关键的类ZookeeperDiscoveryClient即可。 先看看它的getInstances方法，通过serviceId获取一个Service实例。 在ServiceDiscoveryImpl中的方法如下： 可以看到会调用queryForInstances方法，通过name拿到Zookeeper的path，然后获取该path上的所有instancesIds，在通过name和id调用queryForInstances方法查询具体的实例信息。 我们创建一个hello项目，加入Web及Zookeeper Discovery 依赖后，进行必要配置，启动项目，便可以在Zookeeper里看到我们注册的服务信息。 application.properties配置 12345spring.cloud.zookeeper.discovery.register=truespring.cloud.zookeeper.discovery.root=/testspring.cloud.zookeeper.connect-string=127.0.0.1:2181spring.application.name=testserver.port=8081 HelloController内容 1234567RestControllerpublic class HelloController &#123; @RequestMapping("/hello") public String hello()&#123; return "hello"; &#125;&#125; 启用服务发现。 启动项目后通过客户端连接到Zookeeper可以看到服务节点信息。 PS：启动过程中如果出现错误，可能是Zookeeper客户端版本与服务端版本不一致，一般为客户端版本较高出现的问题，应当注意。 我们继续新建一个Test类，如下： 1234567891011121314151617@RunWith(SpringRunner.class)@SpringBootTestpublic class HelloApplicationTest &#123; @Autowired RestTemplate restTemplate; @Autowired DiscoveryClient client; @Test public void contextLoads() &#123; List&lt;String&gt; list=client.getServices(); for(String str:list)&#123; ServiceInstance instance=client.getInstances(str).get(0); String result=restTemplate.getForEntity(instance.getUri(),String.class).getBody(); System.out.println(result); &#125; &#125;&#125; 可以看到获得的ServiceInstance信息如下。 使用restTemplate发送请求，获得结果。 使用RestTemplate时，应当在启动时声明这个Bean，如下。 1234@BeanRestTemplate restTemplate()&#123; return new RestTemplate();&#125; 如果服务调用时出现异常，可能是host的问题，比如我上面的host为DELL-3020-PC，如果不在hosts文件里指定其解析为127.0.0.1，那么请求会出现异常，应当注意。 说到这里，其实Zookeeper作为注册中心的本质，就是把服务的一些信息（host，port，URI，serviceName等等）以顺序临时节点的形式存储在Zookeeper上，通过主动拉取或者通知的方式获取服务信息，服务调用者拿到信息后进行调用服务提供方。 因此我们也可以自己基于Zookeeper实现一个注册中心，但是根据上面我们可以知道，注册中心可以说是系统中的关键部分，如果出现问题，可能导致系统服务不可用，出现严重生产事故，因此，注册中心必须保证高可用性、高性能、实时性（如有服务出现问题，应该从注册中心剔除）等特点。 因此我们现在使用一些开源代码，如Erueka Discovery、Zookeeper Discovery等，毕竟这些代码已经经过了无数人的测验。 我们也应该多学习关于注册中心这一方面的知识，争取可以造一个属于自己的“轮子”。 总结通过对 Spring Cloud 里 Zookeeper Discovery代码的部分分析，了解了注册中心的大致原理，明白了Zookeeper作为注册中心的原理，对于我们来说，也是蛮不错的一次学习过程。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>注册中心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2Fblog%2FHTTP%E5%8D%8F%E8%AE%AE.html</url>
    <content type="text"><![CDATA[前言超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，所有的WWW文件都必须遵守这个标准。 今天我们来详细了解下它的一些特点及Java里对此协议的解析处理。 正文我们发送一个HTTP 请求，在浏览器中可以看到HTTP协议的一些信息。 Request Headers： POST /test HTTP/1.1 POST 表示HTTP的请求方法类型。 /test 表示请求URI。 HTTP/1.1 表示协议和协议版本。 说明：HTTP协议里目前定义了8种请求方法类型。 OPTIONS：返回服务器针对特定资源所支持的HTTP请求方法，也可以利用向web服务器发送“*”的请求来测试服务器的功能性。 HEAD：服务器返回与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以再不必传输整个响应内容的情况下，就可以获取包含在响应小消息头中的元信息。 GET：向特定的资源发出请求。它本质就是发送一个请求来取得服务器上的某一资源。 POST：向指定资源提交数据进行处理请求，数据被包含在请求体中，POST请求可能会导致新的资源的建立或已有资源的修改。 PUT：向指定资源位置上传其最新内容。 DELETE：请求服务器删除指定的资源。 TRACE：回显服务器收到的请求，主要用于测试或诊断 CONNECT：HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 Host: localhost:8080 表示服务器域名地址 Connection: keep-alive 表示连接类型，一直保持连接。 Content-Length: 11 表示参数长度 Accept: application/json, text/javascript, */*; q=0.01 表示接受的数据类型，逗号分隔，优先支持程度从前到后，q表示权重， 0&lt;=q&lt;=1， 权重越大，分号前面这一项支持度越好，0表示不支持，不指定q值默认为1。 Origin: http://localhost:8080 表示源数据地址。 X-Requested-With: XMLHttpRequest XMLHttpRequest对象，可以实现局部更新，ajax使用。 User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1 表示用户代理信息，浏览器信息等。 Content-Type: application/x-www-form-urlencoded; charset=UTF-8 提交数据类型，编码格式。 Referer: http://localhost:8080/ 表示引用源地址 Accept-Encoding: gzip, deflate, br 表示浏览器支持的压缩编码格式。当前浏览器支持 gzip, deflate, br 三种压缩算法。 Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 可以接受的语言。 Cookie: Idea-6ad4ef07=ced8796b-12ab-4db6-9bd7-556be6c5132b; JSESSIONID=3F1698BD54B15FFE91EC948154761956 Cookie信息。 Status Code: 200 状态码。 目前总共有41种状态码。 在HttpServletResponse有定义，有兴趣的可以看看。 主要状态码及含义如下： 状态码 含义 100 表示客户端可以继续 101 表示服务器正在根据升级头切换协议 200 表示请求成功 201 表示请求成功，并在服务器上创建了一个新资源 202 表示服务器接受了处理请求，但未完成 203 表示客户端提供的元信息并非来自服务器 204 表示请求成功，但没有返回新信息 205 表示代理可能被重置导致发送了文档视图 206 表示服务器已经完成了对资源的部分GET请求 300 表示所请求的资源对应一组资源，组里每一种表示都有其特定的位置 301 表示资源已经永久地移动到一个新的位置，并且将来的请求应使用一个新的URI 302 表示资源暂时移动到其它的URI下 303 表示可以在其它的URI下找到对请求的响应 304 表示GET操作发现资源可用且未修改 305 表示资源必须通过代理访问 307 表示资源暂时在其它的URI下，但是这个URI地址应该在响应的时候给出 400 表示客户端发送的请求语法错误 401 表示请求需要HTTP身份验证 402 保留以备将来使用 403 表示服务器接受了请求，但拒绝执行 404 表示请求的资源不可用或者找不到 405 表示对于要请求的资源，请求中指定的方法是不允许的 406 表示请求标识的资源仅能够根据请求中发送的accept标头生成具有不可接受的内容特征的响应实体 407 表示客户端必须先通过代理进行身份验证 408 表示客户端在服务器准备等待的时间内没有生成请求 409 表示由于与资源的当前状态发生冲突，请求无法完成 410 表示该资源在服务器上不再可用，且不知道转发地址 411 表示如果没有定义Content-Length，则无法处理请求 412 表示在服务器上测试时，在一个或多个请求头字段中给出的前提条件评估不通过 413 表示服务器拒绝处理请求，因为请求实体大于服务器能够处理的大小 414 表示服务器拒绝服务请求，因为request-uri长度超过了服务器能够处理的长度 415 表示服务器拒绝服务请求，因为请求的实体的格式不受请求方法的请求资源支持 416 表示服务器不能为请求的字节范围提供服务 417 表示服务器不能满足Expect请求头中给出的期望 500 表示服务器内部的一个错误，它阻止了服务器完成请求 501 表示服务器不支持满足请求所需的功能 502 表示服务器作为代理或网关服务器时接收到了无效响应 503 表示服务器暂时过载，无法处理请求 504 表示服务器作为网关或代理服务器时没有及时收到来自上游服务器的响应 505 表示服务器不支持或拒绝支持请求消息中使用的HTTP协议版本 我们来看一下后端Java部分的一些处理吧。 找到HttpServlet，可以看到它定义的几种请求方法。 同时GET，POST，PUT，DELETE都需要被实现。HEAD、OPTIONS、TRACE已被实现。 我们根据上面的知识，可以自己实现一个HttpServlet，同时可以实现其它的自定义方法，比如有的面试会问如何在请求里新增压缩图片方法（或者其它方法），我们的大致实现过程如下： 我们需要新增自定义协议COMPRESS。 我们新建HttpTestServlet 抽象类，新增方法 doCompress。 这样我们写一个测试Servlet，实现doCompress方法。 发送ajax，type 为 COMPRESS。 如下：可以看到请求方法变成了COMPRESS。 总结通过对HTTP协议的理解，加深了对HTTP协议的认识与认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之分布式锁]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81.html</url>
    <content type="text"><![CDATA[前言Zookeeper是可以用来实现分布式锁的。 要了解它，我们先简单说下分布式锁吧。 我们知道，在系统中，当我们访问公共资源并对资源进行一些操作时，为防止出现问题，需要对公共资源依次访问，如常见的多线程售票模型等。由于在一个系统中，我们可以使用锁（如ReentrantLock）或者synchronized关键字等Java方法处理。 但是，当系统逐渐由单系统转换为分布式系统、微服务时，情况就变得复杂了，比如有某共享资源，比如有1个奖品，一个应用查询到有奖品并尝试发给用户A，另一个应用也查询到有奖品并尝试发给用户B，这样A，B均显示有奖品，实际上我们的奖品数量是不足的。如常见的秒杀系统，抽奖系统等。 这时候就需要一种全局的互斥机制来控制应用对共享资源的访问，这就是所谓的分布式锁。 PS：分布式锁的实现也可以基于缓存（如Redis）实现，亦可以通过数据库（乐观锁等）实现，实际中要确实使用到分布式锁，基于缓存的实现还是要偏多一些的。 根据上面所述，下面的图是比较好理解的。 我们再来说下分布式锁应具备的一些特点。 同一时间只允许一台机器（服务）的一个线程执行。 为整个系统必要业务提供服务，应当是高可用的。 性能应得以保证，不能在获取锁和释放锁过程中浪费太多资源或时间。 分布式锁应当具备失效机制，避免死锁发生。 应当具有可重入特性。 应当有非阻塞的特点，某个服务没有获取到锁，应返回获取失败，不能阻塞。 下面我们来用Zookeeper实现我们的分布式锁。 正文我们在之前封装framework-zookeeper时，用到了下面这个依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 这里面有一个类InterProcessMutex，这是分布式锁使用的关键类。 PS：其实它已经实现了分布式锁，我们来使用下它吧。 我们创建一个Test，如下： 1234567891011121314151617181920212223242526@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; ExecutorService executorService = Executors.newCachedThreadPool(); static int TEST = 5; @Test public void test1() throws Exception&#123; for(int i=0;i&lt;10;i++)&#123; executorService.execute(()-&gt;&#123; try&#123; if(TEST&gt;0)&#123; doSomething(); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125; Thread.sleep(100000);&#125;public void doSomething() throws Exception&#123; Thread.sleep(1000); TEST--; System.out.println(Thread.currentThread().getId()+"--"+Thread.currentThread().getName()+"执行,TEST="+TEST); &#125;&#125; 考虑到多应用分布式比较麻烦，我这里的Test使用多线程模拟分布式请求，用了线程池管理了10个线程，假设TEST静态变量为共享资源，如果TEST数量大于0的时候，我们执行doSomething方法，假设这个方法执行需要一秒，执行后TEST减一。 我们运行Test后，可以看到输出结果。 可以看到出现了负数。 我们引入InterProcessMutex，在判断TEST之前对其加分布式锁，锁的zk基路径我们定为/zwt/lock。调用acquire获取锁，完成业务逻辑后调用release方法释放锁。 1234567891011121314151617181920@AutowiredCuratorZKClient client;@Testpublic void test1() throws Exception&#123; for(int i=0;i&lt;10;i++)&#123; executorService.execute(()-&gt;&#123; try&#123; InterProcessMutex interProcessMutex = new InterProcessMutex(client.getCuratorFramework(),"/zwt/lock"); interProcessMutex.acquire(); if(TEST&gt;0)&#123; doSomething(); &#125; interProcessMutex.release(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125; Thread.sleep(100000);&#125; 运行Test查看结果。 可以看到结果有序的减一最后到0结束。说明了InterProcessMutex实现了我们的分布式锁的功能。 它是如何实现的呢？ 我们在上面的doSomething代码里加一些输出。如下： 12345678910public void doSomething() throws Exception&#123; Thread.sleep(1000); List&lt;String&gt; list=client.getNodes("/zwt/lock"); list.forEach((e)-&gt;&#123; System.out.print(e+" "); &#125;); System.out.println(); TEST--; System.out.println(Thread.currentThread().getId()+"--"+Thread.currentThread().getName()+"执行,TEST="+TEST);&#125; 继续测试，如下结果： 数据大致如下： 可以看到，Zookeeper创建了10个临时顺序节点，每次会找到最小的节点并删除。其实这就是InterProcessMutex这个类的实现分布式锁的原理。 我们可以看下它的相关代码。 我们从acquire方法看起，调用了internalLock方法，而后调用了attemptLock方法，这个方法会通过createsTheLock去创建锁。 可以看到createsTheLock方法里创建了临时有序节点。 再看下internalLockLoop这个方法，有些大。 可以看到会拿到有序的子节点，getSortedChildren。 然后尝试去获取锁（从最小的节点开始），getsTheLock会先获取比自己小的节点，要是自己是最小的节点就会获得锁。 拿到后就设置haveTheLock为true，没有拿到，就添加watcher，监听比自己小的节点。然后根据设置的等待时间判断是否超时从而进行等待或者退出。 最后，如果到了时间或者出现异常，doDelete为true，就会删除节点。 再来看下release方法，里面的主要方法releaseLock，可以看到它调用了上面的deleteOurPath方法删除创建的临时节点。 在锁的获取和释放方法里可以看到下面这些地方，它可以保证我们的分布式锁具有可重入的性质。其通过lockCount（AtomicInteger ）实现的，统计重入次数。 1final AtomicInteger lockCount = new AtomicInteger(1); Zookeeper分布式锁的基本内容就是这些了，我们来总结下Zookeeper分布式锁的步骤： 指定一个存放锁目录（这儿我们指定的/zwt/lock）。 线程A想要获取锁，就需要在该目录下创建临时有序节点。 获取该目录下的所有子节点，然后获取比自己小的兄弟节点，如果不存在，说明自己是最小节点，那么就去获得锁。 线程B同线程A，创建好节点后获取目录下所有子节点，判断自己不是最小的，就会对获得锁的节点添加监听。 线程A处理完后释放锁，删除自己的节点，并通知，线程B监听后判断自己是不是最小节点，是的话会获取锁，不是的话在添加对当前获得锁的线程的监听。 通过上面我们可以看到Zookeeper分布式锁的一些优点，如高可用性（由Zookeeper保证）、可重入性、不会出现死锁（临时节点程序出现异常断开连接后会被删除也就失去了锁）等。 和一些缺点，如需要创建临时节点、删除临时节点，性能上肯定有一些影响。 其它我们可以在对其进行简单封装形成自己的分布式锁工具类。 相关代码如下： 提供一个分布式锁的接口。 1234567891011121314151617181920public interface DistributedReentrantLock &#123; /** * 获得锁 * @return * @throws InterruptedException */ boolean tryLock() throws InterruptedException; /** * 获得锁 * @param timeout * @param unit * @return * @throws InterruptedException */ boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; /** * 解除锁 */ void unlock();&#125; 同时我们使用Zookeeper的InterProcessMutex去完成相关实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class ZKDistributedReentrantLock implements DistributedReentrantLock &#123; private static final Logger logger = LoggerFactory.getLogger(ZKDistributedReentrantLock.class); /** * 线程池 */ private static final ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor( 10, new BasicThreadFactory.Builder().namingPattern("scheduled-pool-%d").build() ); /** * 所有锁的根节点 */ public static final String ROOT_PATH = "/LOCK/"; /** * 每次延迟清理PERSISTENT节点的时间 毫秒 */ private static final long DELAY_TIME_FOR_CLEAN = 1000; /** * zk 共享锁实现 */ private InterProcessMutex interProcessMutex; /** * 锁的ID,对应zk一个PERSISTENT节点,下挂EPHEMERAL节点. */ private String path; /** * zk的客户端 */ private CuratorFramework client; private volatile boolean isLockSuccess; public ZKDistributedReentrantLock(CuratorFramework client, String lockId) &#123; this.client = client; this.path = ROOT_PATH + lockId; interProcessMutex = new InterProcessMutex(client, this.path); &#125; public ZKDistributedReentrantLock(CuratorZKClient zkClient, String lockId) &#123; this.client = zkClient.getCuratorFramework(); this.path = ROOT_PATH + lockId; interProcessMutex = new InterProcessMutex(client, this.path); &#125; /** * 获取锁 * @return * @throws InterruptedException */ @Override public boolean tryLock() throws InterruptedException &#123; return tryLock(-1, null); &#125; /** * 获取锁 * @param timeout * @param unit * @return * @throws InterruptedException */ @Override public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; try &#123; isLockSuccess = interProcessMutex.acquire(timeout, unit); logger.debug("&#123;&#125; lock result:&#123;&#125;",this.path,isLockSuccess); return isLockSuccess; &#125; catch (InterruptedException e) &#123; throw e; &#125; catch (Exception e) &#123; logger.error(e.getMessage(),e); throw new RuntimeException(e.getMessage(),e); &#125; &#125; /** * 释放锁 */ @Override public void unlock() &#123; if(isLockSuccess) &#123; try &#123; isLockSuccess = false; interProcessMutex.release(); &#125; catch (Throwable e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; executorService.schedule(new Cleaner(client, path), DELAY_TIME_FOR_CLEAN, TimeUnit.MILLISECONDS); &#125; logger.debug("&#123;&#125; success unlock.",this.path); &#125; &#125; static class Cleaner implements Runnable &#123; private CuratorFramework client; private String path; public Cleaner(CuratorFramework client, String path) &#123; this.client = client; this.path = path; &#125; @Override public void run() &#123; try &#123; List list = client.getChildren().forPath(path); if (list == null || list.isEmpty()) &#123; client.delete().forPath(path); &#125; &#125; catch (KeeperException.NoNodeException | KeeperException.NotEmptyException e1) &#123; //nothing &#125; catch (Exception e) &#123; //准备删除时,正好有线程创建锁 logger.error(e.getMessage(), e); &#125; &#125; &#125;&#125; 实现还是比较简单的，tryLock方法主要就是使用了interProcessMutex的acquire方法，成功后记录isLockSuccess状态，失败后除了调用release方法、把isLockSuccess变为false外，还应尝试清除刚才已经创建的业务lockId节点(线程池)。 测试相关代码： 12345678910111213ZKConfig zkConfig = new ZKConfig();//....CuratorZKClient zkClient=new CuratorZKClient(zkConfig);DistributedReentrantLock lock = new ZKDistributedReentrantLock(zkClient,"test");try&#123; if(lock.tryLock())&#123; //doSomething &#125;&#125;catch (Exception e)&#123; //...&#125;finally &#123; lock.unlock();&#125; 总结通过对Zookeeper实现分布式锁的学习理解，我们又看到了Zookeeper的另外一个用途，对Zookeeper有了更深入的理解，也是蛮不错的一次学习体验。 有时间我会在研究下其它方式实现的分布式锁及其一些特点。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>分布式锁</tag>
        <tag>InterProcessMutex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之配置中心]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.html</url>
    <content type="text"><![CDATA[前言根据Zookeeper的一些特点，它是可以作为配置中心使用的。 何为配置中心？ 我们在项目开发维护过程中会有很多公共变量或资源，需要统一管理，以前我们把它们写在程序公共类或者配置文件中，可是这样以后有变动，程序就需要重新部署，很是不方便，而且分布式、微服务等技术出现，修改维护多个项目管理也变得复杂。 为了解决以上问题，实现一次打包多地部署需求，减少项目管理及安全风险，我们需要将可变变量外移并通过页面统一可视化管理，基于此，我们统一建设了配置中心。 引入Zookeeper后，我们把数据存放在Zookeeper节点Znode上，可以选择主动轮询查询或者等待Zookeeper通知。当数据发生变化时，我们可以直接通过通知去执行某些业务操作。一般为了数据准确性，我们会主动轮询查询或者通知+轮询的方式。 PS：当然使用数据库保存这些数据也是可以的，采用定期查询的方式，而且有的配置中心就与之类似，我们在这儿不做更广泛讨论。 分析注册中心可以分为服务端和客户端两部分。 服务端一般用于存储配置数据，提供数据管理等等服务。客户端一般为业务端调用数据提供API服务等。 当然现在有一些开源的配置中心，如spring-cloud-config，diamond，disconf，apollo 等，我们以后有接触在具体介绍研究它们。 今天我们基于Zookeeper实现自己的一个简单的注册中心。 如下的配置中心流程图也就比较好理解了。 正文先来实现基于Zookeeper的配置中心客户端吧。 PS：了解这篇文章之前可以先看看 Zookeeper Java客户端Curator 先来了解下curator-recipes 包下的一个类PathChildrenCache。 该类是从本地缓存ZK路径的所有子路径中保存所有数据的一个工具类，它将监视ZK路径、响应更新/创建/删除事件、下拉数据等等，此类不能保证事务处理时的强同步。 这个类有一个全参构造方法： 1public PathChildrenCache(CuratorFramework client, String path, boolean cacheData)&#123;...&#125; Client是我们的ZKClient需要创建，path指要监控的路径，cacheData指是否缓存数据。 同时我们可以为其添加Listener，当节点/子节点数据有变化时，可以进行通知等。 使用该方法： 1pathChildrenCache.getListenable().addListener(pathChildrenCacheListener); 我们想实现自己的配置中心客户端，与SpringBoot进行集成，其目录结构如下创建： ConfigCenterAutoConfig：SpringBoot自动配置类，会提供相应的Bean。 ConfigCenterConfiguration：自动配置类，从properties文件中获取配置属性。 ConfigCenterException：异常处理类。 ConfigCenterListener：配置中心监听listener。 ConfigCenterListenerAdapter：考虑到监听可以有多个，这个类用来处理它们。 LocalCacheService：主要用来定时轮询Zookeeper的配置。 ZKConfigService：主要用来创建Zookeeper连接及添加监听等。 ConfigUtil：工具类。 CacheNodeVo：节点Vo。 ConfigCenterClient：配置中心客户端。 先从配置类说起吧，连接配置文件properties的类ConfigCenterConfiguration。 1234567891011121314@ConfigurationProperties("spring.zookeeper.config-center")public class ConfigCenterConfiguration &#123; //zk地址 private String zkAddress; //业务名称 private String sysName; //连接超时时间 private Integer connectTimeoutMs = 60000; //session过期时间 private Integer sessionTimeoutMs = 60000; //重试间隔 private Integer retryInterval = 1000; ......&#125; 这个类不过多介绍了，就是Zookeeper的配置信息，连接Zookeeper时使用。 我们引入之前封装的framework-zookeeper包，通过自动配置拿到client。 12345&lt;dependency&gt; &lt;groupId&gt;com.zwt&lt;/groupId&gt; &lt;artifactId&gt;framework-zookeeper&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; ConfigCenterAutoConfig部分代码如下，也比较好理解，就是Spring启动后自动配置CuratorClient。 12345678910111213141516171819202122232425262728293031323334353637@Configuration@ConditionalOnClass(CuratorZKClient.class)@EnableConfigurationProperties(ConfigCenterConfiguration.class)public class ConfigCenterAutoConfig implements ApplicationContextAware &#123; private static final Logger log= LoggerFactory.getLogger(ConfigCenterAutoConfig.class); @Autowired private ConfigCenterConfiguration properties; @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = "spring.zookeeper.config-center", name = "enabled", havingValue = "true") CuratorZKClient curatorZKClient ()&#123; ZKConfig config = new ZKConfig(); config.setConnectString(properties.getZkAddress()); config.setNameSpace(properties.getSysName()); config.setSessionTimeoutMs(properties.getSessionTimeoutMs()); config.setConnectTimeoutMs(properties.getConnectTimeoutMs()); config.setRetryInterval(properties.getRetryInterval()); CuratorZKClient zkClient = new CuratorZKClient(config); zkClient.addConnectionListener((state) -&gt; &#123; log.debug("ZKConfigService connectionListener state：" + state); if (state == ZKConstants.State.CONNECTED || state == ZKConstants.State.RECONNECTED) &#123; log.info("ZKConfigService zookeeper is connected..."); &#125; &#125;); zkClient.start(); return zkClient; &#125; private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125; //根据class类型返回bean public static &lt;T&gt; T getBean(Class&lt;T&gt; requireType)&#123; return applicationContext.getBean(requireType); &#125;&#125; 有了CuratorClient，我们创建ZKConfigService，主要为指定节点添加希望的监听。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ZKConfigService&#123; private static final Logger log= LoggerFactory.getLogger(ZKConfigService.class); private static CuratorZKClient zkClient = null; final static String configRootPath = ConfigUtil.getConfigCenterPath(); /** * ZKService初始化 */ public static void init() &#123; //拿到zkClient zkClient = ConfigCenterAutoConfig.getBean(CuratorZKClient.class); try &#123; PathChildrenCache pathChildrenCache = new PathChildrenCache(zkClient.getCuratorFramework(), configRootPath, true); PathChildrenCacheListener pathChildrenCacheListener = (client,event) -&gt; &#123; log.debug("pathChildrenCacheListener eventType：" + event.getType()); ChildData data = event.getData(); if(data!=null)&#123; String dataStr = new String(data.getData(), "UTF-8"); String key = StringUtils.substringAfterLast(data.getPath(), ConfigUtil.SEP_STRING); switch (event.getType()) &#123; case CHILD_ADDED: LocalCacheService.put(key,dataStr); break; case CHILD_UPDATED: LocalCacheService.put(key,dataStr); ConfigCenterListenerAdapter.onChange(key,dataStr); break; case CHILD_REMOVED: LocalCacheService.remove(key); break; default: break; &#125; &#125; &#125;; pathChildrenCache.getListenable().addListener(pathChildrenCacheListener); pathChildrenCache.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; log.info("spring-boot-config ZKConfigService init success."); &#125; /** * 根据key获取值 * @param key * @return */ public static String getKey(String key) &#123; return zkClient.getStringData(ConfigUtil.joinPath(configRootPath, key)); &#125;&#125; 可以看到用到了我们刚才说的PathChildrenCache类，启动后，添加一个Listener，监听节点变化，一方面，我们需要一个类，对节点变化进行通知；另一方面，我们应把数据缓存在本地，如果数据变化后ZK没有通知或者其它情况，我们可以轮询查询后与本地缓存比较，有变化后继续进行我们节点变化的通知。 这就是我们的ConfigCenterListenerAdapter监听处理类和LocalCacheService本地缓存服务。 先说ConfigCenterListenerAdapter吧，可以看到上面代码节点有变化时触发了onChange事件。 由于我们业务可能需要多个监听类，故，我们提供一个监听接口，相关业务类实现这个接口，在注册一下监听即可使用岂不美哉。 考虑到此，我们存储监听类的集合应是静态的。如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ConfigCenterListenerAdapter &#123; private static Logger log= LoggerFactory.getLogger(ConfigCenterListenerAdapter.class); //key和要监听此节点的监听者列表 private static ConcurrentHashMap&lt;String, List&lt;ConfigCenterListener&gt;&gt; someKeyListenerMap = new ConcurrentHashMap&lt;&gt;(); private static List&lt;ConfigCenterListener&gt; allKeyListeners = new CopyOnWriteArrayList&lt;&gt;(); public static boolean addListener(String key, ConfigCenterListener configCenterListener) &#123; if (configCenterListener == null) &#123; return false; &#125; if (key == null || key.trim().length() == 0) &#123; allKeyListeners.add(configCenterListener); return true; &#125; else &#123; List&lt;ConfigCenterListener&gt; listeners = someKeyListenerMap.get(key); if (listeners == null) &#123; listeners = new ArrayList&lt;&gt;(); someKeyListenerMap.put(key, listeners); &#125; listeners.add(configCenterListener); return true; &#125; &#125; public static void onChange(String key, String value) &#123; if (key == null || key.trim().length() == 0) &#123; return; &#125; List&lt;ConfigCenterListener&gt; keyListeners = someKeyListenerMap.get(key); if (keyListeners != null &amp;&amp; keyListeners.size() &gt; 0) &#123; for(ConfigCenterListener listener : keyListeners) &#123; try &#123; listener.onChange(key, value); &#125; catch (Exception e) &#123; log.error(e.getMessage(), e); &#125; &#125; &#125; if (allKeyListeners.size() &gt; 0) &#123; for(ConfigCenterListener confListener : allKeyListeners) &#123; try &#123; confListener.onChange(key, value); &#125; catch (Exception e) &#123; log.error(e.getMessage(), e); &#125; &#125; &#125; &#125;&#125; 12345678public interface ConfigCenterListener &#123; /** * 配置的key有变化触发事件 * @param key * @param value */ void onChange(String key, String value) ;&#125; 我们再来看看我们的主动轮询服务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class LocalCacheService &#123; private static final Logger log= LoggerFactory.getLogger(LocalCacheService.class); //配置缓存信息map private static final ConcurrentHashMap&lt;String, CacheNodeVo&gt; LOCAL_CONFIG_CACHE_MAP = new ConcurrentHashMap&lt;&gt;(); //刷新线程状态 private static boolean refreshThreadStop = false; //一个单线程的线程池（刷新缓存使用） private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat("spring-boot-config-%d").setDaemon(true).build(); private static ExecutorService singleThreadPool = Executors.newFixedThreadPool(1, namedThreadFactory); //一分钟刷新一次 public static void init() &#123; // refresh thread singleThreadPool.submit(()-&gt;&#123; while (!refreshThreadStop) &#123; try &#123; TimeUnit.SECONDS.sleep(60); reloadAll(); log.debug("spring-boot-config, refresh thread reloadAll success."); &#125; catch (Exception e) &#123; log.error("spring-boot-config, refresh thread error."); log.error(e.getMessage(), e); &#125; &#125; log.info("spring-boot-config, refresh thread stopped."); &#125;); log.info("spring-boot-config LocalCacheService init success."); &#125; /** * 通过key获取值 * @param key * @return */ public static String get(String key) &#123; CacheNodeVo cacheNodeVo = LOCAL_CONFIG_CACHE_MAP.get(key); if (cacheNodeVo != null) &#123; return cacheNodeVo.getValue(); &#125; return null; &#125; /** * 放入值 * @param key * @param value */ public static void put(String key, String value) &#123; LOCAL_CONFIG_CACHE_MAP.put(key, new CacheNodeVo(key, value)); &#125; /** * 移除某个值 * @param key */ public static void remove(String key) &#123; LOCAL_CONFIG_CACHE_MAP.remove(key); &#125; /** * 重新加载全部 */ private static void reloadAll() &#123; Set&lt;String&gt; keySet = LOCAL_CONFIG_CACHE_MAP.keySet(); if (keySet.size() &gt; 1) &#123; for(String key : keySet) &#123; String zkValue = ZKConfigService.getKey(key); CacheNodeVo cacheNodeVo = LOCAL_CONFIG_CACHE_MAP.get(key); if (cacheNodeVo != null &amp;&amp; cacheNodeVo.getValue() != null &amp;&amp; cacheNodeVo.getValue().equals(zkValue)) &#123; log.debug("refresh key:&#123;&#125; no changed ", key); &#125; else &#123; LOCAL_CONFIG_CACHE_MAP.put(key, new CacheNodeVo(key, zkValue)); ConfigCenterListenerAdapter.onChange(key, zkValue); &#125; &#125; &#125; &#125;&#125; 可以看到借助了一个定长线程池，每隔60s重载一下数据，有变化会对监听者进行通知。它是通过一个静态的ConcurrentHashMap 来保存数据的。 这儿看到刚才的也会主动通知监听者，这儿也通知监听者，它们会通知两次吗？ 我们可以看到主动通知的时候，也会先把ConcurrentHashMap的值先改变在进行通知，要是出现通知两次的情况，会是概率极低的。要是要求只能通知一次，且业务监听无法重复处理两次数据变化请求，可以在向ConcurrentHashMap里放值时，再检查一下它的当前值，或使用其它方法处理。 两个服务ZK通知和主动轮询处理完成后，提供一个配置中心Client，用于获取值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ConfigCenterClient &#123; private static final Logger log= LoggerFactory.getLogger(ConfigCenterClient.class); static &#123; LocalCacheService.init(); ZKConfigService.init(); &#125; public static String getString(String key, String defaultValue) &#123; //查询本地缓存 String value = LocalCacheService.get(key); if (value != null) &#123; log.debug("get config &#123;&#125; from cache",key); return value; &#125; //没有命中，查询zk中的值，并加入到缓存中，并加watcher value = ZKConfigService.getKey(key); if (value != null) &#123; log.debug("get config &#123;&#125; from zookeeper",key); LocalCacheService.put(key,value); return value; &#125; return defaultValue; &#125; public static String getString(String key) &#123; return getString(key, null); &#125; private static void checkNull(String key,String value) &#123; if (value == null) &#123; throw new ConfigCenterException(String.format("config key [%s] does not exist",key)); &#125; &#125; public static long getLong(String key) &#123; String value = getString(key, null); checkNull(key, value); return Long.valueOf(value); &#125; public static int getInt(String key) &#123; String value = getString(key, null); checkNull(key, value); return Integer.valueOf(value); &#125; public static boolean getBoolean(String key) &#123; String value = getString(key, null); checkNull(key, value); return Boolean.valueOf(value); &#125; public static boolean addListener(String key, ConfigCenterListener configCenterListener)&#123; return ConfigCenterListenerAdapter.addListener(key, configCenterListener); &#125;&#125; 首先应加载LocalCacheService和ZKConfigService，然后实现主要的方法getString，直接去缓存里取，拿不到去Zookeeper里取并放到缓存里，在提供一个addListener方法，可以让用户自己定义想监听的节点。 至此，我们一个简单的配置中心的客户端就完成了，我们把它打包引入一个demo项目测试一下。 创建一个demo项目，引入我们的jar包。 12345&lt;dependency&gt; &lt;groupId&gt;com.zwt&lt;/groupId&gt; &lt;artifactId&gt;config-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 我们创建一个Listener实现。 1234567public class MyListener implements ConfigCenterListener &#123; private static final Logger log= LoggerFactory.getLogger(MyListener.class); @Override public void onChange(String key, String value) &#123; log.info(key+ " changed "+ value); &#125;&#125; 创建测试类，我们循环10次改变节点的值，测试一下我们的程序。 1234567891011121314151617@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; @Autowired CuratorZKClient client; @Test public void contextLoads() throws Exception&#123; //client.createNode("/config/test","Hello World"); ConfigCenterClient.addListener("test",new MyListener()); for(int i = 0;i&lt;10;i++)&#123; Thread.sleep(5000); String str = ConfigCenterClient.getString("test"); System.out.println(str); client.setData("/config/test","Hello World"+i); &#125; &#125;&#125; 可以看到执行结果。 我们还要实现一个配置中心的服务端。 服务端基本上是对Zookeeper数据节点的增删改查这几个逻辑，其核心是Zookeeper保存在节点上的数据。 为了方便对Zookeeper数据进行操作，我们一般创建一个可视化后台管理系统。如下： 这个系统是比较好实现的，引入我们的framework-zookeeper包，里面封装了Zookeeper的增删改查，当然需要创建一个web项目。 这一块就不再过多介绍了，当明白了Zookeeper的增删改查节点数据后，实现起来是比较容易的。 总结今天我们通过Zookeeper实现了一个配置中心，简单了解了它的原理，也对Zookeeper有了一些更深刻的理解。 现在很多开源的配置中心也相当的不错，也是可以学习和理解的，我后面可能也会讲解一些关于这方面的知识。 framework-zookeeper 和 config-spring-boot-starter 的相关代码已上传GitHub，大家如果有兴趣在实践中遇到问题可以过去参考下代码，如有疑问也欢迎与我交流探讨。 配置中心服务端（web项目）由于个人原因和时间原因，只写了个大概，也没有提交Github，后续应该会补上。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>配置中心</tag>
        <tag>ConfigCenter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql 使用Explain分析SQL语句及索引]]></title>
    <url>%2Fblog%2FMySql-%E4%BD%BF%E7%94%A8Explain%E5%88%86%E6%9E%90SQL%E8%AF%AD%E5%8F%A5%E5%8F%8A%E7%B4%A2%E5%BC%95.html</url>
    <content type="text"><![CDATA[前言我们经常会用到数据库，并可能写一些查询语句，大多数情况下，可能都是一些比较简单的增删改查语句。但某些时候，也可能遇到一些较复杂的语句。 在系统数据量不太大的情况下，可能我们SQL语句的好坏对系统性能并不会造成太大影响，但是随着系统数据量增大，数据库压力增大，除一些其它策略（使用缓存数据库、分库分表等等），良好的SQL语句也是提高数据库性能的一个重要指标。 SQL语句我们基本是按照，能精确查询就精确查询，能简化就简化，能使用索引就使用索引（必要条件下）的原则去编写。 我们今天通过Explain这个关键字来分析及优化我们的SQL语句。 正文那就先说下Explain吧。 打开数据库，随便找一张数据表。如下： 可以看到，当我们在我们写的SQL语句前加上Explain后，会得到如下的结果表。 PS：如果SQL语句关联其它表，会有多行结果，后续。 我们先来简单说下结果各个字段的含义。 id 暂且叫主键或者顺序吧，如果不止一次查询搜索（比如关联其它表），会按照顺序列出来。序号越大越先被执行。 select_type 查询类型，有下面几种： SIMPLE：简单SELECT(不使用UNION或子查询等) PRIMARY：最外面的SELECT UNION：UNION中的第二个或后面的SELECT语句 DEPENDENT UNION：UNION中的第二个或后面的SELECT语句，取决于外面的查询 UNION RESULT：UNION的结果。 SUBQUERY：子查询中的第一个SELECT DEPENDENT SUBQUERY：子查询中的第一个SELECT，取决于外面的查询 DERIVED：导出表的SELECT(FROM子句的子查询) table 这行数据和哪张表有关联。 partitions 匹配的哪个分区。 type 这列最重要，显示了连接使用了哪种类别，有无使用索引，是使用Explain命令分析性能瓶颈的关键项之一。 结果值从好到坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL 一般来说，得保证查询至少达到range级别，最好能达到ref，否则就可能会出现性能问题。 possible_keys MySQL可能使用哪个索引在该表中找到行。 key MySQL实际决定使用的键（索引）。如果没有选择索引，值是NULL。 key_len MySQL决定使用的索引长度。如果key是NULL，则key_len为NULL。在不损失精确性的情况下，长度越短越好。 ref 使用哪个列或常数与key一起从表中选择行。 rows MySQL认为它执行查询时必须检查的行数。正常情况下这个值越小越好。 Extra MySQL解决查询的详细信息，也是关键参考项之一。 Distinct：一旦MYSQL找到了与行相联合匹配的行，就不再搜索了。 Not exists：MYSQL 优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了。 Range checked for each Record：没有找到理想的索引，因此对于从前面表中来的每一个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。 Using filesort：MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行。出现这种情况应当优化SQL语句。 Using index：列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候。 Using temporary：说明MySQL需要新建一个临时表来存储结果，当生成的临时表较大时，应当优化SQL语句。 Using where：使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，就会出现这种情况。 Using union：使用了合并，有可能合并索引、表等。一般使用or连接条件时可能会出现。 我们创建一张学生表。 123456789CREATE TABLE `student` ( `id` int(32) NOT NULL AUTO_INCREMENT COMMENT '自增主键', `stu_no` varchar(20) DEFAULT NULL COMMENT '学生编号', `stu_name` varchar(50) DEFAULT NULL COMMENT '学生姓名', `stu_sex` char(2) DEFAULT NULL, `stu_birthday` date DEFAULT NULL COMMENT '学生生日', `stu_class` char(2) DEFAULT NULL COMMENT '学生所在班级', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=50077 DEFAULT CHARSET=utf8; 向表中加入100w数据。（过程略） 关于索引我们统计5班学生的学生数量。 可以看到耗时2s之久。 使用Explain分析可以看到mysql需要检查994511行才能得到结果，已经相当于全表扫描了。Type是ALL，为最差的一种。 我们在stu_class班级这一列上加上索引。 继续查询及分析，可以看到耗时0.07s。 Explain分析可知type为ref满足我们要求，mysql执行这个查询认为只要扫描191502条数据即可，使用了index_1索引。 是不是感觉到索引很厉害？ 我们再来看一下。 这回我们查询5年级所有学生的学生名。如下： 先不使用索引，可以看到，耗时2s左右。 我们给年级加上索引，继续测试。如下： 可以看到耗时竟达到5s左右。 为什么会这样呢？ 一般我们认为，这种属于很特殊的情况，一般在硬盘上，数据是连续存储的，但是引入索引后，数据连续性变差，随机波动性要大，在数据量大的情况下更是。 这种情况下，我们读取大量数据（99497条已接近1/10数据总量），由于随机性变大，故读取时间会变大。因此索引在查询一些小数据量数据的情况下效果明显，或者索引带来的优化已经完全超过了数据读取时间。 关于上面，我们可以看下有索引和无索引SQL执行情况。 有索引情况下: 无索引情况下: 可以看到它们明显的差距就在返回数据这儿了，有索引情况下读取数据浪费了太多时间。 关于索引带来的优化已经完全超过了数据读取时间这个，我们可以看下这个例子。 我们在添加一条姓名索引。 我们查询5班所有姓张的童鞋。 有索引情况下: 无索引情况下: 明显的有索引的完爆无索引查询时间啦，这里就不在具体分析了。 我们可以看下这儿的like，可以直接看下面的图。 明显得到的结论就是当有like查询时，%或者_在前面的是无法使用当前字段索引的（如果有），会进行全表扫描，这一点需要注意。 SQL优化我们接下来分析一些SQL的优化。 1.我想查询生日在1998年至2010年之间的所有学生数量。（生日已建立索引index_3） 可以这样，如下图。 Explain分析仅为index级别，虽然用到了索引index_3。 如何优化下呢？ 我们可以这样处理： 通过Explain分析type也达到了range级别。 PS： 通过上面例子，我们知道我们应该尽量减少对字段的计算操作，以便使其可以正常利用索引，也就是尽量使条件数据与字段数据去匹配，而不是对字段数据进行操作后去匹配条件数据。 2.我想查询生日在98年后且是5班的且姓张的童鞋。 有生日索引index_3，姓名索引index_2，班级索引index_1。 运行上述SQL后我们看到type级别为range，还可以，但是却只用到了index_2索引，时间还可以0.006s。 还有好一点的方法吗？ 我们试着删除上面三个索引，创建一个联合索引。如下： 进行测试。 可以看到时间有提高（反复测试是比单索引要好的），type级别还是range。 PS：联合索引只有在一些特定的情况下能发挥很好的作用，一般单索引可以满足大多需求，所以联合索引的创建是需要慎重的。 联合索引有一些特点需要注意，以免被坑。对于上面的联合索引，index_1(stu_name, stu_birthday, stu_class)，遵循最左原则。 最左原则可以看下面7个例子。 例子1:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01'; Explain分析结果如下： 例子2:1EXPLAIN select * from student s where s.stu_name like '张%'; Explain分析结果如下： 例子3:1EXPLAIN select * from student s where s.stu_class = '5'; Explain分析结果如下： 例子4:1EXPLAIN select * from student s where s.stu_class = '5' and s.stu_birthday &gt;= '1998-01-01'; Explain分析结果如下： 例子5:1EXPLAIN select * from student s where s.stu_class = '5' and s.stu_name like '张%'; Explain分析结果如下： 例子6:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01' and s.stu_name like '张%'; Explain分析结果如下： 例子7:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01' and s.stu_name like '张%' and s.stu_class = '5'; Explain分析结果如下： 可以看到，可以使用索引的查询有 （stu_name）（stu_name, stu_birthday）（stu_name, stu_birthday, stu_class）（stu_name, stu_class），也就是必须有stu_name这个条件。这就是联合索引的最左原则。 3.我们统计姓名相同的学生的学生名及数量。1select s.stu_name,count(1) from student s GROUP BY s.stu_name HAVING count(1)&gt;1 ORDER BY s.stu_name; 这儿就不在贴图了，当stu_name有索引时，查询效率会有明显提升。 在有索引Explain分析后，我们可以看到，Mysql认为这个查询需要检索全部数据（rows 99万）。 这是Mysql认为要检索的行数，实际由于stu_name有索引，是没有检索这么多的。 因为查询语句中出现了group by，对于某张表，正常情况的的认知肯定是检索完全表后才能进行分组归类。所以，在一些经常被group by 或者 order by 的字段上，数据量较大的情况下，且该字段比较多样性（比如性别这种字段顶多有男、女、未知三种，加不加索引都可以），向表中常用的创建时间、修改时间等字段，还是有必要添加索引的。 4.查询所有学生生日，性别，班级相同的学生数量。 我们假设建立了index_1(stu_birthday)索引，index_2(stu_sex)索引，index_3(stu_class)索引。 你可能会想到如下SQL。1select count(1) from student a,(select stu_birthday,stu_class,stu_sex from student) b where a.stu_birthday = b.stu_birthday and a.stu_class=b.stu_class and a.stu_sex=b.stu_sex; 嗯，这个SQL是不正确的，而且耗时无法忍受。 其实不用Explain，单独看它就可以知道它会扫描全表，无论有无索引。而且我们统计学生数量，如果有2个以上情况相同呢？显然上面SQL的3个等于条件会出现问题。 其实我们可以根据分组数量来处理这个问题，如下：1select count(1) from student s ,(select a.stu_birthday,a.stu_sex,a.stu_class from student a GROUP BY a.stu_birthday,a.stu_sex,a.stu_class HAVING count(1)&gt;1) b where s.stu_birthday=b.stu_birthday and s.stu_sex=b.stu_sex and s.stu_class=b.stu_class; 通过学生生日，性别，班级相同，他们的数量要大于1，从而进行数量统计。 耗时在8s左右。我们Explain分析下： 先从id为2的看起，生成了一个临时表a，但一个索引也没用到，向上看它用了学生生日，性别，班级这三个临时表的值做了auto_key（可以理解为临时索引），然后对s表全表扫描得到结果。 那我们的三个索引岂不是创建的是毫无意义的…… 我们可以将它的auto_key变成一个联合索引，index_4(stu_birthday, stu_sex, stu_class)。 这时候查询时间减少到5s左右。 可以看到使用到了index_4，且Extra字段原来使用Using temporary现在变为了Using index。 总结好了，基本上就这些内容，其实也有想写一个多表联查复杂的SQL语句的优化，但是无奈例子比较难找（自己不想再建新表，再插入数据，再想例子），在这儿就不在举例了。 其实实际中SQL语句的编写，复杂的还是比较少的（毕竟有程序结合，一个不行可以拆开嘛）。 这一节主要的内容其实是讲Explain这个关键字的，实际中写完复杂SQL语句可以用它分析下SQL执行情况。 上面也看到了有些索引不是建了就会减少查询时间的，索引的选择还是要认真对待，处理好能提升不少查询效率，另外索引创建也应找到问题的关键点。比如该字段经常被group by，order by ，该字段业务中经常使用等等。 使用Explain可以帮我们分析到索引的利用情况。 另外，Mysql除BTree索引外，还有一种Hash索引。 BTree索引具有范围查找和前缀查找的能力，一个N节点的BTree，检索一条记录时间复杂度为O(LogN)，相当于二分查找。 Hash索引只能做等值查找，但查找时间复杂度为O(1)。 如果值的差异性大，并且以等值查找（=、 &lt;、&gt;、in）为主，Hash索引是更好的选择。如果值的差异性相对较差，并且以范围查找为主，BTree是更好的选择。 一般使用BTree索引的场合较多。 结语结束之前，我们可以看看索引的一些失效场景。 索引列查询出现 is not null 情况，会进行全表扫描不走索引。 列类型是字符串，数据需要引号引起来，否则会进行全表扫描不走索引。 模糊查询like的各种情况，不再详述。 联合索引的各种情况，不再详述。 or的情况，当用or连接条件时，应保证条件都有索引才能使用索引，否则索引无效。 可以看到由于stu_name没有索引，故进行了全表扫描。 stu_sex和stu_birthday均有索引，使用了索引。 如果MySQL觉得全表扫描要比使用索引要快，它也不会使用索引。 如下：由于stu_sex=0 或者 stu_class =1 数据很多，且都要返回，故没有使用它们的索引。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>Explain</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的2018年度总结]]></title>
    <url>%2Fblog%2F%E6%88%91%E7%9A%842018%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[我的2018年度总结本来计划月底写的，可是啊，看到有时力不从心的自己，还是决定提前写了吧。 写这个文章，算是对自己今年一整年的总结吧。 怎么说呢？ 先整体上总结一下吧，感觉自己比17年状态要好，懂得了时间的宝贵吧，自制力上也有些许提高。 语无伦次了呢…… 哎，谁让我是个偏内向的男孩啊。啊啊啊啊啊啊啊，老男人，哈哈。 不知道3、5年后，在看到这篇文章，会想到些什么呢？ 如果文章还在的话…… 不扯那么多矫情的话了…… 我的2018年度总结开篇吧。 先说下自己的一些目标及愿望吧，我比较喜欢列清单的方式。 主要也是由于制定的比较晚吧，可能目标/愿望较少，不过也基本上能代表2018年的一些自己主要的计划/想法了。 总的来说一些目标也太模糊，一些目标也很随意，毕竟第一年制定这么个清单。 比如坚持锻炼，可能那时定个坚持锻炼7天，坚持15天……一点点来，自己就不会太迷茫，开始就感觉没戏一样。 而且总感觉对自己下不去狠心，惩罚写的很潦草，奖励写的不现实。 PS：所以这儿就不贴上面给大家看啦，哈哈。 PS：而且还有一些比较个人隐私的目标和愿望没写到上面。 PS：如果你看到这篇文章，也可以自己制定个计划List，不一定要一年，一个月，半年也是可以的。有的时候越长远，反而越不容易实现。 人都是有惰性的嘛。 总会越来越好的。 2018年的自己，怎么说呢？ 也自信也迷茫，也努力也彷徨，也欢喜也哀伤，也年轻也沧桑。 感觉自己也是蛮幸运的吧。 也受到过不少负面的影响。 有时甚至无聊到想探寻人类的本质…… 有人说，出生那一刻，人命就定了，曾经年少轻狂，以为自己可以拯救世界，却发现世界根本不在乎你…… 哈哈，越长大越“认命”吗？ 罗曼·罗兰有句话说的好啊。 世界上只有一种真正的英雄主义，那就是在认清生活的真相后依然热爱生活。 对于人生，我觉得，或许会认命，但绝不会认输。 好吧，我自己这都是写了啥啊…… 哎，跑题了…… 就这样吧，就这样吧。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper Java客户端Curator]]></title>
    <url>%2Fblog%2FZookeeper-Java%E5%AE%A2%E6%88%B7%E7%AB%AFCurator.html</url>
    <content type="text"><![CDATA[前言我们今天来讲讲Zookeeper Java客户端的一些使用吧。 之前我们说到Zookeeper的安装及简单使用，要介绍它的一些应用场景，要明白它的应用场景，要先理解它客户端的一些操作方法。 Zookeeper的Java客户端，最常使用的便是Apache Curator了，它是Netflix公司开源的一个Zookeeper客户端，与Zookeeper提供的原生客户端ZooKeeper相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量，而且Curator的功能更加强大。 正文要使用Curator客户端，需要下面的两个依赖。 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; curator-recipes包一般能满足我们的需要，要是封装更简便的底层功能的话，curator-framework包必不可少。 创建并启动客户端使用程序创建一个客户端client并启动（连接到Zookeeper）。 Builder模式创建一个客户端。 1234567CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder() .connectString(zkConfig.getConnectString()) .namespace(zkConfig.getNameSpace()) .retryPolicy(new ExponentialBackoffRetry(zkConfig.getRetryInterval(), Integer.MAX_VALUE)) .connectionTimeoutMs(zkConfig.getConnectTimeoutMs()) .sessionTimeoutMs(zkConfig.getSessionTimeoutMs()); client = builder.build(); 客户端启动。 1client.start(); 简单说下连接时的参数: connectString：服务器列表，逗号隔开（host1:port1,host2:port2……） namespace：命名空间，可以用来进行业务区分。 retryPolicy：重试策略,有以下4种重试策略，也可以自己实现重试策略（实现RetryPolicy接口）。RetryOneTime：重试一次。 RetryNTimes：重试N次（需要传入重试间隔参数sleepMsBetweenRetries，及尝试次数n），它继承了抽象类SleepingRetry（每休眠一段时间重试一次）。 RetryForever：一直重试（需要传入重试间隔retryIntervalMs参数）。 BoundedExponentialBackoffRetry：重试次数固定，但每次重试的时间间隔会不断变大（如果一直连不上），需要传入初始等待重试时间baseSleepTimeMs，重试次数maxRetries，及最大等待重试时间maxSleepTimeMs 参数，这个类继承ExponentialBackoffRetry（它又继承SleepingRetry）抽象类。 sessionTimeoutMs：会话超时时间，单位毫秒，默认60000ms。 connectionTimeoutMs：连接创建超时时间，单位毫秒，默认60000ms。 创建数据节点。Zookeeper节点有4种，上篇文章已介绍。 创建持久化节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path, data); 使用create方法，creatingParentsIfNeeded这个方法保证如果有父节点也会一起创建，这在原生客户端是无法实现的。CreateMode 有4种，跟Zookeeper的节点类型对应。forPath方法可以认为最终操作，path表示节点路径，data表示节点数据。data是byte数组，其它类型的数据应转换为byte数组。 注：如果不设置withMode方法，默认创建持久化节点，不设置data，节点默认内容为空。 如下： 1client.create().forPath(path); 创建顺序节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(path, data); 创建临时节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(path, data); 创建临时顺序节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(path, data); 设置节点数据1client.setData().forPath(path, bytes); 直接调用setData方法即可，返回一个Stat（节点信息类）。 获取节点数据1client.getData().forPath(path); 使用getData方法，返回byte数组。 获取子节点1client.getChildren().forPath(path); 使用getChildren方法，返回一个子节点List &lt;String&gt; 列表，数据为各个子节点名称。 删除节点1client.delete().guaranteed().forPath(path); 使用delete方法，guaranteed方法可以保证一定删除。如果某个节点删除失败，会抛出异常，但是如果使用了guaranteed，它会在后台继续进行删除直到删除成功。 删除节点（包括子节点）1client.delete().guaranteed().deletingChildrenIfNeeded().forPath(path); deletingChildrenIfNeeded方法可以保证如果有子节点的话一并删除，原生client是无法实现此功能的（需要我们写方法处理）。 判断节点是否存在1234567891011public boolean checkNodeExist(String path) &#123; boolean exits = false; try &#123; Stat stat = client.checkExists().forPath(path); if (stat != null) &#123; exits = true; &#125; &#125; catch (Exception e) &#123; &#125; return exits;&#125; 使用checkExists方法，最终返回一个Stat，如果Stat为空就说明不存在。 PS：由此我们可以创建一个 createOrUpdate方法，无节点时创建，有节点时更新内容。 1234567891011public void createOrUpdateNode(String path, byte[] data) &#123; try &#123; if(checkNodeExist(path))&#123; setData(path,data); &#125;else&#123; createNode(path, data); &#125; &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125;&#125; 异步处理上面的操作方法，都可以使用异步进行处理的，主要使用了inBackground方法。 如下： 1client.create().inBackground().forPath(path, data); 该方法全参函数如下，且重载了多个方法。 1public T inBackground(BackgroundCallback callback, Object context, Executor executor); BackgroundCallback callback：异步回调函数，处理完成后会回调此函数进行某些逻辑。 Object context：上下文对象。 Executor executor：异步处理的线程，不指定的话将使用内部默认线程处理。 我们可以看下BackgroundCallback 方法 会有两个参数。 123public interface BackgroundCallback &#123; void processResult(CuratorFramework var1, CuratorEvent var2) throws Exception;&#125; 第二个参数CuratorEvent里面包含了此次处理结果的所有信息，包括节点信息等。 123456789101112public interface CuratorEvent &#123; CuratorEventType getType(); int getResultCode(); String getPath(); Object getContext(); Stat getStat(); byte[] getData(); String getName(); List&lt;String&gt; getChildren(); List&lt;ACL&gt; getACLList(); WatchedEvent getWatchedEvent();&#125; CuratorEventType表示事件类型，表示此次操作的事件类型。可以看到它与CuratorFramework里的方法是一一对应的。 getResultCode返回处理结果码。可以在这个枚举里查看各个状态码。 添加watcher12client.getData().usingWatcher(watcher).forPath(path);client.getChildren().usingWatcher(watcher).forPath(path); 使用usingWatcher结合getData或者getChildren方法可以为指定节点或者子节点添加watcher。 Watcher可以为CuratorWatcher或者Zookeeper自带的Watcher。它们有一个event参数。 可以拿到Zookeeper的状态 KeeperState和 事件类型 EventType，从而进行某些必要的操作。 KeeperState枚举和EventType枚举如下图。 事务支持Zookeeper一些操作是支持事务的。 主要用到的方法有inTransaction、and、commit等方法。举例如下： 123456789101112131415161718192021222324252627282930/** * 开启事务 */public CuratorTransaction startTransaction() &#123; return client.inTransaction();&#125;/** * 事务中添加create操作 */public CuratorTransactionFinal addCreateToTransaction(CuratorTransaction transaction, String path) throws Exception &#123; return transaction.create().forPath(path, new byte[0]).and();&#125;/** * 事务中添加delete操作 */public CuratorTransactionFinal addDeleteToTransaction(CuratorTransaction transaction, String path) throws Exception &#123; return transaction.delete().forPath(path).and();&#125;/** * 事务中添加seData操作 */public CuratorTransactionFinal addSetDataToTransaction(CuratorTransaction transaction, String path, byte[] data) throws Exception &#123; return transaction.setData().forPath(path, data).and();&#125;/** * 提交事务 */public Collection&lt;CuratorTransactionResult&gt; commitTransaction(CuratorTransactionFinal transaction) throws Exception &#123; return transaction.commit();&#125; 检查连接情况1client.getZookeeperClient().isConnected(); 关闭连接1client.close(); 提升对Curator客户端有简单理解后，我们把它进行简单功能的封装。 PS：Curator的强大之处在于其增强功能部分，我们会在后面结合Zookeeper应用讨论。 创建项目framework-zookeeper，搭建如下结构： 接口ZKClient，里面有一些Zookeeper客户端的协议，大致如下： 12345678910public interface ZKClient &#123; void start();//启动 boolean isConnected();//连接情况 void close();//关闭 void createNode(String path, byte[] data);//创建永久节点 void createOrUpdateNode(String path, byte[] data); //创建或者更新节点 void createEphemeralNode(String path, byte[] data); String createSequenceNode(String path) ; ......&#125; CuratorZKClient是Curator对接口ZKClient的实现，BaseZKClient是原生客户端对接口ZKClient的实现。 我们来写下CuratorZKClient的一些关键代码。 12345678910111213public class CuratorZKClient implements ZKClient &#123; //zk客户端 private CuratorFramework client; //关闭状态 private volatile boolean closed = false; //zk配置 private ZKConfig zkConfig; //连接状态监听 private Set&lt;ConnectionListener&gt; connectionListeners = new CopyOnWriteArraySet&lt;&gt;(); //节点变化监听及相关Watcher private final ConcurrentMap&lt;String, ConcurrentMap&lt;NodeListener, CuratorWatcher&gt;&gt; nodeListeners = new ConcurrentHashMap&lt;&gt;();......&#125; 提供一个连接监听接口，以便我们可以监听Zookeeper的连接状态并且执行某些操作。 123public interface ConnectionListener &#123; void stateChanged(ZKConstants.ConnectState state);&#125; 及节点变化接口，监测节点变化进行某些操作。 12345public interface NodeListener &#123; void nodeChanged(String path, List&lt;String&gt; nodes);//节点改变 void nodeDelete(String path);//节点删除 void dataChanged(String path, byte[] data);//节点数据改变&#125; ZKConfig是Zookeeper客户端连接的配置，属性值可以配置在properties等配置文件里。 12345678public class ZKConfig &#123; private String connectString; private String nameSpace; private int retryInterval = 1000; private int connectTimeoutMs = 60000; private int sessionTimeoutMs = 60000; ......&#125; CuratorZKClient 里实现接口的start方法是，部分代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940...... public void start() &#123; ....... CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder() .connectString(zkConfig.getConnectString()) .namespace(zkConfig.getNameSpace()) //重试指定的次数, 且每一次重试之间停顿的时间逐渐增加 .retryPolicy(new ExponentialBackoffRetry(zkConfig.getRetryInterval(), Integer.MAX_VALUE)) .connectionTimeoutMs(zkConfig.getConnectTimeoutMs()) .sessionTimeoutMs(zkConfig.getSessionTimeoutMs()); client = builder.build(); //添加Zookeeper状态监听 client.getConnectionStateListenable().addListener(new ConnectionStateListener() &#123; @Override public void stateChanged(CuratorFramework client, ConnectionState zkConnectionState) &#123; ZKConstants.ConnectState state = toConnectionListenerState(zkConnectionState); if (state != null) &#123; for(ConnectionListener connectionListener : connectionListeners) &#123; connectionListener.stateChanged(state); &#125; &#125; &#125; private ZKConstants.ConnectState toConnectionListenerState(ConnectionState zkConnectionState) &#123; switch (zkConnectionState) &#123; case LOST: return ZKConstants.ConnectState.DISCONNECTED; case SUSPENDED: return ZKConstants.ConnectState.DISCONNECTED; case CONNECTED: return ZKConstants.ConnectState.CONNECTED; case RECONNECTED: return ZKConstants.ConnectState.RECONNECTED; default: return null; &#125; &#125; &#125;); client.start(); &#125;..... 这样我们暴露了Zookeeper的连接状态监听接口，以后想监听它的连接状态进行某些操作，直接实现接口，并通过addConnectionListener添加进来即可。 12345678@Overridepublic void addConnectionListener(ConnectionListener listener) &#123; connectionListeners.add(listener); &#125;@Overridepublic void removeConnectionListener(ConnectionListener listener) &#123; connectionListeners.remove(listener);&#125; 其它的方法，比如createNode、deleteNode等，我们拿到client后，按照上面讲述的各个操作便可以写出代码，这里不再赘述。以下是createNode的例子。 12345678910@Overridepublic void createNode(String path, byte[] data) &#123; try &#123; client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path, data); &#125; catch (KeeperException.NodeExistsException e) &#123; log.warn(String.format("create node is exist:%s", path)); &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125;&#125; 再说一下需要实现的NodeListener方法，节点发生变化，主要通过watcher通知。实现一个watcher。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758private class CuratorWatcherImpl implements CuratorWatcher &#123; private volatile NodeListener listener; public CuratorWatcherImpl(NodeListener listener) &#123; this.listener = listener; &#125; public void unWatch() &#123; this.listener = null; &#125; @Override public void process(WatchedEvent event) throws Exception &#123; if (listener != null) &#123; log.debug(event.getPath() + " with event " + event.getType()); switch (event.getType()) &#123; //节点数据变化，调用listener指定方法 case NodeDataChanged: try &#123; byte[] data = client.getData().usingWatcher(this).forPath(event.getPath()); log.debug(event.getPath() + " data after change: " + new String(data)); listener.dataChanged(event.getPath(), data); &#125; catch (Exception e) &#123; log.warn(e.getMessage(), e); &#125; break; //节点删除 case NodeDeleted: //节点创建 case NodeCreated: log.error(event.getPath()); //自己点改变 case NodeChildrenChanged: try &#123; if (event.getType().equals(Watcher.Event.EventType.NodeDeleted)) &#123; listener.nodeDelete(event.getPath()); &#125; else &#123; List&lt;String&gt; nodes = getNodes(event.getPath()); if (nodes != null) &#123; client.getChildren().usingWatcher(this).forPath(event.getPath()); &#125; //监控子节点数据变化 //for(String node : nodes) &#123; // client.getData().usingWatcher(this).forPath( ZKPathMgr.joinPath(event.getPath(),node)); //&#125; log.debug(event.getPath() + " nodes after change: " + nodes); listener.nodeChanged(event.getPath(), nodes); &#125; &#125; catch (KeeperException.NoNodeException e) &#123; log.warn(e.getMessage()); &#125; catch (Exception e) &#123; log.warn(e.getMessage(), e); &#125; break; case None: default: break; &#125; &#125; &#125; &#125; 然后实现NodeListener的添加移除。 123456789101112131415161718192021222324252627282930313233343536@Override public void addNodeListener(String path, NodeListener listener) &#123; ConcurrentMap&lt;NodeListener, CuratorWatcher&gt; listeners = nodeListeners.get(path); if (listeners == null) &#123; nodeListeners.putIfAbsent(path, new ConcurrentHashMap&lt;NodeListener, CuratorWatcher&gt;()); listeners = nodeListeners.get(path); &#125; CuratorWatcher watcher = listeners.get(listener); if (watcher == null) &#123; listeners.putIfAbsent(listener, new CuratorWatcherImpl(listener)); watcher = listeners.get(listener); &#125; addChildrenCuratorWatcher(path, watcher); &#125; @Override public void removeNodeListener(String path, NodeListener listener) &#123; ConcurrentMap&lt;NodeListener, CuratorWatcher&gt; listeners = nodeListeners.get(path); if (listeners != null) &#123; CuratorWatcher watcher = listeners.remove(listener); if (watcher != null) &#123; ((CuratorWatcherImpl) watcher).unWatch(); &#125; &#125; &#125; //添加目录watcher private void addChildrenCuratorWatcher(final String path, CuratorWatcher watcher) &#123; try &#123; client.getData().usingWatcher(watcher).forPath(path); client.getChildren().usingWatcher(watcher).forPath(path); &#125; catch (KeeperException.NoNodeException e) &#123; log.warn(String.format("add watcher node not exist:%s", path)); &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125; &#125; 可以看到主要是是维护一个ConcurrentHashMap，listener为key，watcher为value，节点有变化，通知到listener。 好。到这里基本上一个Zookeeper工具客户端就OK了，BaseZKClient的实现与CuratorZKClient类似，有兴趣的可以自己看看。 测试我们测试下我们的代码。 12345678910111213141516171819public static void main(String[] args) throws Exception&#123; ZKConfig config = new ZKConfig(); config.setConnectString("127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"); config.setConnectTimeoutMs(60000); config.setNameSpace("zwt"); config.setRetryInterval(1000); config.setSessionTimeoutMs(60000); ZKClient client = new CuratorZKClient(config); client.addConnectionListener(new ConnectionListener() &#123; @Override public void stateChanged(ZKConstants.ConnectState state) &#123; System.out.println("ZKState state "+ state.name()); &#125; &#125;); client.start(); client.createNode("/mytest","Hello World"); System.out.println(new String(client.getData("/mytest"),"UTF-8")); client.close();&#125; 运行可以看到输出： 我们使用命令行也可以看到我们新增的test节点及其属性。 PS：可以看到nameSpace 业务命名空间相当于新增一个根节点以区分不同业务，避免节点冲突等作用。 我们在client启动后添加watcher。 12345678910111213141516171819202122......client.start();client.addNodeListener("/mytest", new NodeListener() &#123; @Override public void nodeChanged(String path, List&lt;String&gt; nodes) &#123; System.out.println(path+" node changed"); &#125; @Override public void nodeDelete(String path) &#123; System.out.println(path+" node delete"); &#125; @Override public void dataChanged(String path, byte[] data) &#123; System.out.println(path+" data changed "+ data); &#125;&#125;);System.out.println(new String(client.getData("/mytest"),"UTF-8"));client.setData("/mytest","World");System.out.println(new String(client.getData("/mytest"),"UTF-8"));client.createNode("/mytest/test");client.deleteNodeWithChildren("/mytest");...... 继续测试，结果如下。 到这里，我们基本把客户端操作的基本说完了。关于其它一些Zookeeper客户端，这里就不在过多介绍了，有兴趣的可以继续实现ZKClient接口去完成。 PS：BaseZKClient类是我写的一个原生Zookeeper客户端的集成工具，但有些小问题未处理。 总结通过使用Zookeeper客户端的一些例子，更对Zookeeper有了更深入的了解。 下面的文章我们将结合Curator的一些高级功能及Zookeeper的一些应用来了解Zookeeper的强大之处。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>Curator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper初探]]></title>
    <url>%2Fblog%2FZookeeper%E5%88%9D%E6%8E%A2.html</url>
    <content type="text"><![CDATA[前言前不久研究了下Zookeeper，感觉挺不错的，这几天把成果总结下，方便记忆。 今天先简单介绍下它的安装使用和一些特性吧。 ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 正文Zookeeper的安装与配置首先需要下载安装包。 Zookeeper官网地址 点击这里 我们可以通过镜像的方式将压缩包下载下来。 解压到合适目录，进入到zookeeper目录下的conf目录，需要新建zoo.cfg文件，可以看到里面有一个zoo_sample.cfg文件，可以作为我们的配置参考。 我们来看下它里面的一些常用参数： tickTime=2000 指Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。单位毫秒 initLimit=10 此配置表示，允许 follower（相对于 leader 而言）连接并同步到 leader 的初始化连接时间，它以 tickTime 的倍数来表示。当超过设置倍数的 tickTime 时间，则连接失败。 syncLimit=5 此配置表示，leader 与 follower 之间发送消息，请求和应答时间长度。如果 follower 在设置的时间内不能与leader 进行通信，那么此 follower 将被丢弃。 dataDir=D:/zookeeper1/zookeeper/data 数据存储快照的地址 dataLogDir=D:/zookeeper1/zookeeper/log 数据库事务日志地址，如果不设置此项，默认dataDir的地址 clientPort=2181 供客户端连接的端口，对外提供的端口 server.10=127.0.0.1:2888:3888 集群模式下需要的配置（单机不需要） server.A = B:C:D A：表示这是第几号服务器 B：服务器的 IP 地址 C：集群内机器通讯使用（Leader监听此端口） D：一旦集群中的 Leader 服务器挂了，需要一个端口重新进行选举，选出一个新的 Leader Zookeeper的启动和使用看完上面配置后，我们先配置单机zookeeper对其简单使用。 进入到zookeeper bin目录下，我们可以看到一些脚本，分为cmd脚本和sh脚本。 zkCleanup 清理Zookeeper历史数据脚本，包括日志文件和快照数据文件 zkCli Zookeeper的客户端脚本 zkEnv 设置Zookeeper的环境变量脚本 zkServer Zookeeper服务器的启动、停止、和重启脚本 启动ZookeeperWindows操作： 进入到bin目录下，执行如下命令： zkServer 可以启动zookeeper服务。（需要注意windows下dataDir路径支持/和\\，但使用\会出现问题。） 下图是我公司电脑（Windows）上的zookeeper启动图。 Linux/Mac/Unix操作： 进入到bin目录下，执行如下命令： ./zkServer.sh start 可以启动zookeeper服务。 停止ZookeeperWindows操作： 直接可以Ctrl+C结束命令。 Linux/Mac/Unix操作： ./zkServer.sh stop PS:杀进程也可以结束，不推荐 连接ZookeeperWindows操作： 进入到bin目录下，执行如下命令： zkCli -server localhost:2181 可以连接到zookeeper服务。 下图是Windows电脑上连接到zookeeper服务的图。 Linux/Mac/Unix操作： 进入到bin目录下，执行如下命令： ./zkCli.sh -server localhost:2181 可以连接到zookeeper服务。 Zookeeper常用命令在zookeeper客户端，使用help可以查看可以使用的命令。 我们说几个比较常用的命令。 创建节点： create [-s] [-e] path data acl 使用此命令可以创建一个zookeeper节点。 其中，-s或-e分别指定节点特性，顺序或临时节点，若不指定，则表示持久节点；acl用来进行权限控制。 create -s /zk-test test 表示创建顺序节点zk-test。 可以看到创建的zk-test节点后面添加了一串数字以示区别顺序。 create -e /zk-temp test 表示创建临时节点。临时节点在客户端断开连接后会自动删除。 可以看到创建临时节点重新登陆后节点已被删除。 create /zk-permanent test 创建永久节点zk-permanent。 读取节点： ls path [watch] 可以列出节点下面的所有子节点。 get path [watch] 可以获取根节点数据内容和属性信息。 ls2 path [watch] 列出节点信息。 如下图： 更新节点： set path data [version] 我们将zk-permanent节点下的数据test变为test1。 可以看到dataVersion也发生了变化，用来表示已经进行了更新。 删除节点： delete path [version] 我们删除刚才创建的zk-permanent节点。 可以看到节点已经被删除。 PS：若删除节点存在子节点，那么无法删除该节点，必须先删除子节点，再删除父节点。 Zookeeper部署模式Zookeeper的部署模式一般分为3种，单机模式、伪集群模式、集群模式。 单机模式 单个Zookeeper服务，如上面所述。 伪集群模式 所谓伪集群, 是指在单台机器中启动多个zookeeper进程, 并组成一个集群。 以3个zookeeper服务进程为例。 需要有3个zookeeper项目。 三个项目配置特殊的地方： zookeeper1配置：1234567dataDir=D:/zookeeper1/zookeeper/datadataLogDir=D:/zookeeper1/zookeeper/logclientPort=2181server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 1 zookeeper2配置：1234567dataDir=D:/zookeeper2/zookeeper/datadataLogDir=D:/zookeeper2/zookeeper/logclientPort=2182server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 2 zookeeper3配置：1234567dataDir=D:/zookeeper3/zookeeper/datadataLogDir=D:/zookeeper3/zookeeper/logclientPort=2183server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 3 在同一台机器上，3者的端口号不能相同，不然出现端口冲突。 集群模式 集群模式下配置和伪集群相似。 服务器1配置：123456789tickTime=2000initLimit=10syncLimit=5dataDir=D:/zookeeper1/zookeeper/datadataLogDir=D:/zookeeper1/zookeeper/logclientPort=2181server.1=10.1.100.1:2888:3888server.2=10.1.100.2:2888:3888server.3=10.1.100.3:2888:3888 由于不在一个服务器上，故服务器2，服务器3也如上面的配置即可，它们3个唯一不同的地方是dataDir文件夹里面的myid文件，代表着服务器各自的编号。 Zookeeper特性由上面内容，我们汇总说下Zookeeper的一些性质。 类文件系统（数据节点） Zookeeper有类似于Unix文件系统的节点。它可以自由创建、修改和删除子目录，这些目录项被称为znode，znode可以存储数据。 Zookeeper有4种类型的znode： 1. PERSISTENT 持久化目录节点，客户端与zookeeper断开连接后，该节点依旧存在 2. PERSISTENT_SEQUENTIAL 持久化顺序编号目录节点，客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 3. EPHEMERAL 临时目录节点，客户端与zookeeper断开连接后，该节点被删除 4. EPHEMERAL_SEQUENTIAL 临时顺序编号目录节点，客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 通知机制 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。这是非常重要的一条性质。 高可用性 zookeeper的高可用性通过zookeeper集群实现。 Zookeeper一般有三种集群角色： Leader（领导者） Follower（跟随者） Observer（观察者） 一个 ZooKeeper 集群同一时刻只会有一个 Leader，其他都是 Follower 或 Observer。 ZooKeeper 默认只有 Leader 和 Follower 两种角色，没有 Observer 角色。为了使用 Observer 模式，在任何想变成Observer的节点的配置文件中加入 peerType=observer 并在所有 server 的配置文件中，配置成 observer 模式的 server 的那行配置追加 :observer即可。 ZooKeeper 集群的所有机器通过一个 Leader 选举过程来选定一台机器作为Leader，Leader服务器为客户端提供读和写服务。Follower 和 Observer 都能提供读服务，不能提供写服务。两者唯一的区别在于，Observer机器不参与 Leader 选举过程，也不参与写操作的过半写成功策略，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。 我们简单来看下： 我在自己的Mac机器上创建了4个zookeeper服务并启动（伪集群），其中一个是Observer，另三个其中一个是Leader，其它两个Follower。（3台机器中谁是Leader是选举出来的，一般情况下先启动的会成为Leader）。 配置如下(关键配置)：12345678910111213141516171819202122232425262728# zookeeper1的配置，myid文件里值为1clientPort=2181server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper2的配置，myid文件里值为2clientPort=2182server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper3的配置，myid文件里值为3clientPort=2183server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper4的配置，myid文件里值为4clientPort=2184peerType=observerserver.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer 启动后查看它们的角色： 可以看到zookeeper2是leader，我们停掉zookeeper2. 可以看到leader变成了zookeeper3. 我们继续把zookeeper3停掉。 可以发现zookeeper集群已经不能正常运行了。 zookeeper在执行增删改操作时，只要有半数以上服务器通过，就可以成功执行（不包括observer服务器） 半数以上投票通过：可以这样理解，客户端的增删改操作无论访问到了哪台zookeeper服务器，最终都会被转发给leader服务器，再由leader服务器分给zookeeper集群中所有follower服务器去投票（投票指的是在内存中做增删改操作），半数投票通过就被认为操作可执行（commit），否则不可执行。 由于在增删改操作中需要半数以上服务器通过，来分析以下情况。 2台服务器，至少2台正常运行才行，正常运行1台服务器都不允许挂掉。 3台服务器，至少2台正常运行才行，正常运行可以允许1台服务器挂掉。 4台服务器，至少3台正常运行才行，正常运行可以允许1台服务器挂掉。 ...... 故上面例子当挂掉两台服务器时，zookeeper服务已经不能正常运行了。 由上面可以知道，2n-1台服务器和2n台服务器都最多允许n-1台服务器挂掉（n&gt;=2）。 一般为了节省资源，环境基本配置奇数2n-1台服务器（不包括observer）。 Zookeeper应用场景Zookeeper的应用场景还是很多的，如： 1.命名服务 2.分布式协调服务/通知 3.数据发布与订阅（配置中心） 4.Master选举 5.分布式锁 等。 我会在后面结合例子研究zookeeper的一些应用场景。 结语通过对Zookeeper的一些学习，明白了Zookeeper的一些特点用途，了解了一些简单操作，还是蛮不错的一次学习过程。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成RocketMQ]]></title>
    <url>%2Fblog%2FSpringBoot%E9%9B%86%E6%88%90RocketMQ.html</url>
    <content type="text"><![CDATA[前言在开始之前，大家可以先了解RocketMQ的一些特性。 RocketMQ简介 今天我们接上之前说的，对RocketMQ进行简单使用。主要的也是讲如何在SpringBoot项目中使用RocketMQ。 环境RocketMQ安装我们先在RocketMQ官网上下载最新的MQ版本并进行安装。 可以通过镜像进行下载。 将压缩包解压并放在一个指定文件夹下。（这里要注意的是文件夹路径中尽量不要有空格，像Program Files这种，有可能导致mq无法正常启动） RocketMQ启动通过命令行进入到bin目录下，使用 mqnamesrv -n localhost:9876 （windows）可以启动mq的namesrv。如下图： 使用 mqbroker -n localhost:9876 （windows）可以启动mqbroker。如下图： 注意：上图表示RocketMQ的namesrv和broker启动成功，RocketMQ若正常使用应保证namesrv和broker均启动成功。 与Java集成使用主要依赖于rocketmq-client的jar包，在与SpringBoot进行集成时，应当引入该jar包。 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;$&#123;version.rocketmq&#125;&lt;/version&gt;&lt;/dependency&gt; 例子我们简单使用下该jar包创建消费者和生产者进行消费，来了解下它们的一些参数。 生产者： 12345678910111213141516171819public class ProducerTest &#123; public static void main(String[] args) throws Exception &#123; //producerGroup 生产者组名称 DefaultMQProducer producer = new DefaultMQProducer("producer1"); //设置NamesrvAddr producer.setNamesrvAddr("127.0.0.1:9876"); //设置自动创建Topic producer.setCreateTopicKey("AUTO_CREATE_TOPIC_KEY"); //调用start()方法启动一个producer实例 producer.start(); System.out.println("Producer started"); Message message=new Message(); message.setTopic("Test"); message.setTags("123"); message.setBody(new String("Hello").getBytes()); SendResult result=producer.send(message); System.out.println(result); &#125;&#125; 消费者： 123456789101112131415161718192021222324252627282930313233public class ConsumerTest &#123; public static void main(String[] args) throws Exception&#123; //需要consumerGroup DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("consumer1"); //设置NameServer consumer.setNamesrvAddr("127.0.0.1:9876"); //消费策略 //CONSUME_FROM_LAST_OFFSET 默认策略，从该队列最尾开始消费，即跳过历史消息 //CONSUME_FROM_FIRST_OFFSET 从队列最开始开始消费，即历史消息（还储存在broker的）全部消费一遍 //CONSUME_FROM_TIMESTAMP 从某个时间点开始消费，和setConsumeTimestamp()配合使用，默认是半个小时以前 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET); //Topic和Tag，*代表全部的Tag consumer.subscribe("Test", "*"); //设置一个Listener，主要进行消息的逻辑处理 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; msgs.forEach((a)-&gt;&#123; System.out.println(new String(a.getBody())); &#125;); //返回消费状态 //CONSUME_SUCCESS 消费成功 //RECONSUME_LATER 消费失败，需要稍后重新消费 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); //启动 consumer.start(); System.out.println("Consumer started"); &#125;&#125; 运行这两个类进行测试。 生产者结果： 消费者结果： 备注：如果启动中出现异常com.alibaba.rocketmq.client.exception.MQClientException: No route info of this topic, TestTopic可能是没有开启AUTO_CREATE_TOPIC，我们可以在启动broker的时候加上该参数，mqbroker -n localhost:9876&amp;autoCreateTopicEnable=true 可以保证使用时自动创建Topic。 生产等环境一般需要什么Topic就配置什么，不会开启这个参数让程序自动创建。 简要说明我们对上面例子里的一些参数等做些说明，以便于我们可以更好的封装功能。 可与之前RocketMQ的简介结合理解。 DefaultMQProducer部分参数 producerGroup：生产者组名称。 namesrvAddr：生产者NameSrvAddr的服务地址。 createTopicKey：可以创建指定的Topic。AUTO_CREATE_TOPIC_KEY为自动创建Topic。 其它参数略。 发送Message时，需要设置Message的Topic和Tag，并能收到发送状态结果。 DefaultMQPushConsumer部分参数 consumerGroup：消费者组名称。 namesrvAddr：消费者NameSrvAddr的服务地址。 ConsumeFromWhere：消费策略。 ---&gt; CONSUME_FROM_LAST_OFFSET 默认策略，从该队列最尾开始消费，即跳过历史消息 ---&gt; CONSUME_FROM_FIRST_OFFSET 从队列最开始开始消费，即历史消息（还储存在broker的）全部消费一遍 ---&gt; CONSUME_FROM_TIMESTAMP 从某个时间点开始消费，和setConsumeTimestamp()配合使用，默认是半个小时以前 subscribe方法：使消费者订阅指定的Topic和Tag。 registerMessageListener方法：注册消费者监听，用于消费消息，这里面也是我们业务逻辑的主要内容。有顺序消费和并行消费两种模式。 分别需要实现MessageListenerOrderly接口和MessageListenerConcurrently接口。 我们的例子是并行消费处理的。 小试牛刀每次处理时都要这样写会有很多的与业务无关的代码，也不美观。 我们对其进行必要封装，之前集成了SpringBoot的一个自己封装的starter插件，今天我们把RocketMQ也与SpringBoot集成下。 A 首先新建SpringBoot项目，引入RocketMQ 的jar依赖，不在详述。 然后在项目下创建必要的package。 producer：用来存放我们构建producer的类的包。 consumer：用来存放我们构建consumer的类的包。 listener:用来存放我们构建listener的类的包 factory:用来构建生产者和消费者群组的包。 config：存放SpringBoot配置类的包。 autoware:存放启动配置生效的类的包。 annotation：用来存放注解的包。 先说下简单思路吧。 首先这个生产者和消费者是可以有多个的，然后我们怎么管理它们？生产者可以发送顺序消息和并发消息，消费者可以处理顺序消息和并发消息，同时我们可能有两种业务要使用同一个Listener，如何解耦呢？ 关于管理：我们可以管理生产者和消费者的一个集合来解决。 关于解耦：可以提供一个接口，业务类实现这个接口拿到Message，进行处理。那如何知道这个业务类需要哪个listener呢？自然需要customerId或者listenerId。 好了开始工作。 B 先从配置入手。 下面是生产者和消费者的配置Bean。 由于可以配置多个生产者或者消费者，故使用List处理它们。部分代码如下： C 然后从消费者和生产者的提供入手。也是比较简单的，主要是根据参数生成一个生产者或者消费者，然后暴露一些方法，如start，stop等方法。 提供生产者的类，部分代码如下。 备注：生产环境一般不设置AUTO_CREATE_TOPIC_KEY，需要什么Topic要手动创建加入管理。 创建消费者的类，部分代码如下。 我们认为两个consumerId相等则获取的是一个Consumer，因此需要重写equals方法。 D 创建消费者监听，为处理消息提供一个接口。 使用抽象类部分实现这个接口。 这一步的目的是由于不同的业务逻辑可能用到一个监听，这样可以两个业务逻辑写到两个不同的类中，只需实现IProcessor。 写两个并行处理监听和顺序处理监听，对其进行实现。 并行监听： 1234567891011121314151617181920public class ConcurrentlyRocketMQMessageListener extends AbstractRocketMQMessageListener implements MessageListenerConcurrently &#123; private static final transient Logger logger = LoggerFactory.getLogger(ConcurrentlyRocketMQMessageListener.class); @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; logger.debug("ConcurrentlyRocketMQMessageListener receive message begin,length:&#123;&#125;" , msgs.size()); for(MessageExt msg:msgs ) &#123; for (IProcessor processor : processorList) &#123; try &#123; // 处理消息 process(processor, msg); &#125; catch (Exception ex) &#123; logger.error("ConcurrentlyRocketMQMessageListener error",ex); return ConsumeConcurrentlyStatus.RECONSUME_LATER; &#125; &#125; &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;&#125; 顺序监听： 12345678910111213141516171819public class OrderlyRocketMQMessageListener extends AbstractRocketMQMessageListener implements MessageListenerOrderly &#123; private static final transient Logger logger = LoggerFactory.getLogger(OrderlyRocketMQMessageListener.class); @Override public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; logger.debug("OrderlyRocketMQMessageListener receive message begin,length:&#123;&#125;" , msgs.size()); for(MessageExt msg:msgs ) &#123; for (IProcessor processor : processorList) &#123; try &#123; process(processor, msg); &#125; catch (Exception ex) &#123; logger.error("OrderlyRocketMQMessageListener error",ex); return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT; &#125; &#125; &#125; return ConsumeOrderlyStatus.SUCCESS; &#125;&#125; 然后需要对实现IProcesser接口的类添加如下注解，用来查看调用的哪个Listener。 1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface RocketMQProcessor &#123; /** * 该程序归哪个消费者监听处理 * @return */ String consumerId() default "";&#125; E 好，我们开始构建factory。 使用ConcurrentHashMap存储生产者和消费者集合。 1234//用于存放生产者的map组private static Map&lt;String, RocketMQMessageProducer&gt; producers=new ConcurrentHashMap&lt;&gt;();//用于存放消费者的map组private static Map&lt;String, RocketMQMessageConsumer&gt; consumers=new ConcurrentHashMap&lt;&gt;(); 创建生产者： 1234567891011121314151617181920212223242526272829/** * 创建一个生产者 * @param configuration * @return */public RocketMQMessageProducer createProducer(MqProducerConfiguration configuration)&#123; //如果map里存在这个实例，直接返回 if(producers.get(configuration.getProducerId())!=null)&#123; return producers.get(configuration.getProducerId()); &#125; //创建一个生产者 RocketMQMessageProducer producer=new RocketMQMessageProducer(configuration.getGroupName(), configuration.getNamesrvAddr()); if(configuration.getSendMsgTimeout()!=null)&#123; producer.setSendMsgTimeout(configuration.getSendMsgTimeout()); &#125; if(configuration.getMaxMessageSize()!=null)&#123; producer.setMaxMessageSize(configuration.getMaxMessageSize()); &#125; try &#123; //启动生产者并放入map进行管理 producer.start(); producers.put(configuration.getProducerId(), producer); logger.info("MqProducer start success "+configuration.toString()); &#125; catch (MQClientException e) &#123; logger.error("MqProducer start error "+configuration.toString(),e); throw new RuntimeException(e); &#125; return producer;&#125; 创建消费者： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 创建一个消费者 * @param mqConsumerConfiguration * @return */public RocketMQMessageConsumer createConsumer(MqConsumerConfiguration mqConsumerConfiguration, List&lt;IProcessor&gt; list) &#123; //如果map里存在，直接返回 if (consumers.get(mqConsumerConfiguration.getConsumerId()) != null) &#123; return consumers.get(mqConsumerConfiguration.getConsumerId()); &#125; try &#123; RocketMQMessageConsumer consumer = new RocketMQMessageConsumer(mqConsumerConfiguration.getConsumerId(), mqConsumerConfiguration.getGroupName(), mqConsumerConfiguration.getNamesrvAddr()); consumer.subscribe(mqConsumerConfiguration.getTopicAndTagMap()); //设置消费者其它参数 if(!CollectionUtils.isEmpty(mqConsumerConfiguration.getOptions()))&#123; String consumeFromWhere = mqConsumerConfiguration.getOptions().get("consumeFromWhere"); String consumeThreadMin = mqConsumerConfiguration.getOptions().get("consumeThreadMin"); String consumeThreadMax = mqConsumerConfiguration.getOptions().get("consumeThreadMax"); String pullThresholdForQueue = mqConsumerConfiguration.getOptions().get("pullThresholdForQueue"); String consumeMessageBatchMaxSize = mqConsumerConfiguration.getOptions().get("consumeMessageBatchMaxSize"); String pullBatchSize = mqConsumerConfiguration.getOptions().get("pullBatchSize"); String pullInterval = mqConsumerConfiguration.getOptions().get("pullInterval"); if (StringUtils.isNotBlank(consumeFromWhere)) &#123; if (StringUtils.equals(consumeFromWhere, "CONSUME_FROM_LAST_OFFSET")) &#123; consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET); &#125; else if (StringUtils.equals(consumeFromWhere, "CONSUME_FROM_FIRST_OFFSET")) &#123; consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); &#125; &#125; if (StringUtils.isNotBlank(consumeThreadMin)) &#123; consumer.setConsumeThreadMin(Integer.parseInt(consumeThreadMin)); &#125; if (StringUtils.isNotBlank(consumeThreadMax)) &#123; consumer.setConsumeThreadMax(Integer.parseInt(consumeThreadMax)); &#125; if (StringUtils.isNotBlank(pullThresholdForQueue)) &#123; consumer.setPullThresholdForQueue(Integer.parseInt(pullThresholdForQueue)); &#125; if (StringUtils.isNotBlank(consumeMessageBatchMaxSize)) &#123; consumer.setConsumeMessageBatchMaxSize(Integer.parseInt(consumeMessageBatchMaxSize)); &#125; if (StringUtils.isNotBlank(pullBatchSize)) &#123; consumer.setPullBatchSize(Integer.parseInt(pullBatchSize)); &#125; if (StringUtils.isNotBlank(pullInterval)) &#123; consumer.setPullInterval(Integer.parseInt(pullInterval)); &#125; &#125; //设置消费者监听 if(mqConsumerConfiguration.isOrderly())&#123; OrderlyRocketMQMessageListener orderlyRocketMQMessageListener=new OrderlyRocketMQMessageListener(); orderlyRocketMQMessageListener.setProcessorList(list); consumer.registerMessageListener(orderlyRocketMQMessageListener); &#125;else&#123; ConcurrentlyRocketMQMessageListener concurrentlyRocketMQMessageListener=new ConcurrentlyRocketMQMessageListener(); concurrentlyRocketMQMessageListener.setProcessorList(list); consumer.registerMessageListener(concurrentlyRocketMQMessageListener); &#125; consumer.start(); consumers.put(mqConsumerConfiguration.getConsumerId(), consumer); logger.info("MqConsumer start success "+mqConsumerConfiguration.toString()); logger.info("MqConsumer processors size "+list.size()); return consumer; &#125; catch (Exception e) &#123; logger.error("MqConsumer start error", e); throw new RuntimeException(e); &#125;&#125; 它们的其它方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344 /** * 获取一个生产者 * @param producerId * @return */ public RocketMQMessageProducer getProducer(String producerId)&#123; if(producers.get(producerId)!=null)&#123; return producers.get(producerId); &#125; return null; &#125; /** * 停止某个生产者 * @param producerId */ public void stopProducer(String producerId)&#123; if(producers.get(producerId)!=null)&#123; producers.get(producerId).shutdown(); producers.remove(producerId); logger.info("MqProducer "+producerId+" is shutdown!"); &#125; &#125;/** * 获取一个消费者 * @param customerId * @return */ public RocketMQMessageConsumer getConsumer(String customerId)&#123; if(consumers.get(customerId)!=null)&#123; return consumers.get(customerId); &#125; return null; &#125; /** * 停止某个消费者 * @param customerId */ public void stopConsumer(String customerId)&#123; if(consumers.get(customerId)!=null)&#123; consumers.get(customerId).shutdown(); consumers.remove(customerId); logger.info("MqConsumer "+customerId+" is shutdown!"); &#125; &#125; F 然后到了我们关键的自动配置部分了。 让这个类实现ApplicationContextAware可以拿到applicationContext。 同时生成一个FactoryBean。 主要方法： 12345678910111213141516171819202122232425262728293031323334353637@Bean@ConditionalOnMissingBean@ConditionalOnProperty(prefix = "spring.rocketmq.config", name = "enabled", havingValue = "true")RocketMQFactory rocketMQFactory()&#123; RocketMQFactory rocketMQFactory=new RocketMQFactory(); //处理生产者 if(mqConfigurations.getMqProducerConfigurations()!=null&amp;&amp;mqConfigurations.getMqProducerConfigurations().size()&gt;0)&#123; mqConfigurations.getMqProducerConfigurations().forEach(producerConfiguration -&gt;&#123; rocketMQFactory.createProducer(producerConfiguration); &#125;); &#125; if(mqConfigurations.getMqConsumerConfigurations()!=null&amp;&amp;mqConfigurations.getMqConsumerConfigurations().size()&gt;0)&#123; //处理消费者 mqConfigurations.getMqConsumerConfigurations().forEach(consumerConfiguration-&gt;&#123; //处理Processor final Map&lt;String, Object&gt; annotationMap = applicationContext.getBeansWithAnnotation(RocketMQProcessor.class); List&lt;IProcessor&gt; list = new ArrayList&lt;&gt;(); if(annotationMap!=null)&#123; annotationMap.forEach((key,value)-&gt;&#123; RocketMQProcessor annotation = value.getClass().getAnnotation(RocketMQProcessor.class); if(consumerConfiguration.getConsumerId().equals(annotation.consumerId()))&#123; try&#123; list.add((IProcessor) value); &#125;catch (Exception e)&#123; throw new RuntimeException(e); &#125; &#125; &#125;); &#125; rocketMQFactory.createConsumer(consumerConfiguration,list); &#125;); &#125; return rocketMQFactory;&#125; 总的来说就是拿到配置生产生产者组，生成消费者组。 在生成消费者的时候需要注册监听，一个监听可以有很多业务类，通过注解拿到业务类，放到处理器列表里，再把该监听注册到指定的customerId上。 G 配置spring.factories。 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.zwt.rocketmqspringboot.autoware.RocketMqAutoConfig H 然后我们添加application.properties进行测试。 12345678910111213141516171819202122232425262728## 是否启用RocketMQspring.rocketmq.config.enabled=true## Producer1配置spring.rocketmq.config.mq-producer-configurations[0].group-name=rocketmq-producerspring.rocketmq.config.mq-producer-configurations[0].namesrv-addr=localhost:9876spring.rocketmq.config.mq-producer-configurations[0].producer-id=001## Consumer1配置spring.rocketmq.config.mq-consumer-configurations[0].namesrv-addr=localhost:9876spring.rocketmq.config.mq-consumer-configurations[0].group-name=rocketmq-consumerspring.rocketmq.config.mq-consumer-configurations[0].consumer-id=001spring.rocketmq.config.mq-consumer-configurations[0].topic-and-tag-map.123=123spring.rocketmq.config.mq-consumer-configurations[0].orderly=falseserver.port=8001## Producer2配置spring.rocketmq.config.mq-producer-configurations[1].group-name=rocketmq-producer1spring.rocketmq.config.mq-producer-configurations[1].namesrv-addr=localhost:9876spring.rocketmq.config.mq-producer-configurations[1].producer-id=002## Consumer2配置spring.rocketmq.config.mq-consumer-configurations[1].namesrv-addr=localhost:9876spring.rocketmq.config.mq-consumer-configurations[1].group-name=rocketmq-consumer1spring.rocketmq.config.mq-consumer-configurations[1].consumer-id=002spring.rocketmq.config.mq-consumer-configurations[1].topic-and-tag-map.1234=1234spring.rocketmq.config.mq-consumer-configurations[1].orderly=false 编写一些测试类进行测试。 123456789@Service@RocketMQProcessor(consumerId = "001")public class TestConsumer implements IProcessor &#123; private static final Logger logger = LoggerFactory.getLogger(TestConsumer.class); @Override public void handleMessage(MessageExt msg) throws Exception &#123; System.out.println(new String(msg.getBody())+"TestConsumer"); &#125;&#125; 123456789101112131415161718@Controllerpublic class TestProducer &#123; @Autowired RocketMQFactory rocketMQFactory; @RequestMapping("/test") public String doSomething() throws Exception&#123; Message message=new Message(); message.setTopic("123"); message.setTags("123"); message.setBody(new String("Hello World").getBytes()); SendResult result=rocketMQFactory.getProducer("001").sendMessage(message); System.out.println(result); return result.toString(); &#125;&#125; 可以看到正常运行。 实战我们按照上篇文章那样把它封装成jar包。 完成后进行测试。 新建一个test的SpringBoot项目。作为消费者。 引入我们的包。 PS：这儿没有命名为xxxx-spring-boot-starter的形式。 创建一个消费者。 再新建一个test1的SpringBoot项目。作为生产者。 引入我们的包并进行配置。 启动test消费者，同时使用test1生产者发送一条消息。 可以看到。 生产者发送成功： 消费者处理成功： 总结通过对RocketMQ的集成封装使用，更好地学会了如何使用RocketMQ，及对其的更多理解。 消息中间件在我们软件开发中具有重要作用，应当好好理解。 如果觉得properties配置太繁琐可以改用yml配置，会更简介好看些。 如下： 1234567891011121314151617181920212223spring: rocketmq: config: enabled: true # 是否启用RocketMQ mq-producer-configurations: # 生产者配置 - producerId: TestProducer1 # 生产者1的id groupName: producer1 # 生产者1的组名称 namesrvAddr: 127.0.0.1:9876 # 生产者1的namesrvAddr - producerId: TestProducer2 # 生产者2的id groupName: producer2 # 生产者2的组名称 namesrvAddr: 127.0.0.1:9876 # 生产者2的namesrvAddr mq-consumer-configurations: # 消费者配置 - consumerId: TestConsumer1 # 消费者1的id groupName: consumer1 # 消费者1的组名称 namesrvAddr: 127.0.0.1:9876 # 消费者1的namesrvAddr topicAndTagMap: &#123; TestTopic1:TestTag1 &#125; # 消费者1监听的topic和tag orderly: false # 消费者1是否顺序消费消息 - consumerId: TestConsumer2 # 消费者2的id groupName: consumer2 # 消费者2的组名称 namesrvAddr: 127.0.0.1:9876 # 消费者2的namesrvAddr topicAndTagMap: &#123; TestTopic2:TestTag2 &#125; # 消费者2监听的topic和tag orderly: true # 消费者2是否顺序消费消息 代码地址：rocketmq-spring-boot]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建一个自定义的spring-boot-starter]]></title>
    <url>%2Fblog%2F%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84spring-boot-starter.html</url>
    <content type="text"><![CDATA[前言现在很多项目都使用SpringBoot构建了，不知道大家在使用SpringBoot项目时有没有注意到它的一些jar包的引用。如下图： 我们引入相关starter后，在application.properties或者application.yml里面进行相关配置，便可以尽情使用相关功能了，十分方便。 今天我们构建一个自己的spring-boot-starter，来体验下spring-boot的魅力和乐趣。 正文我之前有一篇文章是ElasticSearch的介绍并且对它的Java客户端封装了下，那个例子中我们最后的配置文件叫property/es-config.properties 不知道大家还有没有印象。要使用那个jar包，我们需要配置一个es-config.properties文件，显然，我想去掉这个文件，改为application.properties的那种配置，和各种spring-boot-starter一样。 好了，我们开始吧。 创建项目首先，我们需要创建一个名为 elasticsearch-spring-boot-starter 的SpringBoot项目（除springboot基础外不需要其他dependencies）。 这儿有一点要注意的，就是项目（后面生成jar包）命名。 备注：Spring官方的starter命名一般会采用 spring-boot-starter-xxxx的形式，而官方建议非官方的starter命名可以遵循 xxxx-spring-boot-starter的形式。 引入依赖然后我们引入我们要编写的starter的一些依赖。（这里对以前封装的ElasticSearch 客户端jar包进行继续优化封装） 备注：dependency里的optional表示可选依赖，因为spring-boot父项目有日志的相关jar，我们不必再次引用。 这样配置的优点大致如下： 节约空间（不用两个jar包重复引用） 避免license许可问题（两个jar版本不同可能出现问题） 避免jar包出现冲突等问题（两个jar版本不同可能出现问题） 构建（改造）项目然后我们把原来的项目改造下。 我们移除了配置解析的一些方法及类，改用自动配置。 使用@ConfigurationProperties注解，指定要进行属性配置的Bean。 使用该注解，需要引入下面的jar包。 这个jar包的@ConfigurationProperties注解可以帮我们把application.properties的属性值映射到Bean里。 这个jar的作用是编译时生成 spring-configuration-metadata.json ，此文件主要给IDE使用，可以进行友好的提示。 我们还需要创建一个自动配置类，名为ElasticSearchStarterAutoConfigure，如下： 123456789101112131415@Configuration@ConditionalOnClass(ElasticSearchClientFactory.class)@EnableConfigurationProperties(ElasticSearchConfiguration.class)public class ElasticSearchStarterAutoConfigure &#123; @Autowired private ElasticSearchConfiguration properties; @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = "spring.elasticserch.zwt", name = "enabled", havingValue = "true") ElasticSearchUtil elasticSearchUtil ()&#123; ElasticSearchClientFactory elasticSearchClientFactory=new ElasticSearchClientFactory(properties); return new ElasticSearchUtil(elasticSearchClientFactory); &#125;&#125; @Configuration注解，我们可以看到它被标识为 @Component注解，可以认为它最终也会成为spring容器里的一个Bean。 @ConditionalOnClass 当指定class存在时进行Bean创建。 @EnableConfigurationProperties 为指定class启用属性配置。 @ConditionalOnMissingBean 当bean不存在时进行bean创建。 @ConditionalOnProperty 用属性对bean进行配置创建。它里面可以enabled可以控制是否启用我们的配置。 然后在resources/META-INF 文件夹下新建spring.factories文件，添加如下内容，用于Spring加载自动配置类。 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.zwt.elasticsearchspringbootstarter.factory.ElasticSearchStarterAutoConfigure 配置完成后，在application.properties里，就可以看到我们的自定义属性啦。 测试创建测试类进行测试。 application.properties配置如下： 1234## es地址spring.elasticserch.zwt.address=127.0.0.1:9300## 是否启用es客户端spring.elasticserch.zwt.enabled=true 可以看到我们的日志输出。 当我们把spring.elasticserch.zwt.enabled设置为false时，在运行可以看到bean创建失败了，即未注入属性配置bean。 提升我们把项目用maven打包发布下（clean install），可以看到大约30多M……. 原因是它里面有原始的springboot项目，还内置了tomcat等，我们如果想把该工具jar包引入到另一个springboot项目里，这显然是不合理的。 我们还需要对其进行优化。 我们删除无用的Application class（springboot启动使用，显然我们封装jar包不是为了封装web项目），application.properties（当我们把我们工具jar引入到新的springboot项目里，里面已经有了application.properties）及test类包。 然后处理下pom文件。 在pom文件里，spring-boot-starter这个依赖我们是不需要打到包里的。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 将其添加属性。 1&lt;optional&gt;true&lt;/optional&gt; 同时build插件做下调整（原来用的springboot的插件）。 build插件基本固定的，代码可以不用看。 12345678910111213141516171819202122232425262728293031&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;attach&gt;true&lt;/attach&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 再次编译，这时候我们就可以看到jar包大小只有10几Kb啦。 应用我们随便创建一个新的springboot项目。引入我们封装的jar包。 配置application.properties文件 1234## es地址spring.elasticserch.zwt.address=127.0.0.1:9300## 是否启用es客户端spring.elasticserch.zwt.enabled=true 新建个测试类，对我们的jar包进行测试。可以看到正常运行。 总结通过一步步改造，我们成功创建了一个spring-boot-starter。 通过实践，我们也能看到springboot基于注解简化代码的配置。也是蛮不错的一次工具封装体验。 补充：上面说的@ConfigurationProperties注解也可以定义自己的配置文件。 如果我们想使用原来的es-config.properties则需要进行如下配置即可。 PS：SpringBoot官方已经有了对elasticsearch client的支持，建议使用官方相关支持工具。 自己实践时，可以简单的写一个HelloWorld去完成一个简单的spring-boot-starter的封装。 相关代码： https://github.com/JavaZWT/framework-base/tree/master/elasticsearch-spring-boot-starter]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>spring-boot-starter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口管理平台YAPI]]></title>
    <url>%2Fblog%2F%E6%8E%A5%E5%8F%A3%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0YAPI.html</url>
    <content type="text"><![CDATA[前言有没有为维护接口文档而烦恼过？ 作为一个良好的开发人员，为使用者提供准确良好的接口文档也是开发的一部分。 也便于测试创建测试用例。 接口管理有很多种方式，很多是提供接口文档供使用者参考，也有一些是通过平台管理。 今天介绍一款非常好用的工具，YAPI，其主要功能是一款优秀的API接口管理平台。旨在为开发、产品、测试人员提供更优雅的接口管理服务。 我在使用这个API管理平台时，感觉还蛮不错的，特此介绍下。 官网地址：https://yapi.ymfe.org/ 我们搭建内网环境，把这款接口管理平台在内网部署下，并且简单使用下。 正文要搭建内网环境，首先要准备以下几个软件。 nodejs （必须） mongodb （必须） git （非必须） git主要是从GitHub上 clone项目的，如果采用可视化安装或者直接下载压缩包是用不到的。 没有以上软件的需要下载安装。 nodejs下载 mongodb下载 安装不做过多介绍，自己参考一些资料即可完成。 对于YAPI服务，我们采用可视化部署，执行命令： 1npm install -g yapi-cli --registry https://registry.npm.taobao.org 注：其他部署方式，详情可以查看官方文档 https://yapi.ymfe.org/devops/index.html 这儿不做讨论。 安装成功后启动 yapi server，执行命令： 1yapi server 通过浏览器打开页面 http://0.0.0.0:9090/ 会看到如下界面： 注意：需要启动mongdb数据库。 mongodb启动备注(路径未加入PATH):12cd /Users/zhangwentong/Applications/mongodb/mongodb-osx-x86_64-4.0.4/bin./mongod --dbpath /Users/zhangwentong/Applications/mongodb/data/db 部署引导完成后。我们进入到vendors 文件夹，启动YAPI服务。如下命令： 12cd my-yapi/vendors/node server/app.js 说明我们安装完成并启动成功了。浏览器打开图上地址。 使用管理员账号登陆，初始密码为 ymfe.org 其他人可以进行注册后登陆。 登陆成功后如下图： 我们可以在个人中心修改密码，这儿就不做过多介绍了。看一看它的主要功能吧。 首先我们创建一个分组。 然后我们在该分组里创建一个项目。 上图的成员列表里可以添加成员，把一起开发的开发者拉进来。 分组动态展示我们分组的一些操作等。 我们可以看到项目里内容如下： 接口主要就是用来管理我们的接口的界面，比如后台提供某些接口，全部放在里面，供其它开发人员查看使用。 动态代表着这个项目里的接口等改动动态，方便人员查看。 成员管理可以管理这个项目的成员，指定组长，开发者，外部接口使用者（访客）等。 然后我们可以添加一些模块接口分类。如下，不同接口放在不同模块下。 继续，我们新增加一个接口来看看吧。 新增OK后对接口进行修改。 当我们接口参数定义好，或者说后台接口完成后（可以测试时），我们应该把接口状态改为已完成。 一个写好的接口demo如下： 同时动态里也可以看到我们对于接口的改动。 同时YAPI还支持测试接口，mock，wiki等，类似于Postman等工具。 这儿就不过多介绍了，对于我们开发人员，最主要的应该属对接口文档的维护了，也就是刚才介绍的部分。 结语今天说到这里也差不多啦，YAPI还可以控制人员的权限、给指定人员发送邮件、设置是否可以注册等其它功能，有兴趣的可以摸索。 我们对YAPI的使用就是用其对接口文档进行管理。它是易用的简便的。 在项目开发过程中，在公司服务器上搭建一套环境，对组成员接口文档维护也是受益的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>YAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 新的日期API]]></title>
    <url>%2Fblog%2FJava-8-%E6%96%B0%E7%9A%84%E6%97%A5%E6%9C%9FAPI.html</url>
    <content type="text"><![CDATA[前言Java 8 中引入了新的一套日期API，相对于之前的Date或者Calendar类，这套API更好的解决了日期和时间的问题。 我们来简单看下。 正文LocalDateTime,LocalDate,LocalTime开始使用日期API时，最先碰到的也是这三个类。 他们的静态工厂of方法可以创建日期实例。如下代码： 1234567//1. 第一部分//创建一个日期LocalDate date=LocalDate.of(2018,11,22);//时分秒LocalTime time=LocalTime.of(13,45,20);//LocalDateTime对象LocalDateTime dateTime=LocalDateTime.of(2018,Month.NOVEMBER,22,13,45,20); 是不是很简单。 要注意这三个类都是final的，即不可被改变的。 LocalDate 表示年月日，LocalTime表示时分秒，LocalDateTime表示年月日时分秒。 他们三个之间的转换也是比较容易的。如下： 1234567891011//2. 第二部分//使用localDate和localTime构造一个LocalDateTimeLocalDateTime dateTime1=LocalDateTime.of(date,time);//使用LocalDate构造一个LocalDateTimeLocalDateTime dateTime2=date.atStartOfDay();//这一天的00:00:00LocalDateTime dateTime3=date.atTime(LocalTime.of(12,12,12));//指定这一天的时间//使用LocalTime构造LocalDateTimeLocalDateTime dateTime4=time.atDate(LocalDate.of(2018,11,22));//指定日期//通过LocalDateTime获取LocalDate和LocalTimeLocalDate date1=dateTime.toLocalDate();LocalTime time1=dateTime.toLocalTime(); 对于固定的日期，我们可以获取它的时间信息，也是比较容易的。如具体年月日的数值，等等。 123456789101112131415161718192021//3. 第三部分//获取日期年份int year=dateTime.getYear();int year1=dateTime.get(ChronoField.YEAR);//获取日期月份Month month=dateTime.getMonth();int month1=month.getValue();int month2=dateTime.get(ChronoField.MONTH_OF_YEAR);//获取当月第几天int day=dateTime.getDayOfMonth();int day1=dateTime.get(ChronoField.DAY_OF_MONTH);//获取星期几DayOfWeek dow=dateTime.getDayOfWeek();//获取该月有几天int len=date.lengthOfMonth();//获取小时数int hour=dateTime.getHour();//获取分钟int minute=dateTime.getMinute();//获取秒数int second=dateTime.getSecond(); 他们还有一些常用的方法，如时间和字符串时间之间的转化、判断闰年、获取当前时间信息、时间的比较、时间的加减天数（年数等）等方法。 相比于我们单独封装处理Date，或者Calendar类，更简便和安全了。 1234567891011121314151617181920212223242526272829//4. 第四部分//是不是闰年boolean leap=date.isLeapYear();//获取当前时间信息LocalDateTime localDateTimeNow=LocalDateTime.now();LocalDate localDateNow=LocalDate.now();LocalTime localTimeNow=LocalTime.now();//字符串转时间LocalDate localDate1=LocalDate.parse("2014-03-18");LocalTime localTime1=LocalTime.parse("13:45:20");LocalDateTime localDateTime1=LocalDateTime.parse("2018/11/22 11:22:33", DateTimeFormatter.ofPattern("yyyy/MM/dd HH:mm:ss"));//时间转换为字符串String localDateStr=localDate1.toString();String localTimeStr=localTime1.format(DateTimeFormatter.ofPattern("HH:mm:ss"));String localDateTimeStr=localDateTime1.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"));//当前时间减10天LocalDateTime localDateTime2=localDateTime1.minusDays(10);//当前时间加1年LocalDateTime localDateTime3=localDateTime1.plusYears(1);//当前时间加1个月LocalDateTime localDateTime4=localDateTime1.minus(-1, ChronoUnit.MONTHS);//当前时间加1个月LocalDateTime localDateTime5=localDateTime1.plus(1, ChronoUnit.MONTHS);//更改日期时间，返回新的对象，原对象不会变化LocalDate localDate=date1.with(ChronoField.MONTH_OF_YEAR,9);//日期时间比较boolean flag=localDateTime2.isAfter(localDateTime3);boolean flag1=localDateTime2.isBefore(localDateTime3);boolean flag2=localDateTime2.isEqual(localDateTime3); 这里面对于时间的加减（plus，minus方法），赋值（with方法）都会生成新的LocalDateTime对象，不会对原来的对象做修改。 Period、Duration 类这两个类都可以表示日期时间的差值，Period的话表示年月日，如两个时间差1年或者-1个月，Duration 的话用来表示天时分秒，比如它可以表示两个时间差相差34.5s这样的数据。 123456789//5. 第五部分//计算两个时间差Duration d1=Duration.between(localDateTime2,localDateTime3);long days1=d1.toDays();Duration d2=Duration.between(localDateTime4,localDateTime5);long hours=d2.toHours();//计算相差时间，结果10天Period period=Period.between(LocalDate.of(2014,3,8),LocalDate.of(2014,3,18));int days2=period.getDays(); Instant和 ZoneId从计算机的角度来看，建模时间最自然的格式是表示一个持续时间段上某个点的单一大整型数。这也是新的 java.time.Instant 类对时间建模的方式，基本上它是以Unix元年时间（传统的设定为UTC时区1970年1月1日午夜时分）开始所经历的秒数进行计算。而每个ZoneId由该地区的ID标识。地区ID都为“{区域}/{城市}”的格式，这些地区集合的设定都由英特网编号分配机构（IANA）的时区数据库提供。 看下它们的用法。 123456789101112131415161718//6. 第六部分//机器时间//以下均表示3s时间Instant.ofEpochSecond(3);Instant.ofEpochSecond(3,0);Instant.ofEpochSecond(2,1000000);Instant.ofEpochSecond(4,-1000000);//当前时间的时间戳Instant.now();//localDateTime转换为instantInstant instantFormDateTime=localDateTime1.toInstant(ZoneOffset.UTC);//获取本地区的zoneIdZoneId romeZone= TimeZone.getDefault().toZoneId();//instant转localDateTimeInstant instant1=Instant.now();LocalDateTime timeFromInstant=LocalDateTime.ofInstant(instant1,romeZone);//将时区设置为欧洲罗马城市。ZoneId romeZone1 = ZoneId.of("Europe/Rome"); 一般传统的Date和LocalDate之间的转换会用到它们（Instant，ZoneId）。 其他在一些项目中，应用Date还是很多的。我们想使用新的日期API，又不太想改动源代码。可以写一些转换的工具类，或者使用Java8的一些日期API对Date进行处理。 如Date转化为LocalDateTime，LocalDateTime转换为Date等。 12345678910111213141516171819202122232425/** * Java8 LocalDateTime对象转Date对象 * @param localDateTime * @return */public static Date localDateTime2Date(LocalDateTime localDateTime)&#123; //获取zoneId ZoneId zone = ZoneId.systemDefault(); //将localDateTime转换为Instant对象 Instant instant = localDateTime.atZone(zone).toInstant(); return Date.from(instant);&#125;/** * Date对象 转 Java8 LocalDateTime对象 * @param date * @return */public static LocalDateTime date2LocalDateTime(Date date)&#123; //根据date拿到Instant Instant instant = date.toInstant(); ZoneId zone = ZoneId.systemDefault(); //转换为LocalDateTime return LocalDateTime.ofInstant(instant, zone);&#125; 将字符串日期格式化为LocalDateTime，或者将LocalDateTime转换为字符串时间。 12345678910111213141516171819/** * 将日期格式化为指定的格式 * @param date * @return */ public static String localDateTime2String(Date date)&#123; LocalDateTime localDateTime=date2LocalDateTime(date); return DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss").format(localDateTime); &#125; /** * 将string时间格式转化为LocalDateTime * @param string * @return */ public static LocalDateTime string2LocalDateTime(String string)&#123; DateTimeFormatter df=DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); return LocalDateTime.parse(string,df); &#125; 有了这些比较基础的方法，我们某些日期便可以使用Java8处理。比如项目使用Date，在判断闰年，或者时间的加减等，可以封装Java8工具类。 12345678910111213141516171819202122232425262728293031/** * 获取两个日期的时间间隔 * @param date1 * @param date2 * @return */public static long diffDays(Date date1,Date date2)&#123; LocalDateTime localDateTime1=date2LocalDateTime(date1); LocalDateTime localDateTime2=date2LocalDateTime(date2); return Duration.between(localDateTime1,localDateTime2).toDays();&#125;/** * 当期日期增加多少天后的日期，负数为减少多少天 * @param date * @param days * @return */public static Date addDays(Date date,int days)&#123; LocalDateTime localDateTime1=date2LocalDateTime(date); LocalDateTime localDateTime2=localDateTime1.minusDays(days); return localDateTime2Date(localDateTime2);&#125;/** * 判断是否是闰年 * @param date * @return */public static boolean isLeapYear(Date date)&#123; LocalDateTime localDateTime=date2LocalDateTime(date); return localDateTime.toLocalDate().isLeapYear();&#125; 这样对于Date的处理，均使用LocalDateTime处理，虽然整个项目把Date改造成LocalDateTime较困难，但是工具类相当于黑匣子，这样慢慢使用LocalDateTime去处理Date，也是蛮不错的一次体验。 总结Java8 新的日期API比较优秀的地方是更直观了，使用更简洁。而且不用担心变量污染问题，想想一般的Date，对日期进行操作，如果不小心没有创建新对象，会把传入的Date改变掉，是很不安全的。 而且它也没有时间从1900年起那种莫名其妙的限制，而且获取到的月份值是1-12，不是0-11，也是符合自然的。 其实Java 8 的日期API不单单完成了对于Java日期的优化，其更多的方法更像是一种工具API，如判断是不是闰年，求两日期之差等等常用方法，都被封装在了Java8的新的日期API里面了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码的优雅]]></title>
    <url>%2Fblog%2F%E4%BB%A3%E7%A0%81%E7%9A%84%E4%BC%98%E9%9B%85.html</url>
    <content type="text"><![CDATA[前言很早之前就想写这么一篇文章，苦于准备不足和时间有限吧，进度一拖再拖。 来到新公司差不多3个多月了，让我感到或者说受益比较大的可以说就是公司的代码风格吧。 估计也是由于有架构师的缘故，大家的代码质量感觉都比较高。 扯远了…… 今天结合实际聊一下关于如何让代码变得规范、优雅吧。 正文遵守规范这一点或许可以说很笼统，目前大家除了遵守公司制定的一些规范，针对于Java我还比较建议的是阅读下阿里的《阿里巴巴Java开发手册》，H5前端的可以看看Github 上的这个规范http://alloyteam.github.io/CodeGuide/ ，也是蛮不错的。 必要的封装当有一些重复，或者有共性的代码时，我们可以对它们进行包装。小到一些特殊的判断，大到常用的设计模式。 如下面的代码，实现相同的功能，明显第二种要好些。 这些必要的封装不仅可以减少一些额外的工作量，还能提高代码可阅读性。 一些工具包，如apache.commons 已经封装了一些通用校验，我们是可以直接拿来使用的。 学会占位符的使用这一点打印日志比较常见到。如下： 可以看到两者的输出是一样的，但是第一种是方便我们进行程序源码阅读的。 除了String.format里的%s占位符，log4j里的{}占位符也可以实现这种作用。 枚举和常量的使用正确使用一些枚举和常量可以提高代码的阅读性。 比如我们创建一个水果枚举。 进行某些方法，这些方法更容易被阅读和维护。 异常的正确使用或许你对异常不感冒，觉得有异常还要try catch是一件很费劲的事情，也影响代码美观，怎么会使代码优雅呢？ 我们可以举个例子看一下异常的一种好玩的用法。 比如有一个业务逻辑，需要通过分布式接口获取其它数据，比如分三步： 这种情况下每个Res我们需要判断成功，才能获取数据。 即retCode为0000时。其它为失败。 这时候我们可以创建异常类。用于对retCode和retInfo进行接收。 如果上面步骤中获取失败，直接上抛。 最后可以使用try catch统一处理返回。 还有一些异常的用法，在这里就不一一说明了。 必要时尝试使用多线程可以在必要时使用多线程来提高程序运行效率。 对于上面的例子，查询可用余额，查询提现次数，查询银行卡信息相互之间是不影响的（他们的结果只取决于userId或者customerId的值）。我们可以使用Future获取结果，减少程序运行时间。 使用多线程分别去请求获取结果，最后在进行处理。 结语规范的代码不仅自己赏心悦目，别人看后也有所启发，养成编写良好代码的习惯，对自己、对别人都是有所帮助的。 其实最重要的还是多学、多练、多思考吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[和朋友的一次关于数组问题讨论]]></title>
    <url>%2Fblog%2F%E5%92%8C%E6%9C%8B%E5%8F%8B%E7%9A%84%E4%B8%80%E6%AC%A1%E6%95%B0%E7%BB%84%E9%97%AE%E9%A2%98%E8%AE%A8%E8%AE%BA.html</url>
    <content type="text"><![CDATA[前言前两天下班和一朋友一起，被问到如下问题：如何移除一个数组中某些符合条件（或不符合条件）的元素形成新的数组，要求数组长度也应该变化（这是我后面加的）。 这两天研究下了这个问题，感觉比较有意思，也从中学到了一些其它该注意的东西。 特此来分享一下。 如有问题欢迎批评指正。 正文对于上面的问题，最简单也最应该想到的一个应该是借助List这个工具类。 是的，我写下大致如下代码： （1）使用List帮助类实现,借助remove方法优点：代码简洁，便于理解，不易出错 缺点：数组和List转换耗时，效率不算太高，使用List remove方法时应注意线程安全问题，如果是基本数据类型，需要转换为包装类，拆箱装箱也是影响效率的一个因素。 代码如下： 123456789101112131415/** * 过滤形成新的数组（转换为List，通过List里面remove(安全的)移除元素） * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray1(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //asList生成的是一个List的内部类，无法调用add remove delete方法会抛出异常 List&lt;T&gt; sourceList= new ArrayList&lt;&gt;(Arrays.asList(sourceArray)); //使用Java8 函数式接口，移除不符合条件的元素 sourceList.removeIf(predicate); return sourceList.toArray((T[]) Array.newInstance(type,0));&#125; 注： Predicate predicate 相当于一个判断条件（lambda表达式，Java8语法），具体的问题也可以写明，就是那种for list 在加上 if 条件判断的，这儿我就不啰嗦了。Java 8里面对于集合类，新增了 removeIf方法，我们可以看下它，其实就是我们上面说的那个。 关于这种写法，有需要注意的几点： for-each，for, iterator 这三种对于List的循环，for，iterator是可以遍历并进行remove操作的，但是for-each是不可以的。有兴趣的可以研究研究，这不是我们的重点。 看到我上面使用的泛型T，其实使用Object[]也是可以的，要注意一个问题，泛型T是不能包含int,long等基本数据类型的，使用的话只能转化为它们的包装类。 Object[]是没有限制的，但是Object[]转换为Integer[]或者int[]或者其他不能直接转换，如下写法是错误的，会出现问题。 12Object [] objects=new Object[10];Integer [] a=(Integer[])objects; 正确的转换方法应该循环里面的元素，并对它们强转添加进数组；或者使用下面的方法。123Object [] objects=new Object[10];//Integer [] a=(Integer[])objects;Integer [] a=(Integer[])Array.newInstance(Integer.class,objects.length); 当我们把数组转换成List的时候，为了不想循环遍历添加，可能会想到使用Arrays.asList(T t) 这个方法，这个方法生成的List是一个Arrays里面的一个内部类ArrayList。 看下这个内部类你会发现它没有对remove、add等方法的实现，也就是继承自AbstractList。 123public E remove(int index) &#123; throw new UnsupportedOperationException();&#125; 也就是执行它们时会抛出异常，如果想使用remove方法，应该把它转为具体实现，如ArrayList。 1List&lt;T&gt; sourceList= new ArrayList&lt;&gt;(Arrays.asList(sourceArray)); 为什么它要使用内部类处理这个方法呢？也是比较有趣的，我这儿还没做深入研究，有时间研究下。 (2)使用两次循环实现。原理是第一次数组循环查找符合条件（或不符合条件）的个数count，后面在创建一个指定长度（原数组总长度-count）的数组，然后在遍历循环一遍原数组，将符合条件（或不符合条件）的元素添加进新的数组。 优点：简单直接，易于理解，基本数据类型数组的处理应该比List方法有优势。 缺点：两次循环应该比较耗时，对于长Array应该显现的明显。 代码如下： 12345678910111213141516171819202122232425262728293031323334/** * 过滤形成新的数组（两次循环查找符合条件的，移动过去） * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray2(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; int count=0; for (T t:sourceArray)&#123; if(predicate.test(t))&#123; count++; &#125; &#125; //都不符合条件 if(count==0)&#123; return Arrays.copyOf(sourceArray,sourceArray.length); &#125; //都符合条件 if(count==sourceArray.length)&#123; return (T[]) Array.newInstance(type,0); &#125; T [] targetArray=(T[]) Array.newInstance(type,sourceArray.length-count); int index=0; for (T t:sourceArray)&#123; if(!predicate.test(t))&#123; targetArray[index]=t; index++; &#125; &#125; return targetArray;&#125; 注意：还有一种情况，如果过滤的数据（符合条件或者不符合条件的）出现次数较低，我们是不是可以考虑一次拿出一整段进行处理。当然，如果频率较高，比如在一堆自然数中取偶数，明显奇数偶数出现频率相近，都为50%，那么我们可能用上面这种方法效率也很好。 (3)预先设置等长数组，而后截取得到目标数组对比上一种方法，这是一种空间换时间的做法。 开始时创建一个和原数组相同大小的数组，遍历后把元素放进去，最后将数组截短。这种方法仅仅循环一次。 代码如下： 123456789101112131415161718192021222324252627/** * 对比第二种方法，这属于空间换时间的做法 * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray3(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //直接创建一个空的一样长的数组 T[] tempArray=(T[]) Array.newInstance(type,sourceArray.length); //不符合条件的数量 int count=0; for (T t:sourceArray)&#123; //拿到不符合过滤条件的，一个个赋值给新数组 if(!predicate.test(t))&#123; tempArray[count]=t; count++; &#125; &#125; //最后这个数组长度&lt;=原数组长度 //特殊处理下 if(count==0)&#123; return (T[]) Array.newInstance(type,0); &#125; return Arrays.copyOf(tempArray,count);&#125; (4)借助List的toArray方法借助List实现循环一次把符合条件的放到里面，再把List强转成数组。也是不错的实现方法。 代码如下： 123456789101112131415161718/** * 借助list拿到符合条件的，在强转成数组 * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray4(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //记录符合条件的元素下标 List&lt;T&gt; targetList=new ArrayList&lt;&gt;(); for (T t:sourceArray)&#123; if(!predicate.test(t))&#123; targetList.add(t); &#125; &#125; return targetList.toArray((T[]) Array.newInstance(type,0));&#125; 无论上面哪种方法，其底层都使用了System.arraycopy方法。 12345678910/*** 数组复制核心方法* @param src 原数组* @param srcPos 原数组要复制的起始位置下标* @param dest 目标数组* @param destPos 目标数组的起始位置下标* @param length 要复制的长度* @return*/public static native void arraycopy(Object src, int srcPos,Object dest, int destPos,int length); (5)Java8串行处理方式Java8中使用Stream操作集合工具类来对其进行处理。分为串行和并行两种方式。先来看看串行。 代码如下： 1234567891011121314/** * Java8 串行流语法(收集符合条件的) * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray5(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; return Arrays.stream(sourceArray).sequential(). filter(predicate). collect(Collectors.toList()). toArray((T[]) Array.newInstance(type,0));&#125; 其底层也是元素的循环遍历。 （6）Java8并行处理方式我们应该知道，无论方法怎样，至少应该遍历数组一次以判断该元素是否符合条件。当数据量较大时，这儿会成为方法运行时间的瓶颈，由于List家族中ArrayList是有序的，我们可以使用多线程对它进行分割，每段进行遍历筛选结果，最后再把结果合起来。 并行流就是利用分支/合并框架实现的，使用了多线程。当数组数据量较大时效率是明显的。 Java8的相关API已经封装好，我们可以直接使用。 代码如下： 1234567891011121314/** * Java8 并行流语法(收集符合条件的) * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray6(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; return Arrays.stream(sourceArray).parallel(). filter(predicate). collect(Collectors.toList()). toArray((T[]) Array.newInstance(type,0));&#125; (7)自己动手，丰衣足食我们其实是可以借助多线程自己实现一个相似工具类的。 可以使用分支/合并框架自己实现一个多线程的处理。 关于这一块，我有一篇文章 一道Java试题引发的思考中有具体例子及测试。 大家可以看下，数据量大的情况下并行效率还是比较明显的。 我这儿对这个例子就不在验证了。 注意：使用并行流（或者说多线程）要注意的点。 首先是数据量，数据量的主要意义就在于单线程处理的耗时（处理数据的时间）已经超过了多线程耗时（数据处理时间+拆分数据时间+合并结果时间），这一点是比较难把控的。其次一点是要确定这些数据可以使用多线程处理，不会产生意外的情况，比如我们这个问题，我想删除两个数相差1的所有元素，剩下的元素生成一个新的数组，多线程显然不易解决这种问题，或者解决起来较复杂。 测试测试的话今天就省了，（3）、（4）、（5）都是不错的写法，（6）话具体问题具体分析，（7）的话有想法的可以试试，（1），（2）不推荐。 因为数据量大小，数据类型都对方法有些影响。 比如较短的原数组，基本数据类型，（3）方法效率很快的，对比（4）、（5）是没有数据拆箱操作的。换成长数组，引用数据类型，（6）可能效率就高了。 结语开始写这篇文章的时候脑子不好使，根本没想到Java8的Stream，失误。我甚至一开始想的都是些可能不安全（（1）方法），或者比较繁琐（（2）方法）的方法，过了一天晾了晾脑子就好使多了。 在处理时，因为数组底层操作都是基于System.arraycopy嘛，我想到是不是循环一次记录符合条件（不符合条件）的元素下标（可用List记录），然后建立一个目标数组，使用System.arraycopy一段一段的将数据copy进去。 无奈才疏学浅，想了半天使用System.arraycopy时的两个起始位置，copy长度始终弄错了，仔细想了下，估计这种方法效率也不怎么高。 哈哈，于是就没写。 大家有什么好的、耳目一新的方法也可以说出来交流交流。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ简介]]></title>
    <url>%2Fblog%2FRocketMQ%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言 为什么要学习RocketMQ？ RocketMQ是一个队列模型的的消息中间件，具有高性能、高可靠、高实时、分布式等特点。 其主要特点如下： 生产者、消费者、队列都可以分布式。 能够保证严格的消息顺序。 提供丰富的消息拉取模式。 高效的订阅者水平扩展能力。 实时的消息订阅机制。 亿级消息堆积能力。 较少的依赖。 RocketMQ的基本原理就是生产者（Producer）向一些队列轮流发送消息，队列集合称为Topic，消费者（Consumer）如果做广播消费，则一个消费者实例消费这个Topic对应的所有队列，如果做集群消费，则多个消费者实例平均消费这个topic对应的队列集合。 消息中间件收发消息的典型模型如下图： 好吧，我承认前言太官方了。我们来看下正文关于RocketMQ的一些特点吧。 正文RocketMQ术语。先介绍RocketMQ的几个术语。 Producer 消息生产者，生产者的作用就是将消息发送到Broker（MQ），生产者本身既可以产生消息。也可以对外提供接口，由外部应用来调用接口，再由生产者将收到的消息发送到 Broker（MQ）。 Producer Group 生产者组，一般来说多个发送同一类消息的生产者称之为一个生产者组。 Consumer 消息消费者，消费 Broker（MQ） 上的消息的应用程序就是消费者，消息的处理取决于业务。 Consumer Group 消费者组，和生产者类似，消费同一类消息的多个 consumer 实例组成一个消费者组。 Topic Topic 是一种消息的逻辑分类，比如说你有支付订单类的消息，也有奖品抽取类的消息，那么就需要进行分类，一个是支付订单 Topic 存放支付订单相关的消息，一个是奖品抽取 Topic 存储抽取奖品结果相关的消息。 Message Message 是消息的载体。一个 Message 必须指定 topic。Message 还有一个可选的 tag 设置，以便消费者可以基于 tag 进行过滤消息。也可以添加额外的键值对等。 Tag 标签可以被认为是对 Topic 进一步细化。一般在相同业务模块中通过引入标签来标记不同用途的消息。 Broker Broker 是 RocketMQ系统的主要角色。Broker 接收来自生产者的消息，储存以及为消费者拉取消息的请求做好准备。 Name Server Name Server 为 producer 和 consumer 提供路由信息。 对于上面的几个概念的理解，可以从一封邮件的发送入手，认为发信人是Producer，收信人为Consumer，Message，Topic和Tag分别指信的内容，信的分类规则等，Broker就相当于邮局了。 RocketMQ集群架构再来看一下RocketMQ常用的物理部署结构。（生产环境常用的RocketMQ集群架构） RocketMQ的集群特点如下： Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server。 Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。要注意Producer并不能和 Broker Slave建立连接。 Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 Broker Master和 Broker Slave之间会进行数据同步，即Data Sync。 数据复制主要有两种： ①同步复制 SYNC_MASTER master 和 slave 都写成功后返回成功状态。好处是如果master出故障，slave上有全部备份，容易恢复。缺点是增大延迟，降低吞吐量。 ②异步复制 ASYNC_MASTER 只要 master 写成功就返回成功状态。好处是低延迟、高吞吐，缺点是如果 master 出故障，数据没有写入 slave，就会有丢失。 master或者 slave在返回处理成功消息时，需要将数据持久化（写入磁盘）。称之为MQ的刷盘。 刷盘策略也有两种： ①同步刷盘 SYNC_FLUSH 返回成功状态时，消息已经被写入磁盘。 消息写入内存后，立即通知刷盘线程，刷盘完成后，返回消息写成功的状态。 ②异步刷盘 ASYNC_FLUSH 返回成功状态时，消息只是被写入内存，写操作返回快，吞吐量达，当内存里的消息积累到一定程度时，统一出发写磁盘动作，快速写入。 RocketMQ部署模式根据上面的描述，我们也大致可以知道RocketMQ通常有3种部署模式。 单master部署 简单来说就是环境上就一台RocketMQ，一般自己开发研究学习会用这种模式。这种部署明显的缺点就是MQ宕机后系统就会挂掉。所以不用于测试或生产环境。 多master部署 就是环境上有多台RocketMQ，都充当master的角色。这种方式部署，如果有一台MQ挂掉或者重启，并不会对整个应用系统造成影响。但是该节点MQ宕机期间，未消费的MQ消息是无法被消费的，只有等该节点故障排除后才能恢复正常。如果无法恢复，则会造成RocketMQ消息的永久丢失。 这种方式，如果是使用的异步刷盘，则故障发生并恢复正常后可能丢失部分消息（写入内存没来得及写入磁盘的消息）；如果同步刷盘，则不会出现此情况，但同步刷盘会对MQ的响应速度造成一些影响。 多master 多slave部署 根据刷盘策略和数据复制策略。我们知道这种部署方式基本有四种组合策略。同步复制同步刷盘明显是效率最差但最安全的，异步复制异步刷盘明显是效率最好但最不安全的。 一般情况下，我们比较常用的是异步刷盘+同步复制的模式。这样即使master 宕机，同步复制的slave也能保证把消息写入磁盘。同时使用异步刷盘策略，因为写入磁盘本身是应用系统中耗时的一个操作，先写入内存，随后写入磁盘，可以保证MQ的响应速度。 结语这一篇文章简单介绍了RocketMQ的一些性质和特点，了解这些特点，才能更好的掌握RocketMQ，后面我们会搭建一个RocketMQ服务进行下学习，并对我们的调用代码做些优化，封装为自己的一个小小工具类。 回答下最开始的问题，因为我们公司用到了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring Transaction 注解不生效bug引发的思考]]></title>
    <url>%2Fblog%2F%E4%B8%80%E4%B8%AASpring-Transaction-%E6%B3%A8%E8%A7%A3%E4%B8%8D%E7%94%9F%E6%95%88bug%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83.html</url>
    <content type="text"><![CDATA[前言某日，在项目测试代码过程中，发现一个问题，对于一个方法A（无事务），调用B方法（有事务），当A，B方法在同一个类中的时候，在B方法上的事务注解是不生效的！ 同事说将B方法写到新的Service类中就可以解决，遂试之，确实得以解决。但不解其原理，问同事、查资料均感觉不如意。故分析了下Spring 事务的部分源码。有所见解，特此记录。 下图就是我描述的这种情况，B事务不生效的问题。 1.测试类 2.实现类 我们经过测试可以发现，当insert方法有事务、但被该实现类内部方法doInsert调用后，即使insert方法出现异常，该方法的数据库操作也不会回滚。 3.数据没有回滚，已经入库 正文要理解研究这种情况，我们先来简单说下Spring 的注解方式的事务实现机制。 事务的一些基础我在一篇文章中有介绍 https://www.sakuratears.top/blog/Spring-Transactional%E6%B3%A8%E8%A7%A320181013/ 不懂得可以先大致看看。 Spring注解方式的事务实现机制在应用系统调用声明@Transactional 的目标方法时，Spring Framework 默认使用 AOP 代理，在代码运行时生成一个代理对象，根据@Transactional 的属性配置信息，这个代理对象决定该声明@Transactional 的目标方法是否由拦截器 TransactionInterceptor 来使用拦截，在 TransactionInterceptor 拦截时，会在在目标方法开始执行之前创建并加入事务，并执行目标方法的逻辑, 最后根据执行情况是否出现异常，利用抽象事务管理器AbstractPlatformTransactionManager 操作数据源 DataSource 提交或回滚事务, 如图。 Spring AOP 代理有 CglibAopProxy 和 JdkDynamicAopProxy 两种，上图以 CglibAopProxy 为例，对于 CglibAopProxy，需要调用其内部类的 DynamicAdvisedInterceptor 的 intercept 方法。对于 JdkDynamicAopProxy，需要调用其 invoke 方法。当然我们也可以使用AspectJ的方式实现AOP代理，这儿不做过多介绍。 事务管理的框架是由抽象事务管理器 AbstractPlatformTransactionManager 来提供的，而具体的底层事务处理实现，由 PlatformTransactionManager 的具体实现类来实现，如事务管理器 DataSourceTransactionManager。不同的事务管理器管理不同的数据资源 DataSource，比如 DataSourceTransactionManager 管理 JDBC 的 Connection。 PlatformTransactionManager，AbstractPlatformTransactionManager 及具体实现类关系图如下。 一次正常事务调试出现问题，debug是比较好的解决方法。我们大致跟下SpringTransaction的使用过程。先从正确流程入手吧。如下：直接将事务注解加在doInsert方法上，明显，这种情况下出现异常事务会回滚。我们debug下事务大致的回滚过程。 DefaultAopProxyFactory里的createAopProxy方法可以拿到看到该方法具体使用的哪种代理。 可以看到我们这个类使用了Cglib代理。使用了Cglib代理，上面讲到 对于 CglibAopProxy，需要调用其内部类的 DynamicAdvisedInterceptor 的 intercept 方法。我们继续断点跟踪下。 一步步进行，如事务图所示，进入了TransactionInterceptor的invoke方法，并执行invokeWithinTransaction方法。 继续跟踪。来到了TransactionAspectSupport，这是spring事务处理的关键类，谨记。 会进行事务的创建，createTransactionIfNecessary getTransaction方法会开启一个事务。 根据上面debug看到的事务管理器是DataSourceTransactionManager， 执行getTransaction会调用它的doBeigin方法。 可以看到把自动提交设置成了false，并且暂时保存了原来的自动提交属性状态。 而后可以看到他将当前事务信息绑定在了ThreadLocal里了。 执行我们添加事务注解的方法，抛出了异常被捕获。 执行completeTransactionAfterThrowing方法，我们的异常正好是这个异常（或者其父类）。 然后执行回滚操作，最终到达下图所示方法（DataSourceTransactionManager的doRollback） 回滚具体代码不在介绍，我们可以看到在回滚时它把原来数据库的自动提交属性改了过来。 最后他会把本次事务状态清除，相当于保存上一次的事务状态。 注意： 在spring启动时获取事务注解时我们可以看到下图。 这个说明Spring AOP 事务注解只能作用于public 方法。 关于事务回滚rollbackFor条件的问题，我们可以看到下图。 当我们事务注解配置具体的回滚条件，如rollbackFor = Exception.class，只要是Exception.class或者其子类，都可以实现事务回滚。它会通过RuleBasedTransactionAttribute.class这个类去校验抛出的异常是否符合条件。进而判断是否需要回滚。 但是当我们不声明rollbackFor 条件时，这儿应该注意一点。它会使用默认的条件，而不是不处理异常。主要由DefaultTransactionAttribute.class 里的rollbackOn方法实现。我们看下这个方法，可以发现，他只会处理RuntimeException和Error。也就是说，如果我们一个方法有事务，但抛出了非RuntimeException（如检查时异常等），且事务没有声明rollbackFor回滚条件，那么，它是不会触发事务回滚的。这一点要注意。 上图调用RuleBasedTransactionAttribute.class的方法，回滚规则为空，使用父类rollbackOn方法。 通过调试可以轻松看到这一情况，这儿不在做过多赘述。 异常事务调试我们来看下事务不成功的情况。就是题目开始的问题。 Spring刚启动时，会扫描需要进行代理的类，生成代理对象，在AdvisedSupport.class类中，把类中的方法缓存起来。 首先查询该方法是不是需要拦截（是不是有事务注解） 在TransactionAttributeSourcePointcut.class 类里的matches方法，查询事务注解情况。 查到了就缓存起来了。 当doInset方法进入时，同样的逻辑。也会缓存起来，但是cached是值为null。 该类的其他方法也会被缓存，没有事务注解的都放为null。 开始执行doInsert方法时，进入CglibAopProxy的intercept方法。 可以看到尝试拿缓存，但缓存的值为空。 尝试获取一下，显然也是没有值的。所以这时候认为不需要进行事务。事务链为空。 就直接执行了方法doInsert。并不会开启事务。（不为空的话会创建一个CglibMethodInvocation并开启事务执行方法，如上面开始的情况）。 当生成一个动态代理对象后，对这个对象引用中方法的调用就是对代理的调用，而这个代理能够代理所有跟特定方法调用相关的拦截器。不过，一旦调用最终抵达了目标对象 （此处为TransactionalTestImpl类的引用），任何对自身的调用例如insert将对this引用进行调用而非代理。这一点意义重大， 它意味着自我调用将不会导致和方法调用关联的通知得到执行的机会。 如果需要insert的事务生效，一种典型的方法就将方法insert放到新的类中，这便很好理解了。因为新的类会生成新的动态代理对象，调用源从而获得通知。 如果我非要在本类中实现通知呢？ 那我们就需要直接获取代理对象调用insert方法了。如下图。 要实现这个功能，需要开启Spring AspectJ支持，我使用的Springboot，启动类上加入如下注解，并引入如下依赖。 这个pom文件你进去可以看到就是引用了AspectJ 的相关jar包。 这个时候我们在测试一下，就会发现事务生效了。 在CglibAopProxy中可以看到如下代码，可以明白开启后它把代理对象绑定到ThreadLocal上等待insert方法执行的通知。 当然，如果这两个方法上都存在事务，它也会进行判断处理，也就是事务的传播属性，他们主要通过AbstractPlatformTransactionManager这个类（这个类也很重要）的getTransaction方法和handleExistingTransaction方法来进行事务传播属性的处理。这儿不做过多讲解，自己看看逻辑处理即可。 getTransaction部分代码： handleExistingTransaction部分代码： 结语总的来说，通过一个问题，我们大致看了下Spring Transactional注解的实现过程。并分析了产生这种问题的原因，通过有效的手段来进行验证。还是蛮不错的一次体验。 下面总结下： A：在Spring中，一个类中无事务注解的方法A调用有事务注解的方法B，默认情况下B出现异常事务是不会进行回滚的。 解决方法： 将B写到一个新的方法中。（原理上是生成不同类的动态代理对象，实际中比较常用的一种手段，但需要管理一个新的类） 如果业务（情形等）允许，可以将事务移动到A上，或者B的事务不动，给A也加一个事务。（根据具体情况讨论，有时候效果很好，有时候不适宜，使用此种方法可能影响程序效率或者产生莫名其妙的bug，慎用） 启用增强型事务，引入AspectJ。（不太常用的一种手段，但如果项目中本来已经引入了AspectJ并且开启了增强型事务管理，何乐而不为呢？） B：研究过程中发现的其他应该注意的坑。 事务注解应当作用在public方法上，需要注意。 如果不设置事务回滚条件（rollbackFor参数为空），它能捕获RuntimeException及其子类 和 Error及其子类 出现的异常情况并回滚，其他异常是无法捕获并回滚的。如IOException（检查型异常）等。 事务的传播属性的几个应该了解，不能乱用，虽然我们可能就用到过或者就用到了Propagation.REQUIRED ，但不代表其它不会用到。 C：本次研究学习我们应该理解掌握的。 Spring事务的处理过程。（Spring AOP的体现，应用反射和动态代理） 事务的一些性质。（事务的传播属性、事务的四大特性等） 其它一些需要学习的地方。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何以并发方式在同一个流上执行多种操作]]></title>
    <url>%2Fblog%2F%E5%A6%82%E4%BD%95%E4%BB%A5%E5%B9%B6%E5%8F%91%E6%96%B9%E5%BC%8F%E5%9C%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E6%B5%81%E4%B8%8A%E6%89%A7%E8%A1%8C%E5%A4%9A%E7%A7%8D%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[前言Java 8中，流有一个非常大的局限性，使用时，对它操作一次仅能得到一个处理结果。当流进行终端操作后，如果你在试图遍历它，就会出现异常。 1java.lang.IllegalStateException: stream has already been operated upon or closed 虽然流就是如此设计的，但是我们有时候就希望可以通过流获取多个结果。或者说，你希望一次性向流中传入多个Lambda表达式。 为了达到这一目标，我们应该需要一个fork类型的方法，对每个复制的流应用不同的函数。理想情况下，这些操作也应该支持并行去拿到运算结果。 这一特性在Java 8中是没有的，不过我们可以利用一个通用API，即Spliterator，尤其是它的延迟绑定能力，结合BlockingQueues和Futures来实现这一特性。 正文复制流要达到此效果，我们首先应该创建一个StreamForker，它会对原始的流进行封装，在此基础上在执行各种操作。我们来看下代码。 12345678910111213141516public class StreamForker&lt;T&gt; &#123; private final Stream&lt;T&gt; stream; private final Map&lt;Object, Function&lt;Stream&lt;T&gt;,?&gt;&gt; forks=new HashMap&lt;&gt;(); public StreamForker(Stream&lt;T&gt; stream) &#123; this.stream = stream; &#125; public StreamForker&lt;T&gt; fork(Object key,Function&lt;Stream&lt;T&gt;,?&gt; f)&#123; //使用一个键对流上的函数进行索引 forks.put(key,f); //返回this从而保证多次顺畅的调用fork方法 return this; &#125; public Results getResults()&#123; //TODO &#125;&#125; fork方法接受两个参数。 Function:对流进行处理，转变成这些操作结果的类型。 key: 通过它拿到结果，这些结果被放到内部的一个Map中。 fork方法需要返回自身，这样可以复制多个操作构成流水线。 如图： 上图不难理解。 而由fork方法添加的操作如何执行呢，就是通过getResults方法的调用触发，该方法返回一个Results接口的实现。接口定义如下： 123public interface Results &#123; public &lt;R&gt; R get(Object key);&#125; 实现Results接口我们使用ForkingStreamConsumer实现Results接口。 123456789public Results getResults()&#123; ForkingStreamConsumer&lt;T&gt; consumer=build(); try&#123; stream.sequential().forEach(consumer); &#125;finally &#123; consumer.finish(); &#125; return consumer;&#125; ForkingStreamConsumer同时实现了Results和Consumer接口。其主要任务就是来处理流元素，将他们分发到多个BlockingQuenes中处理，BlockingQuenes的数量和通过fork方法提交的操作数是一致的。这里的getResults的实现，流应该是顺序处理的，否则，forEach后元素的顺序就会变化。finish方法用来表明队列中没有更多要处理的元素了。build方法主要用于创建ForkingStreamConsumer。代码如下： 123456789101112131415161718private ForkingStreamConsumer&lt;T&gt; build()&#123; //创建由队列组成的列表，每一个队列对应一个操作 List&lt;BlockingQueue&lt;T&gt;&gt; queues=new ArrayList&lt;&gt;(); //建立用于标识操作的键与包含操作结果的Future之间的映射关系 Map&lt;Object,Future&lt;?&gt;&gt; actions= forks.entrySet().stream().reduce( new HashMap&lt;Object,Future&lt;?&gt;&gt;(), (map,e)-&gt;&#123; map.put(e.getKey(),getOperationResult(queues,e.getValue())); return map; &#125;, (m1,m2)-&gt;&#123; m1.putAll(m2); return m1; &#125; ); return new ForkingStreamConsumer&lt;&gt;(queues,actions);&#125; 可以看到，我们先创建了BlockingQuenes列表。接着创建了一个Map，Map的键就是用来标识不同操作的键，值包含着Future里。最终BlockingQuenes和Map会被传递给ForkingStreamConsumer的构造函数。每个Future通过关键方法getOperationResult创建。 来看看getOperationResult的实现。 1234567891011private Future&lt;?&gt; getOperationResult(List&lt;BlockingQueue&lt;T&gt;&gt; queues,Function&lt;Stream&lt;T&gt;,?&gt; f)&#123; //创建一个队列，并将其添加到队列的列表中 BlockingQueue&lt;T&gt; queue=new LinkedBlockingDeque&lt;&gt;(); queues.add(queue); //创建一个Spliterator，遍历队列中的元素 Spliterator&lt;T&gt; spliterator=new BlockingQueueSpliterator&lt;&gt;(queue); //创建一个流，将Spliterator作为数据源 Stream&lt;T&gt; source= StreamSupport.stream(spliterator,false); //创建一个Future对象，以异步方式计算在流上执行特定函数的结果 return CompletableFuture.supplyAsync(()-&gt;f.apply(source));&#125; 该方法创建一个新的BlockingQuene，并将其添加到队列列表。队列会被传递给一个新的BlockingQueueSpliterator对象，后者是一个延迟绑定的Spliterator。然后我们创建一个顺序流对Spliterator进行遍历，最终创建一个Future收集结果。 开发ForkingStreamConsumer代码如下： 1234567891011121314151617181920212223242526public class ForkingStreamConsumer&lt;T&gt; implements Consumer&lt;T&gt;,Results &#123; public static final Object END_OF_STREAM=new Object(); private final List&lt;BlockingQueue&lt;T&gt;&gt; queues; private final Map&lt;Object, Future&lt;?&gt;&gt; actions; public ForkingStreamConsumer(List&lt;BlockingQueue&lt;T&gt;&gt; queues, Map&lt;Object, Future&lt;?&gt;&gt; actions) &#123; this.queues = queues; this.actions = actions; &#125; @Override public &lt;R&gt; R get(Object key) &#123; try &#123; return ((Future&lt;R&gt;)actions.get(key)).get(); &#125;catch(Exception e)&#123; throw new RuntimeException(e); &#125; &#125; @Override public void accept(T t) &#123; //将流中遍历的元素添加到所有的队列中 queues.forEach(q-&gt;q.add(t)); &#125; void finish()&#123; //将最后一个元素添加到队列中，表明该流已经结束 accept((T)END_OF_STREAM); &#125;&#125; 这个类同时实现了Consumer接口和Results接口。 Consumer接口要求实现accept方法，每当ForkingStreamConsumer接受流中的一个元素，它就会将元素添加到所有BlockingQuenes中当所有元素都添加到所有队列后，finish方法将最后一个元素添加到所有队列。处理时碰上这个元素表明后面没有元素要处理了。 Results接口需要实现get方法。一旦处理结束，get方法会获取Map中由键索引的Future，解析到结果后返回。 每有一个操作，就会对应一个BlockingQueueSpliterator。我们来看下BlockingQueueSpliterator的实现。 开发BlockingQueueSpliterator12345678910111213141516171819202122232425262728293031323334public class BlockingQueueSpliterator&lt;T&gt; implements Spliterator&lt;T&gt; &#123; private final BlockingQueue&lt;T&gt; q; public BlockingQueueSpliterator(BlockingQueue&lt;T&gt; q) &#123; this.q = q; &#125; @Override public boolean tryAdvance(Consumer&lt;? super T&gt; action) &#123; T t; while (true)&#123; try &#123; t=q.take(); break; &#125;catch(InterruptedException e)&#123; &#125; &#125; if(t!=ForkingStreamConsumer.END_OF_STREAM)&#123; action.accept(t); return true; &#125; return false; &#125; @Override public Spliterator&lt;T&gt; trySplit() &#123; return null; &#125; @Override public long estimateSize() &#123; return 0; &#125; @Override public int characteristics() &#123; return 0; &#125;&#125; 可以看到该Spliterator未定义任何切割流的策略，仅仅利用了流的延迟绑定能力。也没有实现trySplit方法。由于我们的操作数是不确定的，故estimateSize不能提供任何有意义的数字，返回0.也没有体现Spliterator的特性，故characteristics返回0. 仅仅实现了tryAdvance方法，它从BlockingQueue中取得原始流元素，进一步传给Consumer对象。当返回true时表明还有元素要处理，直到发现最后一个元素时终止。 以上基本上是在一个流上执行多种操作的代码。 我们下面来检测一下正确性。 测试编写测试类。如下数据： 123456789101112131415161718public static void main(String[] args) &#123; //生成1到1000的数组 List&lt;Integer&gt; list1=IntStream.rangeClosed(1,1000).filter(n-&gt;n%2==0).boxed().collect(Collectors.toList()); List&lt;Integer&gt; list2=IntStream.rangeClosed(1,1000).filter(n-&gt;n%5==0).boxed().collect(Collectors.toList()); //同时对list1数据求和，统计list1数据数量，统计list1和list2相同元素，统计list1和list2相同元素的最大值和最小值 Results results=new StreamForker&lt;Integer&gt;(list1.stream()) .fork("sum",s-&gt;s.mapToInt(Integer::intValue).sum()) .fork("count",s-&gt;s.count()) .fork("list3",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).collect(Collectors.toList())) .fork("max",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).max(Comparator.naturalOrder())) .fork("min",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).min(Comparator.naturalOrder())) .getResults(); System.out.println("sum="+results.get("sum")); System.out.println("count="+results.get("count")); System.out.println("max="+((Optional) results.get("max")).get()); System.out.println("min="+((Optional)results.get("min")).get()); ((List&lt;Integer&gt;)results.get("list3")).stream().forEach(System.out::println);&#125; 输出结果： 可以看到，使用了一个流，通过我们实现的方法进行了多次终端操作返回正确结果。 性能问题这是我们用一个流实现多种终端操作的方式，当然这并不意味着会比普通的写法效率高，如果对于上述问题，我们可以分个构建若干个流进行一一实现。 这种一个流进行多个终端操作的情况使用，一定是生成流比较耗费资源性能时才会用到，比如操作一个较大文件时生成的字符流，我们想统计字数，检查某些单词出现的次数，统计行数等等操作，重复生成流显然是耗费资源的。这种情况可以考虑使用这种一个流进行多个终端操作的实现。 当然，具体到具体问题优化，建议认真分析两者的资源消耗。这是比较稳妥的做法。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[封装一个属于自己的Redis API]]></title>
    <url>%2Fblog%2F%E5%B0%81%E8%A3%85%E4%B8%80%E4%B8%AA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84Redis-API.html</url>
    <content type="text"><![CDATA[前言Redis作为一款强大的key-value型数据库，其应用是十分广泛的。在Java语言中，常用来与Redis数据库建立连接用到的是Jedis Pool连接池。 今天我们来简单了解下它们然后实现一个可移植的操作Redis的API。 正文知识准备我们知道Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。并提供了一系列的命令操作这些数据类型。 Jedis相当于对这些操作进行了代码封装，及提供了一些其它常用操作。 我们先来了解下Jedis的连接池配置参数。 commons-pool2 有一个配置类GenericObjectPoolConfig里面的通用参数设置如下： 参数 说明 默认值 备注 maxTotal 说明一个pool最多可以有多少个Jedis实例 8 -1表示不限制 maxIdle 一个pool最多可以有多少个空闲的Jedis实例 8 minIdle 一个pool最少有多少个空闲的Jedis实例 0 可以看到它继承BaseObjectPoolConfig。我们可以看到BaseObjectPoolConfig的参数如下。 部分参数意义如下： 参数 说明 默认值 备注 lifo pool中的idle列表是双端队列，设定是否last in first out true maxWaitMillis 当active数量为max时,等待的时长 -1L(代表一直等) 配合blockWhenExhausted使用 blockWhenExhausted 当active数量为max时，是否阻塞等待一段时间 true testOnCreate 创建实例时有效性检测 false testOnReturn 归还实例时有效性检测 false testOnBorrow 借出实例时有效性检测 false 思路分析首先Redis连接池属性我们应当放置在配置文件里，解析并获得，连接池最好设计成单例的，每次不用在初始化过多连接资源。同时Redis有单机模式和集群模式区分，这两种模式我们也应该区分开来。单机模式下，可以选择多个database，集群模式下只能选择database0.集群模式下，如果redis地址过多，我们如何分开呢？ 我们可以考虑如下样式：address =127.0.0.1:6379;127.0.0.1:6380 每个redis地址用分号分隔，解析配置时把每个解析到并建立连接。 当然，最后完成JedisPool的创建后，我们应该编写工具类对一些常用操作方法进行封装，便于我们使用。 代码我们根据上述思路，构造了如下图所示的小项目。 其中： RedisException是用来统一处理程序过程中的异常的类。 JedisFactory可以认为是一个JedisPool工厂，用来提供单机模式的连接池或者集群模式的连接池。 RedisConfiguration是与配置文件对应的配置类，用于存放配置的数据。 RedisConstants用来放置一些项目中用到的常量。 RedisUtil工具类接口，提供了多种操作Redis的方法。 RedisSingleUtil工具接口的单机模式实现。 RedisClusterUtil工具接口的集群模式实现。 redis-config.properties Redis的配置文件存放 JedisFactory和RedisUtill为主要类。我们看下他们的具体实现。 JedisFactory的主要代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697private volatile JedisPool jedisPool; private volatile JedisCluster jedisCluster; private RedisConfiguration redisConfig; private Pattern addRessPattern = Pattern.compile("^.+[:]\\d&#123;1,5&#125;\\s*(;.+[:]\\d&#123;1,5&#125;\\s*)*[;]?\\s*$"); public JedisFactory(final RedisConfiguration redisConfiguration)&#123; this.redisConfig=redisConfiguration; &#125; public JedisPool getJedisPool()&#123; if(jedisPool==null)&#123; synchronized (JedisFactory.class)&#123; if(jedisPool==null)&#123; init(); &#125; &#125; &#125; return jedisPool; &#125; public JedisCluster getJedisCluster()&#123; if(jedisCluster==null)&#123; synchronized (JedisFactory.class)&#123; if(jedisCluster==null)&#123; init(); &#125; &#125; &#125; return jedisCluster; &#125; public void init()&#123; logger.info("JedisFactory init start..."); try&#123; if(StringUtils.isNotBlank(redisConfig.getLocalPropertiesPath()))&#123; fillData(); &#125; logger.info("redis config is: &#123;&#125;.", redisConfig.toString()); Set&lt;HostAndPort&gt; hostAndPortSet = this.parseHostAndPort(redisConfig.getAddress()); GenericObjectPoolConfig genericObjectPoolConfig = new GenericObjectPoolConfig(); genericObjectPoolConfig.setMaxWaitMillis(redisConfig.getMaxWaitMillis()); genericObjectPoolConfig.setMaxTotal(redisConfig.getMaxTotal()); genericObjectPoolConfig.setMinIdle(redisConfig.getMinIdle()); genericObjectPoolConfig.setMaxIdle(redisConfig.getMaxIdle()); if(redisConfig.getMode()== RedisConstants.REDIS_MODE_SINGLE)&#123; HostAndPort hostAndPort=(HostAndPort)hostAndPortSet.toArray()[0]; jedisPool=new JedisPool(genericObjectPoolConfig, hostAndPort.getHost(), hostAndPort.getPort(), redisConfig.getTimeout(), null,redisConfig.getDatabase()); logger.info("jedisPool init is finished"); &#125;else&#123; if(redisConfig.getDatabase()!=0)&#123; logger.warn("当前配置的database为："+redisConfig.getDatabase()+",集群模式下不能选择database，只能使用database0"); &#125; jedisCluster = new JedisCluster(hostAndPortSet, redisConfig.getTimeout(), redisConfig.getMaxRedirections(), genericObjectPoolConfig); logger.info("jedisCluster init is finished"); &#125; &#125;catch(Exception ex)&#123; throw new RedisException(ex); &#125; &#125; private void fillData() throws Exception &#123; Properties localProperties = PropertiesUtils.loadLocalProperties(redisConfig.getLocalPropertiesPath()); String address=localProperties.getProperty("address", ""); if (StringUtils.isBlank(address)) &#123; throw new RedisException("error:redis config address is blank!"); &#125; // 设置初始值 long maxWaitMillis=Long.parseLong(localProperties.getProperty("maxWaitMillis", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_WAIT_MILLIS))); int maxTotal=Integer.parseInt(localProperties.getProperty("maxTotal", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_TOTAL))); int minIdle=Integer.parseInt(localProperties.getProperty("minIdle", String.valueOf(GenericObjectPoolConfig.DEFAULT_MIN_IDLE))); int maxIdle=Integer.parseInt(localProperties.getProperty("maxIdle", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_IDLE))); int timeout=Integer.parseInt((localProperties.getProperty("timeout", "2000"))); int maxRedirections=Integer.parseInt((localProperties.getProperty("maxRedirections", "6"))); int database=Integer.parseInt((localProperties.getProperty("database", "0"))); //1单机模式，2集群模式 int mode=Integer.parseInt((localProperties.getProperty("mode", String.valueOf(RedisConstants.REDIS_MODE_SINGLE)))); redisConfig.setAddress(address); redisConfig.setMaxWaitMillis(maxWaitMillis); redisConfig.setMaxTotal(maxTotal); redisConfig.setMinIdle(minIdle); redisConfig.setMaxIdle(maxIdle); redisConfig.setTimeout(timeout); redisConfig.setMaxRedirections(maxRedirections); redisConfig.setDatabase(database); redisConfig.setMode(mode); &#125; //部分代码略 对于RedisUtil接口，应有两个实现，单机和集群的，这里为了简化代码，只简单列举了一个方法。 1234public interface RedisUtil &#123; String setString(String key, String value); //部分代码略&#125; 单机模式的实现： 123456789101112public class RedisSingleUtil implements RedisUtil&#123; @Override public String setString(String key, String value) &#123; Jedis jedis = this.getResource(); try&#123; return jedis.set(key, value); &#125;finally&#123; this.closeResource(jedis); &#125; &#125; //部分代码略&#125; 集群模式的实现： 12345678public class RedisClusterUtil implements RedisUtil&#123; @Override public String setString(String key, String value) &#123; JedisCluster cluster=getResource(); return getResource().set(key, value); &#125; //部分代码略&#125; 其他方法及实现不在赘述，有兴趣的可以在 https://github.com/javazwt/framework-base 上查看相关代码。 测试1234567public class RedisTest &#123; private static RedisSingleUtil redisSingleUtil=new RedisSingleUtil(); public static void main(String[] args) &#123; redisSingleUtil.setString("str","123"); redisSingleUtil.getString("str"); &#125;&#125; 可以检测我们的正确性。 结束语经过封装后，我们可以把该工具类使用在任何项目上，提高开发效率，降低项目耦合性，同时对Redis有了更深入的认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[封装构建基于ES的Java API包]]></title>
    <url>%2Fblog%2F%E5%B0%81%E8%A3%85%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8EES%E7%9A%84Java-API%E5%8C%85.html</url>
    <content type="text"><![CDATA[前言前些日子，elastic公司成功上市了。旗下有一款优秀的开源搜索引擎ElasticSearch。 ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。 好了，扯远了，回归正题。今天我们使用Java对它的一些API进行封装，以使其API更具备灵活性。 完成对其简单的使用是比较简单的，我们今天把我们的一些调用代码封装，以便可以在各个项目中自由使用。这才是我们的根本目的。 今天，我们来构建一个工具包类。 环境准备安装ElasticSearch，主要就是为了测试我们的代码，对于有现成环境的，如开发环境，测试环境上的ElasticSearch，这一步可以忽略。 首先请安装ElasticSearch，这里就不介绍了，我这里是安装的ElasticSearch最新版，6.4.0版本。https://www.elastic.co/downloads/elasticsearch 。安装成功后可以将其添加到环境变量中，然后启动。启动成功浏览器访问http://localhost:9200/ 会看到json信息。 安装Kibana，Kibana是ElasticSearch的可视化工具，可以方便的查看ElasticSearch及其运行状态。https://www.elastic.co/products/kibana 。我安装的也是最新版本，6.4.0. 安装成功后加入环境变量，然后启动，访问http://localhost:5601/ 可以看到Kibana界面。 JDK版本为1.8 以上都为准备工作。 架构构建org.elasticsearch.client.transport jar包已经有相关关于ES API的操作。 我们把它封装为专用jar包，建议使用Maven构建。如下： Maven项目首先，我们先建一个名叫 framework-es的Maven项目。如下。 建好各个package。 exception里面定义我们的异常。 factory里面用来生成esclient。 util里面放一些工具类。 vo里面可以放查询对象等。 property为配置文件。 test里面存放测试类。 各个package的名字可以自己定义，描述清目的即可。如下图。 添加依赖添加依赖，我的pom.xml依赖如下。 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.31&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 思路我的思路是构建一个单例的ESClient，用于与ES建立连接，以后增删改查等逻辑均使用此Client，保证资源的高效利用。这个也应当支持ES集群，有多个ES服务端也应当支持。而且地址应为可配置的。 话不多说，上代码，主要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class ElasticSearchClientFactory &#123; private final static Logger logger = LoggerFactory.getLogger(ElasticSearchClientFactory.class); private volatile TransportClient esClient; //ES配置 private ElasticSearchConfiguration esConfig; //校验多个ES地址的正则 private Pattern addRessPattern = Pattern.compile("^.+[:]\\d&#123;1,5&#125;\\s*(;.+[:]\\d&#123;1,5&#125;\\s*)*[;]?\\s*$"); public ElasticSearchClientFactory(final ElasticSearchConfiguration elasticSearchConfiguration)&#123; this.esConfig=elasticSearchConfiguration; &#125; /** * 获取一个单例的ESClient * @return */ public Client getEsClient()&#123; if(esClient==null)&#123; synchronized (ElasticSearchClientFactory.class)&#123; if(esClient==null)&#123; logger.info("ElasticSearchClientFactory init start..."); try&#123; if(StringUtils.isNotBlank(esConfig.getLocalPropertiesPath()))&#123; //获取ES配置信息 fillData(); &#125; logger.info("ESConfig is:&#123;&#125;",esConfig.toString()); //多个ES地址解析 List&lt;HostAndPort&gt; hostAndPortList = this.parseHostAndPortList(esConfig.getAddress()); TransportAddress [] transportAddress=new TransportAddress[hostAndPortList.size()]; for (int i = 0; i &lt; hostAndPortList.size(); i++) &#123; transportAddress[i] = new TransportAddress(InetAddress.getByName(hostAndPortList.get(i).getIp()),hostAndPortList.get(i).getPort()); &#125; //节点名 String nodeName=esConfig.getNodeName()+ UUID.randomUUID(); String clusterName=esConfig.getClusterName(); Settings.Builder settingsBuilder = Settings.builder(); settingsBuilder.put("node.name", nodeName); if(StringUtils.isNotBlank(clusterName))&#123; settingsBuilder.put("cluster.name", clusterName); &#125; settingsBuilder.put("client.transport.sniff", true); Settings settings = settingsBuilder.build(); TransportClient client = new PreBuiltTransportClient(settings); //创建ESClient esClient = client.addTransportAddresses(transportAddress); logger.info("EalsticSearchClientFactory init is finished"); &#125;catch(Exception e)&#123; logger.error("EalsticSearchClientFactory create failed",e); throw new ElasticSearchException("EalsticSearchClientFactory create faile",e); &#125; &#125; &#125; &#125; return esClient; &#125; //其他代码略&#125; 以上代码用于生成一个单例的ESClient类。 123public class ElasticSearchConstants &#123; public final static String DEFAULT_PROPERTIES_PATH="property/es-config.properties";&#125; 同时默认配置文件为property/es-config.properties可手动进行配置，且支持多个地址，地址写法为 127.0.0.1:9200;127.0.0.2:9300 这样，中间应使用;分割。 我们对外提供一个工具类，供使用者进行对数据的操作。如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class ElasticSearchUtil &#123; private final static Logger logger = LoggerFactory.getLogger(ElasticSearchUtil.class); private static final SerializerFeature[] featuresWithNullValue=&#123;SerializerFeature.WriteMapNullValue, SerializerFeature.WriteNullBooleanAsFalse, SerializerFeature.WriteNullListAsEmpty, SerializerFeature.WriteNullNumberAsZero, SerializerFeature.WriteNullStringAsEmpty&#125;; private ElasticSearchClientFactory elasticSearchClientFactory; public void setElasticSearchClientFactory(ElasticSearchClientFactory elasticSearchClientFactory) &#123; this.elasticSearchClientFactory = elasticSearchClientFactory; &#125; public ElasticSearchUtil(ElasticSearchClientFactory elasticSearchClientFactory)&#123; this.elasticSearchClientFactory=elasticSearchClientFactory; &#125; public ElasticSearchUtil(String localPropertiesPath)&#123; ElasticSearchConfiguration esConfig=new ElasticSearchConfiguration(); esConfig.setLocalPropertiesPath(localPropertiesPath); this.elasticSearchClientFactory=new ElasticSearchClientFactory(esConfig); &#125; public ElasticSearchUtil() &#123; ElasticSearchConfiguration esConfig = new ElasticSearchConfiguration(); esConfig.setLocalPropertiesPath(ElasticSearchConstants.DEFAULT_PROPERTIES_PATH); this.elasticSearchClientFactory = new ElasticSearchClientFactory(esConfig); &#125; public ElasticSearchUtil(ElasticSearchConfiguration esConfig) &#123; this.elasticSearchClientFactory = new ElasticSearchClientFactory(esConfig); &#125; public Client getEsClient() &#123; return elasticSearchClientFactory.getEsClient(); &#125; /** * 创建索引 * @param index * @param type * @param id * @param isOnlyCreate * @param jsonString * @return */ protected boolean createDocument(String index,String type,String id,boolean isOnlyCreate,String jsonString)&#123; IndexResponse indexResponse; if(StringUtils.isBlank(id))&#123; indexResponse=elasticSearchClientFactory.getEsClient().prepareIndex(index,type).setCreate(isOnlyCreate).setSource(jsonString, XContentType.JSON).get(); &#125;else&#123; indexResponse=elasticSearchClientFactory.getEsClient().prepareIndex(index,type,id).setCreate(isOnlyCreate).setSource(jsonString,XContentType.JSON).get(); &#125; if(logger.isDebugEnabled())&#123; String _index=indexResponse.getIndex(); String _type=indexResponse.getType(); String _id=indexResponse.getId(); long _version = indexResponse.getVersion(); boolean created = RestStatus.CREATED.equals(indexResponse.status()); logger.debug(String.format("createDocument index:%s,type:%s,id:%s,version:%s,created:%s", _index, _type, _id, _version, created)); &#125; return RestStatus.CREATED.equals(indexResponse.status()); &#125; //其他代码略&#125; 创建好客户端后，我们可以用它执行增删改查，我们在封装一个ESUtil类，用于执行该系列操作，暴露一些特定增删改查接口。如上代码。 注：其他代码略。 这样，完成代码后，工程项目如下图所示。 ElasticSearchException 为自定义异常类。 ElasticSearchClientFactory为主要方法，用来构建一个单例的ESClient ElasticSearchConfiguration为ES配置类 ElasticSearchConstants为常量类，里面存放配置文件的路径 HostAndPort为地址和端口的一个辅助Bean ElasticSearchUtil为主要方法，用于对外提供服务（CRUD） DocumentVo为辅助Bean es-config.properties为配置文件 当我们把项目打包成jar包时，配置文件可以不用打包，这样引入其他项目后，在其他项目里配置配置文件即可。 测试若ES的安装路径D:\Program Files\elasticsearch-6.4.0\bin成功配置到环境变量后，在命令行输入elasticsearch便可启动服务。 若Kibana的安装路径D:\Program Files\kibana-6.4.0-windows-x86_64成功配置到环境变量后，在命令行输入kibana便可启动服务。 访问http://127.0.0.1:5601/ 看到可视化界面。 编写测试类。 12345678910public class ESTest &#123; private static ElasticSearchUtil esUtil=new ElasticSearchUtil(); private static String index="user_index"; private static String type="user_type"; public static void main(String[] args) &#123; JSONObject js=new JSONObject(); js.put("1","2"); esUtil.insertDocument(index,type,"123",js); &#125;&#125; 查看Kibana界面可以看到我们的数据被添加进来了。 结论关于ElasticSearch及Kibana的具体使用我们以后再讲。 我们平时开发中，也可以创建类似的工具包，提高代码使用率，实现软件的高内聚低耦合，同时也是提高自己。 部分未展示代码请见我的GitHub地址: https://github.com/javazwt/]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 Spliterator接口]]></title>
    <url>%2Fblog%2FJava8-Spliterator%E6%8E%A5%E5%8F%A3.html</url>
    <content type="text"><![CDATA[前言Spliterator 是Java8中加入的一个新接口，是“可分迭代器”（splitable iterator）的意思。它也是用来遍历数据源中的元素的，但它是为并行执行而设计的。 其接口主要代码如下： 123456Public interface Spliterator&lt;T&gt;&#123; Boolean tryAdvance(Consumer &lt;? super T&gt;) action; Spliterator&lt;T&gt; trySplit(); Long estimateSize(); Int characteristics();&#125; T 是Spliterator要遍历的元素类型。 tryAdvance 方法的行为类似于普通的Iterator，因为它会按顺序一个一个使用Spliterator中的元素，并且如果还有其他元素要遍历就返回true。 trySplit 是专为Spliterator接口而设计的，因为它可以把一些元素划出去分给第二个Spliterator，让他们两个并行处理。 estimateSize方法估计还剩多少元素需要遍历，因为即使不那么精确，快速算出来的值也有助于让拆分均匀点。 注：将Stream流拆分成多个部分的算法是个递归过程，第一步第一个Spliterator调用trySplit，生成两个Spliterator，第二步这两个Spliterator调用trySplit，生成4个Spliterator，直到调用Spliterator的trySplit 方法后返回null，表示这部分Spliterator不能在分割。 这个拆分过程也受Spliterator本身特性的影响，而特性是通过characteristics方法声明的。 我们来简单看一下它的特性的常用值。 特性 含义 ORDERED 按元素的既定顺序遍历和划分 DISTINCT 对于任一遍历过的元素x，y,x.equals(y)返回false SORTED 遍历元素按照一个预定义顺序排序 SIZED Spliterator由一个已知大小的数据源建立，estimateSize会返回准确值 NONNULL 保证遍历元素不会为空 IMMUTABLE Spliterator的数据源不能被修改，（不能 添加、删除、修改任何元素） CONCURRENT Spliterator的数据源可以被其他线程同时修改而无需同步 SUBSIZED 该Spliterator和从它拆分出来的Spliterator都是SIZED的 例子为什么我们需要了解这个类，有的时候甚至要实现这个类呢？ 我们来看一个例子。 对于下面一个String，我想统计下单词数量。 1static final String WORD="Hello World Happy EveryDay Good good study day day up let us study Spliterator"; 我们需要创建一个counter来累计流中字符，以及在counter中把它们结合起来的逻辑，如下： 12345678910111213141516171819202122232425public class WordCounter &#123; private final int counter; private final boolean lastSpace; public WordCounter(int counter, boolean lastSpace) &#123; this.counter = counter; this.lastSpace = lastSpace; &#125; //遍历一个个的Character public WordCounter accumulate(Character c)&#123; if(Character.isWhitespace(c))&#123; return lastSpace ? this : new WordCounter(counter,true); &#125;else&#123; //上一个字符是空格，而当前遍历的字符不是空格时，将单词计数器加一 return lastSpace ? new WordCounter(counter+1,false):this; &#125; &#125; //合并两个WordCounter，将其计数器加起来 public WordCounter combine(WordCounter wordCounter)&#123; return new WordCounter(counter+wordCounter.counter,wordCounter.lastSpace); &#125; public int getCounter()&#123; return counter; &#125;&#125; 这时候，我们在书写一个规约Character流统计单词个数就很简单了。 1234public static int countWords(Stream&lt;Character&gt; stream)&#123; WordCounter wordCounter=stream.reduce(new WordCounter(0,true),WordCounter::accumulate,WordCounter::combine); return wordCounter.getCounter();&#125; 12Stream&lt;Character&gt; stream= IntStream.range(0,WORD.length()).mapToObj(WORD::charAt);System.out.println(countWords(stream)); 输出14。结果是正确的。 现在我们让他在并行流上进行工作： 1Stream&lt;Character&gt; stream= IntStream.range(0,WORD.length()).mapToObj(WORD::charAt).parallel(); 结果输出26。显然这是不正确的。一脸懵逼。 为什么会出现这种情况呢？ 因为在并行流进行Spliterator分割时，把一个单词拆分成两部分了，导致结果变大。这显然不是我们想看到的。 实践我们要处理这种情况，就要指定分割原则，不要让程序把整个单词切开。 因此我们需要编写自己的Spliterator才能让上述问题在并行流下工作。如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class WordCounterSpliterator implements Spliterator&lt;Character&gt; &#123; private final String string; private int currentChar=0; public WordCounterSpliterator(String string) &#123; this.string = string; &#125; @Override public boolean tryAdvance(Consumer&lt;? super Character&gt; action) &#123; //处理当前字符 action.accept(string.charAt(currentChar++)); //如果还有字符要处理，返回true return currentChar&lt;string.length();&#125; @Override public Spliterator&lt;Character&gt; trySplit() &#123; int currentSize=string.length()-currentChar; //返回null表示要处理的String已经足够小 if(currentSize&lt;10)&#123; return null; &#125; //将试探拆分位置设定到要解析的文字中间 for(int splitPos=currentSize/2+currentChar;splitPos&lt;string.length();splitPos++)&#123; //如果是空格就开始拆分，不是空格将拆分位置前进直到下一个空格 if(Character.isWhitespace(string.charAt(splitPos)))&#123; //创建一个新的WordCounterSpliterator来解析String从开始到拆分位置的部分 Spliterator&lt;Character&gt; spliterator= new WordCounterSpliterator(string.substring(currentChar,splitPos)); //将这个WordCounterSpliterator的起始位置设为拆分位置 currentChar=splitPos; return spliterator; &#125; &#125; return null; &#125; @Override public long estimateSize() &#123; return string.length()-currentChar; &#125; @Override public int characteristics() &#123; return ORDERED+SIZED+SUBSIZED+NONNULL+IMMUTABLE; &#125;&#125; tryAdvance方法把String中当前位置的Character传给了Consumer，并让位置加一。作为参数传递的内部类Consumer，在遍历流时将要处理的Character传递给要执行的函数。如果新的指针位置小于String总长度，说明没有遍历完，返回true继续遍历。 trySplit方法，首先我们设置了一个拆分下限——10个Character，实际应用中我们应尽量提高这个长度避免生成太多的任务。如果长度小于这个数，就返回空无需继续拆分。否则就把试探拆分位置放到要解析的String块中间，但不能直接使用此位置，应该看看是不是空格，如果是就拆分，如果不是，就向前找，找到空格进行拆分，避免把一个单词拆成两份。 estimatedSize方法返回的是这个Spliterator解析的String的总长度和当前遍历位置的差值。 characteristic方法告诉这个框架是ORDERED（String的每个Character的默认顺序），SIZED（estimatedSize方法返回值是精确的），SUBSIZED（trySplit分出来的Spliterator大小也是固定的），NONNULL（String里面的Character不可能为null），IMMUTABLE（String本身就不可变化）。 下面我们测试一下我们的WordCounterSpliterator 。 123Spliterator&lt;Character&gt; spliterator=new WordCounterSpliterator(WORD);Stream&lt;Character&gt; stream= StreamSupport.stream(spliterator,true);System.out.println(countWords(stream)); 可以看到输出结果为14. 结论可以看到，并行流不是所有情况都适用的，有些情况要定制自己的Spliterator才能使并行流正常工作。这个例子或许运行效率并行比不上串行，但是在大数据下，比如分析一个文本文件中的单词数量，就能明显看到并行带来的速度优势了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[和前同事闲聊小记]]></title>
    <url>%2Fblog%2F%E5%92%8C%E5%89%8D%E5%90%8C%E4%BA%8B%E9%97%B2%E8%81%8A%E5%B0%8F%E8%AE%B0.html</url>
    <content type="text"><![CDATA[前言这是一篇技术问题讨论文章~~~ 最近和前同事（朋友）聊天，帮他分析了这么一个需求，蛮有意思的，特来分享下其代码。 和他聊了很久，算是大致上听懂了他的需求，总结下他的需求： 假设有若干方法（任务）(A,B,C,D,E…….)，对于每个方法（任务），同一时间只能有一个用户（线程）访问，现一些用户需要执行若干任务（如用户1执行（ABC），用户2执行（BEF），用户3执行（ACEF）….），对每个用户，用户的任务不要求执行顺序，如何尽可能的提高程序运行的效率。 分析其实这之中比较重要的一点是每个方法同一时间只允许一个用户访问，我们如果给每个方法加上synchronized关键字呢？我从这方面入手没有想到太好的方法。我用了一种方法，就是从队列入手，对每个方法，可以设置一个等待队列，如果有任务正在执行该方法，则将其他的访问该方法的线程挂起，当然，对于每个用户，应该开多个线程去异步执行各个方法。 于是我想到了定长线程池，允许一个线程执行，其他线程进入后会放到等待队列中。对于方法，每有一个方法（任务），就创建一个定长线程池。应当在初始化时就将数据设置好。 代码以下是我的一些实现代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class DoTask &#123; private static Map&lt;String,ExecutorService&gt; poolMap=new HashMap&lt;&gt;(); static&#123; //获取指定class的public方法 Method[] methods=ThingsMethod.class.getDeclaredMethods(); //有多少个方法就创建多少个线程池,一个方法指定一个定长线程池 for(Method method:methods)&#123; ExecutorService pool= Executors.newFixedThreadPool(1); poolMap.put(method.getName(),pool); &#125; &#125; //list里面假设是要执行的任务，变化的 public void doTask(List&lt;String&gt; list)&#123; for(String taskStr:list)&#123; final String id="Thread"+Thread.currentThread().getId(); switch (taskStr)&#123; case "A": try&#123; executeMethod("doSomethingA",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "B": try&#123; executeMethod("doSomethingB",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "C": try&#123; executeMethod("doSomethingC",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "D": try&#123; executeMethod("doSomethingD",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; default: break; &#125; &#125; &#125; public void executeMethod(String methodName,String id)&#123; poolMap.get(methodName).execute(()-&gt;&#123; try&#123; Method method=ThingsMethod.class.getMethod(methodName,String.class); method.invoke(ThingsMethod.class.newInstance(),id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ThingsMethod &#123; public boolean doSomethingA(String id)&#123; System.out.println("线程"+id+"做A事情开始-----&gt;"); //假设做A事情花费1s try&#123; Thread.sleep(1000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做A事情结束-----&gt;"); return true; &#125; public boolean doSomethingB(String id)&#123; System.out.println("线程"+id+"做B事情开始-----&gt;"); //假设做B事情花费3s try&#123; Thread.sleep(3000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做B事情结束-----&gt;"); return true; &#125; public boolean doSomethingC(String id)&#123; System.out.println("线程"+id+"做C事情开始-----&gt;"); //假设做C事情花费2s try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做C事情结束-----&gt;"); return true; &#125; public boolean doSomethingD(String id)&#123; System.out.println("线程"+id+"做D事情开始-----&gt;"); //假设做D事情花费5s try&#123; Thread.sleep(5000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做D事情结束-----&gt;"); return true; &#125;&#125; 编写测试类对其进行测试。 123456789101112131415161718192021222324252627282930313233343536public class MyTest implements Runnable&#123; @Override public void run() &#123; //随机假设我们的任务是这样的 List&lt;String&gt; s=new ArrayList&lt;&gt;(); String [] strings=new String[]&#123;"A","B","C","D"&#125;; int l=(int)Math.ceil(Math.random()*strings.length); for(int i=0;i&lt;l;i++)&#123; s.add(strings[i]); &#125; //获取结果 System.out.println(); System.out.print("Thread"+Thread.currentThread().getId()+"随机生成的任务--》"); for(int i=0;i&lt;s.size();i++)&#123; System.out.print(s.get(i)); &#125; System.out.println(); DoTask doTask=new DoTask(); doTask.doTask(s); &#125; public static void main(String[] args) &#123; MyTest test = new MyTest(); Thread t1 = new Thread(test); Thread t2 = new Thread(test); Thread t3 = new Thread(test); Thread t4 = new Thread(test); Thread t5 = new Thread(test); Thread t6 = new Thread(test); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); t6.start(); &#125;&#125; 输出结果： 12345678910111213141516171819202122232425262728293031323334353637Thread11随机生成的任务--》AThread13随机生成的任务--》AThread15随机生成的任务--》ABThread14随机生成的任务--》ABCDThread12随机生成的任务--》ABCDThread16随机生成的任务--》A线程Thread16做A事情开始-----&gt;线程Thread14做C事情开始-----&gt;线程Thread12做B事情开始-----&gt;线程Thread12做D事情开始-----&gt;线程Thread16做A事情结束-----&gt;线程Thread13做A事情开始-----&gt;线程Thread14做C事情结束-----&gt;线程Thread12做C事情开始-----&gt;线程Thread13做A事情结束-----&gt;线程Thread12做A事情开始-----&gt;线程Thread12做B事情结束-----&gt;线程Thread14做B事情开始-----&gt;线程Thread12做A事情结束-----&gt;线程Thread14做A事情开始-----&gt;线程Thread12做C事情结束-----&gt;线程Thread14做A事情结束-----&gt;线程Thread11做A事情开始-----&gt;线程Thread12做D事情结束-----&gt;线程Thread14做D事情开始-----&gt;线程Thread11做A事情结束-----&gt;线程Thread15做A事情开始-----&gt;线程Thread14做B事情结束-----&gt;线程Thread15做B事情开始-----&gt;线程Thread15做A事情结束-----&gt;线程Thread15做B事情结束-----&gt;线程Thread14做D事情结束-----&gt; 可以看到运行结果是正确的。 讨论我还想到一种就是对各个方法（任务）添加synchronized关键字，这些方法对用户共享，这样每当有用户请求时，开线程池执行各个方法。如下： 123456789101112131415161718192021222324252627282930public class DoTask &#123; private static ThingsMethod method=new ThingsMethod(); public void doTask(List&lt;String&gt; list)&#123; for(String taskStr:list)&#123; ExecutorService executor= Executors.newCachedThreadPool(); executor.execute(()-&gt;&#123; final String id="Thread"+Thread.currentThread().getId(); doSomething(taskStr,id,method); &#125;); &#125; &#125;public void doSomething(String taskStr,String id,ThingsMethod method)&#123; switch (taskStr)&#123; case "A": method.doSomethingA(id); break; case "B": method.doSomethingA(id); break; case "C": method.doSomethingA(id); break; case "D": method.doSomethingA(id); break; default: break; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ThingsMethod &#123; public synchronized boolean doSomethingA(String id)&#123; System.out.println("线程"+id+"做A事情开始-----&gt;"); //假设做A事情花费1s try&#123; Thread.sleep(1000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做A事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingB(String id)&#123; System.out.println("线程"+id+"做B事情开始-----&gt;"); //假设做B事情花费3s try&#123; Thread.sleep(3000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做B事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingC(String id)&#123; System.out.println("线程"+id+"做C事情开始-----&gt;"); //假设做C事情花费2s try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做C事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingD(String id)&#123; System.out.println("线程"+id+"做D事情开始-----&gt;"); //假设做D事情花费5s try&#123; Thread.sleep(5000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做D事情结束-----&gt;"); return true; &#125;&#125; 由于在主线程上开启了多线程，所以这种方法观看日志比较不方便。这种方法的正确性也还没来得及验证。 如果大家有什么好的其他想法也可以讨论交流。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java8 Stream API简介]]></title>
    <url>%2Fblog%2FJava8-Stream-API%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言今天来简单讲下Java8新增的Stream API。 Stream是Java8新增的对于集合框架操作的一个类，我们也称之为流操作。使用Stream，可以简化很多重复代码，使用并行流时有时还能提高程序运行效率。我们来看下。 正文要把一个集合，List、Set或其他集合类转换为一个流，需要调用stream方法（转换为串行流）或者parallelStream方法（转换为并行流）。 Stream的常用方法如下表格介绍： 常用方法 说明 参数 举例 filter(Predicate&lt;? super T&gt; predicate) 过滤，返回一个匹配给定布尔表达式的数据流 一个返回值为boolean的lambda表达式 例1 map(Function&lt;? super T, ? extends R&gt; mapper) 匹配，对流的每个元素都应用给定的函数以获得新流 任一符合条件的lambda表达式 例2 mapToInt(ToIntFunction&lt;? super T&gt; mapper) 将流数据转换为IntStream流 流元素转为int的lambda表达式 例3 mapToLong(ToLongFunction&lt;? super T&gt; mapper) 将流数据转换为LongStream流 流元素转为long的lambda表达式 略 mapToDouble(ToDoubleFunction&lt;? super T&gt; mapper) 将流数据转换为DoubleStream流 流元素转为Double的lambda表达式 略 flatMap(Function&lt;? super T, ? extends Stream&lt;? extends R&gt;&gt; mapper) 返回一个对流的每个元素都应用给定的函数的新流。这个入参lambda包含有过滤流，此方法类似于平常的双重循环 一个包含过滤流的lambda表达式 例4 flatMapToInt(Function&lt;? super T, ? extends IntStream&gt; mapper) 参考mapToInt 参考mapToInt 略 flatMapToLong(Function&lt;? super T, ? extends LongStream&gt; mapper) 参考mapToLong 参考mapToLong 略 flatMapToDouble(Function&lt;? super T, ? extends DoubleStream&gt; mapper) 参考mapToDouble 参考mapToDouble 略 distinct() 去除流中的重复元素 无参数 例5 sorted() 对流中元素进行排序，使用自然顺序。 无参数 例6 sorted(Comparator&lt;? super T&gt; comparator) 对流中的元素进行排序，按照指定的规则 流中元素的比较规则 例7 peek(Consumer&lt;? super T&gt; action) 截断流后进行某些操作 指定的操作，无返回值lambda 例8 limit(long maxSize) 获取流中的前n个元素的流 限定值 例9 skip(long n) 获取从n开始后面元素的流，不包括n，如果n超出流长度，返回一个空流 起始长度 例10 forEach(Consumer&lt;? super T&gt; action) 循环流中的每个元素，执行指定的lambda表达式 无返回值的lambda表达式 例11 forEachOrdered(Consumer&lt;? super T&gt; action) 按照默认顺序循环流中的元素，执行指定的lambda表达式 无返回值的lambda表达式 略 toArray() 返回流中元素组成的数组 无参数 例12 toArray(IntFunction&lt;A[]&gt; generator) 将流中元素转换为指定类型的数组，数组类型必须是流元素的本身或者超类 略 略 reduce(T identity, BinaryOperator&lt;T&gt; accumulator) 归约，将流中的元素按照指定表达式归并。 起始值（默认值），指定归约公式 例13 reduce(BinaryOperator&lt;T&gt; accumulator) 同上，不同的是这个没有设置起始值，故结果可能为空，返回的是Optional对象 指定的归约公式 略 reduce(U identity,BiFunction&lt;U,? super T,U&gt; accumulator,BinaryOperator&lt;U&gt; combiner) 归约全参方法，可以指定初始值，归约方法，归约方式 略 例14 collect(Collector&lt;? super T, A, R&gt; collector) 收集器，比较常用的是toList，将流元素变为集合 略 略 collect(Supplier&lt;R&gt; supplier,BiConsumer&lt;R, ? super T&gt; accumulator,BiConsumer&lt;R, R&gt; combiner) 收集器，可以指定收集规则，收集方法等 略 略 min(Comparator&lt;? super T&gt; comparator) 根据指定规则获取流中最小值 排序规则lambda表达式 例15 max(Comparator&lt;? super T&gt; comparator) 根据指定规则获取流中最大值 排序规则lambda表达式 例15 count() 统计流中元素数量 无参数 例16 anyMatch(Predicate&lt;? super T&gt; predicate) 查找流中是否有元素符合条件，一旦查到，就返回true 条件的lambda表达式 例17 allMatch(Predicate&lt;? super T&gt; predicate) 查找流中的元素是否全部匹配指定表达式，全部符合才返回true 条件的lambda表达式 例18 noneMatch(Predicate&lt;? super T&gt; predicate) 查找流中的元素是否全部不匹配指定表达式，全部不匹配才返回true 条件的lambda表达式 略 findFirst() 查找流的第一个元素，如果流为空，返回空（Optional的空） 无参数 略 findAny() 随便从流中拿一个元素，如果流为空，返回空（Optional的空） 无参数 略 empty() 静态方法，返回一个空流 略 例19 of(T t) 静态方法，生成一个元素的流 略 例19 of(T… values) 静态方法，生成若干元素的流 略 例19 iterate(final T seed, final UnaryOperator&lt;T&gt; f) 静态方法，生成一个有规律的流 起始值，指定的lambda表达式 例19 generate(Supplier&lt;T&gt; s) 静态方法，生成指定规则的数据流，流数据的生成完全取决于lambda表达式 生成流数据的lambda表达式 例19 concat(Stream&lt;? extends T&gt; a, Stream&lt;? extends T&gt; b) 静态方法，将两个数据流合并，两个流的类型须一致 两个数据流 例19 例子举例：假设有一个1-100的List集合list1,50-150的List集合list2. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//例1：需要一个1-100的偶数List集合。list1.stream().filter(n-&gt;n%2==0).collect(Collectors.toList());//例2：获取list1各元素加上100，再取基数的List集合。list1.stream().map(n-&gt;n+100).map(n-&gt;n%2!=0).collect(Collectors.toList());//例3：将list1的流转换为IntStream。list1.stream().mapToInt(n-&gt;n.intValue());//例4：list1和list2的交集集合。list1.stream().flatMap(i-&gt;list2.stream().map(j-&gt;i==j)).collect(Collectors.toList());//例5：list1和list2的并集集合。list1.addAll(list2);list1.stream().distinct();//例6：对list1进行排序。list1.stream().sorted().collect(Collectors.toList());//例7：对list1进行倒序排序。list1.stream().sorted(Comparator.reverseOrder()).collect(Collectors.toList());//例8：截断查看流的每一步得到的流元素。list1.stream().map(n-&gt;n+100).peek(System.out::println).map(n-&gt;n%2!=0).peek(System.out::println).collect(Collectors.toList());//例9：取list1的前10个元素。list1.stream().limit(10).collect(Collectors.toList());//例10：取list1的后10个元素。list1.stream().skip(list1.size()-10).collect(Collectors.toList());//例11：输出list1的各个元素。list1.stream().forEach(System.out::println);//例12：将list1元素转为数组。list1.stream().toArray();//例13：对list1元素求和，对list2元素求差。list1.stream().reduce(0,(a,b)-&gt;a+b);list2.stream().reduce(0,(a,b)-&gt;a-b);//例14：对list1按照最大值到最小值求和。list1.stream().reduce(0,(a,b)-&gt;a+b, BinaryOperator.maxBy(Comparator.naturalOrder()));//例15：取list1的最大最小值list1.stream().max(Comparator.naturalOrder());list1.stream().min(Comparator.naturalOrder());//例16：求list1的元素个数list1.stream().count();//例17：查询list1中有没有17的倍数（除了17）。list1.stream().anyMatch(n-&gt;n!=17&amp;&amp;n%17==0);//例18：查询list1中所有元素是不是都是2的倍数。list1.stream().allMatch(n-&gt;n%2==0);//例19：Stream.empty();//空流Stream.of(1);//生成一个元素的流Stream.of(1,2,3,4);//生成若干元素的流Stream.iterate(1,n-&gt;2*n);//生成指定规则的数据流Stream.generate(()-&gt;Math.random());//生成指定规则的数据流Stream.concat(list1.stream(),list2.stream());//将两个数据流合并 结语Java8 Stream API的一些常用方法基本如上，Stream API为操作集合类提供了更方便快捷的方式，值得我们掌握。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你该相信什么]]></title>
    <url>%2Fblog%2F%E4%BD%A0%E8%AF%A5%E7%9B%B8%E4%BF%A1%E4%BB%80%E4%B9%88.html</url>
    <content type="text"><![CDATA[前言写这篇文章，或是怀念过去的自己，或是憧憬自己未来。可是啊，无论如何，要活好当下。 正文工作总的来说差不多四年了，或许已从职场菜鸟变化为职场老手了。 可是，岁月的变迁确无法掩饰我这颗浮躁的心，哎，想想自己，确实很浮躁啊。 曾经，想静心，于是开始练字，练了很长一段时间，可是后面确不知不觉的放弃了，至今剩下的字帖和钢笔仍静静地躺在我的桌子上。 后来，心血来潮搭建了属于自己的博客，然后创建了微信公众号，平时写些文章来总结自己和工作，我希望自己可以坚持下去，也相信自己可以坚持下去。 世上大多数人是平凡的，可是，要有一颗不甘平凡的心。 最怕一生碌碌无为，还安慰平凡难能可贵。 任何事都不应成为懒惰的借口。 不诉困苦于环境，不寄期望于运气，不服挫折于命运。 还记得那个年少的自己吗？或许他变了，可是他还在，他一直陪着你。 相信自己。做自己的英雄。 努力吧，骚年！ 加油！ 总结好了，喝饱鸡汤了。 努力去“搬砖”吧！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 接口]]></title>
    <url>%2Fblog%2FJava8-%E6%8E%A5%E5%8F%A3.html</url>
    <content type="text"><![CDATA[前言Java 8已经推出相当长一段时间了，其中，接口部分有一些变化。我们来研究下它。 问题我们知道，对于一个接口，如果我们声明好后，如果再想对其增加新的方法是困难的，因为我们要改变所有其实现类，也就是每个实现类都要对其新方法进行实现。如下图： 这显然是不现实的，如果我们直接把方法写在实现类里，接口中没有此方法，就破坏了我们的多态性。 对于某些已经发布的应用，无论哪种做法都是比较繁重且不被推荐的。 接口默认实现还好，Java大神们已经意识到了这个问题，于是在Java8中，引入了对接口的默认方法实现。 什么是默认方法实现呢？ 简单来说，就是允许接口定义默认方法，在接口中需要有方法体的具体实现，实现类默认继承该方法（如果不重写默认方法的话）。同时为区分默认方法，默认方法在接口中采用default关键字标明。如下图： 这样，如果我们新增一个接口方法（对于已经发布的接口），可以使用默认实现，就不会出现我们上述的问题。 思考你一定会说，这和抽象类有什么区别呢？ 当然还是有区别的，Java8以后，接口和抽象类的几点总结如下： 新的问题接口引入了默认方法后，就会有新的问题，好在Java已经替我们解决了，我们来看下。 情况一123456789101112131415public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B extends A&#123; default void doSomething()&#123; System.out.println("Interface B"); &#125;&#125;public class C implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 结果：输出 Interface B 情况二1234567public class D implements A &#123;&#125;public class C extends D implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 结果：输出 Interface B 情况三如果D是这样呢？ 12345public class D implements A &#123; public void doSomething()&#123; System.out.println("Class D"); &#125;&#125; 结果：输出Class D 如果D不对doSomething提供实现（D为抽象的类），则C需要为doSomething提供实现。 情况四如果B接口不在继承A接口。如下： 12345678910public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B&#123; default void doSomething()&#123; System.out.println("Interface B"); &#125;&#125; 那么我们C类必须为doSomething提供实现，当然我们可以具体制定使用哪个接口的doSomething方法，如下： 123456public class C implements A, B &#123; @Override public void doSomething() &#123; B.super.doSomething(); &#125;&#125; 情况五如果两个函数不一样但差距很小呢？如下： 123456789101112131415public interface A &#123; default int doSomething()&#123; return 100; &#125;&#125;public interface B&#123; default long doSomething()&#123; return 200; &#125;&#125;public class C implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 在IDEA里我们可以看到，类C是无法编译的，这是不被允许的。 情况六1234567891011121314public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B extends A&#123;&#125;public interface C extends A&#123;&#125;public class D implements B,C &#123; public static void main(String[] args) &#123; new D().doSomething(); &#125;&#125; 输出结果Interface A 因为只有A声明了一个默认方法，这个接口是D的父接口，故输出Interface A。如果B也提供了一个默认方法，签名和A一致，那么编译器会选择B的默认方法，如果B添加一个相同签名的抽象方法，则D需要为其提供实现，如果B,C都有相同签名的默认方法doSomething，则会出现冲突，需要我们为doSomething提供实现或者指定使用B，C中的哪个方法。 结论解决问题的三条规则： 如果一个类使用相同的函数签名从多个地方（比如另一个类或者接口）继承了方法，通过三条规则可进行判断。 类中的方法优先级最高。类或者父类中声明的方法优先级高于任何声明为默认方法的优先级。 如果无法依据第一条判断，那么子接口的优先级更高：函数签名相同时，优先选择拥有最具体实现的默认方法的接口，及如果B继承了A，那么B就比A更加具体。 最后，如果还是无法判断，继承了多个接口的类必须通过显式覆盖和调用期望的方法，显式地选择使用哪一个默认方法的实现。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Callable原理分析]]></title>
    <url>%2Fblog%2FCallable%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[前言今天我们来分析实现多线程的一种方式，实现Callable接口。这种方式有种特殊的地方，就是可以拿到线程返回值。具体怎么实现的呢？我们来研究下。 栗子我们先举个简单的栗子来看下Callable接口吧。要首先明白，线程池提交实现Callable接口的线程后会返回一个Future对象，这个对象里包含程序的运行结果。 123456789101112public class CallableTest &#123; public static void main(String[] args) throws ExecutionException, InterruptedException,TimeoutException&#123; //创建一个线程池 ExecutorService executor = Executors.newCachedThreadPool(); Future&lt;String&gt; future = executor.submit(()-&gt; &#123; //System.out.println("CallableTest"); TimeUnit.SECONDS.sleep(5); return "CallableTest"; &#125;); System.out.println(future.get()); &#125;&#125; 可以看到我们可以通过future.get()拿到结果”CallableTest”。我们也可以设置指定时间后拿到结果，如指定6s后拿到结果。 1System.out.println(future.get(4,TimeUnit.SECONDS)); 可以看到也拿到了返回结果，如果我们设置4s拿到结果，小于程序运行时间5s，可以看到它抛出了超时异常。java.util.concurrent.TimeoutException。 原理是不是很神奇？ 我们来研究下Callable接口获取返回值的原理。 我们先来看看ExecutorService的submit方法，它接受一个Callable对象，返回一个Future对象。 1&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); 及它的实现。AbstractExecutorService的submit方法。 12345678910public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125;protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable);&#125;void execute(Runnable command); ThreadPoolExecutor的execute是对其实现。 可以看到创建了一个FutureTask对象并执行。 FutureTask对象实现了Runable接口，我们来看下它。 看一下它的run方法。 1234567891011121314151617181920212223242526272829303132333435public void run() &#123; if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //拿到结果设置ran为true result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; //异常设置结果为空ran为false并设置异常 result = null; ran = false; setException(ex); &#125; //ran为true时放入结果 if (ran) set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; 1234567public V get() throws InterruptedException, ExecutionException &#123; int s = state; //会一直挂起知道处理业务的线程完成，唤醒等待线程 if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125; 我们调用get方法时，他先查询线程状态，如果未完成，就调用awaitDone方法。 1234567891011121314151617181920212223242526272829303132333435363738private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; //如果设置了超时时间就获取截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; /循环监视线程的状态 for (;;) &#123; if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; else if (s == COMPLETING) // cannot time out yet Thread.yield(); else if (q == null) q = new WaitNode(); else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); else if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; else LockSupport.park(this); &#125; &#125; 带有超时时间的get，到达时间后，会判断线程状态，如果未完成，抛出超时异常。 12345678910public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; if (unit == null) throw new NullPointerException(); int s = state; if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s);&#125; 总结因此，带有返回值得异步线程基本上可以这样理解。 由于有返回值，如果未设置等待时间，会等线程执行完成后返回，基本类似同步。其原理是线程运行后，如果未完成，会放入等待队列。直到线程状态变化（完成或者异常等）。如果设置了等待时间，则到时间后无论线程状态是否完成都会返回线程状态。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道Java试题引发的思考]]></title>
    <url>%2Fblog%2F%E4%B8%80%E9%81%93Java%E8%AF%95%E9%A2%98%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83.html</url>
    <content type="text"><![CDATA[前言无意中看到了自己入门Java时的一本Java基础书，看到了其中一个小结的习题，颇为简单。求一个数组所有数据的和。 分析那时入门Java，看到以前自己在书上写下的for循环。大致如下： 1234567public static Long sum(int [] numbers)&#123; long result=0L; for(int i=0;i&lt;numbers.length;i++)&#123; result+=numbers[i]; &#125; return result;&#125; 现已入行Java颇有时日，遂想到当n逐渐变大时，其执行效率会逐渐降低的。 近日对多线程有些许研究，故想到可以将数据平均分为2份，3份或者多份，每份进行和值，最后相加得到结果。也是一个不错的想法。 好在Java里有这么一个框架，名字叫分支/合并框架。我们来配合例子研究下。 分支/合并框架分支/合并框架的目的是以递归的方式将可以并行的任务分为更小的任务，然后将每个子任务的结果合并起来生成整体的结果。它是ExecutorService接口的一个实现，它把子任务分配给线程池（ForkJoinPool）中的工作线程。那我们如何来定义任务和子任务呢？ 要把任务提交到ForkJoinPool，必须创建RecursiveTask&lt; R &gt;的一个子类，其中R是并行化任务产生的结果类型。它其中有唯一一个抽象方法compute，用来定义将任务拆分成子任务的逻辑，以及无法再拆分或不方便拆分时，生成单个子任务结果的逻辑。 伪代码如下： 1234567if(任务足够小或者不可分)&#123; 顺序执行该任务&#125;else&#123; 将任务分为两个子任务 递归调用本方法，拆分每个子任务，等待所有子任务完成 合并每个子任务结果&#125; 那如何定义一个任务是否可以在拆分呢？ 一般来说没有明确的标准决定一个任务是否可以在拆分，但是有几种试探方法可以帮助我们做出决定，分支/合并框架采用了一种称为工作窃取的技术来解决这个问题。每个线程的子任务都保存在一个双向链式队列里，每完成一个任务，就从队列头部获取一个任务，如果有线程先执行完成，它不会“闲”下来，而是去未完成的队列尾部“窃取”一个任务进行完成，直至所有子任务完成返回结果。 实践我们用分支/合并框架对数组数据进行并行求和。 代码如下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ForkJoinSumCalculator extends RecursiveTask&lt;Long&gt; &#123; //要求和的数组 private final int[] numbers; //子任务处理的数组的起始位置 private final int start; //子任务处理的数组的终止位置 private final int end; //不再将任务划分的子任务数组大小 public static final long THRESHOLD=10000; public ForkJoinSumCalculator(int[] numbers)&#123; this(numbers,0,numbers.length); &#125; private ForkJoinSumCalculator(int[] numbers,int start,int end)&#123; this.numbers=numbers; this.start=start; this.end=end; &#125; @Override protected Long compute() &#123; int length=end-start; //小于等于阈值，计算结果 if(length&lt;=THRESHOLD)&#123; return computeSequentially(); &#125; //创建一个子任务来为数组的前一半求和 ForkJoinSumCalculator leftTask=new ForkJoinSumCalculator(numbers,start,start+length/2); //利用另一个ForkJoinPool线程异步执行新创建的子任务 leftTask.fork(); //创建一个任务为数组的后一半求和 ForkJoinSumCalculator rightTask=new ForkJoinSumCalculator(numbers,start+length/2,end); //同步执行第二个子任务，有可能允许进一步递归划分 Long rightResult=rightTask.compute(); //读取第一个子任务的结果，没有完成就等待 Long leftResult=leftTask.join(); //合并结果 return rightResult+leftResult; &#125; //子任务数组求和 private long computeSequentially()&#123; long sum=0; for(int i=start;i&lt;end;i++)&#123; sum+=numbers[i]; &#125; return sum; &#125;&#125; 这样，我们在编写一个方法并行对数组求和就很简单了。 1234public static long forkJoinSum(int [] numbers)&#123; ForkJoinTask&lt;Long&gt; task=new ForkJoinSumCalculator(numbers); return new ForkJoinPool().invoke(task);&#125; 我们可以写一个测试方法，测试这两种方法的执行效率。 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) &#123; //构建一个数组 int [] numbers=new int[100000000]; for(int i=0;i&lt;numbers.length;i++)&#123; numbers[i]=(int)(Math.random() * Integer.MAX_VALUE); &#125; //分支/合并框架计算执行速度 long fastest=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); forkJoinSum(numbers); long duration=(System.nanoTime()-start); if(duration&lt;fastest)&#123; fastest=duration; &#125; &#125; System.out.println("分支/合并最快执行速度为"+fastest+"ns"); //普通方法计算执行速度 long fastest1=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); sum(numbers); long duration=(System.nanoTime()-start); if(duration&lt;fastest1)&#123; fastest1=duration; &#125; &#125; System.out.println("普通算法最快执行速度为"+fastest1+"ns"); &#125; 输出如下： 12分支/合并最快执行速度为25894038ns普通算法最快执行速度为38811709ns 可以看到速度是有明显提升的。 其他源数据问题这个计算的数组之所以随机，是因为我之前测试了1-n的和计算，发现for循环居然比分支/合并框架快！！我加大了n值也是如此，所以对于这种比较特殊的计算，Java虚拟机或者编译器对它们的计算做了优化，因此用这些数据测试时，可能得不到我们想要的结果，这也是并发处理比较难的地方。有的时候我们多线程处理的代码可能还没有单线程快，或者快是快了，但结果错了。 拥抱Java8看到上面我们为了求个和冒着出错的风险写了一个sum类，是不是心里退却了？不过啊，Java 8已经给我们提供了类似的功能啦。 它就是全新的Stream API，这个我们有时间在介绍。先看看Stream API对于这个是如何求和的。 12Arrays.stream(numbers).sum();Arrays.stream(numbers).parallel().sum(); 是不是很简单？？ 第一个为串行求和，即单线程，第二个为并行求和。 我们来测试下他们的效率。 1234567891011121314151617181920212223242526//Stream API 串行方法计算执行速度 long fastest2=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); //sum(numbers); Arrays.stream(numbers).sum(); long duration=(System.nanoTime()-start); if(duration&lt;fastest2)&#123; fastest2=duration; &#125; &#125; System.out.println("Stream API 串行 最快执行速度为"+fastest2+"ns"); //Stream API 并行方法计算执行速度 long fastest3=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); //sum(numbers); Arrays.stream(numbers).parallel().sum(); long duration=(System.nanoTime()-start); if(duration&lt;fastest3)&#123; fastest3=duration; &#125; &#125; System.out.println("Stream API 并行 最快执行速度为"+fastest3+"ns"); 输出如下结果： 1234分支/合并最快执行速度为25316712ns普通算法最快执行速度为38812671nsStream API 串行 最快执行速度为36572646nsStream API 并行 最快执行速度为24291637ns 可以看到，并行情况下时间与刚才写的分支/合并框架相近。 其实：并行流背后使用的基础框架就是分支/合并框架。 这只是最简单的求和例子，遇到实际问题，可能使用Stream流比普通实现简单快速，因此，Stream是值得我们学习的。 结论学习就是不断思考不断进步的过程，有的时候看看自己之前写的代码，应该少一些“我去，写的什么玩意，原来？”之类的抱怨，多一些深入的思考及优化。 相关代码均已上传Github ： https://github.com/JavaZWT/framework-base]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Transactional注解]]></title>
    <url>%2Fblog%2FSpring-Transactional%E6%B3%A8%E8%A7%A3.html</url>
    <content type="text"><![CDATA[前言使用@Transactional 注解有一段时间了，今天来对它进行下总结。 再说这个之前先说下事务。 事务的基本要素（ACID） 原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 说明Transactional注解的主要参数如下： 下面来简单介绍下它的参数： value：一般用来配置指定的事务管理器。 propagation：事务的传播属性，有七种。见枚举Propagation，默认REQUIRED。 1234567891011121314//支持当前事务，如果不存在就创建一个REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED),//支持当前事务，如果不存在就以非事务的方式运行SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS),//支持当前事务，不存在就抛出异常MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY),//创建一个新事务，如果当前有事务就暂停当前事务REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW),//以非事务方式运行，如果当前有事务就暂停当前事务NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED),//以非事务的方式运行，如果当前有事务就抛出异常NEVER(TransactionDefinition.PROPAGATION_NEVER),//如果当前有事务，就加入当前事务。NESTED(TransactionDefinition.PROPAGATION_NESTED); isolation：隔离级别。事务的隔离级别有4种。我们看一下Isolation枚举类。 12345678910//使用默认的隔离级别，取决于底层数据库的默认隔离级别DEFAULT(TransactionDefinition.ISOLATION_DEFAULT),//读未提交，这种隔离级别最低，会出现脏读，不可重复读，虚读（幻读）等情况。一般不用。READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED),//读已提交，这种隔离级别可以防止脏读的产生，但是无法避免不可重复读和虚读（幻读）的出现。READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED),//可重复读，这种隔离级别可以防止脏读、不可重复读的出现，但是无法避免虚读（幻读）的产生。REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ),//串行化，这种隔离级别最高，可以避免脏读、不可重复读和虚读（幻读）的产生。SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。 Mysql默认隔离级别为可重复读。 关于事务的并发问题 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 timeout : 事务的超时时间，默认为-1，即事务直到完成都不会超时。 readOnly：是否只读，一般用来标识。拥有这个标识，写入操作不一定会失败，取决于数据库系统。 rollbackFor：事务回滚条件。参数为继承Throwable的class类。 rollbackForClassName：事务回滚条件。可以接受String数组。 noRollbackFor： 事务不会回滚条件。 noRollbackForClassName：事务不会回滚条件。 原理Spring的Transaction注解是如何实现并生效的呢？我们来探究下。 我们首先来看看SpringTransactionAnnotationParser这个类，这个是解析transaction注解的类。调用AnnotationUtils类的getAnnotation方法拿到注解信息。 可以看到AnnotationTransactionAttributeSource这个类调用了parseTransactionAnnotation方法。 我们来看下AbstractFallbackTransactionAttributeSource这个类的computeTransactionAttribute方法。调用了上图的findTransactionAttribute方法。getTransactionAttribute方法，由于获取注解信息耗时，故spring先从缓存里获取注解事务信息，查不到在用程序获取。 TransactionAspectSupport这个类的createTransactionIfNecessary方法，调用getTransactionAttribute方法。 我们可以看到AbstractTransactionAspect.aj文件使用了createTransactionIfNecessary方法。这个应用了aspectj。Before指在事务开始之前获取注解信息。After throwing 指当方法抛出异常后的执行动作。这里一般会进行回滚操作。After returning指当方法返回前的执行动作。这里一般会提交事务。After 指当事务完成后的动作。这里会清空当前事务注解信息。 结论事务的隔离级别和传播属性都是我们应该掌握和学习的，对于Spring的@Transactional 注解，我们不但要会使用，而且应该深入去理解它的实现原理。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道面试题引发的思考（递归）]]></title>
    <url>%2Fblog%2F%E4%B8%80%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83.html</url>
    <content type="text"><![CDATA[前言某日，去某互联网公司面试，被问到了如下一道面试题： 题目很简单，有一队人，已知第一个人8岁，后一个人比前一个人大两岁，以此类推，问第8个人多少岁？第N个人多少岁。 我拿过笔和纸，不假思索的写下了如下答案： 1234567static int getAge(int n)&#123; if(n==1)&#123; return 8; &#125;else&#123; return getAge(n-1)+2; &#125;&#125; 面试官又问我还有什么需要注意的吗？我说要注意参数不能小于1. 显然，面试官不是很满意，又问我如果N很大会怎么样？ 我想了想，说，会出现OOM异常吧，或者超了int的范围。 感觉他还是不满意。 思考自那过去一段时间，我自认为答得没有问题，也答到了点上（只可惜面试没过~O(∩_∩)O哈哈~）。 最近又想到了这个问题，决定研究研究。 实践我用自己的方法，进行了数据测试。当然我把为了测试效果明显，我加大了测试值。 123public static void main(String[] args) &#123; System.out.println(getAge(1000000));&#125; 结果使我惊讶，不是OOM异常，而是堆栈异常。 1Exception in thread "main" java.lang.StackOverflowError 后面我了解到，递归调用，可以假想成一个函数调用另一个函数，而每个函数相当于占用一个栈帧，这些栈帧以先进后出的方式排列起来形成栈。如下图： 这样，函数会追寻到栈顶，拿到getAge(1)的值后逐渐返回。如下。 1234getAge(1)=8getAge(2)=getAge(1)+2=10getAge(3)=getAge(2)+2=12getAge(4)=getAge(3)+2=14 可以知道，如果堆栈深度不够的话，就会出现异常。 我们上图所示的异常就是这个原因。 提升改进一在研究这个问题时，我发现了一种递归，尾递归。 如下所示： 1234567static int getAge1(int n,int result)&#123; if(n==1)&#123; return result; &#125;else&#123; return getAge1(n-1,result+2); &#125;&#125; 这种递归我们可以看到，带了一个参数result，当他运行到n==1时，直接返回了result，不用在一层层回退进行计算。如下： 12345 getAge(4,8)= getAge(3,8+2)= getAge(2,8+2+2)= getAge(1,8+2+2+2)= 14 一些编译器发现这些函数可以在一个栈帧里进行完成，就会复用栈帧，优化代码。 可惜的是，到目前为止，JAVA在HotSpot（Oracle的JVM）上使用时，并不支持尾递归优化。据说IBM的JVM支持尾递归优化，有兴趣的童鞋可以试下。 所以这种方案的测试结果肯定也是StackOverflowError啦。 改进二循环解决: 那时脑子笨，现在想想，这道题用循环也是可以解决的，且不用担心堆栈溢出问题。如下： While循环： 1234567static int getAge2(int n,int result,int step)&#123; while(n&gt;1)&#123; result+=step; n--; &#125; return result;&#125; For循环: 123456static int getAge3(int start,int end,int firstValue,int step)&#123; for(int i=start;i&lt;end;i++)&#123; firstValue+=step; &#125; return firstValue;&#125; 且速度也提高了不少。 改进三这么有规律的数据，当然乘法就可以解决，如下： 123static int getAge4(int start,int end,int firstValue,int step)&#123; return firstValue+(end-start)*step;&#125; 以上例子没有对入参做校验处理，结合实际场景，其实需要处理的，在此略掉了。 总结JDK源码中很少有递归，因为递归不能被优化，当数据过大时，很容易出现堆栈溢出，我们也应该慎用，基本能有递归解决的问题也可以用循环解决。 可以看出，对于一个问题，实现的方案可能不止一种。 看到自己想了及总结了多种方法，感觉很诧异，面试时却只想到了递归一种，而且实现很不好。还是自己基础不够扎实，应该多学多看多练。 其他上面都说要多学多看多练啦….. 这是我测试IBM JDK对于尾递归（getAge1方法）的结果。 说明IBM JDK确实对尾递归有优化，我又测试了getAge方法（普通递归），结果和Oracle JDK一样，抛出 java.lang.StackOverflowError 异常。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于注解的通用数据验证]]></title>
    <url>%2Fblog%2F%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E7%9A%84%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E9%AA%8C%E8%AF%81.html</url>
    <content type="text"><![CDATA[前言在项目开发交互过程中，难免会遇到一些数据校验。以校验客户端发送数据的合法性，对于一些非空校验，我们也许可以使用@NonNull，@NotNull 等注解，可是对于一些常规的，如手机号，身份证等等的校验，我们就还要判断处理每个请求的参数的合法性。 但是合法性的判断是难以避免的，我们是否可以精简工作量、提高工作效率呢。 思考我们或许应该从@NonNull @NotNull等其他注解那里受到些启发。 我们或许可以结合正则表达式及注解对某些通用数据进行验证。 注解可以设置参数，我们可以设置参数为校验规则，通过枚举列举出来，同时也应该允许用户自定义正则等校验。 我们知道，注解有三种类型 RetentionPolicy.SOURCE RetentionPolicy.CLASS RetentionPolicy.RUNTIME。 SOURCE主要用于编译之前，编译过程中会被丢弃如@Override注解。 CLASS主要用于编译，运行时会被丢弃。 RUNTIME在源码，编译，运行时始终会存在。 可以利用反射，拿到具有特定注解的bean，并处理。所以我们定义的注解应该是RUNTIME类型。同时声明注作用范围为FIELD及PARAMETER。 实践定义注解1234567891011121314151617181920/** * 数据验证注解 */@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.FIELD,ElementType.PARAMETER&#125;)public @interface DataValid &#123; //是否可以为空 boolean nullable() default false; //提供几种常用的正则验证 RegexType regexType() default RegexType.NONE; //自定义正则验证 String regexExpression() default ""; //参数或者字段描述 String description() default "";&#125; 定义如上注解，nullable用来校验参数是否可空，默认不可以为空，false。 同时提供几种通用的正则校验，用枚举列出，如手机号码校验，身份证信息校验等等。 同时如果没有规定的正则表达式，可以让用户自定义自己的正则表达式。 另增加描述字段，用来说明这个paramer的用途。 定义常用正则枚举12345678910111213141516/** * 正则类型枚举 */public enum RegexType &#123; NONE, SPECIALCHAR, CHINESE, EMAIL, IP, NUMBER, NUMBERORNIL, PHONENUMBER, ID; &#125; 列出几种常用枚举。非空，特殊字符，中文，邮箱，IP，数字等等 枚举规则定义了枚举，要定义它们的具体对应的方法，以便后续调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164/** * 常用正则表达式 */public class RegexUtils &#123; /** * 判断是否是正确的IP地址 * * @param ip * @return boolean true,通过，false，没通过 */ public static boolean isIp(String ip) &#123; if (null == ip || "".equals(ip)) return false; String regex = "^(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|[1-9])\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)$"; return ip.matches(regex); &#125; /** * 判断是否是正确的邮箱地址 * * @param email * @return boolean true,通过，false，没通过 */ public static boolean isEmail(String email) &#123; if (null == email || "".equals(email)) return false; String regex = "\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*"; return email.matches(regex); &#125; /** * 判断是否含有中文，仅适合中国汉字，不包括标点 * @param text * @return boolean true,通过，false，没通过 */ public static boolean isChinese(String text) &#123; if (null == text || "".equals(text)) return false; Pattern p = Pattern.compile("[\u4e00-\u9fa5]"); Matcher m = p.matcher(text); return m.find(); &#125; /** * 判断是否正整数 * * @param number * 数字 * @return boolean true,通过，false，没通过 */ public static boolean isNumber(String number) &#123; if (null == number || "".equals(number)) return false; String regex = "[0-9]*"; return number.matches(regex); &#125; /** * 判断是否正整数(可以为空) * * @param number * 数字 * @return boolean true,通过，false，没通过 */ public static boolean isNumberOrNil(String number) &#123; if(null == number) return true; if ("".equals(number.trim())) return true; String regex = "[0-9]*"; return number.matches(regex); &#125; /** * 判断几位小数(正数) * * @param decimal * 数字 * @param count * 小数位数 * @return boolean true,通过，false，没通过 */ public static boolean isDecimal(String decimal, int count) &#123; if (null == decimal || "".equals(decimal)) return false; String regex = "^(-)?(([1-9]&#123;1&#125;\\d*)|([0]&#123;1&#125;))(\\.(\\d)&#123;" + count + "&#125;)?$"; return decimal.matches(regex); &#125; /** * 判断是否是手机号码 * * @param phoneNumber * 手机号码 * @return boolean true,通过，false，没通过 */ public static boolean isPhoneNumber(String phoneNumber) &#123; if (null == phoneNumber || "".equals(phoneNumber)) return false; String regex = "^1[3|4|5|6|7|8|9][0-9]\\d&#123;8&#125;$"; return phoneNumber.matches(regex); &#125; /** * 判断身份证号格式正确性 * * @param ID * 身份证号 * @return boolean true,通过，false，没通过 */ public static boolean isID(String ID) &#123; if (null == ID || "".equals(ID)) return false; String regex = "^(\\d&#123;14&#125;[0-9a-zA-Z])|(\\d&#123;17&#125;[0-9a-zA-Z])$"; return ID.matches(regex); &#125; /** * 判断是否含有特殊字符 * * @param text * @return boolean true,通过，false，没通过 */ public static boolean hasSpecialChar(String text) &#123; if (null == text || "".equals(text)) return false; if (text.replaceAll("[a-z]*[A-Z]*\\d*-*_*\\s*", "").length() == 0) &#123; // 如果不包含特殊字符 return true; &#125; return false; &#125; /** * 适应CJK（中日韩）字符集，部分中日韩的字是一样的 */ public static boolean isChinese2(String strName) &#123; char[] ch = strName.toCharArray(); for (int i = 0; i &lt; ch.length; i++) &#123; char c = ch[i]; if (isChinese(c)) &#123; return true; &#125; &#125; return false; &#125; private static boolean isChinese(char c) &#123; Character.UnicodeBlock ub = Character.UnicodeBlock.of(c); if (ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_COMPATIBILITY_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_A || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_B || ub == Character.UnicodeBlock.CJK_SYMBOLS_AND_PUNCTUATION || ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS || ub == Character.UnicodeBlock.GENERAL_PUNCTUATION) &#123; return true; &#125; return false; &#125;&#125; 实现及调用基本数据都定义及处理好了，我们应该建立注解与方法之间的关联，RUNTIME类型的注解在程序运行时也会被保留，我们可以利用反射，拿到具体注解参数信息，进行相关处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113/** * 注解解析Service */public class ValidateService &#123; /*private static DataValid dataValid;*/ public ValidateService() &#123; super(); &#125; /** * 解析入口 * @param object * @throws Exception */ public static void valid(Object object) throws Exception&#123; //获取object的类型 Class&lt;? extends Object&gt; clazz=object.getClass(); //获取该类型声明的成员 Field[] fields=clazz.getDeclaredFields(); //遍历属性 for(Field field:fields)&#123; //对于private私有化的成员变量，通过setAccessible来修改器访问权限 field.setAccessible(true); validate(field,object); //重新设置会私有权限 field.setAccessible(false); &#125; &#125; public static void validate(Field field,Object object) throws Exception&#123; String description = null; Object value = null; DataValid dataValid = null; //获取对象的成员的注解信息 dataValid=field.getAnnotation(DataValid.class); value=field.get(object); if(dataValid==null)return; description=dataValid.description().equals("")?field.getName():dataValid.description(); /*************注解解析工作开始******************/ if(!dataValid.nullable() &amp;&amp; dataValid.regexType() != RegexType.NUMBERORNIL)&#123; if(value==null|| StringUtils.isBlank(value.toString()))&#123; throw new Exception(description+"不能为空"); &#125; &#125; if(dataValid.regexType()!=RegexType.NONE)&#123; switch (dataValid.regexType()) &#123; case NONE: break; case SPECIALCHAR: if(RegexUtils.hasSpecialChar(value.toString()))&#123; throw new Exception(description+"不能含有特殊字符"); &#125; break; case CHINESE: if(RegexUtils.isChinese2(value.toString()))&#123; throw new Exception(description+"不能含有中文字符"); &#125; break; case EMAIL: if(!RegexUtils.isEmail(value.toString()))&#123; throw new Exception(description+"邮箱地址格式不正确"); &#125; break; case IP: if(!RegexUtils.isIp(value.toString()))&#123; throw new Exception(description+"IP地址格式不正确"); &#125; break; case NUMBER: if(!RegexUtils.isNumber(value.toString()))&#123; throw new Exception(description+"不是数字"); &#125; break; case NUMBERORNIL: if(value == null)&#123; break; &#125; if(!RegexUtils.isNumberOrNil(value.toString()))&#123; throw new Exception(description+"格式不正确"); &#125; break; case PHONENUMBER: if(!RegexUtils.isPhoneNumber(value.toString()))&#123; throw new Exception("手机号格式不正确"); &#125; break; case ID: if(!RegexUtils.isID(value.toString()))&#123; throw new Exception("身份证号格式不正确"); &#125; break; default: break; &#125; &#125; if(!dataValid.regexExpression().equals(""))&#123; if(value.toString().matches(dataValid.regexExpression()))&#123; throw new Exception(description+"格式不正确"); &#125; &#125; /*************注解解析工作结束******************/ &#125;&#125; 如上代码。 当然，到具体业务层，应该调用这个Service的valid方法去校验参数。 结论可以看到，经过这样，我们可以把一些常用的校验通过这种方式封装，大大简化代码量，使业务层更注重业务。 这种也可以添加自己的通用类型，灵活性很强。 这个小小的简单工具最主要的就是利用了Java的反射机制。 以上。 今天就到这里啦，中秋节快乐～～]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK里那些有趣的代码（1）]]></title>
    <url>%2Fblog%2FJDK%E9%87%8C%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%881%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言今天我们来看一下JDK里一些好玩的代码。我们来分析下。 在Integer源码里，我们可以看到这样一段代码： 12345678final static int [] sizeTable = &#123; 9, 99, 999, 9999, 99999, 999999, 9999999,99999999, 999999999, Integer.MAX_VALUE &#125;;// Requires positive xstatic int stringSize1(int x) &#123; for (int i=0; ; i++) if (x &lt;= sizeTable[i]) return i+1;&#125; 这段代码是在干啥？？？ 我们仔细读下，其实就会发现他其实是计算传入的int型x的位数，要求x为正数。 嗯，不难理解。 他为什么要这么写呢？ 思考对于我们来讲，当拿到一个int型正整数，如何用程序算出它的位数呢？ 一般不假思索的可能想到String的length方法计算。如下： 123static int myStringSize1(int x)&#123; return String.valueOf(x).length();&#125; 嗯，代码量很少。。。。。。 还有吗？ 对，还可以用除法，除以10，计算位数。 12345678static int myStringSize2(int x)&#123; int num=1; while(x&gt;10)&#123; x=x/10; num++; &#125; return num;&#125; 嗯。。。。不错不错～ 当然能用除法解决的基本上也可以用乘法解决，int的最大位数为10位，所以可以乘以10，判断大小。 12345678910// Requires positive xstatic int stringSize2(int x) &#123; int p = 10; for (int i=1; i&lt;11; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 10;&#125; 好吧，目前我就想到了除JDK以外的这三种方法。 我们测试一下我们的代码正确性。 12345678910public static void main(String[] args) &#123; int [] x=new int[]&#123;5,10,333,6666,77777,123456,1234567,87654321,999999999,1111111111,Integer.MAX_VALUE&#125;; for(int i=0;i&lt;x.length;i++)&#123; int a1=Test.stringSize1(x[i]); int a2=Test.stringSize2(x[i]); int a3=Test.myStringSize1(x[i]); int a4=Test.myStringSize2(x[i]); System.out.println(a1+"---"+a2+"---"+a3+"---"+a4); &#125;&#125; 运行结果： 结果还是木有问题哒。 提升JDK的这种写法有什么好处呢？？？ 可以看到，JDK的方法把一部分数据计算变成了数据比较，相当于优化吧。 那它的执行效率怎么样呢？ 我们写的方法的效率又如何呢？ 我们来测试下吧！ 实践我们可以制造一个随机的定长int数组，看看它们执行耗时，同时统计若干组数据，进行比较。 话不多说，直接写代码并记录到Excel里供分析。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public static void main(String[] args) throws Exception&#123; List&lt;List&lt;Long&gt;&gt; rowList= Lists.newArrayList(); List&lt;String&gt; titleList=Lists.newArrayList(); titleList.add("JDK方法"); titleList.add("乘法"); titleList.add("String方法"); titleList.add("除法"); for(int s=0;s&lt;50;s++)&#123; List&lt;Long&gt; cellList=Lists.newArrayList(); int [] xArrays=new int [10000]; for(int i=0;i&lt;xArrays.length;i++)&#123; xArrays[i]=1 + (int)(Math.random()*Integer.MAX_VALUE); &#125; //System.out.println("当前S值为"+s); long start1=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; stringSize1(xArrays[i]); &#125; long end1=System.nanoTime(); long time1=(end1-start1)/1000; System.out.println("JDK方法耗时---》"+time1+"ms"); cellList.add(time1); long start4=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; stringSize2(xArrays[i]); &#125; long end4=System.nanoTime(); long time4=(end4-start4)/1000; System.out.println("乘法耗时---》"+time4+"ms"); cellList.add(time4); long start2=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; myStringSize1(xArrays[i]); &#125; long end2=System.nanoTime(); long time2=(end2-start2)/1000; System.out.println("String方法耗时---》"+time2+"ms"); cellList.add(time2); long start3=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; myStringSize2(xArrays[i]); &#125; long end3=System.nanoTime(); long time3=(end3-start3)/1000; System.out.println("除法耗时---》"+time3+"ms"); cellList.add(time3); rowList.add(cellList); &#125; WriteExcelUtil.writeExecl(titleList,rowList,"/Users/zhangwentong/Desktop/workbook.xlsx"); &#125; 我记录了50组数据，每组里面每个计算位数的方法执行10000次，得到如下结果。 把它绘制成折线图。 可以看到，String的length方法效率是最差的。。。。 我们其实看一下String.valueOf(x).length()这个源码，就知道为什么这么慢了。 然后除法也稍微逊色一点。。。 乘法和JDK的效率都可以说不错。。。。。但JDK方法其实要好一点。。。。 而且开始创建的sizeTable是占据较少空间，但却降低了CPU的计算次数（乘法需要每次乘以十在比较计算）。 当然，因为int的位数只有10位，可以写一个sizeTable，当数据量大时，比如long，最大19位，写一个19个数的sizeTable？ 一堆9？？？ 哈哈，我们可以看看Long里面的计算位数的方法。 12345678910// Requires positive xstatic int stringSize(long x) &#123; long p = 10; for (int i=1; i&lt;19; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 19;&#125; 人家当然用的乘法啦。。。。。 结论我们可以看到，我们最容易想到的String.length方法确是效率最低的。。。。 JDK源码里一些方法会被大量调用，当然要做到最好的优化啦。。。 多读些源码，深入思考，多加练习，有助于提升自己。 今天就到这儿吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）线上某应用的FULLGC分析]]></title>
    <url>%2Fblog%2F%E8%BD%AC-%E7%BA%BF%E4%B8%8A%E6%9F%90%E5%BA%94%E7%94%A8%E7%9A%84FULLGC%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[原文链接：http://jm.taobao.org/2016/03/23/3782/ 这2天，排除线上某应用启动内存变化频繁的问题时，额外发现了一个fullgc的问题，分享给大家。 过程如下：抽了台线上机器，想看下这段时间机器的gc情况，发现里面有好几个FullGc的日志： T23:23:02.009+0800: 21860.015: [Full GC 21860.015: [CMS: 2361237K-&amp;gt;1111804K(4718592K), 4.9917540 secs] 2532961K-&amp;gt;1111804K(5190464K), [CMS Perm : 17397K-&amp;gt;17240K(131072K)], 4.9918770 secs] [Times: user=4.96 sys=0.03, real=4.99 secs] JVM参数设置如下： -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=60 参数的意思是：在旧区到60％的时候，会触发一次cmsgc，应该出现如下日志： T20:10:37.803+0800: 3246087.559: [CMS-concurrent-mark-start] T20:10:38.463+0800: 3246088.220: [CMS-concurrent-mark: 0.661/0.661 secs] [Times: user=3.17 sys=0.56, real=0.66 secs] T20:10:38.463+0800: 3246088.220: [CMS-concurrent-preclean-start] T20:10:38.552+0800: 3246088.309: [CMS-concurrent-preclean: 0.069/0.089 secs] [Times: user=0.14 sys=0.04, real=0.09 secs]_&lt;/span&gt; T20:10:38.552+0800: 3246088.309: [CMS-concurrent-abortable-preclean-start] 而现在日志里面都是old区到2.3G（50%）的时候，就会触发一次FullGc，而且gc日志里面没有一次正常的cmsgc，现在是什么原因在半路截胡了？ 开始怀疑JVM参数是否设置生效，通过jinfo进行查看： jinfo -flag UseCMSInitiatingOccupancyOnly 20195 jinfo -flag CMSInitiatingOccupancyFraction 20195 一切正常。 出现Fullgc，当时我想可能的原因有以下几个情况： cmsgc失败导致（GC日志中没有相关cmsgc失败的日志） JMAP -histo：现场（人为执行肯定不是） 大对象分配时，空间不够导致（当时还剩下50%内存，并且如果大对象分配，gc日志里面是会有如下WARN的） 内存碎片导致？（由于系统会经常分配一些大数组，这个会加剧碎片化） 第四点是最可能的原因了。于是，接下来怎么验证是否是它导致的呢？加上PrintGCReason，先打印出fullgc的原因, 命令如下： /java/bin/jinfo -flag +PrintGCReason 第二天，查看日志，如下： GC Cause: Heap Inspection Initiated GC T16:16:01.880+0800: 687439.886: [Full GC 687439.886: [CMS: 2362138K-&amp;gt;1180717K(4718592K), 5.6573690 secs] 2700275K-&amp;gt;1180717K(5190464K), [C MS Perm : 17531K-&amp;gt;17488K(131072K)], 5.6574950 secs] [Times: user=5.59 sys=0.06, real=5.65 secs] GC原因：堆检查启动GC，FullGc的原因是这个，看不明白，咨询过后，说这个很可能是因为JAMP -hist继：活导致的FullGc。 那如果是这样，就有可能是有脚本或者定时任务，也可能是什么其他东西，去执行了这个命令，反正据我了解的cs没有做这事。接下来就是找这个“凶手”了，这事情没做过，没啥头绪，看进程也看不出什么，想grep所有脚本，懒癌又发作了，还是先去群里咨询下有啥简单又省力的办法吧，一下搞定： [ ~]$ crontab -l */1 * * * * /home/bin/config-monitor.sh &amp;gt;&amp;gt; /home/logs/config-monitor.log 2&amp;gt;&amp;amp;1 [logs]$ cat /home/bin/config-monitor.sh |grep &quot;jmap&quot; jmaplog=&quot;/home/jmap.log&quot;; if (count == 3) { / run jmap print &quot;run jmap command : /java/bin/jmap -histo:live &quot;pid&quot; |head -n 20&quot;; system(&quot;/java/bin/jmap -histo:live &quot;pid&quot; |head -n 20&quot;)&amp;gt;jmaplog; print &quot;#######Server has recovered after running jmap######&quot;; 有个定时任务跑一个叫config-monitor.sh的脚本，里面做的事情，基本就是监视内存各个区的比例，超过一定比例，就通过jamp -histo：现场触发下fullgc，防止溢出===》这个定时任务是cs以前遗留下来的，一直没发现，后续就是评估是否去掉这个定时任务，整个过程告一段落。 总结： 问题可能出现的原因，要尽快动手去验证，不要只停留在思考的层面; 出现fullgc的时候，可以通过加上PrintGCReason，查看具体GC原因。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）为什么 String hashCode 方法选择数字31作为乘子]]></title>
    <url>%2Fblog%2F%E4%B8%BA%E4%BB%80%E4%B9%88-String-hashCode-%E6%96%B9%E6%B3%95%E9%80%89%E6%8B%A9%E6%95%B0%E5%AD%9731%E4%BD%9C%E4%B8%BA%E4%B9%98%E5%AD%90.html</url>
    <content type="text"><![CDATA[原文地址：https://segmentfault.com/a/1190000010799123 背景某天，我在写代码的时候，无意中点开了 String hashCode 方法。然后大致看了一下 hashCode 的实现，发现并不是很复杂。但是我从源码中发现了一个奇怪的数字，也就是本文的主角31。这个数字居然不是用常量声明的，所以没法从字面意思上推断这个数字的用途。后来带着疑问和好奇心，到网上去找资料查询一下。在看完资料后，默默的感叹了一句，原来是这样啊。那么到底是哪样呢？在接下来章节里，请大家带着好奇心和我揭开数字31的用途之谜。 选择数字31的原因在详细说明 String hashCode 方法选择数字31的作为乘子的原因之前，我们先来看看 String hashCode 方法是怎样实现的，如下： 123456789101112public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; 上面的代码就是 String hashCode 方法的实现，是不是很简单。实际上 hashCode 方法核心的计算逻辑只有三行，也就是代码中的 for 循环。我们可以由上面的 for 循环推导出一个计算公式，hashCode 方法注释中已经给出。如下： s[0]31^(n-1) + s131^(n-2) + … + s[n-1] 这里说明一下，上面的 s 数组即源码中的 val 数组，是 String 内部维护的一个 char 类型数组。这里我来简单推导一下这个公式： 假设 n=3 i=0 -&gt; h = 31 * 0 + val[0] i=1 -&gt; h = 31 * (31 * 0 + val[0]) + val[1] i=2 -&gt; h = 31 * (31 * (31 * 0 + val[0]) + val[1]) + val[2] h = 31*31*31*0 + 31*31*val[0] + 31*val[1] + val[2] h = 31^(n-1)*val[0] + 31^(n-2)*val[1] + val[2] 上面的公式包括公式的推导并不是本文的重点，大家了解了解即可。接下来来说说本文的重点，即选择31的理由。从网上的资料来看，一般有如下两个原因： 第一、31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。另外一些相近的质数，比如37、41、43等等，也都是不错的选择。那么为啥偏偏选中了31呢？请看第二个原因。 第二、31可以被 JVM 优化，31 * i = (i &lt;&lt; 5) - i。 上面两个原因中，第一个需要解释一下，第二个比较简单，就不说了。下面我来解释第一个理由。一般在设计哈希算法时，会选择一个特殊的质数。至于为啥选择质数，我想应该是可以降低哈希算法的冲突率。至于原因，这个就要问数学家了，我几乎可以忽略的数学水平解释不了这个原因。上面说到，31是一个不大不小的质数，是优选乘子。那为啥同是质数的2和101（或者更大的质数）就不是优选乘子呢，分析如下。 这里先分析质数2。首先，假设 n = 6，然后把质数2和 n 带入上面的计算公式。并仅计算公式中次数最高的那一项，结果是2^5 = 32，是不是很小。所以这里可以断定，当字符串长度不是很长时，用质数2做为乘子算出的哈希值，数值不会很大。也就是说，哈希值会分布在一个较小的数值区间内，分布性不佳，最终可能会导致冲突率上升。 上面说了，质数2做为乘子会导致哈希值分布在一个较小区间内，那么如果用一个较大的大质数101会产生什么样的结果呢？根据上面的分析，我想大家应该可以猜出结果了。就是不用再担心哈希值会分布在一个小的区间内了，因为101^5 = 10,510,100,501。但是要注意的是，这个计算结果太大了。如果用 int 类型表示哈希值，结果会溢出，最终导致数值信息丢失。尽管数值信息丢失并不一定会导致冲突率上升，但是我们暂且先认为质数101（或者更大的质数）也不是很好的选择。最后，我们再来看看质数31的计算结果： 31^5 = 28629151，结果值相对于32和10,510,100,501来说。是不是很nice，不大不小。 上面用了比较简陋的数学手段证明了数字31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。接下来我会用详细的实验来验证上面的结论，不过在验证前，我们先看看 Stack Overflow 上关于这个问题的讨论，Why does Java’s hashCode() in String use 31 as a multiplier?。其中排名第一的答案引用了《Effective Java》中的一段话，这里也引用一下： The value 31 was chosen because it is an odd prime. If it were even and the multiplication overflowed, information would be lost, as multiplication by 2 is equivalent to shifting. The advantage of using a prime is less clear, but it is traditional. A nice property of 31 is that the multiplication can be replaced by a shift and a subtraction for better performance: 31 * i == (i &lt;&lt; 5) - i. Modern VMs do this sort of optimization automatically. 简单翻译一下： 选择数字31是因为它是一个奇质数，如果选择一个偶数会在乘法运算中产生溢出，导致数值信息丢失，因为乘二相当于移位运算。选择质数的优势并不是特别的明显，但这是一个传统。同时，数字31有一个很好的特性，即乘法运算可以被移位和减法运算取代，来获取更好的性能：31 * i == (i &lt;&lt; 5) - i，现代的 Java 虚拟机可以自动的完成这个优化。 排名第二的答案设这样说的： As Goodrich and Tamassia point out, If you take over 50,000 English words (formed as the union of the word lists provided in two variants of Unix), using the constants 31, 33, 37, 39, and 41 will produce less than 7 collisions in each case. Knowing this, it should come as no surprise that many Java implementations choose one of these constants. 这段话也翻译一下： 正如 Goodrich 和 Tamassia 指出的那样，如果你对超过 50,000 个英文单词（由两个不同版本的 Unix 字典合并而成）进行 hash code 运算，并使用常数 31, 33, 37, 39 和 41 作为乘子，每个常数算出的哈希值冲突数都小于7个，所以在上面几个常数中，常数 31 被 Java 实现所选用也就不足为奇了。 上面的两个答案完美的解释了 Java 源码中选用数字 31 的原因。接下来，我将针对第二个答案就行验证，请大家继续往下看。 实验及数据可视化本节，我将使用不同的数字作为乘子，对超过23万个英文单词进行哈希运算，并计算哈希算法的冲突率。同时，我也将针对不同乘子算出的哈希值分布情况进行可视化处理，让大家可以直观的看到数据分布情况。本次实验所使用的数据是 Unix/Linux 平台中的英文字典文件，文件路径为 /usr/share/dict/words。 哈希值冲突率计算计算哈希算法冲突率并不难，比如可以一次性将所有单词的 hash code 算出，并放入 Set 中去除重复值。之后拿单词数减去 set.size() 即可得出冲突数，有了冲突数，冲突率就可以算出来了。当然，如果使用 JDK8 提供的流式计算 API，则可更方便算出，代码片段如下： 123456789101112131415161718192021222324252627public static Integer hashCode(String str, Integer multiplier) &#123; int hash = 0; for (int i = 0; i &lt; str.length(); i++) &#123; hash = multiplier * hash + str.charAt(i); &#125; return hash;&#125; /** * 计算 hash code 冲突率，顺便分析一下 hash code 最大值和最小值，并输出 * @param multiplier * @param hashs */public static void calculateConflictRate(Integer multiplier, List&lt;Integer&gt; hashs) &#123; Comparator&lt;Integer&gt; cp = (x, y) -&gt; x &gt; y ? 1 : (x &lt; y ? -1 : 0); int maxHash = hashs.stream().max(cp).get(); int minHash = hashs.stream().min(cp).get(); // 计算冲突数及冲突率 int uniqueHashNum = (int) hashs.stream().distinct().count(); int conflictNum = hashs.size() - uniqueHashNum; double conflictRate = (conflictNum * 1.0) / hashs.size(); System.out.println(String.format("multiplier=%4d, minHash=%11d, maxHash=%10d, conflictNum=%6d, conflictRate=%.4f%%", multiplier, minHash, maxHash, conflictNum, conflictRate * 100));&#125; 结果如下： 从上图可以看出，使用较小的质数做为乘子时，冲突率会很高。尤其是质数2，冲突率达到了 55.14%。同时我们注意观察质数2作为乘子时，哈希值的分布情况。可以看得出来，哈希值分布并不是很广，仅仅分布在了整个哈希空间的正半轴部分，即 0 ~ 2^31-1。而负半轴 -2^31 ~ -1，则无分布。这也证明了我们上面断言，即质数2作为乘子时，对于短字符串，生成的哈希值分布性不佳。然后再来看看我们之前所说的 31、37、41 这三个不大不小的质数，表现都不错，冲突数都低于7个。而质数 101 和 199 表现的也很不错，冲突率很低，这也说明哈希值溢出并不一定会导致冲突率上升。但是这两个家伙一言不合就溢出，我们认为他们不是哈希算法的优选乘子。最后我们再来看看 32 和 36 这两个偶数的表现，结果并不好，尤其是 32，冲突率超过了了50%。尽管 36 表现的要好一点，不过和 31，37相比，冲突率还是比较高的。当然并非所有的偶数作为乘子时，冲突率都会比较高，大家有兴趣可以自己验证。 哈希值分布可视化上一节分析了不同数字作为乘子时的冲突率情况，这一节来分析一下不同数字作为乘子时，哈希值的分布情况。在详细分析之前，我先说说哈希值可视化的过程。我原本是打算将所有的哈希值用一维散点图进行可视化，但是后来找了一圈，也没找到合适的画图工具。加之后来想了想，一维散点图可能不合适做哈希值可视化，因为这里有超过23万个哈希值。也就意味着会在图上显示超过23万个散点，如果不出意外的话，这23万个散点会聚集的很密，有可能会变成一个大黑块，就失去了可视化的意义了。所以这里选择了另一种可视化效果更好的图表，也就是 excel 中的平滑曲线的二维散点图（下面简称散点曲线图）。当然这里同样没有把23万散点都显示在图表上，太多了。所以在实际绘图过程中，我将哈希空间等分成了64个子区间，并统计每个区间内的哈希值数量。最后将分区编号做为X轴，哈希值数量为Y轴，就绘制出了我想要的二维散点曲线图了。这里举个例子说明一下吧，以第0分区为例。第0分区数值区间是[-2147483648, -2080374784)，我们统计落在该数值区间内哈希值的数量，得到 &lt;分区编号, 哈希值数量&gt; 数值对，这样就可以绘图了。分区代码如下： 1234567891011121314151617181920212223242526 /** * 将整个哈希空间等分成64份，统计每个空间内的哈希值数量 * @param hashs */public static Map&lt;Integer, Integer&gt; partition(List&lt;Integer&gt; hashs) &#123; // step = 2^32 / 64 = 2^26 final int step = 67108864; List&lt;Integer&gt; nums = new ArrayList&lt;&gt;(); Map&lt;Integer, Integer&gt; statistics = new LinkedHashMap&lt;&gt;(); int start = 0; for (long i = Integer.MIN_VALUE; i &lt;= Integer.MAX_VALUE; i += step) &#123; final long min = i; final long max = min + step; int num = (int) hashs.parallelStream() .filter(x -&gt; x &gt;= min &amp;&amp; x &lt; max).count(); statistics.put(start++, num); nums.add(num); &#125; // 为了防止计算出错，这里验证一下 int hashNum = nums.stream().reduce((x, y) -&gt; x + y).get(); assert hashNum == hashs.size(); return statistics;&#125; 本文中的哈希值是用整形表示的，整形的数值区间是 [-2147483648, 2147483647]，区间大小为 2^32。所以这里可以将区间等分成64个子区间，每个自子区间大小为 2^26。详细的分区对照表如下： 分区编号 分区下限 分区上限 分区编号 分区下限 分区上限 0 -2147483648 -2080374784 32 0 67108864 1 -2080374784 -2013265920 33 67108864 134217728 2 -2013265920 -1946157056 34 134217728 201326592 3 -1946157056 -1879048192 35 201326592 268435456 4 -1879048192 -1811939328 36 268435456 335544320 5 -1811939328 -1744830464 37 335544320 402653184 6 -1744830464 -1677721600 38 402653184 469762048 7 -1677721600 -1610612736 39 469762048 536870912 8 -1610612736 -1543503872 40 536870912 603979776 9 -1543503872 -1476395008 41 603979776 671088640 10 -1476395008 -1409286144 42 671088640 738197504 11 -1409286144 -1342177280 43 738197504 805306368 12 -1342177280 -1275068416 44 805306368 872415232 13 -1275068416 -1207959552 45 872415232 939524096 14 -1207959552 -1140850688 46 939524096 1006632960 15 -1140850688 -1073741824 47 1006632960 1073741824 16 -1073741824 -1006632960 48 1073741824 1140850688 17 -1006632960 -939524096 49 1140850688 1207959552 18 -939524096 -872415232 50 1207959552 1275068416 19 -872415232 -805306368 51 1275068416 1342177280 20 -805306368 -738197504 52 1342177280 1409286144 21 -738197504 -671088640 53 1409286144 1476395008 22 -671088640 -603979776 54 1476395008 1543503872 23 -603979776 -536870912 55 1543503872 1610612736 24 -536870912 -469762048 56 1610612736 1677721600 25 -469762048 -402653184 57 1677721600 1744830464 26 -402653184 -335544320 58 1744830464 1811939328 27 -335544320 -268435456 59 1811939328 1879048192 28 -268435456 -201326592 60 1879048192 1946157056 29 -201326592 -134217728 61 1946157056 2013265920 30 -134217728 -67108864 62 2013265920 2080374784 31 -67108864 0 63 2080374784 2147483648 接下来，让我们对照上面的分区表，对数字2、3、17、31、101的散点曲线图进行简单的分析。先从数字2开始，数字2对于的散点曲线图如下： 上面的图还是很一幕了然的，乘子2算出的哈希值几乎全部落在第32分区，也就是 [0, 67108864)数值区间内，落在其他区间内的哈希值数量几乎可以忽略不计。这也就不难解释为什么数字2作为乘子时，算出哈希值的冲突率如此之高的原因了。所以这样的哈希算法要它有何用啊，拖出去斩了吧。接下来看看数字3作为乘子时的表现： 3作为乘子时，算出的哈希值分布情况和2很像，只不过稍微好了那么一点点。从图中可以看出绝大部分的哈希值最终都落在了第32分区里，哈希值的分布性很差。这个也没啥用，拖出去枪毙5分钟吧。在看看数字17的情况怎么样： 数字17作为乘子时的表现，明显比上面两个数字好点了。虽然哈希值在第32分区和第34分区有一定的聚集，但是相比较上面2和3，情况明显好好了很多。除此之外，17作为乘子算出的哈希值在其他区也均有分布，且较为均匀，还算是一个不错的乘子吧。 接下来来看看我们本文的主角31了，31作为乘子算出的哈希值在第33分区有一定的小聚集。不过相比于数字17，主角31的表现又好了一些。首先是哈希值的聚集程度没有17那么严重，其次哈希值在其他区分布的情况也要好于17。总之，选31，准没错啊。 最后再来看看大质数101的表现，不难看出，质数101作为乘子时，算出的哈希值分布情况要好于主角31，有点喧宾夺主的意思。不过不可否认的是，质数101的作为乘子时，哈希值的分布性确实更加均匀。所以如果不在意质数101容易导致数据信息丢失问题，或许其是一个更好的选择。 写在最后经过上面的分析与实践，我想大家应该明白了 String hashCode 方法中选择使用数字31作为乘子的原因了。 本文为转载，原文地址：https://segmentfault.com/a/1190000010799123]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReentrantLock那些事]]></title>
    <url>%2Fblog%2F%E8%B0%88%E8%B0%88ReentrantLock.html</url>
    <content type="text"><![CDATA[前言在说ReentrantLock之前，我们先说说并发吧。 在JDK1.5之前，并发处理常用的关键字synchronized。使用synchronized关键字，锁的获取和释放是隐式的，synchronized主要通过系统的monitorenter指令实现的。 那时候synchronized可以称为重量级锁，执行效率不是很高。 而Doug Lea编写的util.concurrent 包被纳入JSR-166标准。这里面就包含了ReentrantLock。 ReentrantLock为编写并发提供了更多选择。 使用ReentrantLock的通常用法如下：123456789101112public class X &#123; private final ReentrantLock lock = new ReentrantLock(); public void m() &#123; lock.lock(); try &#123; //TODO &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 原理ReentrantLock主要是通过AbstractQueuedSynchronizer实现的，是一个重入锁，即一个线程加锁后仍然可以获得锁，不会出现自己阻塞自己的情况。 UML图我们看一下它们的UML图。 可以看到ReentrantLock实现了Lock接口。 锁类型ReentrantLock的两种锁类型，公平锁和非公平锁。 源码分析我们先来看下ReentrantLock的构造方法。 123456public ReentrantLock() &#123; sync = new NonfairSync();&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 可以看到默认无参构造方法为非公平锁实现。如果想定义公平锁实现，可以传入true来控制。 它的lock方法： 12345678public void lock() &#123; sync.acquire(1); &#125; public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; 公平锁和非公平锁各有自己的实现方式。我们来看下他们的tryAcquire方法。 非公平锁源码： 123456789101112131415161718192021222324static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125;final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125; 可以看到，非公平锁首先判断AQS（AbstractQueuedSynchronizer）中的state是否为0，0表示没有线程持有该锁，当前线程就尝试获取锁。 如果不是0，那在判断是不是当前线程持有该锁，如果是，就会增加state，改变state状态。（因此ReentranLock支持重入）。 公平锁源码： 1234567891011121314151617181920212223static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; @ReservedStackAccess protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 1234567public final boolean hasQueuedPredecessors() &#123; Node t = tail; Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 公平锁的tryAcquire方法，可以看到，相比非公平锁，多了hasQueuedPredecessors方法，这个方法是判断队列中是否有其他线程，如果没有，线程才会尝试获取锁，如果有，会先把锁分配给队列的线程，因此称为公平锁。 这儿可以看到，非公平锁的效率比公平锁要高。 这是tryAcquire方法，如果尝试获取锁失败了呢？ 那就会执行acquireQueued(addWaiter(Node.EXCLUSIVE), arg)方法啦。 我们先来看一下addWaiter方法。 12345678910111213141516private Node addWaiter(Node mode) &#123; Node node = new Node(mode); for (;;) &#123; Node oldTail = tail; if (oldTail != null) &#123; node.setPrevRelaxed(oldTail); if (compareAndSetTail(oldTail, node)) &#123; oldTail.next = node; return node; &#125; &#125; else &#123; initializeSyncQueue(); &#125; &#125;&#125; 可以看到，这个方法会把线程添加到队列尾，同时，for(;;)循环保证添加成功，直到return出去。 添加后，调用acquireQueued方法，这个方法为挂起等待线程。 看下该方法源码： 1234567891011121314151617181920212223final boolean acquireQueued(final Node node, int arg) &#123; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125;&#125;private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 可以看到，如果节点为头节点，就尝试获取一次锁，如果成功，就返回。 否则判断该线程是否需要挂起，如果需要的化就调用parkAndCheckInterrupt挂起。 调用LockSupport.park方法挂起线程，直到被唤醒。 selfInterrupt方法： 12345678910111213141516static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125;public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); b.interrupt(this); return; &#125; &#125; interrupt0();&#125; 调用interrupt方法，中断正在执行的线程（如果不是当前线程的话）。 释放锁unlock方法： 公平锁和非公平锁释放锁的方法是一样的。 123456789101112131415161718192021222324public void unlock() &#123; sync.release(1); &#125; public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; 可以看到首先会判断当前线程是否是获得锁的线程，如果是重入锁需要将state减完才算是完全释放锁。 释放后调用unparkSuccessor唤起挂起线程。 总结 非公平锁的效率是比公平锁要高的。 ReentranLock支持重入，因为增加了对自身线程的处理，通过state可以控制。 解锁操作应放到finally块里，避免使用锁时出现资源无法释放的问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[越努力越幸运]]></title>
    <url>%2Fblog%2F%E8%B6%8A%E5%8A%AA%E5%8A%9B%E8%B6%8A%E5%B9%B8%E8%BF%90.html</url>
    <content type="text"><![CDATA[前言感觉最近自己浮躁的很，不知为什么。 准备写些东西源自3个月前了，那时候想自己记录些知识，一方面便于加深记忆，一方面不让自己太“闲”。 或许心态本来就不太正，浮浮躁躁，有的时候写点东西感觉就是为了应付内心那个自己。 兴趣与工作从事Java开发已经3-4年了，也算是个半路出家的吧，总应该比别人多付出些努力。 人如何克服惰性？ 关键是培养自己的兴趣爱好吧。 3-4年工作期间，换了2、3家公司，每次面试的时候，总感觉自己欠点什么，说不出来的感觉。大概是基础比较薄弱吧。 回想起自己曾经从采矿专业转到软件开发，培训的时间，那时候是为了什么？那时候不是对软件的兴趣吗？ 初心呢？初衷呢？ 自己也渴望成为一个大牛，无论是架构师、全栈还是资深大咖。 阅读与写作曾说写作可以平复心态，阅读可以使人优秀。 是的呀，无论是阅读或者写作，都可以使人思考。 思考是什么、为什么、怎么办。 提升任何的辉煌与成功都是一步步慢慢实现的。 越努力越幸运。 创造性的思维也是必须的。 时刻牢记你想要的是什么。 结语兄台，就和我一起干了这碗鸡汤吧！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList、LinkedList和Vector分析]]></title>
    <url>%2Fblog%2FArrayList%E5%92%8CVector%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[前言List家族常用的类有3个，ArrayList、LinkedList、Vector。 ArrayList和Vector的底层是基于数组实现的，LinkedList的底层是基于链表实现的。 三者的比较如下： ArrayList和LinkedList不是线程安全的，Vector是线程安全的。 对于随机访问（get和set），ArrayList的性能要优于LinkedList。 对于add和remove操作，LinkedList和ArrayList性能差距不是很大。 三者均实现了Collection接口。 分析UML图三者与其他类的继承实现关系UML图如下。 ArrayList： LinkedList： Vector： 源码分析核心实现ArrayList是基于数组实现。 12transient Object[] elementData;private int size; LinkedList是基于链表实现。 123transient int size = 0;transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; Vector是基于数组实现。 123protected Object[] elementData;protected int elementCount;protected int capacityIncrement; 核心参数ArrayList 可以设置初始大小（由于数组实现的原因），不设置默认为10。 LinkedList不需要设置参数（由于使用链表实现，无界）。 Vector不仅可以设置初始大小，还可以设置容量增幅。 1234567891011121314151617181920212223//ArrayList传参构造函数 public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125; &#125; //LinkedList构造函数 public LinkedList() &#123; &#125; //Vector传参构造函数 public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement; &#125; 关于初始化大小和扩容机制下面讲。 核心方法我们主要分析get、set、add、remove这几个方法。对于ArrayList和Vector，还要分析扩容方法。 get、set方法ArrayList get，set方法： 12345678910public E get(int index) &#123; Objects.checkIndex(index, size); return elementData(index);&#125;public E set(int index, E element) &#123; Objects.checkIndex(index, size); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; Vector get，set方法： 1234567891011121314public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125;public synchronized E set(int index, E element) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 二者的逻辑一样，get方法判断是否下标越界，不越界返回index下的数值。set方法判断是否越界，不越界将新值放到指定下标上。它俩的区别在与synchronized关键字，正好说明了Vector是线程安全的。 LinkedList get，set方法： 1234567891011121314151617181920212223242526public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;public E set(int index, E element) &#123; checkElementIndex(index); Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 可以看到LinkedList的get方法会先检查是否越界，不越界返回指定下标node的item值。set方法也是先检查越界情况，不越界将该点的node的item赋为新值。取node指定位置上的值时要循环遍历，所以对于随机的get，set，ArrayList的性能要优于LinkedList的。 add、remove方法ArrayList add、remove方法： 1234567891011121314151617181920212223242526272829303132333435363738public boolean add(E e) &#123; modCount++; add(e, elementData, size); return true;&#125;private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) elementData = grow(); elementData[s] = e; size = s + 1;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); System.arraycopy(elementData, index, elementData, index + 1, s - index); elementData[index] = element; size = s + 1;&#125;public E remove(int index) &#123; Objects.checkIndex(index, size); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; Vector add、remove方法： 12345678910111213141516171819202122232425public synchronized boolean add(E e) &#123; modCount++; add(e, elementData, elementCount); return true;&#125;private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) elementData = grow(); elementData[s] = e; elementCount = s + 1;&#125;public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue;&#125; 它们里面有add、remove方法不止一个，我们只拿一个来举例。 可以看到，Vector和ArrayList十分相近了，除了synchronized关键字。 add方法当elementData.length和elementCount相等时（容量满），会执行扩容操作，并将元素放到指定位置。 remove方法先判断下标是否越界，不越界会删除指定位置的元素，并且将数组重新拷贝合并。 同时它们有一个计数器modCount，在HashMap那边已经讲过，是用来fast-fail的，当多个线程同时操作，modCount不一致，就会抛出异常。 LinkedList的add、remove方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125;E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 可以看到，LinkedList的add方法开始也会校验指针位置，然后如果在末尾，就在链表最后面添加节点，否则就插入到链表指定位置上。 remove方法校验指针位置后，会删除指定位置上的node。 上面可以看到，对于add和remove，ArrayList数组要进行扩容或者删除部分长度，执行Sysetm.arraycopy方法，这是要消耗一些性能的，对于LinkedList，不需要维护容量问题，但是每次新增或者删除时，都会创建或删除一个Node对象，也是要消耗一些性能的。 扩容方法对于ArrayList或者Vector，扩容方法如下： ArrayList 扩容方法：1234567891011121314151617181920212223private static final int DEFAULT_CAPACITY = 10; private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; Vector扩容方法： 12345678910111213141516171819202122232425public Vector() &#123; this(10); &#125; private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt;= 0) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 上面代码可以看到，对于ArrayList，如果不传入初始容量，默认为10。容量达到最值，执行扩容，每次扩容 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 默认原容量的1.5倍。 Vector，如果不传入初始容量和自增容量，默认初始容量也为10.扩容时执行int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ?capacityIncrement : oldCapacity); 默认为原容量的2倍。 两者的最大值容量均为Integer.MAX_VALUE. LinkedList由于是链表实现，没有容量限制。无需扩容。 代码我们从代码的角度比较下ArrayList和LinkedList，Vector。 我们构建一个有200W数据的ArrayList和LinkedList。 1234567891011121314151617181920212223242526public static void main(String[] args)&#123; List&lt;Integer&gt; list = new ArrayList(); //List&lt;Integer&gt; list = new LinkedList&lt;Integer&gt;(); //Vector&lt;Integer&gt; list=new Vector&lt;&gt;(); for (int i = 0; i &lt; 2000000; i++) &#123; list.add(i); &#125; Integer tmp; long start=System.currentTimeMillis() ; //ForEach for(Integer s:list)&#123; tmp=s; &#125; System.out.println("foreach spend:"+(System.currentTimeMillis()-start)); start = System.currentTimeMillis(); for(Iterator&lt;Integer&gt; it = list.iterator(); it.hasNext();)&#123; tmp=it.next(); &#125; System.out.println("Iterator spend;"+(System.currentTimeMillis()-start)); start=System.currentTimeMillis(); int size=list.size(); for(int i=0;i&lt;size;i++)&#123; tmp=list.get(i); &#125; System.out.println("for spend;"+(System.currentTimeMillis()-start));&#125; 某一次的结果： 数组 forEach遍历 Iterator遍历 for遍历 ArrayList 9ms 9ms 12ms LinkedList 17ms 16ms ??? Vector 44ms 55ms 41ms 由于for循环遍历是随机访问，故LinkedList在数据量很大的情况下时间消耗会很长，基本不能接受。由于Vector线程安全，synchronized，故其整体效率会比ArrayList低些。在实际开发中，应用的ArrayList还是比较多的。 结语以上就是对ArrayList、LinkedList、Vector的全部分析。对于不同的应用场景，合理的选择List的类型也是至关重要的。三种List都是比较基础的知识，应当学习和掌握。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StringBuffer和StringBuilder]]></title>
    <url>%2Fblog%2FStringBuffer%E5%92%8CStringBuilder.html</url>
    <content type="text"><![CDATA[前言我们知道，对于一般大量频繁的String操作，我们不建议也不应该直接用String进行相加操作，而我们应借助StringBuffer或者StringBuilder来实现。 StringBuffer是线程安全的，而StringBuilder是线程不安全的。 由此看来，StringBuilder对String的操作快，不安全，适合单线程；StringBuilder对String的操作较StringBuilder慢，安全，适合多线程和单线程。 我们今天分析一下二者的源码。 分析class定义两者的class定义。 1234567891011//StringBuffer public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125;//StringBuilderpublic final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125; 二者的UML图如下： 可以看到二者均继承AbstractStringBuilder类，且都实现了Serializable和CharSequence接口。即二者分别是AbstractStringBuilder类的安全和不安全的一种实现。 构造器我们先来分析下StringBuffer。 12345678910public StringBuffer() &#123; super(16);&#125;public StringBuffer(int capacity) &#123; super(capacity);&#125;public StringBuffer(String str) &#123; super(str.length() + 16); append(str);&#125; 可以看到，当我们new StringBuffer时，如果什么也不传，默认赋予16数组长度，如果传入一个String，则长度为String.length()+16。 append方法在看一下append方法。关键字synchronized 对该方法进行了加锁，保证安全，toStringCache 赋值为空。然后调用AbstractStringBuilder的append方法。 StringBuffer append方法。123456@Overridepublic synchronized StringBuffer append(String str) &#123; toStringCache = null; super.append(str); return this;&#125; AbstractStringBuilder里的方法。 123456789101112131415161718192021222324252627public AbstractStringBuilder append(String str) &#123; if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this;&#125;private AbstractStringBuilder appendNull() &#123; int c = count; ensureCapacityInternal(c + 4); final char[] value = this.value; value[c++] = 'n'; value[c++] = 'u'; value[c++] = 'l'; value[c++] = 'l'; count = c; return this;&#125;private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) &#123; value = Arrays.copyOf(value, newCapacity(minimumCapacity)); &#125;&#125; String里的getChars方法。 123456789101112public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) &#123; if (srcBegin &lt; 0) &#123; throw new StringIndexOutOfBoundsException(srcBegin); &#125; if (srcEnd &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(srcEnd); &#125; if (srcBegin &gt; srcEnd) &#123; throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); &#125; System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin);&#125; 对于这段代码的理解。追加一个Str时，如果是null，则调用appendNull方法，在后面直接加一个null字符串。如果不为空，拿到字符串长度，进行容量扩容为当前容量+str的长度，调用String的getChars方法，将字符串数组加在后面，这最后是个char数组。 通过上图可以看到 AbstractStringBuilder是基于char数组实现的，count用于统计当前长度。 toString方法我们看一下toString方法。可以发现他把字符串数组先放到了缓存数组，然后在返回一个String。当StringBuffer变化时，如append，则直接把toStringCache 赋值为空。 1234567@Overridepublic synchronized String toString() &#123; if (toStringCache == null) &#123; toStringCache = Arrays.copyOfRange(value, 0, count); &#125; return new String(toStringCache, true);&#125; insert方法我们再来分析一下StringBuffer的insert，由于重载方法较多，我们只分析insert String的代码。 StringBuffer insert方法。123456@Overridepublic synchronized StringBuffer insert(int offset, String str) &#123; toStringCache = null; super.insert(offset, str); return this;&#125; AbstractStringBuilder里的insert方法。 123456789101112public AbstractStringBuilder insert(int offset, String str) &#123; if ((offset &lt; 0) || (offset &gt; length())) throw new StringIndexOutOfBoundsException(offset); if (str == null) str = "null"; int len = str.length(); ensureCapacityInternal(count + len); System.arraycopy(value, offset, value, offset + len, count - offset); str.getChars(value, offset); count += len; return this;&#125; 可以看到与append大致相同，就是调用System.arraycopy的时候插入的位置发生了变化。 其他方法（delete，replace等）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//StringBuffer @Override public synchronized StringBuffer delete(int start, int end) &#123; toStringCache = null; super.delete(start, end); return this; &#125; //AbstractStringBuilder public AbstractStringBuilder delete(int start, int end) &#123; if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (end &gt; count) end = count; if (start &gt; end) throw new StringIndexOutOfBoundsException(); int len = end - start; if (len &gt; 0) &#123; System.arraycopy(value, start+len, value, start, count-end); count -= len; &#125; return this; &#125; //StringBuffer @Override public synchronized StringBuffer replace(int start, int end, String str) &#123; toStringCache = null; super.replace(start, end, str); return this; &#125; //AbstractStringBuilder public AbstractStringBuilder replace(int start, int end, String str) &#123; if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (start &gt; count) throw new StringIndexOutOfBoundsException("start &gt; length()"); if (start &gt; end) throw new StringIndexOutOfBoundsException("start &gt; end"); if (end &gt; count) end = count; int len = str.length(); int newCount = count + len - (end - start); ensureCapacityInternal(newCount); System.arraycopy(value, end, value, start + len, count - end); str.getChars(value, start); count = newCount; return this; &#125; 可以看到他们均使用了System.arraycopy方法。这儿不再一一赘述。 其他我们再看一下StringBuilder的源码。发现它与StringBuffer差别很小。 不同点： 增删改操作上没有synchronized关键字。 没有private transient char[] toStringCache;的定义。 第一点正好验证了StringBuilder不是线程安全的，第二点StringBuffer中toStringCache的引入是为了在多线程并发下读取写入数据起到一定的缓存缓冲作用。 结论我们可以看到，无论StringBuilder还是StringBuffer，都是AbstractStringBuilder的实现类。 AbstractStringBuilder对字符串的操作，实质是将它存储在一个char数组中，这样减小了内存开销。我们知道，如果使用String连加，会创造大量String对象，GC来不及回收，导致OOM异常或内存开销增大。StringBuffer和StringBuilder对String的操作完美的解决了这个问题，且相当于提供了操作字符串更加直观的方法（如insert，delete，append等）。这是值得我们借鉴和学习的。 以上就是StringBuffer和StringBuilder的源码分析。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer源码解析]]></title>
    <url>%2Fblog%2FInteger%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[前言今天我们来分析一下Integer源码。 Integer是八种包装类里面的比较常用的一种。那在使用时有什么注意及学习的地方呢？ 让我们一起来看一下 分析Integer是包java.lang下的一个类。 123public final class Integer extends Number implements Comparable&lt;Integer&gt; &#123;//code&#125; 其被定义成final类型，继承Number类实现Comparable接口。 123@Native public static final int MIN_VALUE = 0x80000000;@Native public static final int MAX_VALUE = 0x7fffffff;public static final Class&lt;Integer&gt; TYPE = (Class&lt;Integer&gt;) Class.getPrimitiveClass("int"); 可以看出，其定义了Integer的最大值为2^31-1，最小值为-2^31。Integer的基本数据类型为int。 我们来看一下Integer的toString方法，是比较有趣的。 12345678public static String toString(int i) &#123; if (i == Integer.MIN_VALUE) return "-2147483648"; int size = (i &lt; 0) ? stringSize(-i) + 1 : stringSize(i); char[] buf = new char[size]; getChars(i, size, buf); return new String(buf, true);&#125; 方法中使用到了stringSize函数，就是求这个Integer数的长度，我们来看看他是如何实现的。 123456789final static int [] sizeTable = &#123; 9, 99, 999, 9999, 99999, 999999, 9999999, 99999999, 999999999, Integer.MAX_VALUE &#125;;// Requires positive xstatic int stringSize(int x) &#123; for (int i=0; ; i++) if (x &lt;= sizeTable[i]) return i+1;&#125; 可以看到这段代码在计算Integer数长度时，构建了一个一维数组，然后拿x与数组每个值进行比较。而未使用我们经常说的除法或乘法计算长度。我们可以看下源码里的注释。 1234567891011121314151617// I use the "invariant division by multiplication" trick to// accelerate Integer.toString. In particular we want to// avoid division by 10.//// The "trick" has roughly the same performance characteristics// as the "classic" Integer.toString code on a non-JIT VM.// The trick avoids .rem and .div calls but has a longer code// path and is thus dominated by dispatch overhead. In the// JIT case the dispatch overhead doesn't exist and the// "trick" is considerably faster than the classic code.//// TODO-FIXME: convert (x * 52429) into the equiv shift-add// sequence.//// RE: Division by Invariant Integers using Multiplication// T Gralund, P Montgomery// ACM PLDI 1994 我们知道计算机在计算除法效率要比加减乘法低。所以为了避免除法，提高计算效率，采用此种方法。 正好我们可以看看Long的toString方法里的stringSize方法。 12345678910// Requires positive xstatic int stringSize(long x) &#123; long p = 10; for (int i=1; i&lt;19; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 19;&#125; 可以看到使用了乘法。你或许会问为什么没有像Integer那样构建一个数组去比较？额，如果要构造数组，那要构造一个19位的数组，里面有1-19位的数，代码写起来很多很臃肿吧，而且构造好的数组会长期放在内存中，我们知道，在实际应用中，Integer的使用频率要比Long高多了，长期让Long里面的一个数组占据内存空间也不太合理。以上是我个人见解。 我们再来看一下parseInt方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public static int parseInt(String s, int radix) throws NumberFormatException&#123; /* * WARNING: This method may be invoked early during VM initialization * before IntegerCache is initialized. Care must be taken to not use * the valueOf method. */ if (s == null) &#123; throw new NumberFormatException("null"); &#125; if (radix &lt; Character.MIN_RADIX) &#123; throw new NumberFormatException("radix " + radix + " less than Character.MIN_RADIX"); &#125; if (radix &gt; Character.MAX_RADIX) &#123; throw new NumberFormatException("radix " + radix + " greater than Character.MAX_RADIX"); &#125; int result = 0; boolean negative = false; int i = 0, len = s.length(); int limit = -Integer.MAX_VALUE; int multmin; int digit; if (len &gt; 0) &#123; char firstChar = s.charAt(0); if (firstChar &lt; '0') &#123; // Possible leading "+" or "-" if (firstChar == '-') &#123; negative = true; limit = Integer.MIN_VALUE; &#125; else if (firstChar != '+') throw NumberFormatException.forInputString(s); if (len == 1) // Cannot have lone "+" or "-" throw NumberFormatException.forInputString(s); i++; &#125; multmin = limit / radix; while (i &lt; len) &#123; // Accumulating negatively avoids surprises near MAX_VALUE digit = Character.digit(s.charAt(i++),radix); if (digit &lt; 0) &#123; throw NumberFormatException.forInputString(s); &#125; if (result &lt; multmin) &#123; throw NumberFormatException.forInputString(s); &#125; result *= radix; if (result &lt; limit + digit) &#123; throw NumberFormatException.forInputString(s); &#125; result -= digit; &#125; &#125; else &#123; throw NumberFormatException.forInputString(s); &#125; return negative ? result : -result;&#125; 我们可以看到此方法首先进行异常处理，然后判断传入String是否有正负号，然后截取位数，使用乘法，用减法得到int值，然后判断正负并返回结果。 我们再来看下Integer的内部类IntegerCache。 这儿可以说是一个坑，也是比较有意思的地方。 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 很容易理解这段代码，初始化Integer后，IntegerCache会缓存[-128,127]之间的数据，这个区间的上限可以配置，取决于java.lang.Integer.IntegerCache.high这个属性，这个属性在VM参数里为-XX:AutoBoxCacheMax=2000进行设置调整或者VM里设置-Djava.lang.Integer.IntegerCache.high=2000。所以Integer在初始化完成后会缓存[-128,max]之间的数据。 并且我们可以看到valueOf方法。 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 可以看到valueOf方法，在cache范围内，返回的是缓存的值，是相同的对象，不在cache范围内，才会新建Integer。 由于有了Integer缓存，我们可以测试以下代码。 12345678Integer a=1;Integer b=1;Integer c=new Integer(1);Integer d=1000;Integer e=1000;System.out.println(a==b);System.out.println(b==c);System.out.println(d==e); 可以看到结果为true,false,false。 这样，我们在比较Integer时，如果仅仅比较值相等，建议使用equals方法比较。 我们可以看下Integer的equals方法。 123456public boolean equals(Object obj) &#123; if (obj instanceof Integer) &#123; return value == ((Integer)obj).intValue(); &#125; return false;&#125; 可以看到它会先判断类型是否符合，然后进行拆箱比较操作。 同样，在Long，Byte，Short，我们也可以看到缓存，其缓存数据长度均是-128到127。 123456789101112131415161718192021222324252627282930313233//Long private static class LongCache &#123; private LongCache()&#123;&#125; static final Long cache[] = new Long[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Long(i - 128); &#125; &#125; //Byte private static class ByteCache &#123; private ByteCache()&#123;&#125; static final Byte cache[] = new Byte[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Byte((byte)(i - 128)); &#125; &#125; //Short private static class ShortCache &#123; private ShortCache()&#123;&#125; static final Short cache[] = new Short[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Short((short)(i - 128)); &#125; &#125; 关于缓存的意义：在该范围内数据比较常用，添加缓存提高性能。不用每次都新建，浪费系统资源。 同时根据Integer的hashCode方法，我们可以看到，Integer的hashCode返回本身的int值。 1234@Overridepublic int hashCode() &#123; return Integer.hashCode(value);&#125; 结论以上就是Integer的源码分析，可以看到，对于偏底层的一些调用频繁的类，Java都做了很多方面的优化。包括从性能及内存开销等诸多方面。是值得我们学习和理解的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 数据结构之链表]]></title>
    <url>%2Fblog%2FJava-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[前言链表是一种物理存储单元上非连续、非顺序的存储结构，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储数据元素的数据域，另一个是存储下一个结点地址的指针域。 由于链表不必按照顺序存储，故在插入数据时可以达到O(1)的复杂度，但是查找的时候就需要遍历，时间复杂度为O(n)。 分类链表根据实现方式一般有三种分类：单向链表、循环链表、双向链表。 单向链表单向链表指的是链表中的元素的指向只能指向链表中的下一个元素或者为空，元素之间不能相互指向。也就是一种线性链表。 图示： 普通单向链表用Java代码实现一普通的单向链表。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class SingleLinkList &#123; private int size;//链表节点的个数 private Node head;//头节点 public SingleLinkList()&#123; size = 0; head = null; &#125; //链表的每个节点类 private class Node&#123; private Object data;//每个节点的数据 private Node next;//每个节点指向下一个节点的连接 public Node(Object data)&#123; this.data = data; &#125; &#125; //在链表头添加元素 public Object addHead(Object obj)&#123; Node newHead = new Node(obj); if(size == 0)&#123; head = newHead; &#125;else&#123; newHead.next = head; head = newHead; &#125; size++; return obj; &#125; //在链表头删除元素 public Object deleteHead()&#123; Object obj = head.data; head = head.next; size--; return obj; &#125; //查找指定元素，找到了返回节点Node，找不到返回null public Node find(Object obj)&#123; Node current = head; int tempSize = size; while(tempSize &gt; 0)&#123; if(obj.equals(current.data))&#123; return current; &#125;else&#123; current = current.next; &#125; tempSize--; &#125; return null; &#125; //删除指定的元素，删除成功返回true public boolean delete(Object value)&#123; if(size == 0)&#123; return false; &#125; Node current = head; Node previous = head; while(current.data != value)&#123; if(current.next == null)&#123; return false; &#125;else&#123; previous = current; current = current.next; &#125; &#125; //如果删除的节点是第一个节点 if(current == head)&#123; head = current.next; size--; &#125;else&#123;//删除的节点不是第一个节点 previous.next = current.next; size--; &#125; return true; &#125; //判断链表是否为空 public boolean isEmpty()&#123; return (size == 0); &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 栈具有先进后出的原则，所以单向链表可以用来实现栈。Java代码如下： 12345678910111213141516171819202122232425262728293031public class StackSingleLinkList &#123; public class StackSingleLink &#123; private SingleLinkList link; public StackSingleLink()&#123; link = new SingleLinkList(); &#125; //添加元素 public void push(Object obj)&#123; link.addHead(obj); &#125; //移除栈顶元素 public Object pop()&#123; Object obj = link.deleteHead(); return obj; &#125; //判断是否为空 public boolean isEmpty()&#123; return link.isEmpty(); &#125; //打印栈内元素信息 public String display()&#123; return link.display(); &#125; &#125;&#125; 我们可以看出，如果对链表的最后一个元素进行操作，需要遍历到链表尾部，在进行操作，十分消耗资源。 双端链表还有一种单向链表称为双端链表。这种链表有一个特点，即在链表内添加了对链表尾部的引用。这使得链表可以方便的操作尾部元素。 Java代码实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class DoublePointLinkList &#123; private Node head;//头节点 private Node tail;//尾节点 private int size;//节点的个数 private class Node&#123; private Object data; private Node next; public Node(Object data)&#123; this.data = data; &#125; &#125; public DoublePointLinkList()&#123; size = 0; head = null; tail = null; &#125; //链表头新增节点 public void addHead(Object data)&#123; Node node = new Node(data); if(size == 0)&#123;//如果链表为空，那么头节点和尾节点都是该新增节点 head = node; tail = node; size++; &#125;else&#123; node.next = head; head = node; size++; &#125; &#125; //链表尾新增节点 public void addTail(Object data)&#123; Node node = new Node(data); if(size == 0)&#123;//如果链表为空，那么头节点和尾节点都是该新增节点 head = node; tail = node; size++; &#125;else&#123; tail.next = node; tail = node; size++; &#125; &#125; //删除头部节点，成功返回true，失败返回false public boolean deleteHead()&#123; if(size == 0)&#123;//当前链表节点数为0 return false; &#125; if(head.next == null)&#123;//当前链表节点数为1 head = null; tail = null; &#125;else&#123; head = head.next; &#125; size--; return true; &#125; //判断是否为空 public boolean isEmpty()&#123; return (size ==0); &#125; //获得链表的节点个数 public int getSize()&#123; return size; &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 双端链表可以用来实现队列，相关实现如下： 123456789101112131415161718192021222324252627public class QueueLinkList &#123; private DoublePointLinkList dp; public QueueLinkList()&#123; dp = new DoublePointLinkList(); &#125; public void insert(Object data)&#123; dp.addTail(data); &#125; public void delete()&#123; dp.deleteHead(); &#125; public boolean isEmpty()&#123; return dp.isEmpty(); &#125; public int getSize()&#123; return dp.getSize(); &#125; public String display()&#123; return dp.display(); &#125;&#125; 有序链表上面所说的单链表数据都是无序的，我们可以构建一个有序的单向链表。即有序链表。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class OrderLinkList &#123; private Node head; private int size; private class Node&#123; private int data; private Node next; public Node(int data)&#123; this.data = data; &#125; &#125; public OrderLinkList()&#123; size=0; head = null; &#125; //插入节点，并按照从小打到的顺序排列 public void insert(int value)&#123; Node node = new Node(value); Node pre = null; Node current = head; while(current != null &amp;&amp; value &gt; current.data)&#123; pre = current; current = current.next; &#125; if(pre == null)&#123; head = node; head.next = current; &#125;else&#123; pre.next = node; node.next = current; &#125; size++; &#125; //删除头节点 public void deleteHead()&#123; head = head.next; size--; &#125; //判断是否为空 public boolean isEmpty()&#123; return (size ==0); &#125; //获取长度 public int getSize() &#123; return size; &#125; public String display()&#123; StringBuilder sb=new StringBuilder(); Node current = head; while(current != null)&#123; sb.append(current.data+" "); current = current.next; &#125; return sb.toString(); &#125;&#125; 对于有序链表，可以看出，插入或删除某一项最多需要O(n)的时间复杂度（遍历），但如果我们每次只删除最小值，且对插入没有过高要求的话，有序链表是一个不错的选择，比如优先级队列就可以利用有序链表实现。 比如我们插入int数并以最小值为优先级，每次取最小的int值的队列。 123456789101112131415161718192021222324252627public class QueueOrderLinkList &#123; private OrderLinkList dp; public QueueOrderLinkList()&#123; dp = new OrderLinkList(); &#125; public void insert(int data)&#123; dp.insert(data); &#125; public void delete()&#123; dp.deleteHead(); &#125; public int getSize() &#123; return dp.getSize(); &#125; public boolean isEmpty() &#123; return dp.isEmpty(); &#125; public String display()&#123; return dp.display(); &#125;&#125; 单向链表的用途可以说是十分广泛的。 双向链表双向链表即是这样一个有序的结点序列，每个链表元素既有指向下一个元素的指针，又有指向前一个元素的指针，其中每个结点都有两种指针，即left和right。left指针指向左边结点，right指针指向右边结点。所以双向链表是可以从两个方向进行遍历的。 图示： 双向链表的Java实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class DoubleWayLinkList &#123; private Node head;//表示链表头 private Node tail;//表示链表尾 private int size;//表示链表的节点个数 private class Node&#123; private Object data; private Node next; private Node prev; public Node(Object data)&#123; this.data = data; &#125; &#125; public DoubleWayLinkList()&#123; size = 0; head = null; tail = null; &#125; //在链表头增加节点 public void addHead(Object value)&#123; Node newNode = new Node(value); if(size == 0)&#123; head = newNode; tail = newNode; size++; &#125;else&#123; head.prev = newNode; newNode.next = head; head = newNode; size++; &#125; &#125; //在链表尾增加节点 public void addTail(Object value)&#123; Node newNode = new Node(value); if(size == 0)&#123; head = newNode; tail = newNode; size++; &#125;else&#123; newNode.prev = tail; tail.next = newNode; tail = newNode; size++; &#125; &#125; //删除链表头 public Node deleteHead()&#123; Node temp = head; if(size != 0)&#123; head = head.next; head.prev = null; size--; &#125; return temp; &#125; //删除链表尾 public Node deleteTail()&#123; Node temp = tail; if(size != 0)&#123; tail = tail.prev; tail.next = null; size--; &#125; return temp; &#125; //获得链表的节点个数 public int getSize()&#123; return size; &#125; //判断链表是否为空 public boolean isEmpty()&#123; return (size == 0); &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 使用双向链表可以构建双端队列。在这儿就不上代码了，和之前的队列构造类似。 循环链表循环链表指的是在单向链表和双向链表的基础上，将两种链表的最后一个结点指向第一个结点从而实现循环。 图示： 循环链表的Java实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class CircleLinkList&lt;T&gt; &#123; // 链表的每个节点类 private class Node&lt;T&gt; &#123; private Object data;// 每个节点的数据 private Node&lt;T&gt; next;// 每个节点指向下一个节点的连接 public Node(Object data) &#123; this.data = data; &#125; &#125; Node&lt;T&gt; head, tail; Node&lt;T&gt; p; int size = 0; public CircleLinkList() &#123; this.head = null; tail = head; p = head; &#125; public int length() &#123; return size; &#125; /** * 添加节点 * * @param data */ public void add(T data) &#123; Node node = new Node&lt;T&gt;(data); if (head == null) &#123; head = node; tail = head; p = head; size++; &#125; else &#123; node.next = head; head = node; tail.next = head; p = head; size++; &#125; &#125; /** * 得到数据 * * @param index * @return */ public T get(int index) &#123; int i = 0; p = head; while (i != index &amp;&amp; p != tail) &#123; i++; p = p.next; &#125; return (T) p.data; &#125; /** * @return */ public boolean isEmpty() &#123; if (head != null) return false; else return true; &#125;&#125; 同样，使用循环链表可以实现循环队列。 总结链表作为数据结构的一部分，应用是十分广泛的，我们上面说明了几种链表在不同情况下的应用，链表是我们应当学会掌握和使用的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一款基于JQuery和bootstrap的单页面WebApp框架]]></title>
    <url>%2Fblog%2F%E4%B8%80%E6%AC%BE%E5%9F%BA%E4%BA%8EJQuery%E5%92%8Cbootstrap%E7%9A%84%E5%8D%95%E9%A1%B5%E9%9D%A2WebApp%E6%A1%86%E6%9E%B6.html</url>
    <content type="text"><![CDATA[前言现在React.js，Vue.js等MVVM框架逐渐火了起来，今天，先暂时不说这些。 我在之前呆过一家公司，他们前端封装了一套基于JQuery和bootstrap的单页面WebApp框架，虽然技术栈方面可能比较落后了，且是基于CSS，Html，JavaScript分层的。与现在的模块化开发有些不同。但是它的一些封装思想是值得我们学习的。 而且这个框架，让我这个前端小白（我是后端开发）对前端JS产生了很大兴趣，特此写下这篇文章来分析和纪念这个框架。 框架主页：https://github.com/JavaZWT/JQueryApp 简介package.json这款框架也是基于Node.js搭建的，主要是用了gulp，还包含了css,html,js压缩，js校验等包。其环境启动包依赖如下图： 在项目的package.json里可以看到。 关于为什么引用这些插件的说明： js，img，png等的压缩，可大幅度减小生成的资源包体积，在生成APK后用户当然是希望安装包要小越好。如果gulp到服务器WebRoot目录下，当用户访问项目资源时，体积小的静态资源更能减小用户流量开支，提高系统响应速度，增强用户体验。 创建一个前端webserver服务器，相当于提供了一个前端APK环境（可以这么理解），大部分bug在浏览器上就可以复现和解决，减少了打包到apk里的繁琐流程，提高开发效率。 根据以上介绍，大家应该知道node_modules,gulpfile.js,package.json,package-lock.json都不会被打到前台资源包中。 package.json里的依赖，都可以通过npm install （同级目录执行）的方式安装，安装后就会生成node_modules文件夹。 安装启动前端环境主要有以下三个步骤。 npm install -g gulp 安装全局gulp，使gulp可以在任何目录下使用，和java环境变量差不多。 npm install 安装环境依赖，需要到有package.json的文件夹下执行。 gulp 启动前端webserver gulpfile.js再来说下gulpfile.js 光有插件也不行啊，这个文件就是把插件们拿来用的。 关于这个js，我们简单说下配置及使用。 参数配置envConfig env：启动模式，有三种，DEV，DEBUG，PRO。DEV和DEBUG模式下不会压缩js和图片，节省时间，便于调试。 distPath: 生成的前端资源文件路径，有的gulpfile.js里面没有它，其他是分开的，那个可以生成发布在webapp目录下的资源文件。 webserverConfig 服务器启动配置参数 port ：默认打开的端口号 open：默认代开的文件名 host：默认打开的地址，不配置的话默认localhost livereload:即时刷新，主要用来检测文件变化。 项目架构图 架构说明： public 前端项目文件夹。 css 用于存放页面css样式的文件夹，理论上里面的文件都会被放入生成的前端资源包。 html 用于放置前端页面的文件夹，理论上里面的文件都会被放入生成的前端资源包。 js 用于放置前端js的文件夹，理论上里面的文件都会被放入生成的前端资源包。 node_modules npm install 后生成的文件夹，是前端资源包的打包环境及在Web浏览器上查看及debug App提供支持。 resources 用于放置前端静态资源的文件夹，如图片，字体，静态json等，理论上里面的文件都会被放入前端资源包。 gulpfile.js 构建前端环境，生成前端资源包的主要文件。 index.html 前端主要页面，所有资源理论上都会被按需加载进来。 package-lock.json npm install 之后生成的文件，存放着一些npm之后的包的信息。 package.json 构建前端环境，生成前端资源包的主要文件，里面放置着构建环境及生成资源包所需的依赖。 分析分析这个框架，我们先从index.html入手，单页面的操作实现主要是将目标html移到这个index.html里实现的。 可以看下index.html里面的几个主要id，如上图。 index.html引入了相关js及css。 我们看下api.js和app.js 项目启动后会调用到此方法，详见index.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function loadViewport()&#123; var path; if(location.hash.indexOf("?")&gt;-1)&#123; path = location.hash.substr(0,location.hash.indexOf("?")); &#125;else&#123; path = location.hash; &#125; path = (path || '#' + api.route.default.path).slice(1); //如果没有配置路径则不做操作 if(!api.route[path])&#123; api.globalLoading.hide(); console.log('[APP] 没有查询到路径:'+ path + ',不进行加载'); &#125;else if(api.curr.clean)&#123; switch(api.curr.clean.length)&#123; case 0://如果不包含参数 则认为这是一个同步 直接执行完成后 加载页面 api.curr.clean(); load(); break; case 1://如果只接收一个参数,则传入回调函数 等待页面完成后自行处理 api.curr.clean(load); break; case 2://如果接收两个参数 则分别是 要跳转页面的hash,回调函数 api.curr.clean(path,load); break; default://如果接收大于2个参数 则只传入三个 分别是 要跳转页面的hash,要跳转页面的配置在route中的数据,回调函数 api.curr.clean(path,api.route[path],load); &#125; &#125;else&#123; load(); &#125; function load()&#123; api.globalLoading.show(); console.log('[APP] 开始加载页面['+path+']'); api.loadPath(path,function(data)&#123; console.log('[APP] 加载页面&lt;p&gt;['+path+']&lt;/p&gt;完成'); api.plugins.header(data,path); //如果页面没有设置 则默认隐藏全局loading if(!data || !data.selfHideLoading) api.globalLoading.hide(); &#125;); &#125; &#125; api.loadViewport = loadViewport; /** * 当浏览器的hash发生变化时进行页面的加载 */ window.onhashchange = loadViewport; 其原理就是利用location.hash的？后半段，进行地址跳转，加载在route里配置的相关html及js，加载之前移除原来的html和js，以实现页面跳转。 方法及使用api.js1、route,modal 新增一个页面，可以添加在route里，新增一个模态框，可以添加在modal里。 2、api.globalLoading 全局loading，有两个方法: api.globalLoading.show(); api.globalLoading.hide(); 一般应用在ajax发送前后，当需要等待操作处理时，也可以直接调用。 3、api.globalShadow 全局shadow，是一个z-index为2000的透明遮罩层。 有两个方法： api.globalShadow.hide(); api.globalShadow.show(); 一般调用模态框时会被加载，自己写一些插件集成在里面时可以自己控制处理下。 4、api.modal.exec 调用模态框时会使用此方法，由于模态框和页面不太一样，模态框关闭后再打开可以保证数据不用重新加载。 5、api.loadJS 框架中加载js用到的方法，分为串行加载和并行加载，各有利弊。 6、api.loadModal modal加载方法，刚才提到的那个是这一个的进一步封装，这里面的方法显示了modal是如何被加载的，这里应注意，modal默认只会被加载一次。 7、api.loadPath 页面跳转实现的主要方法，这个方法制定跳转path后，会在route里寻找，若没有此页面，则不会跳转。 8、loadViewport 刚才提到了，这个可以控制页面hash路由跳转。也是页面跳转的主要方法。 app.js1、app.alert 消息提示框方法，只有一个确认键。 2、app.confirm 消息提示框，有确认和取消两个按钮。 3、app.ajax 基于JQuery ajax后自己封装的一套ajax，增加里请求头签名等等。 cordova.js自己封装的一个cordova工具，根据设备类型不同（Android，iOS）调用不同的cordova插件。 GlobalValue.js全局变量控制插件，通过sessionStorage存储变量。 总结还有很多部分没有介绍到，但框架的关键部分都介绍了，要想清楚明白，可以自己在框架上写个页面或者完善个插件来试试。加深对它的理解。 虽然在现在前端框架日新月异的今天，虚拟dom流行的今天，感觉这个框架已经过时了，但是思想永不过时，框架设计上的一些思想还是值得我们学习和借鉴的。 关于这个框架有什么问题欢迎留言，也可以直接在GitHub上提问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Stack类简介]]></title>
    <url>%2Fblog%2FJava-Stack%E7%B1%BB%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言栈（Stack）是限定只能在一段进行插入和删除操作的线性表。 进行插入和删除操作的一端称为“栈顶”（top），另一端称为“栈底”（bottom）。 栈的插入操作称为“入栈”(push)，栈的删除 操作称为“出栈”（pop）。 栈具有后进先出（LIFO），先进后出(FILO)的特性。 Stack类Java工具包下的Stack类继承于Vector，由此可见Stack底层是由数组实现的。 Stack和Collection的关系如下图： 我们来看下Stack的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package java.util;publicclass Stack&lt;E&gt; extends Vector&lt;E&gt; &#123; /** * 构造函数 */ public Stack() &#123; &#125; /** * 将一个元素压入栈顶 */ public E push(E item) &#123; addElement(item); return item; &#125; /** * 取出栈顶的一个元素，并删除 */ public synchronized E pop() &#123; E obj; int len = size(); obj = peek(); removeElementAt(len - 1); return obj; &#125; /** * 取出栈顶元素，不删除 */ public synchronized E peek() &#123; int len = size(); if (len == 0) throw new EmptyStackException(); return elementAt(len - 1); &#125; /** * 判断栈是不是空 */ public boolean empty() &#123; return size() == 0; &#125; /** * 查找元素在栈的位置，没有返回-1 */ public synchronized int search(Object o) &#123; int i = lastIndexOf(o); if (i &gt;= 0) &#123; return size() - i; &#125; return -1; &#125; private static final long serialVersionUID = 1224463164541339165L;&#125; 根据源码，可以发现Stack的方法调用了Vector类的方法，实现了线程安全。 我们主要看一下Vector里的下面三个方法： 12345678910111213141516171819202122232425262728293031323334//添加一个元素 public synchronized void addElement(E obj) &#123; modCount++; //确认容量，不够会扩容 ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = obj; &#125; //移除指定位置的元素 public synchronized void removeElementAt(int index) &#123; modCount++; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt;= " + elementCount); &#125; else if (index &lt; 0) &#123; throw new ArrayIndexOutOfBoundsException(index); &#125; int j = elementCount - index - 1; if (j &gt; 0) &#123; System.arraycopy(elementData, index + 1, elementData, index, j); &#125; elementCount--; //赋空，便于垃圾回收 elementData[elementCount] = null; &#125; //找出在指定位置的元素 public synchronized E elementAt(int index) &#123; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt;= " + elementCount); &#125; return elementData(index); &#125; 关联方法如下： 1234567891011121314151617181920212223242526272829// private void ensureCapacityHelper(int minCapacity) &#123; 如果长度超了就扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; //扩容方法 private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; //新的容量定义：如果有容量增量且大于0，取增量，否则执行2倍扩容 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); //扩容后容量比传入容量还小，就取传入容量。 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //新容量比数组最大长度还大 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //计算容量 newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); &#125; //计算容量 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 实践我们如何用数组实现自己的一个stack呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Stack &#123; //栈元素组 private Object[] members; //指针 private int size; //自定义初始栈长度 public Stack(int initCapacity) throws Exception&#123; if(initCapacity&lt;=0) &#123; throw new Exception(); &#125; this.members=new Object[initCapacity]; &#125; //默认栈长度为10 public Stack() &#123; this.members=new Object[10]; &#125; //元素入栈 public synchronized void push(Object o)&#123; ensureCapacity(size+1); members[size++]=o; &#125; //元素出栈 public synchronized Object pop() throws Exception&#123; if(size&lt;=0) &#123; throw new Exception(); &#125; return members[--size]; &#125; //查看栈顶元素 public synchronized Object peek() throws Exception&#123; if(size&lt;=0) &#123; throw new Exception(); &#125; return members[size-1]; &#125; //确认容量 private synchronized void ensureCapacity(int minCapacity) &#123; //size+1比数组长度要长，扩容 if(minCapacity-members.length&gt;0) &#123; int oldCapacity = members.length; Object oldMembers=members; //扩容到二倍 int newCapacity = 2 * oldCapacity ; //扩容后还不够或者超过int最大值，就直接赋值size+1 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; members=new Object[newCapacity]; //拷贝数组 System.arraycopy(oldMembers, 0, members, 0, size); oldMembers=null; &#125; &#125;&#125; 以上代码就是一个简易的Stack的实现方式。 代码见： https://github.com/JavaZWT/sakuratears 总结Stack类在编程过程中用到的不是很多，但是计算机栈内存机制遵循先进后出原则，学习Stack类，可以帮助我们加深对程序及数据结构的理解。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java正则表达式使用]]></title>
    <url>%2Fblog%2FJava%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言1.什么是正则表达式？ 正则表达式是一种可以用于模式匹配和替换的强有力的工具。 2.为什么要使用正则表达式？有什么优点缺点？ 正则表达式在参数校验、参数匹配方面可以发挥重大作用。 优点：更少的代码，更高的效率。 缺点：需要一定时间的学习及记忆，不然理解晦涩。 正则表达式符号及其含义下表是正则表达式的所有符号及含义。 常用的符号.表示任意字符\s空格字符(空格键, tab, 换行, 换页, 回车)\S非空格字符([^\s])\d 一个数字，(相当于[0-9] )\D一个非数字的字符，(相当于[^0-9] )\w一个单词字符(word character) (相当于 [a-zA-Z_0-9] )\W一个非单词的字符，[^\w]^一行的开始$一行的结尾\b一个单词的边界 \B一个非单词的边界\G前一个匹配的结束 []匹配方括号内的一个字符例如:[abc] 表示字符 a，b，c 中的任意一个(与 a|b|c 相同) [a-zA-Z]表示从a到z或A到Z当中的任意一个字符表示次数的符号*重复零次或更多次例如:a* 匹配零个或者多个 a+重复一次或更多次例如:a+ 匹配一个或者多个 a?重复零次或一次例如:a? 匹配零个或一个 a{n}重复n次例如:a{4}匹配4个a {n,}重复 n 次或更多次例如:a{4,} 匹配至少 4 个 a{n,m}重复n到m次例如:a{4,10} 匹配 4~10 个 a 正则表达式实例java 正则表达式通过 java.util.regex 包下的 Pattern 类与 Matcher 类实现: 正则表达式使用总共分三步: 构造一个模式. Pattern p=Pattern.compile(“[a-z]*”); 建造一个匹配器 Matcher m = p.matcher(str); 进行判断，得到结果 boolean b = m.matches(); Matcher 类提供三个匹配操作方法,三个方法均返回 boolean 类型,当匹配到 时返回 true,没匹配到则返回 false m.matches() matches()对整个字符串进行匹配,只有整个字符串都匹配了才返回 true m.lookingAt() lookingAt()对前面的字符串进行匹配,只有匹配到的字符串在最前面才返 回 true m.find() find()对字符串进行匹配,匹配到的字符串可以在任何位置 Matcher 类的其他方法: int groupcount() 返回此匹配器模式中的捕获组数。 String replaceAll(String replacement) 用给定的 replacement 全部替代匹配的 部分 String repalceFirst(String replacement) 用给定的 replacement 替代第一次匹 配的部分 appendReplacement(StringBuffer sb,String replacement) 根 据 模 式 用 replacement 替换相应内容,并将匹配的结果添加到 sb 当前位置之后 StringBuffer appendTail(StringBuffer sb) 将输入序列中匹配之后的末尾字串 添加到 sb 当前位置之后. group(n) 0 代表永远都是匹配整个表达式的字符串的那部分 n&lt;&gt;0 时代表 第 n 组匹配的部分 正则表达式的用法 字符匹配 1234Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(str); // 操作的字符串boolean b = m.matches(); //返回是否匹配的结果 System.out.println(b); 分割字符串 12Pattern pattern = Pattern.compile(expression); //正则表达式String[] strs = pattern.split(str); //操作字符串 得到返回的字符串数组 替换字符串 123Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串String s = m.replaceAll(str); //替换后的字符串 查找替换指定字符串 12345678910Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串 StringBuffer sb = new StringBuffer();int i = 0;while (m.find()) &#123; m.appendReplacement(sb, str); i++; //字符串出现次数&#125;m.appendTail(sb);//从截取点将后面的字符串接上 String s = sb.toString(); 查找输出字符串 1234567Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串while (m.find()) &#123; //m.start() 返回匹配到的子字符串在字符串中的索引位置. //m.end()返回匹配到的子字符串的最后一个字符在字符串中的索引位置. //m.group()返回匹配到的子字符串&#125; 常用的正则表达式 手机号码校验 12345 public static boolean isPhone(String str) &#123; Pattern p = Pattern.compile("^[1][3,5,7,8]+\\d&#123;9&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 邮箱地址校验 12345 public static boolean isEmail(String str) &#123; Pattern p = Pattern.compile("^[a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\\.[a-zA-Z0-9-]+)*\\.[a-zA-Z0-9]&#123;2,6&#125;$"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; IP地址校验 12345 public static boolean isIPv4(String str) &#123; Pattern p = Pattern.compile("\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 是不是汉字 12345 public static boolean isChinese(String str) &#123; Pattern p = Pattern.compile("^[\\u4e00-\\u9fa5]+$"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; QQ号校验 12345 public static boolean isQQ(String str)&#123; Pattern p = Pattern.compile("[1-9][0-9]&#123;4,13&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 身份证号校验 1234567891011public static boolean isIDNumber(String IDNumber) &#123; if (IDNumber == null || "".equals(IDNumber)) &#123; return false; &#125; // 定义判别用户身份证号的正则表达式（15位或者18位，最后一位可以为字母） String regularExpression = "(^[1-9]\\d&#123;5&#125;(18|19|20)\\d&#123;2&#125;((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d&#123;3&#125;[0-9Xx]$)|" + "(^[1-9]\\d&#123;5&#125;\\d&#123;2&#125;((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d&#123;3&#125;$)"; boolean matches = IDNumber.matches(regularExpression); //详细校验略 return matches;&#125; 邮编校验 12345 public static boolean isMail(String str) &#123; Pattern p = Pattern.compile("[1-9]\\d&#123;5&#125;(?!\\d)"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 其他 其他常用校验不在一一说明。 可以通过 https://github.com/JavaZWT/sakuratears 获取。 结语如果我们能够熟练掌握正则表达式，一定会对我们的开发提供很大的帮助。 正则表达式整体上相对于代码来说是比较晦涩的，用好它需要一定的功力及学习能力。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（2）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%EF%BC%882%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言关于之前的部分请参考我的《搭建自己的Hexo博客（1）》这篇文章。 创建Github账户我们把博客发布到Github上。 首先申请Github账号：https://github.com/ 新建一个Repository 红框内为项目名称，应写成 yourname.github.io 这种形式。 如我注册的用户名是JavaZWT，则项目名应写为 JavaZWT.github.io 新建好后，我们开始发布我们的博客。 博客发布进入到原来的blog文件夹。 我们要记住以下指令： 博客本地调试启动：hexo s 博客部署文件生成：hexo g 博客发布：hexo d 博客部署文件夹清空： hexo clean hexo g 后，要发布的博客文件会生成在public文件夹下。 hexo d 会把这个文件夹下的文件deploy到Github上。 _config.yml配置文件里，指明自己的Github地址上新建的yourname.github.io的项目。 12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:JavaZWT/JavaZWT.github.io.git 正常的发布流程： 新写了一篇博文 通过hexo s本地调试通过后 hexo clean 清空public文件夹及db文件 hexo g 重新生成部署文件 hexo d 发布到Github 当我们发布成功后，通过 https://yourname.github.io/ 便可以在外网访问自己的blog啦。 添加域名成功访问之后，我们觉得这个网址不好，想换个自己喜欢的域名怎么办？ 这里我使用了阿里云申请了域名。 首先登陆阿里云，没有账号请注册申请。 找到域名与网站（万网）菜单。 找到自己喜欢的域名进行购买。 得到域名后，我们需要配置域名解析。把域名指向我们的博客。 在域名解析列表里我们新增两条记录，如下图红框部分。 其中记录值可以写 https://yourname.github.io/ 相当于把这个域名指向了https://yourname.github.io/这个域名。 或者我们可以查到 https://yourname.github.io/ 这个域名的IP，直接将新的域名指向这个IP即可。 配置好后等一段时间，我们访问新的域名，就可以跳转到我们的博客啦。 其他优化添加音乐播放功能找到 /themes/next/layout/_custom/sidebar.swig 文件 登陆网易云音乐，对于一首歌，生成外链播放器。 将上述代码复制到sidebar.swig文件里。 生成如下图所示效果。 添加博客编写功能写博客时是不是很不方便？ 我们安装hexo-admin插件吧。这个插件可以是写博客更快捷方便。 npm install hexo-admin –save 安装好后，启动调试博客 hexo s 输入 http://localhost:4000/admin/ 是不是看到进入了博客管理菜单？ 通过这个插件可以方便的管理和书写博客。 压缩博客博客里有一些图片或者css，js过大怎么办，这样会导致博客加载缓慢。 我们在发布博客时可以先对其进行压缩。 在blog目录下新建gulpfile.js 有如下内容： 123456789101112131415161718192021222324252627282930313233343536373839404142var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');var imagemin = require('gulp-imagemin');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public/images 目录 imagesgulp.task('images', function() &#123; return gulp.src(['./public/**/*.png', './public/**/*.jpg', './public/**/*.gif', './public/**/*.svg']) .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html', 'minify-css', 'minify-js', 'images']); 同时在package.json里添加这几个包。 &quot;gulp-minify-css&quot;: &quot;^1.2.4&quot;, &quot;gulp-uglify&quot;: &quot;^3.0.0&quot;, &quot;gulp-htmlmin&quot;: &quot;^4.0.0&quot;, &quot;gulp-htmlclean&quot;: &quot;^2.7.22&quot;, &quot;gulp&quot;: &quot;^3.9.1&quot;, &quot;gulp-imagemin&quot;: &quot;^4.1.0&quot;, 添加完后执行 npm install。 或者直接一个个的 npm install 包名 –save 也是可以的。 弄好后，在hexo g 指令后进行 压缩操作，执行 gulp 命令，可对public文件夹下的资源进行压缩。 结语一个博客的基本搭建到这里基本就结束了，如果想提高性能及速度，可以自己申请个服务器，或者使用CDN进行加速，基本都涉及到一定的费用。 博客还有一些比较小的美化的地方就不一一说明，大家可以自己尝试，自己动手，把自己的博客搭建的更漂亮。 毕竟，搭建博客这件事我们应本着学习，兴趣，快乐的本质去出发，自己摸索自己研究一些比较好玩的东西，这样才是做这件事的意义。 附上我的博客地址： https://www.sakuratears.top/ 大家可以参考下，有什么问题可以随时联系我，一定会为大家解答。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（1）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[前言我的Hexo博客很早就建好了，但一直没时间把搭建博客的步骤和经验好好总结。今天，我把在搭建博客中注意到的点及经验总结下来。一方面与大家分享，另一方面也加深自己的记忆，总结经验。 环境安装Node.js及npm要搭建Hexo博客，首先需要安装Node.js环境，去Node.js官网下载Node并安装。 NodeJs官网：https://nodejs.org/ 在命令行界面输入node -v 查询版本号。 输入npm -v 可查询npm版本号。 安装Hexo使用npm指令安装hexo npm install hexo-cli -g 安装成功后输入hexo -v可查询hexo版本信息 博客初始化安装好hexo后，我们找到一个自己文件夹，通过指令进入该文件夹。 使用指令： ①hexo init blog ②cd blog ③npm install ④hexo s 会看到命令行输出如下信息 打开浏览器，输入地址便可以看到blog搭建成功了。 美化博客虽然blog搭建好了，但是总感觉不是很好看。 推荐一款hexo主题Next，其有很多种blog主题供我们选择。 安装Next主题首先cd到刚刚建好的blog目录下，执行命令： git clone https://github.com/iissnan/hexo-theme-next themes/next 如果没有安装git，那么安装下吧。 请去git官网下载git： https://git-scm.com/downloads 如果命令执行成功，在blog下的themes包下，我们可以看到next文件夹，那就是刚刚下好的next主题啦。themes包下那个landscape包，就是hexo自带的默认主题。 如果不成功，那么我们可以直接下载next主题，https://github.com/iissnan/hexo-theme-next/releases，下载source code，然后把文件解压，重命名为next放到themes包下就可以了啦。 完成后，在blog包下，有个_config.yml文件，打开它，里面配置为theme: next即可加载next主题。 Hexo的_config.yml文件上文提到的这个配置文件是Hexo的核心配置文件，我们来说下这个文件吧 请先对yml语法有一定了解，本文不做说明。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: SakuraTears的博客 //博客的标题subtitle: 越努力越幸运 //博客简介description: 越努力越幸运 //博客描述keywords: 'Java,努力,随笔,学习,SakuraTears,博客,生活' //博客关键字author: SakuraTears //博客作者language: zh-Hans //博客语言timezone: //时区avatar: /uploads/avatar.jpg //头像#google_analytics: your-analytics-id //谷歌统计，开启的话请输入自己的id#baidu_analytics: your-analytics-id //百度统计，开启的话请输入自己的id# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://www.sakuratears.top //自己的网站地址root: / //根目录permalink: blog/:title:year:month:day/ permalink_defaults:# Directorysource_dir: source //源码位置，请默认public_dir: public //生成的文件位置，请默认tag_dir: tags //标签文件位置，请默认archive_dir: archives //归档文件位置，请默认category_dir: categories //分类文件位置，请默认code_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new posts //新文章默认名字default_layout: post //默认样式titlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: //高亮 enable: true //启用 line_number: true //linenumber启用 auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 3 //每页三篇文章 order_by: -date //按日期排序 # Category &amp; Tagdefault_category: uncategorized //按默认策略分类category_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD //日期时间格式time_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10 //归档每页10篇文章pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/plugins: hexo-generate-feed## Themes: https://hexo.io/themes/theme: next //hexo主题# hexo-admin authentificationadmin: username: sakuratears password_hash: $2a$10$HhGu7w5fYH6e7wmb2tXta.FvJvGWlzosdUgImJxGhxU0K3YAkXIkm secret: what's your name?# Live2D //live2D插件，后面介绍## https://github.com/xiazeyu/live2d-widget.js## https://l2dwidget.js.org/docs/class/src/index.js~L2Dwidget.html#instance-method-initlive2d: model: scale: 1 hHeadPos: 0.5 vHeadPos: 0.618 display: superSample: 2 width: 150 height: 300 position: right hOffset: 0 vOffset: -20 mobile: show: false scale: 0.1 motion: true react: opacityDefault: 0.7 opacityOnHover: 0.2##local_search //搜索插件search: path: search.xml field: post format: html limit: 10000# Deployment //部署## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:JavaZWT/JavaZWT.github.io.git Next主题的_config.yml文件在next包下，也有一个_config.yml文件，这个文件是配置next主题的，也简单介绍下。 12345678910menu: //next主题菜单 home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap commonweal: /404/ || heartbeat support: /support/ || support 1scheme: Pisces //主题类型 我选的Pisces主题。 12345social: //社交信息 GitHub: https://github.com/JavaZWT || github Email: mailto:971258230@qq.com || envelope QQ: http://wpa.qq.com/msgrd?v=3&amp;uin=971258230&amp;site=qq&amp;menu=yes || qq CSDN: https://blog.csdn.net/JavaZWT || copyright 上面都是一些基本配置，下面我们对blog进行更多的美化。 添加动态旋转头像打开/themes/next/source/css/_common/components/sidebar/sidebar-author.styl文件 将代码替换为： 1234567891011121314151617181920212223242526272829.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; border-radius: 60%; transition: 2.5s all; &#125;.site-author-image:hover &#123; transform: rotate(360deg);&#125;.site-author-name &#123; margin: $site-author-name-margin; text-align: $site-author-name-align; color: $site-author-name-color; font-weight: $site-author-name-weight;&#125;.site-description &#123; margin-top: $site-description-margin-top; text-align: $site-description-align; font-size: $site-description-font-size; color: $site-description-color;&#125; 后重新发布，可以看到头像可以旋转啦。 添加本地搜索插件安装hexo-generator-searchdb： npm install hexo-generator-searchdb –save 同时在hexo的_config.yml里配置如下参数： 123456##local_searchsearch: path: search.xml field: post format: html limit: 10000 Next的_config.yml配置如下： 1234567local_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 重新发布，可以看到新增了搜索功能。 更换背景图片找到 /themes/next/source/css/_custom/custom.styl文件 加入如下代码： 123456789body &#123; background:url("/images/background.jpg"); background-attachment: fixed; // 不随屏幕滚动而滚动 background-repeat: no-repeat; // 如果背景图不够屏幕大小则重复铺，改为no-repeat则表示不重复铺 background-size: 100%; // 等比例铺满屏幕&#125;.main-inner &#123; opacity: 0.8;&#125; 重新发布，可以看到背景图片变化了。 添加live2D功能安装live2D插件hexo-helper-live2d npm install hexo-helper-live2d –save Hexo的 _config.yml文件里配置如下参数 12345678910111213141516171819live2d: model: scale: 1 hHeadPos: 0.5 vHeadPos: 0.618 display: superSample: 2 width: 150 height: 300 position: right hOffset: 0 vOffset: -20 mobile: show: false scale: 0.1 motion: true react: opacityDefault: 0.7 opacityOnHover: 0.2 重新发布，便看到萌萌的二次元人物出现啦。 添加评论插件这里用的Valine评论插件，Valine插件需要leancloud支持，请先注册leancloud，注册成功拿到id和key。 12345678910valine: enable: true appid: //你的id appkey: //你的key notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: (*^_^*)看了这么多，可能你有话想说 # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 配置成功后，重新发布便能看到文章可以评论啦。 开启访客统计当我们注册好leancloud后。 1234leancloud_visitors: enable: true app_id: //你的id app_key: //你的key 找到代码，填入自己的id和key。 重新发布，在页面上可以看到博客的访客数量。 开启字数统计安装字数统计插件：hexo-wordcount npm install hexo-wordcount –save Next配置文件如下参数开启： 123456post_wordcount: item_text: true wordcount: true min2read: true totalcount: true separated_meta: true 重新发布便拥有了字数统计功能。 续文章还没写完，后面会继续更新。 欢迎大家关注我的博客 https://www.sakuratears.top/]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java通过反射获取class信息]]></title>
    <url>%2Fblog%2FJava%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96class%E4%BF%A1%E6%81%AF.html</url>
    <content type="text"><![CDATA[前言在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为反射机制。 我们知道，Java语言不是动态语言，但是他却有非常突出的动态相关机制，反射机制。 代码我们可以通过反射机制获取一个class的相关信息。 1. 利用Javassist获取class信息 Javassist是一个动态类库，可以用来检查、”动态”修改以及创建 Java类。其功能与jdk自带的反射功能类似，但比反射功能更强大。 123456789101112131415161718192021222324252627282930313233343536373839404142public static void getClassInfoByJavassist(Class clazz) &#123; ClassPool classPool = ClassPool.getDefault(); classPool.insertClassPath(new ClassClassPath(clazz)); try &#123; System.out.println("class--&gt;"+clazz.getName()); CtClass ctClass = classPool.get(clazz.getName()); //获取常量信息 CtField[] fields=ctClass.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].getConstantValue()+"--&gt;type="+fields[i].getType().getName()); &#125; //获取方法信息 CtMethod[] ctMethods = ctClass.getMethods(); for (CtMethod ctMethod : ctMethods) &#123; //排除equals，hash，toString等方法 if (!clazz.getName().equals(ctMethod.getDeclaringClass().getName())) &#123; continue; &#125; MethodInfo methodInfo = ctMethod.getMethodInfo(); CodeAttribute codeAttribute = methodInfo.getCodeAttribute(); LocalVariableAttribute attr = (LocalVariableAttribute) codeAttribute.getAttribute(LocalVariableAttribute.tag); if (attr == null) &#123; // exception &#125; String[] paramNames = new String[ctMethod.getParameterTypes().length]; TreeMap&lt;Integer, String&gt; sortMap = new TreeMap&lt;Integer, String&gt;(); for (int i = 0; i &lt; attr.tableLength(); i++) sortMap.put(attr.index(i), attr.variableName(i)); int pos = Modifier.isStatic(ctMethod.getModifiers()) ? 0 : 1; paramNames = Arrays.copyOfRange(sortMap.values().toArray(new String[0]), pos, paramNames.length + pos); CtClass[] types=ctMethod.getParameterTypes(); for(int i=0;i&lt;paramNames.length;i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+ctMethod.getName()+"--&gt;isStatic="+Modifier.isStatic(ctMethod.getModifiers())+"--&gt;paramsType="+types[i].getName()+"--&gt;paramsName="+paramNames[i]); &#125; &#125; &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 我们新建一个Demo.class 1234567891011121314151617181920 public class Demo &#123; public static String getName(String str) &#123; String s="123"; return str+s; &#125; public Integer doSomething(String str,double a,Map&lt;String, String&gt; map,List&lt;String&gt; list) &#123; //doSomething Integer i=0; return i; &#125; private static final int NUM=1; private static String s="1234"; static &#123; s="5678"; &#125;&#125; 调用方法 getClassInfoByJavassist(Demo.class)，输出class信息。 12345678class--&gt;com.zwt.reflect.Demoobject=NUM--&gt;value=1--&gt;type=intobject=s--&gt;value=null--&gt;type=java.lang.Stringclass=Demo--&gt;method=getName--&gt;isStatic=true--&gt;paramsType=java.lang.String--&gt;paramsName=strclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.lang.String--&gt;paramsName=strclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=double--&gt;paramsName=aclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.util.Map--&gt;paramsName=mapclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.util.List--&gt;paramsName=list 2. 通过spring里的LocalVariableTableParameterNameDiscoverer获取paramsName，使用jdk自带reflect反射类获取class其他信息 在jdk1.8以下java版本中，根据jdk自带reflect包，可以拿到大部分class信息，唯一拿不到的是参数name，我们可以借助spring包里的LocalVariableTableParameterNameDiscoverer去获取paramsName。 1234567891011121314151617181920212223public static void getClassInfoBySpringAndReflect(Class clazz) &#123; try &#123; LocalVariableTableParameterNameDiscoverer u = new LocalVariableTableParameterNameDiscoverer(); Method[] methods =clazz.getDeclaredMethods(); Field[] fields=clazz.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; //设置成可以access的，否则get(clazz)报错，无法读取private属性 fields[i].setAccessible(true); System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].get(clazz)+"--&gt;type="+fields[i].getType().getName()); &#125; for(Method method:methods) &#123; //使用spring LocalVariableTableParameterNameDiscoverer 获取paramsName String[] params = u.getParameterNames(method); Class&lt;?&gt; [] classType=method.getParameterTypes(); for (int i = 0; i &lt; params.length; i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+method.getName()+"--&gt;isStatic="+Modifier.isStatic(method.getModifiers())+"--&gt;paramsType="+classType[i].getName()+"--&gt;paramsName="+params[i]); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 调用getClassInfoBySpringAndReflect(Demo.class)，也可以拿到class信息。 3.使用jdk1.8及以上java版本获取class信息 若jdk版本较高，完全可以不用借助第三方jar包而获取class信息。 jdk1.8以上，添加了Parameter类，可以获取参数paramsName. 123456789101112131415161718192021public static void getClassInfoByJava8AndReflect(Class clazz)&#123; try &#123; Method[] methods = clazz.getDeclaredMethods(); Field[] fields=clazz.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; //设置成可以access的，否则get(clazz)报错，无法读取private属性 fields[i].setAccessible(true); System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].get(clazz)+"--&gt;type="+fields[i].getType().getName()); &#125; for (Method method : methods) &#123; //使用jdk1.8Parameter获取paramsNames Parameter[] params = method.getParameters(); Class&lt;?&gt; [] classType=method.getParameterTypes(); for (int i = 0; i &lt; params.length; i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+method.getName()+"--&gt;isStatic="+Modifier.isStatic(method.getModifiers())+"--&gt;paramsType="+classType[i].getName()+"--&gt;paramsName="+params[i]); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 调用getClassInfoByJava8AndReflect(Demo.class)，获取class信息。 其他 反射优点： 合理的使用反射机制可以有效降低代码冗余及代码量。并且可以让应用程序实现一些几乎不可能做到的事情。反射属于Java语言里比较高级的一个特性。 反射缺点： 如果不合理的使用反射，可能降低系统性能。 而且非常重要的一点，我们看如上代码，有一句fields[i].setAccessible(true); 这是在设置私有属性可以访问，显然，这破坏了代码的抽象性，而且可能导致安全问题的产生。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据传输安全性保证]]></title>
    <url>%2Fblog%2F%E6%B5%85%E8%B0%88HTTP%E4%B8%8EHTTPS.html</url>
    <content type="text"><![CDATA[前言数据传输过程中，安全性十分重要，尤其是对于一些敏感的用户信息，其安全性保证更应当被重视。 现代软件开发中，无论是基于敏捷式的软件开发，或者基于前后端分离的软件开发，都离不开提供对外接口，而请求这些接口数据的安全性应当得到保证。 方法保证传输数据的安全性，主要有以下方法： 数据加密 数据加密一直是保密数据的重要部分，常见的加密算法有可逆加密算法和不可逆加密算法，可逆加密算法又分为对称加密算法和非对称加密算法。 比如一个系统的登陆操作，客户输入用户名登陆，如果不进行任何保障措施，用户名和密码明文传输，被不法分子截获数据后，显然是不安全的。如果我们这时对密码进行不可逆加密，如md5，对用户名进行可逆加密，如des，这时候在截获数据时，得到的将是一串密文，显然，即使要破解，也需要相当时间。 但这样，有一个明显问题，就是接口吞吐量下降，明显，加密情况下，由于需要解密数据，接口的响应速度会下降。 可能，对于一些非重要数据，我们这样牺牲系统性能换取来的安全可能有些过了。 数据签名 数据签名又是什么呢？它和数据加密的区别呢？ 数据签名，相当于对传输的数据，进行一些不可逆加密算法后，如md5，生成一段签名字符串sign。 比如上述列子中，登陆操作中如果还要传输IP，地点等等数据，这些数据明显没那么重要，这时可以对全部传输数据进行签名，生成sign，将其传入后端，后端用同样算法及密钥计算比较sign，如果一致认为数据正确，直接拿到IP，地点等数据（不用解密，相对于解密各个信息，理论上所有信息计算签名要节省时间），不一致则认为被修改过，返回错误信息。 session，token机制 session（cookie）和token机制的出现是为了校验用户状态的。 比如不法分子知道了我们的后台接口，恶意伪造大量数据攻击，即使这些数据不正确，而服务器每次都需要校验这些数据的正确性，显然带来大量性能消耗。 我们当然可以进行一些优化操作，如对于同一个IP，短时间大量请求则封掉该IP一段时间，但这不是太合理的。 设想，如果用户登陆后，保存状态，只有登陆的用户可以访问这些接口，每次请求到来，均先校验用户登陆状态，对于session，如果没有sessionid或者sessionid错误或者过期则直接返回登陆界面。对于token，与session同理，没有token或者token错误或者过期的直接返回登陆页面。 这样，我们开始校验token或者session，就可以拒绝大量伪造请求。 Https（数字证书机制） 上面，无论数据加密还是签名，我们发现最重要的就是加密方法和加密密钥。 对于两台服务器交互，可能不用太担心，但是如果是webapp或者原生app，不法分子反编译前端代码后，就有可能拿到加密方法和加密key，怎么办呢？ 这就属于Https要解决的事情，下篇文章会介绍https，这儿先简单说下： 在加密算法中，有一种叫做非对称加密的算法，有公钥和私钥组成，他有个特点：公钥加密的数据，只有私钥能解密；私钥加密的数据，只有公钥能解密。 https就是需要让客户端与服务器端安全地协商出一个对称加密算法。剩下的就是通信时双方使用这个对称加密算法进行加密解密。 ①客户端启动，发送请求到服务端，服务端通过非对称加密算法（如RSA）生成公钥pubkey1和私钥prikey1。 ②服务端将公钥pubkey1发给客户端，客户端用自己的非对称加密算法也生成一套公钥pubkey2和私钥prikey2，并将公钥pubkey2通过pubkey1加密后返回服务端。 ③服务端用私钥prikey1解密后拿到pubkey2，并将确定好的未来交互的对称加密算法和密钥通过pubkey2加密，返回客户端。 ④客户端用私钥pubkey2解密数据，拿到服务器给定的加密算法和密钥，双方开始用其数据通信。 这样仍有一个问题，如何证明公钥pubkey1加密的这串数字是客户端来的，即证明他就是他。。。 这就是https的数字证书，相当于网络中心的部分，证明他就是他。数字证书就是来干这个的。 其他 安全性保证还有其他可以自己操作的方法。 如对于两台稳定的服务器交互，直接进行IP校验或许比token，session机制更好更方便。及一些其他的操作，如同一IP短时间大量错误报文，可以将其暂时拉入黑名单。等等。 其他数据安全一直是数据交互的永恒话题，保证数据安全，肯定是需要消耗一定的系统性能的。如何选择合适的安全保证机制，是每一个软件开发工程师应该了解和掌握的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UTF-8和GBK等编码格式转换问题]]></title>
    <url>%2Fblog%2FUTF-8%E5%92%8CGBK%E7%AD%89%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言在开发过程中，我们可能会遇到Java各种编码格式之间的转换问题。下面我们来研究下UTF-8和GBK等编码格式之间的相互转化。 实践在进行编码转换时，我们用ISO-8859-1编码来接受和保存数据，并转换为相应编码。 为什么采用ISO-8859-1编码作为中间转存方案呢？ 下面我们通过程序验证： 通过ISO-8859-1转存： 123456789101112131415161718192021public static void test(String str1,String encode) throws UnsupportedEncodingException &#123; System.out.println("字符串："+str1); //将str转为原编码字节流 byte[] byteArray1=str1.getBytes(encode); System.out.println(byteArray1.length); //转换为一个ISO-8859-1的字符串 String str2=new String(byteArray1,"ISO-8859-1"); System.out.println("转成ISO-8859-1："+str2); //转回为byte数组 byte[] byteArray2=str2.getBytes("ISO-8859-1"); System.out.println(byteArray2.length); //重新用目标编码格式编码 String str3=new String(byteArray2,encode); System.out.println("字符串："+str3); &#125; public static void main(String[] args) throws UnsupportedEncodingException &#123; String str1="你好"; String str2="你好呀"; test(str1,"UTF-8"); test(str2,"UTF-8"); &#125; 运行结果： 字符串：你好 6 转成ISO-8859-1：ä½ å¥½ 6 字符串：你好 字符串：你好呀 9 转成ISO-8859-1：ä½ å¥½å 9 字符串：你好呀 通过GBK转存： 123456789101112131415161718192021 public static void test(String str1,String encode) throws UnsupportedEncodingException &#123; System.out.println("字符串："+str1); //将str转为原编码字节流 byte[] byteArray1=str1.getBytes(encode); System.out.println(byteArray1.length); //转换为一个GBK的字符串 String str2=new String(byteArray1,"GBK"); System.out.println("转成GBK："+str2); //转回为byte数组 byte[] byteArray2=str2.getBytes("GBK"); System.out.println(byteArray2.length); //重新用目标编码格式编码 String str3=new String(byteArray2,encode); System.out.println("字符串："+str3); &#125; public static void main(String[] args) throws UnsupportedEncodingException &#123; String str1="你好"; String str2="你好呀"; test(str1,"UTF-8"); test(str2,"UTF-8");&#125; 运行结果： 字符串：你好 6 转成GBK：浣犲ソ 6 字符串：你好 字符串：你好呀 9 转成GBK：浣犲ソ鍛� 9 字符串：你好�? 可以看到，当用GBK暂存UTF-8编码字符串时，字符串汉字出现了乱码。 为什么会这样？ 分析我们新增一个方法，将byte数组打印出来： 123456789public static void printHex(byte[] byteArray) &#123; StringBuffer sb = new StringBuffer(); for (byte b : byteArray) &#123; sb.append(Integer.toHexString((b &gt;&gt; 4) &amp; 0xF)); sb.append(Integer.toHexString(b &amp; 0xF)); sb.append(" "); &#125; System.out.println(sb.toString());&#125;; 这样上面两个的运行结果分别如下：ISO-8859-1: 字符串：你好 e4 bd a0 e5 a5 bd 转成ISO-8859-1：ä½ å¥½ e4 bd a0 e5 a5 bd 字符串：你好 字符串：你好呀 e4 bd a0 e5 a5 bd e5 91 80 转成ISO-8859-1：ä½ å¥½å e4 bd a0 e5 a5 bd e5 91 80 字符串：你好呀 GBK: 字符串：你好 e4 bd a0 e5 a5 bd 转成GBK：浣犲ソ e4 bd a0 e5 a5 bd 字符串：你好 字符串：你好呀 e4 bd a0 e5 a5 bd e5 91 80 转成GBK：浣犲ソ鍛� e4 bd a0 e5 a5 bd e5 91 3f 字符串：你好�? 可以看到，UTF-8转换为GBK在转换回来时，最后的80变成了3f，为什么会这样？ 我们使用”你好呀” 三个字来分析，它的UTF-8 的字节流为： [e4 bd a0] [e5 a5 bd] [e5 91 80] 我们按照三个字节一组分组，用GBK处理，因为GBK是双字节编码，如下按照两两一组进行分组： [e4 bd] [a0 e5] [a5 bd] [e5 91] [80 ?] 不够了，怎么办？它把 0x8d当做一个未知字符，用一个半角Ascii字符的 “？” 代替，变成了： [e4 bd] [a0 e5] [a5 bd] [e5 91] \3f 数据被破坏了。 为什么 ISO-8859-1 没问题呢？ 因为 ISO-8859-1 是单字节编码，因此它的分组方案是： [e4] [bd] [a0] [e5] [a5] [bd] [e5] [91] [80] 因此中间不做任何操作，因此数据没有变化。 问题你也许会问到，比如将“你好呀”三个字先由UTF-8转为ISO-8859-1，再由ISO-8859-1转为GBK，结果也是乱码啊，不是和下面的代码一样么，性质上？12String isoFont = new String(chinese.getBytes("UTF-8"),"ISO-8859-1");String gbkFont = new String(isoFont.getBytes("ISO-8859-1"),"GBK"); 1String gbkFont = new String(chinese.getBytes("UTF-8"),"GBK"); 两者的性质确实是一样的。 那与上面说的不矛盾吗？ 不矛盾。上面的代码，第一步你指定了字符串编码格式为UTF-8，第二步你将其转换为GBK，肯定会乱码。可以认为你拿一个UTF-8的字符串去转GBK字符串，其实在程序里这种写法本身是错误的！ 我们来看下面一段代码： 12345678910111213141516public static void test2() throws UnsupportedEncodingException &#123; String chinese = "你好呀"; //GBK 测试 String gbkChinese = new String(chinese.getBytes("GBK"),"ISO-8859-1"); System.out.println(gbkChinese); printHex(gbkChinese.getBytes("ISO-8859-1")); String gbkTest = new String(gbkChinese.getBytes("ISO-8859-1"),"GBK"); System.out.println(gbkTest); //UTF-8测试 String utf8Chinese = new String(chinese.getBytes("UTF-8"),"ISO-8859-1"); System.out.println(utf8Chinese); printHex(utf8Chinese.getBytes("ISO-8859-1")); String utfTest = new String(utf8Chinese.getBytes("ISO-8859-1"),"UTF-8"); System.out.println(utfTest); &#125; 输出结果： ÄãºÃÑ½ c4 e3 ba c3 d1 bd 你好呀 ä½ å¥½å e4 bd a0 e5 a5 bd e5 91 80 你好呀 可以看到， GBK分组 ：[c4 e3]–&gt;你 [ba c3]–&gt;好 [d1 bd]–&gt;呀 UTF-8分组：[e4 bd a0]–&gt;你 [e5 a5 bd]–&gt;好 [e5 91 80]–&gt;呀 字符串“你好呀”在GBK编码和UTF-8编码里生成的byte数据流是不一样的。 结论所以如何正确将两种编码格式数据进行转换？ 注意：这儿的转换指的是这样，比如一个GBK编码文件，里面有“你好呀”字符串，写入到UTF-8编码文件里仍然是“你好呀”。 我们新建一个GBK编码文件，里面有你好呀，三个字符，同时将三个字用UTF-8，写入到另一个文件里。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Test2 &#123; public static void main(String[] args) throws Exception &#123; String line = readInFile("/Users/zhangwentong/junrongdai/gbk.txt", "GBK"); System.out.println(line); writeInFile("/Users/zhangwentong/junrongdai/utf8.txt", line, "UTF-8"); &#125; public static String readInFile(String fileName, String charset) &#123; File gbkfile = new File(fileName); String line = ""; FileInputStream gbkIO = null; InputStreamReader gbkISR = null; BufferedReader br = null; try &#123; gbkIO = new FileInputStream(gbkfile); gbkISR = new InputStreamReader(gbkIO, charset); br = new BufferedReader(gbkISR); String rline = ""; while ((rline = br.readLine()) != null) &#123; line += rline; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if(br!=null) fos.close(); if(gbkISR!=null) gbkISR.close(); if(gbkIO!=null) gbkIO.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return line; &#125; public static void writeInFile(String fileName, String content, String charset) &#123; File f = new File(fileName); FileOutputStream fos = null; try &#123; if (!f.exists()) &#123; f.createNewFile(); &#125; fos = new FileOutputStream(f); fos.write(content.getBytes(charset)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 可以测试下上段代码，GBK文字被转为了UTF-8文字。反过来一个UTF-8文件写入到GBK也是可以实现的。 所以，在读取和写入文字时，指定文字的编码格式，再进行读取和写入操作，便不会有乱码的产生。否则读取和写入时会按照执行操作的class文件的编码格式进行写入和读取。 结语欢迎光临我的博客 https://www.sakuratears.top 我的GitHub地址 https://github.com/javazwt]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap实现原理]]></title>
    <url>%2Fblog%2FHashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[前言今天我们来聊聊HashMap。 Java集合类里的HashMap，实现Map接口，是个非线程安全的类。HashMap允许key和value有null值，且循环遍历为无序的，HashMap底层主要是通过数组+链表实现的，同时JDK8引入红黑树优化，提高HashMap的性能。 要了解HashMap，我们可以从几方面下手。 分析我们先来看下HashMap的两个参数： initialCapacity：初始容量，默认16 loadFactor：负载因子，默认0.75 现在我们执行了下面一段代码，根据代码来分析HashMap： 12Map&lt;String,String&gt; map=new HashMap&lt;&gt;();map.put("0","0"); 创建一个HashMap，其会初始化以下数据： 123456static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64; DEFAULT_INITIAL_CAPACITY: 初始容量，也就是默认会创建 16 个箱子，箱子的个数不能太多或太少。如果太少，很容易触发扩容，如果太多，遍历哈希表会比较慢。 MAXIMUM_CAPACITY: 哈希表最大容量，一般情况下只要内存够用，哈希表不会出现问题。 DEFAULT_LOAD_FACTOR: 默认的负载因子。因此初始情况下，当键值对的数量大于 16 * 0.75 = 12 时，就会触发扩容。 TREEIFY_THRESHOLD: 如果哈希函数不合理，即使扩容也无法减少箱子中链表的长度，因此处理方案是当链表太长时，转换成红黑树。这个值表示当某个箱子中，链表长度大于 8 时，有可能会转化成树。 UNTREEIFY_THRESHOLD: 在哈希表扩容时，如果发现链表长度小于 6，则会由树重新退化为链表。 MIN_TREEIFY_CAPACITY: 在转变成树之前，还会有一次判断，只有键值对数量大于 64 才会发生转换。这是为了避免在哈希表建立初期，多个键值对恰好被放入了同一个链表中而导致不必要的转化。 根据HashMap源码，可以看到源码中有两个static final class Node &lt; K,V &gt; 和 TreeNode &lt; K,V &gt;分别为链表和红黑树链表。 本文不对红黑树链表的实现做过多分析。 我们来看下HashMap的put方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//put方法，调用putVal方法 public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; //HashMap放值方法 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //构建一个链表数组tab，链表p，长度n，索引i Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //把table的值赋给tab，如果tab是空或者长度为0 if ((tab = table) == null || (n = tab.length) == 0) //调用resize方法，并获得tab长度 n = (tab = resize()).length; //计算索引并获得tab索引下的值，如果为空直接将值添加 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果key值相同，直接替换value值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果key不相同，判断p是不是TreeNode，是的话就执行红黑树放入值操作 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; //如果key上没值就放入普通链表 p.next = newNode(hash, key, value, null); //如果链表长度超了8 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //尝试将链表转化为红黑树（不一定会转化） treeifyBin(tab, hash); break; &#125; //如果key上有值就覆盖掉value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果节点value不为空，即key上有值，把这个值返回去 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //记录改变次数（fast-fail机制） ++modCount; //如果长度超过当前，就进行扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 我们再来看下，resize方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081final Node&lt;K,V&gt;[] resize() &#123; //获取旧的tab Node&lt;K,V&gt;[] oldTab = table; //旧的容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; //旧的阀值 int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; //如果容量超了MAXIMUM_CAPACITY，最大阀值定为Integer.MAX_VALUE if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //如果新容量赋值后小于MAXIMUM_CAPACITY并且旧容量不小于初始值 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果定义了初始容量 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults //否则为初始化，所有均为默认值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //为新的阀值赋值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap的get方法 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; //调用getNode方法 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;//获取HashMap Value值final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //先判断链表第一个值是不是结果 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //否则循环链表找值 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 我们看一下hash获取方法： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 代码的意思是：如果Key值为null，返回0；如果Key值不为空，返回原hash值和原hash值无符号右移16位的值按位异或的结果。可以看到当key=null时，hash为0. 问题 为什么HashMap的初始长度默认16，负载因子默认0.75，且长度建议取2的倍数？ &ensp;&ensp;&ensp;&ensp; 通过以上的代码我们可以知道这两个值主要影响的threshold的大小，这个值的数值是当前桶数组需不需要扩容的边界大小，我们都知道桶数组如果扩容，会申请内存空间，然后把原桶中的元素复制进新的桶数组中，这是一个比较耗时的过程。既然这样，那为何不把这两个值都设置大一些呢，threshold是两个数的乘积，设置的大些不就减小了扩容次数吗？ &ensp;&ensp;&ensp;&ensp; 原因是这样的，如果桶初始化桶数组设置太大，就会浪费内存空间，16是一个折中的大小，既不会像1，2，3那样放几个元素就扩容，也不会像几千几万那样可以只会利用一点点空间从而造成大量的浪费。 &ensp;&ensp;&ensp;&ensp; 加载因子设置为0.75而不是1，是因为设置过大，桶中键值对碰撞的几率就会越大，同一个桶位置可能会存放好几个value值，这样就会增加搜索的时间，性能下降，设置过小也不合适，如果是0.1，那么10个桶，threshold为1，你放两个键值对就要扩容，太浪费空间了。 &ensp;&ensp;&ensp;&ensp; HashMap为了存取高效，要尽量较少碰撞，就是要尽量把数据分配均匀，每个链表长度大致相同，这个实现就在把数据存到哪个链表中的算法；这个算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算，源码中做了优化hash&amp;(length-1)，hash%length==hash&amp;(length-1)的前提是length是2的n次方；为什么这样能均匀分布减少碰撞呢？&ensp;&ensp;&ensp;&ensp; 2的n次方实际就是1后面n个0，2的n次方-1 实际就是n个1；例如长度为9时候，3&amp;(9-1)=0 2&amp;(9-1)=0 ，都在0上，碰撞了；例如长度为8时候，3&amp;(8-1)=3 2&amp;(8-1)=2 ，不同位置上，不碰撞； HashMap的fast-fail机制？ &ensp;&ensp;&ensp;&ensp; 在put方法里，我们注意到一个参数，modCount，每当HashMap改变，modCount都会改变,在HashMap的remove，clear等对HashMap的变动操作中，都可以看到对此参数的操作，同时我们还能看到如下代码： 12if (modCount != expectedModCount) throw new ConcurrentModificationException(); &ensp;&ensp;&ensp;&ensp; 这就是HashMap的fast-fail机制，HashMap会记录Map的改变次数，如果多个线程操作HashMap，会导致modCount和expectedModCount不一致，就会抛出ConcurrentModificationException异常，说明你的代码里有多线程对HashMap的操作，这是不被允许的，这样也可以快速检索程序错误，但是我们不能指望HashMap的fast-fail机制来检索错误，我们更应该在编码中注意HashMap是线程不安全的，多线程情况可以考虑ConcurrentHashMap,Hashtable或者使用Collections.synchronizedMap(map)等操作。 深度分析 关于hash桶索引计算函数 i=(n-1)&amp;hash 好处。 &ensp;&ensp;&ensp;&ensp; 观察HashMap将元素放入tab操作如下图，n在1处被赋值为tab.length，在2处，有一个关键算法，i=(n-1)&amp;hash,i为tab下标，这样做有什么好处呢？ （1）保证不会发生数组越界 &ensp;&ensp;&ensp;&ensp; 首先我们要知道的是，在HashMap，数组的长度按规定是2的幂。因此，数组的长度的二进制形式是：10000…000, 1后面有偶数个0。 那么，length - 1 的二进制形式就是01111…111, 0后面有偶数个1。最高位是0, 和hash值相“与”，结果值一定不会比数组的长度值大，因此也就不会发生数组越界。 （2）保证元素尽可能的均匀分布 &ensp;&ensp;&ensp;&ensp; 由上边的分析可知，length若是一个偶数，length - 1一定是一个奇数。假设现在数组的长度length为16，减去1后length - 1就是15，15对应的二进制是：1111。现在假设有两个元素需要插入，一个哈希值是8，二进制是1000，一个哈希值是9，二进制是1001。和1111“与”运算后，结果分别是1000和1001，它们被分配在了数组的不同位置，这样，哈希的分布非常均匀。那么，如果数组长度是奇数呢？减去1后length - 1就是偶数了，偶数对应的二进制最低位一定是 0，例如14二进制1110。对上面两个数子分别“与”运算，得到1000和1000。结果都是一样的值。那么，哈希值8和9的元素都被存储在数组同一个index位置的链表中。在操作的时候，链表中的元素越多，效率越低，因为要不停的对链表循环比较。 关于”扰动函数”，(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16) ？ &ensp;&ensp;&ensp;&ensp; 在对数据进行hash计算时，可以看到，不仅仅是取了数据的hashCode，而是将hashCode和hashCode无符号右移16位的值进行异或运算。 &ensp;&ensp;&ensp;&ensp; 我们知道，key.hashCode返回一个int值，这个值一般比hash桶数组长度要大，比如一个长度为16的hash桶，放入String abc (hashCode为96354)，直接进行桶索引计算，i=(n-1)&amp;hashCode 可以得出(15&amp;96354)=2,索引值为2，如果是abcd，计算（15&amp;2987074）=2，索引值也为2。 &ensp;&ensp;&ensp;&ensp; 可以看出，即使hashCode散列再离散，计算索引值时低位才是主要影响原因，而特征较大的高位（96354和2987074高位特征较大）根本不参与运算，这样hash冲突也会较高。而右移16位（32位的一半，int最大32位），正好为32位一半，这样可以把前16位认为高位，后16位认为低位，然后进行异或操作，高16位的信息被变相保存了下来，增大了随机性。 &ensp;&ensp;&ensp;&ensp; 可以看出这样操作后abc的下标为3（二进制11），abcd的下标为15（二进制1111）。 &ensp;&ensp;&ensp;&ensp; Peter Lawrey有一篇关于hash冲突率比较的文章《An introduction to optimising a hashing strategy》，大家可以看看。 &ensp;&ensp;&ensp;&ensp; https://www.javacodegeeks.com/2015/09/an-introduction-to-optimising-a-hashing-strategy.html 为什么引入负载因子这个概念？ &ensp;&ensp;&ensp;&ensp; 负载因子的引入，可以来说是时间复杂度和空间复杂度的折中。（大数据统计下）负载因子越低，一般认为空间开销越大，查询时间开销越低（hash碰撞低），大量hash数组，少量链表；负载因子越高，一般认为空间开销越低，查询时间开销越高（hash碰撞高），少量hash桶数组，大量链表。负载因子的引入恰可以增加HashMap不同场景使用的灵活性。 结语欢迎光临我的博客 https://www.sakuratears.top 我的GitHub地址 https://github.com/javazwt]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池]]></title>
    <url>%2Fblog%2FJava%E7%BA%BF%E7%A8%8B%E6%B1%A0.html</url>
    <content type="text"><![CDATA[前言Java中，创建线程的方式一般有三种方法： 继承Thread类创建线程 实现Runnable接口创建线程 使用Callable和Future创建线程 关于三种创建方法本文不再赘述。 可以看出，以上创建线程的方式，都缺乏对线程的管理，我们设想，如果线程在调用过程中使用了某一资源，当该资源处理缓慢或异常时，可能产生大量线程等待的情况，严重时可能造成OOM异常。 针对以上情况，应该对创建线程进行管理，这样线程池便产生了，好在在jdk1.5时，Doug Lea大神已经帮我们实现了这些功能，它们均在java.util.concurrent包下。建议大家想学习多线程，把该包下的源码理解，一定对多线程会有更深入的理解。 本文重点讲述线程池，会对以下这几个类（接口）进行重点讲解。 Executor，ExecutorService，Executors，AbstractExecutorService，ThreadPoolExecutor 线程池的创建我们先来简单说下线程池的使用： 缓存型线程池 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 Executors.newCachedThreadPool 简单使用： 123456789101112131415ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; final int index = i; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); &#125; &#125;); &#125; 定长线程池 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 Executors.newFixedThreadPool 简单使用： 123456789101112131415ExecutorService fixedThreadPool = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 定时及周期性任务性线程池 创建一个定长线程池，支持定时及周期性任务执行。 Executors.newScheduledThreadPool 简单使用： 1234567ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id="+Thread.currentThread().getId()+";5s后,每2s执行一次"); &#125; &#125;, 5, 2, TimeUnit.SECONDS); 单线程型线程池 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 Executors.newSingleThreadExecutor 简单使用： 123456789101112131415ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("Thread id="+Thread.currentThread().getId()+";index="+index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 自定义线程池 创建一个自定义线程池，以优化线程池。 根据Executors源码，可以看出： 12345678910111213141516171819 public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 线程池的实现使用ThreadPoolExecutor这个类实现的。这个类全参参数有以下几个： 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ① corePoolSize：核心线程池大小 取值范围 0-Integer.MaxValue ② maximumPoolSize：最大线程池大小 取值范围 0-Integer.MaxValue ③ keepAliveTime：线程空闲时存活时间 ④ unit：线程空闲时存活时间单位 ⑤ workQueue：工作队列类型，线程队列类型 队列分类： 直接提交策略：SynchronousQueue，其无法设置队列长度，所有线程均直接提交给线程池。 无界队列：LinkedBlockingQueue，如果默认不设置初始长度，这个队列是无界的，可缓存大量等待线程。 有界队列：ArrayBlockingQueue，必须设置初始长度，线程池满，且达到队列最大长度后执行拒绝策略。 ⑥ threadFactory：线程工厂 ⑦ handler：线程池饱和后的拒绝策略 ThreadPoolExecutor定义了四种，我们也可以自己定义： ThreadPoolExecutor.AbortPolicy：拒绝该任务并抛出异常 ThreadPoolExecutor.CallerRunsPolicy：直接在 execute 方法的调用线程中运行被拒绝的任务；如果执行程序已关闭，则会丢弃该任务 ThreadPoolExecutor.DiscardOldestPolicy：直接丢弃正在执行的任务，并执行该任务 ThreadPoolExecutor.DiscardPolicy：丢弃该任务 可以看出，当业务情况复杂时，Executors里提供的几种基本的线程池已经不能满足我们的要求，需要我们根据情况自定义线程池，而且可以举个例子，比如对于newCachedThreadPool创建线程池的方法，它传入的maximumPoolSize为Integer的Max值，如果业务资源异常，创建大量线程而不释放，newCachedThreadPool这种创建线程池的方法也能导致OOM异常。 而我们声明最大线程池大小，并声明拒绝策略。如下： 1234567891011121314151617181920ExecutorService myExecutor = new ThreadPoolExecutor(5, 10, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(5), new AbortPolicy()); for (int i = 0; i &lt; 11; i++) &#123; final int index = i; try &#123; myExecutor.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); try &#123; Thread.sleep(10000000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 可以有效防止OOM异常以及及时发现系统运行问题。 自定义线程池也是被推荐的创建线程池的方法。 源码分析下面我们主要对ThreadPoolExecutor这个类进行分析。 我们先看下它的execute方法： 123456789101112131415161718192021222324252627282930public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //获取线程数量 int c = ctl.get(); //如果线程池线程数量小于核心线程数，那么试着向核心线程池添加一个线程 if (workerCountOf(c) &lt; corePoolSize) &#123; //添加成功就返回 if (addWorker(command, true)) return; //添加不成功就再次拿到线程数量 c = ctl.get(); &#125; //如果添加失败了，或者线程池数量达到了核心线程池数量 //那么判断下运行状态，然后试着向工作等待队列里添加此线程 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); //如果不是运行状态，那么试着从工作队列移除此线程 if (! isRunning(recheck) &amp;&amp; remove(command)) //成功就进行拒绝策略处理 reject(command); //如果核心线程池和队列都满了，达到CAPACITY else if (workerCountOf(recheck) == 0) //那么尝试将任务添加至非核心线程池 addWorker(null, false); &#125; //如果添加失败，执行拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 再看下addWorker方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; //sakuratears int c = ctl.get(); //拿下线程运行状态 int rs = runStateOf(c); //如果程序停止或者状态不是暂停并且任务不为空并且任务队列不为空，添加失败 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //数量大于CAPACITY或者数量大于corePoolSize（向核心线程池添加时）或者maximumPoolSize（向非核心线程池添加时），返回添加失败 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //试着计算下当前线程数量 if (compareAndIncrementWorkerCount(c)) //成功跳出循环 break retry; //不成功就重读ctl c = ctl.get(); //如果当前状态与开始线程状态不一致 if (runStateOf(c) != rs) //重试循环 continue retry; &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; //获得锁 final ReentrantLock mainLock = this.mainLock; //创建一个新的Worker w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; //加锁 mainLock.lock(); try &#123; int c = ctl.get(); int rs = runStateOf(c); //如果是运行状态，或者核心线程池暂停，但要将线程添加到非核心线程池中 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) throw new IllegalThreadStateException(); //添加线程 workers.add(w); int s = workers.size(); //记录曾经有过的最大线程数 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; //解锁 mainLock.unlock(); &#125; if (workerAdded) &#123; //启动 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) //线程启动失败，执行失败操作 addWorkerFailed(w); &#125; return workerStarted;&#125; 更多代码不一一赘述。上面代码基本是线程池的核心原理。 通俗点讲，线程池工作分为下面几步： 根据传入参数，设置核心线程池数量，最大线程池数量，拒绝策略，线程工作队列 当添加一个线程时，如果线程池线程数小于核心线程数，直接开启一个新线程执行任务。 如果核心线程池满了，那么把它添加到工作队列中。 如果核心线程池和工作队列都满了，则开启非核心线程执行任务。 如果全部都满了，执行拒绝策略。 以上就是对线程池的全部分析。 关于我的个人博客： https://www.sakuratears.top GitHub地址： https://github.com/javazwt 欢迎关注。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未闻花名]]></title>
    <url>%2Fblog%2F%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.html</url>
    <content type="text"><![CDATA[已知花意未见其花已见其花未闻花名再见其花落泪千溟未闻花名但识花香已知花名花已不在未闻花名但识花香再遇花时泪已千行 某天，你无端想起一个人，她曾让你对明天有所期许，但她却完全没有出现在你的明天里。————《再见金华站》]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MarkDown语法]]></title>
    <url>%2Fblog%2FMarkDown%E8%AF%AD%E6%B3%95.html</url>
    <content type="text"><![CDATA[前言最近自己搭建了一个博客网站，写文章时用到了markdown语法，之前虽未接触过markdown语法，但在两三天的使用过程中感觉markdown语法很棒，写法简洁，而且支持Html语言。特总结一些常用的语法如下： 语法汇总标题markdown有6种标题，写法如下： 级别 写法 1级 # 大标题 2级 ## 前言 3级 ### 语法汇总 4级 #### 标题 5级 ##### 标题5 6级 ###### 标题6 注意：#号和文字之间有空格 段落写完一段话，一般空出一行便可以分为两个段落，如果不主动空行，其便会认为为一个段落。也可以在一段话的结束输入两个空格，markdown也会换行。 注意：这两种换行，第一种间距大。我现在这段话分别用了空格换行和空行换行。 tab键这是一段用了tab缩进的话。 这是用了两个tab缩进的话。 这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长的话。 注意：当一段话过长时，这样生成的句子可能不换行，而是在下面有一个拉动条。 资源引用 引用静态资源时，如img，png，gif等 语法：![图片名称](图片的绝对路径或者相对路径) ![upload successful](/images/pasted-10.png) 引用网址时，直接贴上网址就能识别 我的博客：https://javazwt.github.io/ 表格语法： | Name | Value | |-------------|:-----------: | | sakuratears | 999999 | 效果： Name Value sakuratears 999999 注意：在表头下方的分隔线标记中加入 :，即可标记下方单元格内容的对齐方式，默认左对齐： :— 代表左对齐:–: 代表居中对齐—: 代表右对齐 left（:— ） center（:—-: ） right（ —-:） left center right 加粗 斜体 删除线加粗语法： **要加粗的字** 加粗效果： 要加粗的字 斜体语法：*斜体的字* 斜体效果： 斜体的字 删除线语法： ~~要删除的字~~ 删除线效果： 要删除的字 引用如果需要引用文献资料或者别的引用，可以用大于号解决： 语法：&gt;要引用的文字 效果： 要引用的文字 列表有序列表语法：在指定序号后面加上点（.） 并空格 1. aaa 2. bbb 3. ccc 效果： aaa bbb ccc 无序列表语法：在文字前面加上* 或者 + 或者 - 并空格 * aaa * bbb * ccc 效果： aaa bbb ccc 注意： 如果生成多级列表，可以这样： - 1.a - 1.1.aa - 1.1.1.aaa - 2.b - 3.c 效果图： 1.a 1.1.aa 1.1.1.aaa 2.b 3.c 代码支持语法： 1//代码片段 效果：一段java代码 1System.out.println("Hello World"); 其他由于Markdown支持Html语法，故Markdown以上无法实现的可以借助Html实现，如居中实现： &lt;center&gt;这段话要居中&lt;/center&gt; 效果：这段话要居中]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Html5+JQuery页面标签的禁用问题]]></title>
    <url>%2Fblog%2FHtml5-JQuery%E9%A1%B5%E9%9D%A2%E6%A0%87%E7%AD%BE%E7%9A%84%E7%A6%81%E7%94%A8%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言在做移动应用项目时遇到的问题，完成某些操作后返回到前面页面要保证前面页面的标签内容不可被编辑，修改。有如下方法： 方法 对于input，select框，设置disabled=’disabled’。 如： 12$('#testid1 input').attr('disabled','disabled');$('#testid2 select').attr('disabled','disabled'); 对于一些点击触发事件的button，可以用unbind()移除事件方法。 如： 1$(".testclass3").unbind(); 注：在某些情况下unbind慎用，如页面初始化时调用click事件加载其他事件属性，unbind()可能会导致初始化时无法加载。 插件禁用，有些页面可能使用某些插件，对于插件的禁用，可以设置插件的事件方法return false; 如： 123$("#test #test4").on("touchmove",function()&#123; return false;&#125;); 这个是我们项目一个滑动插件触摸禁用的例子。 对于一些比较特殊的，如li标签，可以使用class样式进行处理。 这是一个两个li标签切换禁用的例子。 例： 1234$("#test_li").addClass('disabledTab');.disabledTab&#123; pointer-events: none;//这样鼠标或触摸就无法选中li标签&#125;]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云主机环境搭建]]></title>
    <url>%2Fblog%2F%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[前言现在很多服务都部署在云服务上，最近项目也要发布到云上，特写此文章。有需要部署云服务的小伙伴可以参考下。 云主机参数在部署云服务时，一般需要知道如下参数： 云ECS主机服务器远程连接： url XXXXXXXX 用户名：XXXXXXX 密码：XXXXXXX 超级管理员：用户名：XXXX 密码 ：XXXX 硬件配置： 实例规格：2 核 4GB 网络：带宽 5Mbps 操作系统：Ubuntu 16.04 64位 系统盘：高效云盘（100 GB） 工具准备远程连接工具 XShell连接工具 Xftp文件上传管理工具 两工具下载地址：https://www.netsarang.com/products/xsh_overview.html 其他说明：由于当前Ubuntu 16.04 64位 Linux系统弃用了一些旧的远程ssh加密方式，故原先的ssh2工具不能连接，当然也可以修改linux配置文件使其支持旧的ssh加密方式，详见http://blog.csdn.net/liucky/article/details/51889112 等文章。本服务器未修改也不建议修改linux配置，进而选择使用XSHell等工具连接。 服务器搭建工具 Linux 64位 jdk压缩包 jdk-7u79-linux-x64.tar.gz Tomcat压缩包 For Linux 64 apache-tomcat-7.0.79.tar.gz Linux 64位 Mysql 安装包 （在线安装可不使用） mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz 编译好的项目 demo 数据库表 demo 连接云主机 连接到Linux服务器 打开XShell，输入前面的云主机配置，连接到Linux远程服务器。如图： 登录成功后如图 使用 pwd 命令可查看当前路径 目前/home 文件夹下已使xxxxxx用户拥有读、写、执行的权限 上传文件 进入/home文件夹 我们在其目录下建三个文件夹jdk,mysql,tomcat用于存放文件 创建文件夹，可以使用 mkdir 文件夹名 这种方式，也可以直接打开xftp 创建，如图： 上传并解压 通过Xftp将 jdk，tomcat，mysql，项目包demo，数据表全部传上去 对于tar.gz文件进行解压 tar -xzvf jdk-7u79-linux-x64.tar.gz 注意：项目demo要放到tomcat webapps目录下，故建议将tomcat 解压后再上传demo项目 配置环境变量 找到jdk路径 同时找到 /etc/profile 使用vi /etc/profile 编辑该文件，设置环境变量 export JAVA_HOME=”/home/jdk/jdk1.7.0_79” export PATH=”$PATH:$JAVA_HOME/bin” export JRE_HOME=”$JAVA_HOME/jre” export CLASSPATH=”.:$JAVA_HOME/lib:$JRE_HOME/lib” 或者通过Xftp把profile文件down下来，修改好后再上传回去 弄好后要使用source /etc/profile 命令进行更新使其变量生效 同时 使用命令 java -version java javac 正常则表示配置成功了 启动Tomcat 由于tomcat自带的startup.sh脚本看不到日志输出，因此写了个脚本startTomcat.sh脚本 进入到tomcat bin目录下，使用命令 sh startTomcat.sh 启动tomcat，看到starup毫秒数表示启动成功。 启动后，在本地机器上访问 http://xxxxxxxxxxx:8080/ 可以看到久违的tomcat页面 bin里的shutdown.sh用于停止tomcat服务，sh shutdown.sh 成功后上传项目 demo到 tomcat webapps文件夹 安装MySQL接下来开始安装Mysql 对于阿里云，我们可以采取在线安装的方式快速安装MySQL 使用Linux root用户登录， 使用命令： sudo apt-get install mysql-server apt-get isntall mysql-client sudo apt-get install libmysqlclient-dev 安装过程中会提示输入root用户密码，设置所需的root用户密码即可。 使用以下命令查看MySQL是否安装成功 sudo netstat -tap | grep mysql 可以看到MySQL处于listen状态，配置成功。 关于离线安装，暂时没弄成功，有时间在补上。 导入数据表将数据表数据导入到MySQL数据库。mysql命令service mysql start 启动service mysql stop 停止 service mysql restart 重启 使用root用户登录MySQ mysql -u root -p 输入密码登录成功。 设置必要数据 进入mysql命令行 查看数据库编码还有表名大小写情况（Linux系统下MySQL表名会区分大小写） 查看字符编码 mysql&gt; show variables like ‘character%’; 查看表名大小写 mysql&gt; show Variables like ‘%table_names’; 如果不是utf-8编码（右图所示为utf-8编码格式），表名默认区分大小写 lower_case_table_names=0 则需要修改默认配置 cd etc/mysql/mysql.conf.d/ vi mysqld.cnf 加入如下配置： character-set-server=utf8 collation-server=utf8_general_ci skip-character-set-client-handshake lower_case_table_names=1 重启mysql服务 开启访问权限 我们可以在开启外网MySQL访问权限在Windows机器上用GUI工具导入MySQL表数据 开启外网权限： mysql&gt; SELECT User, Host FROM mysql.user; mysql&gt; update user set host = ‘%’ where user =’root’; mysql&gt; flush privileges; 同时将etc/mysql/mysql.conf.d/mysqld.cnf 配置文件里的 bind-address = 127.0.0.1注释掉。 这样在MySQL可视化工具如Navicat上远程连接MySQL，设置utf-8编码格式后将数据表导入即可。 注意事项及其他 启动tomcat及MySQL后，通过外网便可以正常访问项目。输入用户名密码后登陆。 注意事项。 使用root用户时，应尽量减少敏感操作，如 rm ， mv ，cp -r 等命令的执行。 tomcat请使用tomcat用户启动，不建议使用root用户启动。 MySQL开启外网访问后，如果无法访问，可能是阿里云禁用了3306端口的外网访问，需要登录阿里云开启，当成功导入数据后，十分建议关闭MySQL远程访问权限。 如果要设置tomcat默认端口为80，也需要登录阿里云设置。 如果操作过程中出现异常破坏了Linux系统，可以登录阿里云进行系统还原。 部署项目时，项目的一些配置文件如数据库连接，请求地址一定要配置正确。 如果要设置tomcat默认端口为80，则需要用root启动，因为linux只有root才能启动1024以内的端口，同时需要修改tomcat server.xml配置文件及demo访问地址。 文档PPT简单阿里云主机搭建文档资源：https://download.csdn.net/download/javazwt/10448764]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>云部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java排序算法]]></title>
    <url>%2Fblog%2FJava%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[排序算法分类十种常见排序算法可以分为两大类： 非线性时间比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此称为非线性时间比较类排序。 线性时间非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此称为线性时间非比较类排序。 如下图： 排序算法复杂度及稳定性如下图： 相关概念： 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数。 排序算法冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 动图演示 代码实现JavaScript代码 12345678910111213function bubbleSort(arr) &#123; var len = arr.length; for (var i = 0; i &lt; len; i++) &#123; for (var j = 0; j &lt; len - 1 - i; j++) &#123; if (arr[j] &gt; arr[j+1]) &#123; // 相邻元素两两对比 var temp = arr[j+1]; // 元素交换 arr[j+1] = arr[j]; arr[j] = temp; &#125; &#125; &#125; return arr; &#125; Java代码 123456789101112public static int[] bubbleSort(int[] a)&#123; for(int i=0;i&lt;a.length-1;i++)&#123; for(int j=i+1;j&lt;a.length;j++)&#123; if(a[i]&gt;a[j])&#123; int temp=a[i]; a[i]=a[j]; a[j]=temp; &#125; &#125; &#125; return a; &#125; 简单选择排序（Selection Sort）选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 算法描述n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n)。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n]分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； n-1趟结束，数组有序化了。 动图演示 代码实现JavaScript代码 12345678910111213141516function selectionSort(arr) &#123; var len = arr.length; var minIndex, temp; for (var i = 0; i &lt; len - 1; i++) &#123; minIndex = i; for (var j = i + 1; j &lt; len; j++) &#123; if (arr[j] &lt; arr[minIndex]) &#123; // 寻找最小的数 minIndex = j; // 将最小数的索引保存 &#125; &#125; temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; &#125; return arr; &#125; Java代码 12345678910111213141516public static int[] simpleSelectSort(int[] a)&#123; for(int i=0;i&lt;a.length-1;i++)&#123; int min=i; for(int j=i+1;j&lt;a.length;j++)&#123; if(a[min]&gt;a[j])&#123; min=j; &#125; &#125; if(min!=i)&#123; int temp=a[i]; a[i]=a[min]; a[min]=temp; &#125; &#125; return a; &#125; 算法分析表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 插入排序（Insertion Sort）插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 动图演示 代码实现JavaScript代码 1234567891011121314function insertionSort(arr) &#123; var len = arr.length; var preIndex, current; for (var i = 1; i &lt; len; i++) &#123; preIndex = i - 1; current = arr[i]; while (preIndex &gt;= 0 &amp;&amp; arr[preIndex] &gt; current) &#123; arr[preIndex + 1] = arr[preIndex]; preIndex--; &#125; arr[preIndex + 1] = current; &#125; return arr; &#125; Java代码 1234567891011121314public static int[] straightInsertionSort(int[] a)&#123; for(int i=1;i&lt;a.length;i++)&#123; for(int j=i;j&gt;0;j--)&#123; if(a[j]&lt;a[j-1])&#123; int temp=a[j-1]; a[j-1]=a[j]; a[j]=temp; &#125;else&#123; break; &#125; &#125; &#125; return a; &#125; 算法分析插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 希尔排序（Shell Sort）1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。 算法描述先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 动图演示 代码实现JavaScript代码 123456789101112131415161718function shellSort(arr) &#123; var len = arr.length, temp, gap = 1; while (gap &lt; len / 3) &#123; // 动态定义间隔序列 gap = gap * 3 + 1; &#125; for (gap; gap &gt; 0; gap = Math.floor(gap / 3)) &#123; for (var i = gap; i &lt; len; i++) &#123; temp = arr[i]; for (var j = i-gap; j &gt; 0 &amp;&amp; arr[j]&gt; temp; j-=gap) &#123; arr[j + gap] = arr[j]; &#125; arr[j + gap] = temp; &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920public static int[] shellsSort(int a[]) &#123; int d = a.length; while (true) &#123; d = d / 2; for (int x = 0; x &lt; d; x++) &#123; for (int i = x + d; i &lt; a.length; i = i + d) &#123; int temp = a[i]; int j; for (j = i - d; j &gt;= 0 &amp;&amp; a[j] &gt; temp; j = j - d) &#123; a[j + d] = a[j]; &#125; a[j + d] = temp; &#125; &#125; if (d == 1) &#123; break; &#125; &#125; return a; &#125; 算法分析希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的。 归并排序（Merge Sort）归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 算法描述 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 动图演示 代码实现JavaScript代码 123456789101112131415161718192021222324252627282930function mergeSort(arr) &#123; // 采用自上而下的递归方法 var len = arr.length; if (len &lt; 2) &#123; return arr; &#125; var middle = Math.floor(len / 2), left = arr.slice(0, middle), right = arr.slice(middle); return merge(mergeSort(left), mergeSort(right)); &#125; function merge(left, right) &#123; var result = []; while (left.length&gt;0 &amp;&amp; right.length&gt;0) &#123; if (left[0] &lt;= right[0]) &#123; result.push(left.shift()); &#125; else &#123; result.push(right.shift()); &#125; &#125; while (left.length) result.push(left.shift()); while (right.length) result.push(right.shift()); return result; &#125; Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static int[] mergeSort(int a[])&#123; sort(a,0,1); return a; &#125; private static void sort(int[] a, int s, int len) &#123; int size = a.length; int mid = size / (len &lt;&lt; 1); int c = size &amp; ((len &lt;&lt; 1) - 1); // -------归并到只剩一个有序集合的时候结束算法-------// if (mid == 0) return; // ------进行一趟归并排序-------// for (int i = 0; i &lt; mid; ++i) &#123; s = i * 2 * len; merge(a, s, s + len, (len &lt;&lt; 1) + s - 1); &#125; // -------将剩下的数和倒数一个有序集合归并-------// if (c != 0) merge(a, size - c - 2 * len, size - c, size - 1); // -------递归执行下一趟归并排序------// sort(a, 0, 2 * len); &#125; private static void merge(int[] a, int s, int m, int t) &#123; int[] tmp = new int[t - s + 1]; int i = s, j = m, k = 0; while (i &lt; m &amp;&amp; j &lt;= t) &#123; if (a[i] &lt;= a[j]) &#123; tmp[k] = a[i]; k++; i++; &#125; else &#123; tmp[k] = a[j]; j++; k++; &#125; &#125; while (i &lt; m) &#123; tmp[k] = a[i]; i++; k++; &#125; while (j &lt;= t) &#123; tmp[k] = a[j]; j++; k++; &#125; System.arraycopy(tmp, 0, a, s, tmp.length); &#125; 算法分析归并排序是一种稳定的排序方法。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(nlogn）的时间复杂度。代价是需要额外的内存空间。 快速排序（Quick Sort）快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法描述快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 动图演示 代码实现JavaScript代码 1234567891011121314151617181920212223242526272829303132function quickSort(arr, left, right) &#123; var len = arr.length, partitionIndex, left = typeof left != 'number' ? 0 : left, right = typeof right != 'number' ? len - 1 : right; if (left &lt; right) &#123; partitionIndex = partition(arr, left, right); quickSort(arr, left, partitionIndex-1); quickSort(arr, partitionIndex+1, right); &#125; return arr; &#125; function partition(arr, left ,right) &#123; // 分区操作 var pivot = left, // 设定基准值（pivot） index = pivot + 1; for (var i = index; i &lt;= right; i++) &#123; if (arr[i] &lt; arr[pivot]) &#123; swap(arr, i, index); index++; &#125; &#125; swap(arr, pivot, index - 1); return index-1; &#125; function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; Java代码 12345678910111213141516171819202122232425262728public static int[] quickSort(int a[],int low,int high)&#123; if(low&lt;high)&#123; int privotLoc=partition(a,low,high);//将表一分为二 quickSort(a,low,privotLoc-1); quickSort(a,privotLoc+1,high); &#125; return a; &#125; private static int partition(int a[],int low,int high)&#123; int privotKey=a[low];//基准元素 while(low&lt;high)&#123; //从表的两端交替向中间扫描 while(low&lt;high&amp;&amp;a[high]&gt;=privotKey)&#123; //从high 所指位置向前搜索，至多到low+1 位置。将比基准元素小的交换到低端 --high; //从右找比基准元素少的 &#125; a[low]=a[high]; //如果比基准元素小，交换位置 a[high]=privotKey; while(low&lt;high&amp;&amp;a[low]&lt;=privotKey)&#123; ++low; //从右找比基准元素大的 &#125; a[high]=a[low]; //如果比基准元素大，交换位置 a[low]=privotKey; &#125; return low; &#125; 堆排序（Heap Sort）堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 算法描述 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 动图演示 代码实现JavaScript代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344var len; // 因为声明的多个函数都需要数据长度，所以把len设置成为全局变量 function buildMaxHeap(arr) &#123; // 建立大顶堆 len = arr.length; for (var i = Math.floor(len/2); i &gt;= 0; i--) &#123; heapify(arr, i); &#125; &#125; function heapify(arr, i) &#123; // 堆调整 var left = 2 * i + 1, right = 2 * i + 2, largest = i; if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123; largest = left; &#125; if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123; largest = right; &#125; if (largest != i) &#123; swap(arr, i, largest); heapify(arr, largest); &#125; &#125; function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; function heapSort(arr) &#123; buildMaxHeap(arr); for (var i = arr.length - 1; i &gt; 0; i--) &#123; swap(arr, 0, i); len--; heapify(arr, 0); &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public static int[] heapSort(int a[])&#123; buildMaxHeapTree(a); sort(a); return a; &#125; private static void buildMaxHeapTree(int a[])&#123; //没有子节点的才需要创建最大堆，从最后一个的父节点开始 int startIndex=getParentIndex(a.length-1); //从尾端开始创建最大堆，每次都是正确的堆 for(int i=startIndex;i&gt;=0;i--)&#123; maxHeap(a,a.length,i); &#125; &#125; private static void maxHeap(int[] data, int heapSize, int index) &#123; //当前点与左右子节点比较 int left = getChildLeftIndex(index); int right = getChildRightIndex(index); int largest = index; if (left &lt; heapSize &amp;&amp; data[index] &lt; data[left]) &#123; largest = left; &#125; if (right &lt; heapSize &amp;&amp; data[largest] &lt; data[right]) &#123; largest = right; &#125; //得到最大值后可能需要交换，如果交换了，其子节点可能就不是最大堆了，需要重新调整 if (largest != index) &#123; int temp = data[index]; data[index] = data[largest]; data[largest] = temp; maxHeap(data, heapSize, largest); &#125; &#125; private static int getParentIndex(int current)&#123; return (current-1)&gt;&gt;2; &#125; private static int getChildLeftIndex(int current)&#123; return(current&lt;&lt;1)+1; &#125; private static int getChildRightIndex(int current)&#123; return(current&lt;&lt;1)+2; &#125; private static void sort(int[] data)&#123; //末尾与头交换，交换后调整最大堆 for(int i=data.length-1;i&gt;0;i--)&#123; int temp=data[0]; data[0]=data[i]; data[i]=temp; maxHeap(data,i,0); &#125; &#125; 计数排序（Counting Sort）计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 动图演示 代码实现JavaScript代码 12345678910111213141516171819202122function countingSort(arr, maxValue) &#123; var bucket = new Array(maxValue + 1), sortedIndex = 0; arrLen = arr.length, bucketLen = maxValue + 1; for (var i = 0; i &lt; arrLen; i++) &#123; if (!bucket[arr[i]]) &#123; bucket[arr[i]] = 0; &#125; bucket[arr[i]]++; &#125; for (var j = 0; j &lt; bucketLen; j++) &#123; while(bucket[j] &gt; 0) &#123; arr[sortedIndex++] = j; bucket[j]--; &#125; &#125; return arr; &#125; Java代码 123456789101112131415161718192021222324public static int[] countSort(int[]a)&#123; int b[] = new int[a.length]; int max = a[0],min = a[0]; for(int i:a)&#123; if(i&gt;max)&#123; max=i; &#125; if(i&lt;min)&#123; min=i; &#125; &#125;//这里k的大小是要排序的数组中，元素大小的极值差+1 int k=max-min+1; int c[]=new int[k]; for(int i=0;i&lt;a.length;++i)&#123; c[a[i]-min]+=1;//优化过的地方，减小了数组c的大小 &#125; for(int i=1;i&lt;c.length;++i)&#123; c[i]=c[i]+c[i-1]; &#125; for(int i=a.length-1;i&gt;=0;--i)&#123; b[--c[a[i]-min]]=a[i];//按存取的方式取出c的元素 &#125; return b; &#125; 算法分析计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 桶排序（Bucket Sort）桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 算法描述 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来。 图片演示 代码实现JavaScript代码 12345678910111213141516171819202122232425262728293031323334353637383940function bucketSort(arr, bucketSize) &#123; if (arr.length === 0) &#123; return arr; &#125; var i; var minValue = arr[0]; var maxValue = arr[0]; for (i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &lt; minValue) &#123; minValue = arr[i]; // 输入数据的最小值 &#125; else if (arr[i] &gt; maxValue) &#123; maxValue = arr[i]; // 输入数据的最大值 &#125; &#125; // 桶的初始化 var DEFAULT_BUCKET_SIZE = 5; // 设置桶的默认数量为5 bucketSize = bucketSize || DEFAULT_BUCKET_SIZE; var bucketCount = Math.floor((maxValue - minValue) / bucketSize) + 1; var buckets = new Array(bucketCount); for (i = 0; i &lt; buckets.length; i++) &#123; buckets[i] = []; &#125; // 利用映射函数将数据分配到各个桶中 for (i = 0; i &lt; arr.length; i++) &#123; buckets[Math.floor((arr[i] - minValue) / bucketSize)].push(arr[i]); &#125; arr.length = 0; for (i = 0; i &lt; buckets.length; i++) &#123; insertionSort(buckets[i]); // 对每个桶进行排序，这里使用了插入排序 for (var j = 0; j &lt; buckets[i].length; j++) &#123; arr.push(buckets[i][j]); &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526272829public static void basket(int data[])//data为待排序数组 &#123; int n = data.length; int bask[][] = new int[10][n]; int index[] = new int[10]; int max = Integer.MIN_VALUE; for (int i = 0; i &lt; n; i++) &#123; max = max &gt; (Integer.toString(data[i]).length()) ? max : (Integer.toString(data[i]).length()); &#125; String str; for (int i = max - 1; i &gt;= 0; i--) &#123; for (int j = 0; j &lt; n; j++) &#123; str = ""; if (Integer.toString(data[j]).length() &lt; max) &#123; for (int k = 0; k &lt; max - Integer.toString(data[j]).length(); k++) str += "0"; &#125; str += Integer.toString(data[j]); bask[str.charAt(i) - '0'][index[str.charAt(i) - '0']++] = data[j]; &#125; int pos = 0; for (int j = 0; j &lt; 10; j++) &#123; for (int k = 0; k &lt; index[j]; k++) &#123; data[pos++] = bask[j][k]; &#125; &#125; for (int x = 0; x &lt; 10; x++) index[x] = 0; &#125; &#125; 算法分析桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 基数排序（Radix Sort）基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 算法描述 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 动图演示 代码实现JavaScript代码 123456789101112131415161718192021222324var counter = []; function radixSort(arr, maxDigit) &#123; var mod = 10; var dev = 1; for (var i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123; for(var j = 0; j &lt; arr.length; j++) &#123; var bucket = parseInt((arr[j] % mod) / dev); if(counter[bucket]==null) &#123; counter[bucket] = []; &#125; counter[bucket].push(arr[j]); &#125; var pos = 0; for(var j = 0; j &lt; counter.length; j++) &#123; var value = null; if(counter[j]!=null) &#123; while ((value = counter[j].shift()) != null) &#123; arr[pos++] = value; &#125; &#125; &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526public static void sort(int[] number, int d) //d表示最大的数有多少位 &#123; int k = 0; int n = 1; int m = 1; //控制键值排序依据在哪一位 int[][] temp = new int[10][number.length]; //数组的第一维表示可能的余数0-9 int[] order = new int[10]; //数组orderp[i]用来表示该位是i的数的个数 while (m &lt;= d) &#123; for (int i = 0; i &lt; number.length; i++) &#123; int lsd = ((number[i] / n) % 10); temp[lsd][order[lsd]] = number[i]; order[lsd]++; &#125; for (int i = 0; i &lt; 10; i++) &#123; if (order[i] != 0) for (int j = 0; j &lt; order[i]; j++) &#123; number[k] = temp[i][j]; k++; &#125; order[i] = 0; &#125; n *= 10; k = 0; m++; &#125; &#125; 算法分析基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n&gt;&gt;k，因此额外空间需要大概n个左右。 测试新建Test类，测试十种排序算法时间消耗。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119 public static void main(String[] args) &#123; // int arrayLength=10; // int range=100; int arrayLength=100000; int range=100000; //生成一个指定长度数据的int数组 int[] a=new int[arrayLength]; for (int i=0;i&lt;arrayLength;i++)&#123; a[i]=(int)(Math.random()*range); &#125; //创建十个数组用于测试，否则会冲突 int[] a1= a.clone(); int[] a2= a.clone(); int[] a3= a.clone(); int[] a4= a.clone(); int[] a5= a.clone(); int[] a6= a.clone(); int[] a7= a.clone(); int[] a8= a.clone(); int[] a9= a.clone(); int[] a10= a.clone(); System.out.println(a1.toString()); System.out.println(a2.toString()); System.out.println(a3.toString()); System.out.println(a4.toString()); System.out.println(a5.toString()); System.out.println(a6.toString()); System.out.println(a7.toString()); System.out.println(a8.toString()); System.out.println(a9.toString()); System.out.println(a10.toString()); System.gc(); //直接插入排序法 long starta1=System.currentTimeMillis(); a1= StraightInsertionSort.straightInsertionSort(a1); long enda1=System.currentTimeMillis(); System.out.println("插入排序法耗时："+(enda1-starta1)+"ms"); //冒泡排序法 long starta2=System.currentTimeMillis(); a2= BubbleSort.bubbleSort(a2); long enda2=System.currentTimeMillis(); System.out.println("冒泡排序法耗时："+(enda2-starta2)+"ms"); //直接(简单)选择排序法 long starta3=System.currentTimeMillis(); a3= SimpleSelectionSort.simpleSelectSort(a3); long enda3=System.currentTimeMillis(); System.out.println("直接选择排序法耗时："+(enda3-starta3)+"ms"); //堆排序 long starta4=System.currentTimeMillis(); a4= HeapSort.heapSort(a4); long enda4=System.currentTimeMillis(); System.out.println("堆排序法耗时："+(enda4-starta4)+"ms"); //归并排序 long starta5=System.currentTimeMillis(); a5= MergeSort.mergeSort(a5); long enda5=System.currentTimeMillis(); System.out.println("归并排序法耗时："+(enda5-starta5)+"ms"); //快速排序算法 long starta6=System.currentTimeMillis(); a6= QuickSort.quickSort(a6,0,a6.length-1); long enda6=System.currentTimeMillis(); System.out.println("快速排序耗时："+(enda6-starta6)+"ms"); //计数排序算法 long starta7=System.currentTimeMillis(); a7= CountingSort.countSort(a7); long enda7=System.currentTimeMillis(); System.out.println("计数排序耗时："+(enda7-starta7)+"ms"); //希尔排序算法 long starta8=System.currentTimeMillis(); a8= ShellsSort.shellsSort(a8); long enda8=System.currentTimeMillis(); System.out.println("希尔排序耗时："+(enda8-starta8)+"ms"); //桶排序算法 long starta9=System.currentTimeMillis(); a9= BucketSort.basket(a9); long enda9=System.currentTimeMillis(); System.out.println("桶排序耗时："+(enda9-starta9)+"ms"); //基数排序算法 long starta10=System.currentTimeMillis(); a10= RadixSort.sort(a10,6); long enda10=System.currentTimeMillis(); System.out.println("基数排序耗时："+(enda10-starta10)+"ms"); // ArrayHelper.printLine(a1); // System.out.println(); // ArrayHelper.printLine(a2); // System.out.println(); // ArrayHelper.printLine(a3); // System.out.println(); // ArrayHelper.printLine(a4); // System.out.println(); // ArrayHelper.printLine(a5); // System.out.println(); // ArrayHelper.printLine(a6); // System.out.println(); // ArrayHelper.printLine(a7); // System.out.println(); // ArrayHelper.printLine(a8); // System.out.println(); // ArrayHelper.printLine(a9); // System.out.println(); // ArrayHelper.printLine(a10); System.gc(); &#125; 运行结果： 其他 本文为转载，十种算法的Java实现方式及测试为原创。 本文转载自：https://www.cnblogs.com/onepixel/articles/7674659.html 文中涉及到的Java代码地址：https://github.com/JavaZWT/sort]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JavaScript</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之代理模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言代理模式：即通过代理对象访问目标对象，实现目标对象的方法。这样做的好处是：可以在目标对象实现的基础上，增强额外的功能操作，实现对目标功能的扩展。 这涉及到一个编程思想：不要随意去修改别人已经写好的代码或者方法（有坑）。如果需要修改，可以通过代理模式实现。 写法实现代理模式通常有三种实现写法：静态代理、动态代理、Cglib代理。 我们依次说下这三种代理模式： 我们现有 AnimalsDao接口和其实现AnimalsDaoImpl实现类，有两个方法，run和eat。 12345public interface AnimalsDao &#123; //提供两个方法 void run(); void eat(); &#125; 12345678910public class AnimalsDaoImpl implements AnimalsDao&#123; @Override public void run() &#123; System.out.println("run"); &#125; @Override public void eat() &#123; System.out.println("eat"); &#125; &#125; 现在我们想在run或者eat方法里之前做一些操作。 静态代理12345678910111213141516171819public class StaticProxy implements AnimalsDao&#123; @Override public void run() &#123; System.out.println("StaticProxy---------&gt;"); dao.run(); &#125; @Override public void eat() &#123; System.out.println("StaticProxy---------&gt;"); dao.eat(); &#125; private AnimalsDao dao; public StaticProxy(AnimalsDao dao)&#123; this.dao=dao; &#125; &#125; 说明：静态代理通过实现目标对象接口，然后调用相同方法来实现代理。这种方式的缺点显而易见，当目标对象接口方法变动时，直接影响到代理类，需要对代理类进行修改，十分不方便。而且如果目标对象接口方法较多时，代理类也十分臃肿，不便维护。 动态代理1234567891011121314151617181920public class DynamicProxy &#123; //要代理的对象 private Object obj; public DynamicProxy(Object obj)&#123; this.obj=obj; &#125; public Object getObjProxyIntance()&#123; return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //执行目标对象方法 System.out.println("DynamicProxy---------&gt;"); return method.invoke(obj,args); &#125; &#125;); &#125; &#125; 说明：动态代理模式主要借助JDK代理对象API java.lang.reflect.Proxy来实现的，所以也称作JDK代理。我们看一下JDK这个类，其中重要的一个方法如下： 这个方法的三个参数： ClassLoader loader 目标对象类加载器 Class&lt;?&gt;[] interfaces 目标对象接口类型 InvocationHandler h 事物处理，在这里面可以实现自己想要的逻辑 根据以上，可以看出动态代理实现要求目标对象必须有实现接口。代理类不必实现接口。 Cglib代理要实现Cglib代理，必须引入cglib.jar 包，由于Spring-core包中已经包含了cglib功能，且大部分Java项目均引入了spring 相关jar包，这边使用spring的cglib来讲解。（他俩实现方式都是一样的） 1234567891011121314151617181920212223242526public class CglibProxy implements MethodInterceptor &#123; //目标对象 private Object obj; public CglibProxy(Object obj)&#123; this.obj=obj; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(obj.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("CglibProxy---------&gt;"); return method.invoke(obj,objects); &#125; &#125; 说明：可以看出，Cglib代理模式实现不需要目标对象一定实现接口，故目标对象如果没有实现接口，可以使用cglib代理模式。其实Spring的代理模式也是这么实现的。 应用实例Spring的代理模式：如果容器目标对象有接口，使用JDK动态代理，如果没有接口，使用cglib动态代理。 参考资料 https://www.cnblogs.com/cenyu/p/6289209.html Spring 4.0 源代码 GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之工厂模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言工厂模式，顾名思义，即用来实例化一个个对象的模式。把相同类别的对象用工厂模式代替new生成，提高代码质量和系统扩展性。 写法分类 工厂模式的写法通常分为四类，简单工厂、多方法工厂、普通工厂、抽象工厂。 在分别说明他们之前，我们先创建几个对象。 抽象的Animals类，Plants类 1234public abstract class Animals &#123; //动物的描述 public abstract String desc(); &#125; 及Animals的三个基本实现，Tiger类、Bird类、Fish类，Plants的实现Tree类。 1234567public class Tiger extends Animals&#123; @Override public String desc() &#123; System.out.println("这是一只老虎"); return "这是一只老虎"; &#125; &#125; 1234567public class Bird extends Animals &#123; @Override public String desc() &#123; System.out.println("这是一只鸟"); return "这是一只鸟"; &#125; &#125; 1234567public class Fish extends Animals &#123; @Override public String desc() &#123; System.out.println("这是一条鱼"); return "这是一条鱼"; &#125; &#125; 1234public abstract class Plants &#123; //植物的描述 public abstract String desc(); &#125; 123456public class Tree extends Plants &#123; @Override public String desc() &#123; return "这是一棵树"; &#125; &#125; 下面来说说这四种工厂模式。 简单工厂模式1234567891011121314151617 public class AnimalsFactory &#123; public static final String Tiger="Tiger"; public static final String Bird="Bird"; public static final String Fish="Fish"; public static Animals getAnimal(String name)&#123; switch (name)&#123; case Tiger: return new Tiger(); case Bird: return new Bird(); case Fish: return new Fish(); default: return null; &#125; &#125; &#125; 调用：12Animals a=AnimalsFactory.getAnimal("Tiger"); a.desc(); 说明：简单工厂模式易于理解，但扩展性差，如我想在增加一种动物，则还需要修改逻辑内容。而且如果设置额外入参，对参数的管理也较为不便。 多方法工厂模式1234567891011public class AnimalsFactory &#123; public static Animals getTiger()&#123; return new Tiger(); &#125; public static Animals getBird()&#123; return new Bird(); &#125; public static Animals getFish()&#123; return new Fish(); &#125; &#125; 调用：12Animals b= AnimalsFactory.getFish(); b.desc(); 说明：这种模式使用方便，容错率高，且可适应不同需求，如我现在想创建一个羽毛为蓝色的小鸟，明显多方法工厂模式比简单工厂模式实现简单。 普通工厂模式123public abstract class PlantsFactory &#123; public abstract Plants getPlant(); &#125; 123456public class TreeFactory extends PlantsFactory&#123; @Override public Plants getPlant() &#123; return new Tree(); &#125; &#125; 调用：12PlantsFactory p=new TreeFactory(); p.getPlant().desc(); 说明：这种工厂模式划分成两层：抽象工厂层+具体的工厂子类层。 当我们不需要Tree，或者需要其他时，都可以新增或删除一个类实现，这种工厂模式看着很简洁明了易于理解。 抽象工厂模式12345678910public abstract class AbstractFactory&lt;T&gt; &#123; static&#123; // do something &#125; public void doSomeThing()&#123; //do something &#125; private int age; public abstract T getT(); &#125; 123456public class BirdFactory extends AbstractFactory&lt;Animals&gt; &#123; @Override public Animals getT() &#123; return new Bird(); &#125; &#125; 123456public class TreeFactory extends AbstractFactory&lt;Plants&gt; &#123; @Override public Plants getT() &#123; return new Tree(); &#125; &#125; 调用：123AbstractFactory factory= new TreeFactory(); Tree t=(Tree)factory.getT(); t.desc(); 说明：可以看出，这种方法把工厂本身也抽象了，如果对于两个或多个简单工厂模式，相似点较多，比如PlantsFactory和AnimalsFactory，作为生物，它们也有相似之处，dosomething部分的代码。其实抽象工厂模式跟普通工厂模式的区别就在于把工厂也抽象了一层。 工厂模式实例 Spring Bean对象的创建 抽象工厂模式的使用 线程池 Executors 多方法工厂模式的使用 其他：Spring框架中大量使用了工厂模式，单例模式等设计模式，有时间阅读些源代码，对技术能力的提升有很大帮助。 参考资料 https://blog.csdn.net/zxt0601/article/details/52798423 Spring 4.0 源码 JDK8源码 GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之单例模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言单例模式，顾名思义，即只产生唯一实例的设计模式。一个类只有一个实例,即一个类只有一个对象实例。 优点1. 减少系统开销及资源消耗。 2. 设置全局访问点，优化共享资源的使用。 注：对于以上优点，我们可以拿Java配置读取举例，对于固定的配置文件，我们完全可以设计一个单例类，读取配置文件，而避免每次都读取文件造成的系统开销，同时也优化了配置文件的使用。 应用场景 Servlet 开源框架Spring中的默认bean配置模式 数据库连接池 项目中对于配置文件的处理等等 写法分类懒汉式单例模式（非线程安全）123456789101112public class Demo_Singleton_01 &#123; private static Demo_Singleton_01 instance=null; private Demo_Singleton_01()&#123; &#125; public static Demo_Singleton_01 getInstance()&#123; if(instance==null)&#123; instance=new Demo_Singleton_01(); &#125; return instance; &#125; &#125; 说明：这种写法不常用，虽然实现了懒加载（lazy-loading），但未考虑线程安全问题，不适用于多线程情况。 懒汉式单例模式（线程安全）123456789101112public class Demo_Singleton_02 &#123; private static Demo_Singleton_02 instance=null; public static synchronized Demo_Singleton_02 getInstance()&#123; if(instance==null)&#123; instance=new Demo_Singleton_02(); &#125; return instance; &#125; private Demo_Singleton_02()&#123; &#125; &#125; 说明：相对于demo1，这种写法实现了懒加载（lazy-loading），也考虑到线程安全问题，可适用于多线程情况。 饿汉式单例模式123456789public class Demo_Singleton_03 &#123; private static final Demo_Singleton_03 instance=new Demo_Singleton_03(); private Demo_Singleton_03()&#123; &#125; public static Demo_Singleton_03 getInstance()&#123; return instance; &#125; &#125; 说明：饿汉式单例模式的明显缺点是无法实现懒加载，在类被创建后就创建了唯一实例。 饿汉式单例模式（变种）123456789101112public class Demo_Singleton_04 &#123; private static Demo_Singleton_04 instance=null; static&#123; instance=new Demo_Singleton_04(); &#125; private Demo_Singleton_04()&#123; &#125; public static Demo_Singleton_04 getInstance()&#123; return instance; &#125; &#125; 说明：可以看到这种方法和demo3的性质是一样的，只不过改为了静态块加载唯一实例。 静态内部类模式1234567891011public class Demo_Singleton_05 &#123; private static class Singleton&#123; private static final Demo_Singleton_05 instance=new Demo_Singleton_05(); &#125; private Demo_Singleton_05()&#123; &#125; public static final Demo_Singleton_05 getInstance()&#123; return Singleton.instance; &#125; &#125; 说明：这种方法既可以实现懒加载，也能保证线程安全。 双重锁单例模式12345678910111213141516 public class Demo_Singleton_06 &#123; private static volatile Demo_Singleton_06 instance=null; private Demo_Singleton_06()&#123; &#125; public static Demo_Singleton_06 getInstance()&#123; if(instance==null)&#123; synchronized(Demo_Singleton_06.class)&#123; if(instance==null)&#123; instance=new Demo_Singleton_06(); &#125; &#125; &#125; return instance; &#125; &#125; 说明：可以看出，双重锁单例模式只有在第一次创建对象的时候才同步，因为创建对象后就没必要了，（实例唯一）相比懒汉式，效率明显提高，相比饿汉式，不用在类加载的时候就创建实例，而是等到什么时候想调用的时候在创建实例。线程安全，效率高。 枚举法123456public enum Demo_Singleton_07 &#123; instance; public void doSomething()&#123; &#125; &#125; 说明：枚举法代码十分直观，线程安全，但是无法实现懒加载。 参考资料 https://blog.csdn.net/goodlixueyong/article/details/51935526 https://www.cnblogs.com/Ycheng/p/7169381.html GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2Fhello-world.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>学习</category>
      </categories>
  </entry>
</search>
