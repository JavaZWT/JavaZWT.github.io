<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2Fblog%2FHTTP%E5%8D%8F%E8%AE%AE20181229%2F</url>
    <content type="text"><![CDATA[前言超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，所有的WWW文件都必须遵守这个标准。 今天我们来详细了解下它的一些特点及Java里对此协议的解析处理。 正文我们发送一个HTTP 请求，在浏览器中可以看到HTTP协议的一些信息。 Request Headers： POST /test HTTP/1.1 POST 表示HTTP的请求方法类型。 /test 表示请求URI。 HTTP/1.1 表示协议和协议版本。 说明：HTTP协议里目前定义了8中请求方法类型。 OPTIONS：返回服务器针对特定资源所支持的HTTP请求方法，也可以利用向web服务器发送“*”的请求来测试服务器的功能性。 HEAD：服务器返回与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以再不必传输整个响应内容的情况下，就可以获取包含在响应小消息头中的元信息。 GET：向特定的资源发出请求。它本质就是发送一个请求来取得服务器上的某一资源。 POST：向指定资源提交数据进行处理请求，数据被包含在请求体中，POST请求可能会导致新的资源的建立或已有资源的修改。 PUT：向指定资源位置上传其最新内容。 DELETE：请求服务器删除指定的资源。 TRACE：回显服务器收到的请求，主要用于测试或诊断 CONNECT：HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 Host: localhost:8080 表示服务器域名地址 Connection: keep-alive 表示连接类型，一直保持连接。 Content-Length: 11 表示参数长度 Accept: application/json, text/javascript, */*; q=0.01 表示接受的数据类型，逗号分隔，优先支持程度从前到后，q表示权重， 0&lt;=q&lt;=1， 权重越大，分号前面这一项支持度越好，0表示不支持，不指定q值默认为1。 Origin: http://localhost:8080 表示源数据地址。 X-Requested-With: XMLHttpRequest XMLHttpRequest对象，可以实现局部更新，ajax使用。 User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1 表示用户代理信息，浏览器信息等。 Content-Type: application/x-www-form-urlencoded; charset=UTF-8 提交数据类型，编码格式。 Referer: http://localhost:8080/ 表示引用源地址 Accept-Encoding: gzip, deflate, br 表示浏览器支持的压缩编码格式。当前浏览器支持 gzip, deflate, br 三种压缩算法。 Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 可以接受的语言。 Cookie: Idea-6ad4ef07=ced8796b-12ab-4db6-9bd7-556be6c5132b; JSESSIONID=3F1698BD54B15FFE91EC948154761956 Cookie信息。 Status Code: 200 状态码。 目前总共有41种状态码。 在HttpServletResponse由定义，有兴趣的可以看看。 主要状态码及含义如下： 状态码 含义 100 表示客户端可以继续 101 表示服务器正在根据升级头切换协议 200 表示请求成功 201 表示请求成功，并在服务器上创建了一个新资源 202 表示服务器接受了处理请求，但未完成 203 表示客户端提供的元信息并非来自服务器 204 表示请求成功，但没有返回新信息 205 表示代理可能被重置导致发送了文档视图 206 表示服务器已经完成了对资源的部分GET请求 300 表示所请求的资源对应一组资源，组里每一种表示都有其特定的位置 301 表示资源已经永久地移动到一个新的位置，并且将来的请求应使用一个新的URI 302 表示资源暂时移动到其它的URI下 303 表示可以在其它的URI下找到对请求的响应 304 表示GET操作发现资源可用且未修改 305 表示资源必须通过代理访问 307 表示资源暂时在其它的URI下，但是这个URI地址应该在响应的时候给出 400 表示客户端发送的请求语法错误 401 表示请求需要HTTP身份验证 402 保留以备将来使用 403 表示服务器接受了请求，但拒绝执行 404 表示请求的资源不可用或者找不到 405 表示对于要请求的资源，请求中指定的方法是不允许的 406 表示请求标识的资源仅能够根据请求中发送的accept标头生成具有不可接受的内容特征的响应实体 407 表示客户端必须先通过代理进行身份验证 408 表示客户端在服务器准备等待的时间内没有生成请求 409 表示由于与资源的当前状态发生冲突，请求无法完成 410 表示该资源在服务器上不再可用，且不知道转发地址 411 表示如果没有定义Content-Length，则无法处理请求 412 表示在服务器上测试时，在一个或多个请求头字段中给出的前提条件评估不通过 413 表示服务器拒绝处理请求，因为请求实体大于服务器能够处理的大小 414 表示服务器拒绝服务请求，因为request-uri长度超过了服务器能够处理的长度 415 表示服务器拒绝服务请求，因为请求的实体的格式不受请求方法的请求资源支持 416 表示服务器不能为请求的字节范围提供服务 417 表示服务器不能满足Expect请求头中给出的期望 500 表示服务器内部的一个错误，它阻止了服务器完成请求 501 表示服务器不支持满足请求所需的功能 502 表示服务器作为代理或网关服务器时接收到了无效响应 503 表示服务器暂时过载，无法处理请求 504 表示服务器作为网关或代理服务器时没有及时收到来自上游服务器的响应 505 表示服务器不支持或拒绝支持请求消息中使用的HTTP协议版本 我们来看一下后端Java部分的一些处理吧。 找到HttpServlet，可以看到它定义的几种请求方法。 同时GET，POST，PUT，DELETE都需要被实现。HEAD、OPTIONS、TRACE已被实现。 我们根据上面的知识，可以自己实现一个HttpServlet，同时可以实现其它的方法，如下： 我们新建HttpTestServlet 抽象类，新增方法 doCompress。 这样我们写一个测试Servlet，实现doCompress方法。 发送ajax，type 为 COMPRESS。 如下：可以看到请求方法变成了COMPRESS。 总结通过对HTTP协议的理解，加深了对HTTP协议的认识与认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之分布式锁]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%8120181226%2F</url>
    <content type="text"><![CDATA[前言Zookeeper是可以用来实现分布式锁的。 要了解它，我们先简单说下分布式锁吧。 我们知道，在系统中，当我们访问公共资源并对资源进行一些操作时，为防止出现问题，需要对公共资源依次访问，如常见的多线程售票模型等。由于在一个系统中，我们可以使用锁（如ReentrantLock）或者synchronized关键字等Java方法处理。 但是，当系统逐渐由单系统转换为分布式系统、微服务时，情况就变得复杂了，比如有某共享资源，比如有1个奖品，一个应用查询到有奖品并尝试发给用户A，另一个应用也查询到有奖品并尝试发给用户B，这样A，B均显示有奖品，实际上我们的奖品数量是不足的。如常见的秒杀系统，抽奖系统等。 这时候就需要一种全局的互斥机制来控制应用对共享资源的访问，这就是所谓的分布式锁。 PS：分布式锁的实现也可以基于缓存（如Redis）实现，亦可以通过数据库（乐观锁等）实现，实际中要确实使用到分布式锁，基于缓存的实现还是要偏多一些的。 根据上面所述，下面的图是比较好理解的。 我们再来说下分布式锁应具备的一些特点。 同一时间只允许一台机器（服务）的一个线程执行。 为整个系统必要业务提供服务，应当是高可用的。 性能应得以保证，不能在获取锁和释放锁过程中浪费太多资源或时间。 分布式锁应当具备失效机制，避免死锁发生。 应当具有可重入特性。 应当有非阻塞的特点，某个服务没有获取到锁，应返回获取失败，不能阻塞。 下面我们来用Zookeeper实现我们的分布式锁。 正文我们在之前封装framework-zookeeper时，用到了下面这个依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 这里面有一个类InterProcessMutex，这是分布式锁使用的关键类。 PS：其实它已经实现了分布式锁，我们来使用下它吧。 我们创建一个Test，如下： 1234567891011121314151617181920212223242526@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; ExecutorService executorService = Executors.newCachedThreadPool(); static int TEST = 5; @Test public void test1() throws Exception&#123; for(int i=0;i&lt;10;i++)&#123; executorService.execute(()-&gt;&#123; try&#123; if(TEST&gt;0)&#123; doSomething(); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125; Thread.sleep(100000);&#125;public void doSomething() throws Exception&#123; Thread.sleep(1000); TEST--; System.out.println(Thread.currentThread().getId()+"--"+Thread.currentThread().getName()+"执行,TEST="+TEST); &#125;&#125; 考虑到多应用分布式比较麻烦，我这里的Test使用多线程模拟分布式请求，用了线程池管理了10个线程，假设TEST静态变量为共享资源，如果TEST数量大于0的时候，我们执行doSomething方法，假设这个方法执行需要一秒，执行后TEST减一。 我们运行Test后，可以看到输出结果。 可以看到出现了负数。 我们引入InterProcessMutex，在判断TEST之前对其加分布式锁，锁的zk基路径我们定为/zwt/lock。调用acquire获取锁，完成业务逻辑后调用release方法释放锁。 1234567891011121314151617181920@AutowiredCuratorZKClient client;@Testpublic void test1() throws Exception&#123; for(int i=0;i&lt;10;i++)&#123; executorService.execute(()-&gt;&#123; try&#123; InterProcessMutex interProcessMutex = new InterProcessMutex(client.getCuratorFramework(),"/zwt/lock"); interProcessMutex.acquire(); if(TEST&gt;0)&#123; doSomething(); &#125; interProcessMutex.release(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125; Thread.sleep(100000);&#125; 运行Test查看结果。 可以看到结果有序的减一最后到0结束。说明了InterProcessMutex实现了我们的分布式锁的功能。 它是如何实现的呢？ 我们在上面的doSomething代码里加一些输出。如下： 12345678910public void doSomething() throws Exception&#123; Thread.sleep(1000); List&lt;String&gt; list=client.getNodes("/zwt/lock"); list.forEach((e)-&gt;&#123; System.out.print(e+" "); &#125;); System.out.println(); TEST--; System.out.println(Thread.currentThread().getId()+"--"+Thread.currentThread().getName()+"执行,TEST="+TEST);&#125; 继续测试，如下结果： 数据大致如下： 可以看到，Zookeeper创建了10个临时顺序节点，每次会找到最小的节点并删除。其实这就是InterProcessMutex这个类的实现分布式锁的原理。 我们可以看下它的相关代码。 我们从acquire方法看起，调用了internalLock方法，而后调用了attemptLock方法，这个方法会通过createsTheLock去创建锁。 可以看到createsTheLock方法里创建了临时有序节点。 再看下internalLockLoop这个方法，有些大。 可以看到会拿到有序的子节点，getSortedChildren。 然后尝试去获取锁（从最小的节点开始），getsTheLock会先获取比自己小的节点，要是自己是最小的节点就会获得锁。 拿到后就设置haveTheLock为true，没有拿到，就添加watcher，监听比自己小的节点。然后根据设置的等待时间判断是否超时从而进行等待或者退出。 最后，如果到了时间或者出现异常，doDelete为true，就会删除节点。 再来看下release方法，里面的主要方法releaseLock，可以看到它调用了上面的deleteOurPath方法删除创建的临时节点。 在锁的获取和释放方法里可以看到下面这些地方，它可以保证我们的分布式锁具有可重入的性质。其通过lockCount（AtomicInteger ）实现的，统计重入次数。 1final AtomicInteger lockCount = new AtomicInteger(1); Zookeeper分布式锁的基本内容就是这些了，我们来总结下Zookeeper分布式锁的步骤： 指定一个存放锁目录（这儿我们指定的/zwt/lock）。 线程A想要获取锁，就需要在该目录下创建临时有序节点。 获取该目录下的所有子节点，然后获取比自己小的兄弟节点，如果不存在，说明自己是最小节点，那么就去获得锁。 线程B同线程A，创建好节点后获取目录下所有子节点，判断自己不是最小的，就会对获得锁的节点添加监听。 线程A处理完后释放锁，删除自己的节点，并通知，线程B监听后判断自己是不是最小节点，是的话会获取锁，不是的话在添加对当前获得锁的线程的监听。 通过上面我们可以看到Zookeeper分布式锁的一些优点，如高可用性（由Zookeeper保证）、可重入性、不会出现死锁（临时节点程序出现异常断开连接后会被删除也就失去了锁）等。 和一些缺点，如需要创建临时节点、删除临时节点，性能上肯定有一些影响。 其它我们可以在对其进行简单封装形成自己的分布式锁工具类。 相关代码如下： 提供一个分布式锁的接口。 1234567891011121314151617181920public interface DistributedReentrantLock &#123; /** * 获得锁 * @return * @throws InterruptedException */ boolean tryLock() throws InterruptedException; /** * 获得锁 * @param timeout * @param unit * @return * @throws InterruptedException */ boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; /** * 解除锁 */ void unlock();&#125; 同时我们使用Zookeeper的InterProcessMutex去完成相关实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class ZKDistributedReentrantLock implements DistributedReentrantLock &#123; private static final Logger logger = LoggerFactory.getLogger(ZKDistributedReentrantLock.class); /** * 线程池 */ private static final ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor( 10, new BasicThreadFactory.Builder().namingPattern("scheduled-pool-%d").build() ); /** * 所有锁的根节点 */ public static final String ROOT_PATH = "/LOCK/"; /** * 每次延迟清理PERSISTENT节点的时间 毫秒 */ private static final long DELAY_TIME_FOR_CLEAN = 1000; /** * zk 共享锁实现 */ private InterProcessMutex interProcessMutex; /** * 锁的ID,对应zk一个PERSISTENT节点,下挂EPHEMERAL节点. */ private String path; /** * zk的客户端 */ private CuratorFramework client; private volatile boolean isLockSuccess; public ZKDistributedReentrantLock(CuratorFramework client, String lockId) &#123; this.client = client; this.path = ROOT_PATH + lockId; interProcessMutex = new InterProcessMutex(client, this.path); &#125; public ZKDistributedReentrantLock(CuratorZKClient zkClient, String lockId) &#123; this.client = zkClient.getCuratorFramework(); this.path = ROOT_PATH + lockId; interProcessMutex = new InterProcessMutex(client, this.path); &#125; /** * 获取锁 * @return * @throws InterruptedException */ @Override public boolean tryLock() throws InterruptedException &#123; return tryLock(-1, null); &#125; /** * 获取锁 * @param timeout * @param unit * @return * @throws InterruptedException */ @Override public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; try &#123; isLockSuccess = interProcessMutex.acquire(timeout, unit); logger.debug("&#123;&#125; lock result:&#123;&#125;",this.path,isLockSuccess); return isLockSuccess; &#125; catch (InterruptedException e) &#123; throw e; &#125; catch (Exception e) &#123; logger.error(e.getMessage(),e); throw new RuntimeException(e.getMessage(),e); &#125; &#125; /** * 释放锁 */ @Override public void unlock() &#123; if(isLockSuccess) &#123; try &#123; isLockSuccess = false; interProcessMutex.release(); &#125; catch (Throwable e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; executorService.schedule(new Cleaner(client, path), DELAY_TIME_FOR_CLEAN, TimeUnit.MILLISECONDS); &#125; logger.debug("&#123;&#125; success unlock.",this.path); &#125; &#125; static class Cleaner implements Runnable &#123; private CuratorFramework client; private String path; public Cleaner(CuratorFramework client, String path) &#123; this.client = client; this.path = path; &#125; @Override public void run() &#123; try &#123; List list = client.getChildren().forPath(path); if (list == null || list.isEmpty()) &#123; client.delete().forPath(path); &#125; &#125; catch (KeeperException.NoNodeException | KeeperException.NotEmptyException e1) &#123; //nothing &#125; catch (Exception e) &#123; //准备删除时,正好有线程创建锁 logger.error(e.getMessage(), e); &#125; &#125; &#125;&#125; 实现还是比较简单的，tryLock方法主要就是使用了interProcessMutex的acquire方法，成功后记录isLockSuccess状态，失败后除了调用release方法、把isLockSuccess变为false外，还应尝试清除刚才已经创建的业务lockId节点(线程池)。 测试相关代码： 12345678910111213ZKConfig zkConfig = new ZKConfig();//....CuratorZKClient zkClient=new CuratorZKClient(zkConfig);DistributedReentrantLock lock = new ZKDistributedReentrantLock(zkClient,"test");try&#123; if(lock.tryLock())&#123; //doSomething &#125;&#125;catch (Exception e)&#123; //...&#125;finally &#123; lock.unlock();&#125; 总结通过对Zookeeper实现分布式锁的学习理解，我们又看到了Zookeeper的另外一个用途，对Zookeeper有了更深入的理解，也是蛮不错的一次学习体验。 有时间我会在研究下其它方式实现的分布式锁及其一些特点。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>分布式锁</tag>
        <tag>InterProcessMutex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之配置中心]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%8320181225%2F</url>
    <content type="text"><![CDATA[前言根据Zookeeper的一些特点，它是可以作为配置中心使用的。 何为配置中心？ 我们在项目开发维护过程中会有很多公共变量或资源，需要统一管理，以前我们把它们写在程序公共类或者配置文件中，可是这样以后有变动，程序就需要重新部署，很是不方便，而且分布式、微服务等技术出现，修改维护多个项目管理也变得复杂。 为了解决以上问题，实现一次打包多地部署需求，减少项目管理及安全风险，我们需要将可变变量外移并通过页面统一可视化管理，基于此，我们统一建设了配置中心。 引入Zookeeper后，我们把数据存放在Zookeeper节点Znode上，可以选择主动轮询查询或者等待Zookeeper通知。当数据发生变化时，我们可以直接通过通知去执行某些业务操作。一般为了数据准确性，我们会主动轮询查询或者通知+轮询的方式。 PS：当然使用数据库保存这些数据也是可以的，采用定期查询的方式，而且有的配置中心就与之类似，我们在这儿不做更广泛讨论。 分析注册中心可以分为服务端和客户端两部分。 服务端一般用于存储配置数据，提供数据管理等等服务。客户端一般为业务端调用数据提供API服务等。 当然现在有一些开源的配置中心，如spring-cloud-config，diamond，disconf，apollo 等，我们以后有接触在具体介绍研究它们。 今天我们基于Zookeeper实现自己的一个简单的注册中心。 如下的配置中心流程图也就比较好理解了。 正文先来实现基于Zookeeper的配置中心客户端吧。 PS：了解这篇文章之前可以先看看 Zookeeper Java客户端Curator 先来了解下curator-recipes 包下的一个类PathChildrenCache。 该类是从本地缓存ZK路径的所有子路径中保存所有数据的一个工具类，它将监视ZK路径、响应更新/创建/删除事件、下拉数据等等，此类不能保证事务处理时的强同步。 这个类有一个全参构造方法： 1public PathChildrenCache(CuratorFramework client, String path, boolean cacheData)&#123;...&#125; Client是我们的ZKClient需要创建，path指要监控的路径，cacheData指是否缓存数据。 同时我们可以为其添加Listener，当节点/子节点数据有变化时，可以进行通知等。 使用该方法： 1pathChildrenCache.getListenable().addListener(pathChildrenCacheListener); 我们想实现自己的配置中心客户端，与SpringBoot进行集成，其目录结构如下创建： ConfigCenterAutoConfig：SpringBoot自动配置类，会提供相应的Bean。 ConfigCenterConfiguration：自动配置类，从properties文件中获取配置属性。 ConfigCenterException：异常处理类。 ConfigCenterListener：配置中心监听listener。 ConfigCenterListenerAdapter：考虑到监听可以有多个，这个类用来处理它们。 LocalCacheService：主要用来定时轮询Zookeeper的配置。 ZKConfigService：主要用来创建Zookeeper连接及添加监听等。 ConfigUtil：工具类。 CacheNodeVo：节点Vo。 ConfigCenterClient：配置中心客户端。 先从配置类说起吧，连接配置文件properties的类ConfigCenterConfiguration。 1234567891011121314@ConfigurationProperties("spring.zookeeper.config-center")public class ConfigCenterConfiguration &#123; //zk地址 private String zkAddress; //业务名称 private String sysName; //连接超时时间 private Integer connectTimeoutMs = 60000; //session过期时间 private Integer sessionTimeoutMs = 60000; //重试间隔 private Integer retryInterval = 1000; ......&#125; 这个类不过多介绍了，就是Zookeeper的配置信息，连接Zookeeper时使用。 我们引入之前封装的framework-zookeeper包，通过自动配置拿到client。 12345&lt;dependency&gt; &lt;groupId&gt;com.zwt&lt;/groupId&gt; &lt;artifactId&gt;framework-zookeeper&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; ConfigCenterAutoConfig部分代码如下，也比较好理解，就是Spring启动后自动配置CuratorClient。 12345678910111213141516171819202122232425262728293031323334353637@Configuration@ConditionalOnClass(CuratorZKClient.class)@EnableConfigurationProperties(ConfigCenterConfiguration.class)public class ConfigCenterAutoConfig implements ApplicationContextAware &#123; private static final Logger log= LoggerFactory.getLogger(ConfigCenterAutoConfig.class); @Autowired private ConfigCenterConfiguration properties; @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = "spring.zookeeper.config-center", name = "enabled", havingValue = "true") CuratorZKClient curatorZKClient ()&#123; ZKConfig config = new ZKConfig(); config.setConnectString(properties.getZkAddress()); config.setNameSpace(properties.getSysName()); config.setSessionTimeoutMs(properties.getSessionTimeoutMs()); config.setConnectTimeoutMs(properties.getConnectTimeoutMs()); config.setRetryInterval(properties.getRetryInterval()); CuratorZKClient zkClient = new CuratorZKClient(config); zkClient.addConnectionListener((state) -&gt; &#123; log.debug("ZKConfigService connectionListener state：" + state); if (state == ZKConstants.State.CONNECTED || state == ZKConstants.State.RECONNECTED) &#123; log.info("ZKConfigService zookeeper is connected..."); &#125; &#125;); zkClient.start(); return zkClient; &#125; private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125; //根据class类型返回bean public static &lt;T&gt; T getBean(Class&lt;T&gt; requireType)&#123; return applicationContext.getBean(requireType); &#125;&#125; 有了CuratorClient，我们创建ZKConfigService，主要为指定节点添加希望的监听。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ZKConfigService&#123; private static final Logger log= LoggerFactory.getLogger(ZKConfigService.class); private static CuratorZKClient zkClient = null; final static String configRootPath = ConfigUtil.getConfigCenterPath(); /** * ZKService初始化 */ public static void init() &#123; //拿到zkClient zkClient = ConfigCenterAutoConfig.getBean(CuratorZKClient.class); try &#123; PathChildrenCache pathChildrenCache = new PathChildrenCache(zkClient.getCuratorFramework(), configRootPath, true); PathChildrenCacheListener pathChildrenCacheListener = (client,event) -&gt; &#123; log.debug("pathChildrenCacheListener eventType：" + event.getType()); ChildData data = event.getData(); if(data!=null)&#123; String dataStr = new String(data.getData(), "UTF-8"); String key = StringUtils.substringAfterLast(data.getPath(), ConfigUtil.SEP_STRING); switch (event.getType()) &#123; case CHILD_ADDED: LocalCacheService.put(key,dataStr); break; case CHILD_UPDATED: LocalCacheService.put(key,dataStr); ConfigCenterListenerAdapter.onChange(key,dataStr); break; case CHILD_REMOVED: LocalCacheService.remove(key); break; default: break; &#125; &#125; &#125;; pathChildrenCache.getListenable().addListener(pathChildrenCacheListener); pathChildrenCache.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; log.info("spring-boot-config ZKConfigService init success."); &#125; /** * 根据key获取值 * @param key * @return */ public static String getKey(String key) &#123; return zkClient.getStringData(ConfigUtil.joinPath(configRootPath, key)); &#125;&#125; 可以看到用到了我们刚才说的PathChildrenCache类，启动后，添加一个Listener，监听节点变化，一方面，我们需要一个类，对节点变化进行通知；另一方面，我们应把数据缓存在本地，如果数据变化后ZK没有通知或者其它情况，我们可以轮询查询后与本地缓存比较，有变化后继续进行我们节点变化的通知。 这就是我们的ConfigCenterListenerAdapter监听处理类和LocalCacheService本地缓存服务。 先说ConfigCenterListenerAdapter吧，可以看到上面代码节点有变化时触发了onChange事件。 由于我们业务可能需要多个监听类，故，我们提供一个监听接口，相关业务类实现这个接口，在注册一下监听即可使用岂不美哉。 考虑到此，我们存储监听类的集合应是静态的。如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ConfigCenterListenerAdapter &#123; private static Logger log= LoggerFactory.getLogger(ConfigCenterListenerAdapter.class); //key和要监听此节点的监听者列表 private static ConcurrentHashMap&lt;String, List&lt;ConfigCenterListener&gt;&gt; someKeyListenerMap = new ConcurrentHashMap&lt;&gt;(); private static List&lt;ConfigCenterListener&gt; allKeyListeners = new CopyOnWriteArrayList&lt;&gt;(); public static boolean addListener(String key, ConfigCenterListener configCenterListener) &#123; if (configCenterListener == null) &#123; return false; &#125; if (key == null || key.trim().length() == 0) &#123; allKeyListeners.add(configCenterListener); return true; &#125; else &#123; List&lt;ConfigCenterListener&gt; listeners = someKeyListenerMap.get(key); if (listeners == null) &#123; listeners = new ArrayList&lt;&gt;(); someKeyListenerMap.put(key, listeners); &#125; listeners.add(configCenterListener); return true; &#125; &#125; public static void onChange(String key, String value) &#123; if (key == null || key.trim().length() == 0) &#123; return; &#125; List&lt;ConfigCenterListener&gt; keyListeners = someKeyListenerMap.get(key); if (keyListeners != null &amp;&amp; keyListeners.size() &gt; 0) &#123; for(ConfigCenterListener listener : keyListeners) &#123; try &#123; listener.onChange(key, value); &#125; catch (Exception e) &#123; log.error(e.getMessage(), e); &#125; &#125; &#125; if (allKeyListeners.size() &gt; 0) &#123; for(ConfigCenterListener confListener : allKeyListeners) &#123; try &#123; confListener.onChange(key, value); &#125; catch (Exception e) &#123; log.error(e.getMessage(), e); &#125; &#125; &#125; &#125;&#125; 12345678public interface ConfigCenterListener &#123; /** * 配置的key有变化触发事件 * @param key * @param value */ void onChange(String key, String value) ;&#125; 我们再来看看我们的主动轮询服务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class LocalCacheService &#123; private static final Logger log= LoggerFactory.getLogger(LocalCacheService.class); //配置缓存信息map private static final ConcurrentHashMap&lt;String, CacheNodeVo&gt; LOCAL_CONFIG_CACHE_MAP = new ConcurrentHashMap&lt;&gt;(); //刷新线程状态 private static boolean refreshThreadStop = false; //一个单线程的线程池（刷新缓存使用） private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat("spring-boot-config-%d").setDaemon(true).build(); private static ExecutorService singleThreadPool = Executors.newFixedThreadPool(1, namedThreadFactory); //一分钟刷新一次 public static void init() &#123; // refresh thread singleThreadPool.submit(()-&gt;&#123; while (!refreshThreadStop) &#123; try &#123; TimeUnit.SECONDS.sleep(60); reloadAll(); log.debug("spring-boot-config, refresh thread reloadAll success."); &#125; catch (Exception e) &#123; log.error("spring-boot-config, refresh thread error."); log.error(e.getMessage(), e); &#125; &#125; log.info("spring-boot-config, refresh thread stopped."); &#125;); log.info("spring-boot-config LocalCacheService init success."); &#125; /** * 通过key获取值 * @param key * @return */ public static String get(String key) &#123; CacheNodeVo cacheNodeVo = LOCAL_CONFIG_CACHE_MAP.get(key); if (cacheNodeVo != null) &#123; return cacheNodeVo.getValue(); &#125; return null; &#125; /** * 放入值 * @param key * @param value */ public static void put(String key, String value) &#123; LOCAL_CONFIG_CACHE_MAP.put(key, new CacheNodeVo(key, value)); &#125; /** * 移除某个值 * @param key */ public static void remove(String key) &#123; LOCAL_CONFIG_CACHE_MAP.remove(key); &#125; /** * 重新加载全部 */ private static void reloadAll() &#123; Set&lt;String&gt; keySet = LOCAL_CONFIG_CACHE_MAP.keySet(); if (keySet.size() &gt; 1) &#123; for(String key : keySet) &#123; String zkValue = ZKConfigService.getKey(key); CacheNodeVo cacheNodeVo = LOCAL_CONFIG_CACHE_MAP.get(key); if (cacheNodeVo != null &amp;&amp; cacheNodeVo.getValue() != null &amp;&amp; cacheNodeVo.getValue().equals(zkValue)) &#123; log.debug("refresh key:&#123;&#125; no changed ", key); &#125; else &#123; LOCAL_CONFIG_CACHE_MAP.put(key, new CacheNodeVo(key, zkValue)); ConfigCenterListenerAdapter.onChange(key, zkValue); &#125; &#125; &#125; &#125;&#125; 可以看到借助了一个定长线程池，每隔60s重载一下数据，有变化会对监听者进行通知。它是通过一个静态的ConcurrentHashMap 来保存数据的。 这儿看到刚才的也会主动通知监听者，这儿也通知监听者，它们会通知两次吗？ 我们可以看到主动通知的时候，也会先把ConcurrentHashMap的值先改变在进行通知，要是出现通知两次的情况，会是概率极低的。要是要求只能通知一次，且业务监听无法重复处理两次数据变化请求，可以在向ConcurrentHashMap里放值时，再检查一下它的当前值，或使用其它方法处理。 两个服务ZK通知和主动轮询处理完成后，提供一个配置中心Client，用于获取值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ConfigCenterClient &#123; private static final Logger log= LoggerFactory.getLogger(ConfigCenterClient.class); static &#123; LocalCacheService.init(); ZKConfigService.init(); &#125; public static String getString(String key, String defaultValue) &#123; //查询本地缓存 String value = LocalCacheService.get(key); if (value != null) &#123; log.debug("get config &#123;&#125; from cache",key); return value; &#125; //没有命中，查询zk中的值，并加入到缓存中，并加watcher value = ZKConfigService.getKey(key); if (value != null) &#123; log.debug("get config &#123;&#125; from zookeeper",key); LocalCacheService.put(key,value); return value; &#125; return defaultValue; &#125; public static String getString(String key) &#123; return getString(key, null); &#125; private static void checkNull(String key,String value) &#123; if (value == null) &#123; throw new ConfigCenterException(String.format("config key [%s] does not exist",key)); &#125; &#125; public static long getLong(String key) &#123; String value = getString(key, null); checkNull(key, value); return Long.valueOf(value); &#125; public static int getInt(String key) &#123; String value = getString(key, null); checkNull(key, value); return Integer.valueOf(value); &#125; public static boolean getBoolean(String key) &#123; String value = getString(key, null); checkNull(key, value); return Boolean.valueOf(value); &#125; public static boolean addListener(String key, ConfigCenterListener configCenterListener)&#123; return ConfigCenterListenerAdapter.addListener(key, configCenterListener); &#125;&#125; 首先应加载LocalCacheService和ZKConfigService，然后实现主要的方法getString，直接去缓存里取，拿不到去Zookeeper里取并放到缓存里，在提供一个addListener方法，可以让用户自己定义想监听的节点。 至此，我们一个简单的配置中心的客户端就完成了，我们把它打包引入一个demo项目测试一下。 创建一个demo项目，引入我们的jar包。 12345&lt;dependency&gt; &lt;groupId&gt;com.zwt&lt;/groupId&gt; &lt;artifactId&gt;config-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 我们创建一个Listener实现。 1234567public class MyListener implements ConfigCenterListener &#123; private static final Logger log= LoggerFactory.getLogger(MyListener.class); @Override public void onChange(String key, String value) &#123; log.info(key+ " changed "+ value); &#125;&#125; 创建测试类，我们循环10次改变节点的值，测试一下我们的程序。 1234567891011121314151617@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; @Autowired CuratorZKClient client; @Test public void contextLoads() throws Exception&#123; //client.createNode("/config/test","Hello World"); ConfigCenterClient.addListener("test",new MyListener()); for(int i = 0;i&lt;10;i++)&#123; Thread.sleep(5000); String str = ConfigCenterClient.getString("test"); System.out.println(str); client.setData("/config/test","Hello World"+i); &#125; &#125;&#125; 可以看到执行结果。 我们还要实现一个配置中心的服务端。 服务端基本上是对Zookeeper数据节点的增删改查这几个逻辑，其核心是Zookeeper保存在节点上的数据。 为了方便对Zookeeper数据进行操作，我们一般创建一个可视化后台管理系统。如下： 这个系统是比较好实现的，引入我们的framework-zookeeper包，里面封装了Zookeeper的增删改查，当然需要创建一个web项目。 这一块就不再过多介绍了，当明白了Zookeeper的增删改查节点数据后，实现起来是比较容易的。 总结今天我们通过Zookeeper实现了一个配置中心，简单了解了它的原理，也对Zookeeper有了一些更深刻的理解。 现在很多开源的配置中心也相当的不错，也是可以学习和理解的，我后面可能也会讲解一些关于这方面的知识。 framework-zookeeper 和 config-spring-boot-starter 的相关代码已上传GitHub，大家如果有兴趣在实践中遇到问题可以过去参考下代码，如有疑问也欢迎与我交流探讨。 配置中心服务端（web项目）由于个人原因和时间原因，只写了个大概，也没有提交Github，后续应该会补上。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>配置中心</tag>
        <tag>ConfigCenter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql 使用Explain分析SQL语句及索引]]></title>
    <url>%2Fblog%2FMySql-%E4%BD%BF%E7%94%A8Explain%E5%88%86%E6%9E%90SQL%E8%AF%AD%E5%8F%A5%E5%8F%8A%E7%B4%A2%E5%BC%9520181219%2F</url>
    <content type="text"><![CDATA[前言我们经常会用到数据库，并可能写一些查询语句，大多数情况下，可能都是一些比较简单的增删改查语句。但某些时候，也可能遇到一些较复杂的语句。 在系统数据量不太大的情况下，可能我们SQL语句的好坏对系统性能并不会造成太大影响，但是随着系统数据量增大，数据库压力增大，除一些其它策略（使用缓存数据库、分库分表等等），良好的SQL语句也是提高数据库性能的一个重要指标。 SQL语句我们基本是按照，能精确查询就精确查询，能简化就简化，能使用索引就使用索引（必要条件下）的原则去编写。 我们今天通过Explain这个关键字来分析及优化我们的SQL语句。 正文那就先说下Explain吧。 打开数据库，随便找一张数据表。如下： 可以看到，当我们在我们写的SQL语句前加上Explain后，会得到如下的结果表。 PS：如果SQL语句关联其它表，会有多行结果，后续。 我们先来简单说下结果各个字段的含义。 id 暂且叫主键或者顺序吧，如果不止一次查询搜索（比如关联其它表），会按照顺序列出来。序号越大越先被执行。 select_type 查询类型，有下面几种： SIMPLE：简单SELECT(不使用UNION或子查询等) PRIMARY：最外面的SELECT UNION：UNION中的第二个或后面的SELECT语句 DEPENDENT UNION：UNION中的第二个或后面的SELECT语句，取决于外面的查询 UNION RESULT：UNION的结果。 SUBQUERY：子查询中的第一个SELECT DEPENDENT SUBQUERY：子查询中的第一个SELECT，取决于外面的查询 DERIVED：导出表的SELECT(FROM子句的子查询) table 这行数据和哪张表有关联。 partitions 匹配的哪个分区。 type 这列最重要，显示了连接使用了哪种类别，有无使用索引，是使用Explain命令分析性能瓶颈的关键项之一。 结果值从好到坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL 一般来说，得保证查询至少达到range级别，最好能达到ref，否则就可能会出现性能问题。 possible_keys MySQL可能使用哪个索引在该表中找到行。 key MySQL实际决定使用的键（索引）。如果没有选择索引，值是NULL。 key_len MySQL决定使用的索引长度。如果key是NULL，则key_len为NULL。在不损失精确性的情况下，长度越短越好。 ref 使用哪个列或常数与key一起从表中选择行。 rows MySQL认为它执行查询时必须检查的行数。正常情况下这个值越小越好。 Extra MySQL解决查询的详细信息，也是关键参考项之一。 Distinct：一旦MYSQL找到了与行相联合匹配的行，就不再搜索了。 Not exists：MYSQL 优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了。 Range checked for each Record：没有找到理想的索引，因此对于从前面表中来的每一个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。 Using filesort：MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行。出现这种情况应当优化SQL语句。 Using index：列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候。 Using temporary：说明MySQL需要新建一个临时表来存储结果，当生成的临时表较大时，应当优化SQL语句。 Using where：使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，就会出现这种情况。 Using union：使用了合并，有可能合并索引、表等。一般使用or连接条件时可能会出现。 我们创建一张学生表。 123456789CREATE TABLE `student` ( `id` int(32) NOT NULL AUTO_INCREMENT COMMENT '自增主键', `stu_no` varchar(20) DEFAULT NULL COMMENT '学生编号', `stu_name` varchar(50) DEFAULT NULL COMMENT '学生姓名', `stu_sex` char(2) DEFAULT NULL, `stu_birthday` date DEFAULT NULL COMMENT '学生生日', `stu_class` char(2) DEFAULT NULL COMMENT '学生所在班级', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=50077 DEFAULT CHARSET=utf8; 向表中加入100w数据。（过程略） 关于索引我们统计5班学生的学生数量。 可以看到耗时2s之久。 使用Explain分析可以看到mysql需要检查994511行才能得到结果，已经相当于全表扫描了。Type是ALL，为最差的一种。 我们在stu_class班级这一列上加上索引。 继续查询及分析，可以看到耗时0.07s。 Explain分析可知type为ref满足我们要求，mysql执行这个查询认为只要扫描191502条数据即可，使用了index_1索引。 是不是感觉到索引很厉害？ 我们再来看一下。 这回我们查询5年级所有学生的学生名。如下： 先不使用索引，可以看到，耗时2s左右。 我们给年级加上索引，继续测试。如下： 可以看到耗时竟达到5s左右。 为什么会这样呢？ 一般我们认为，这种属于很特殊的情况，一般在硬盘上，数据是连续存储的，但是引入索引后，数据连续性变差，随机波动性要大，在数据量大的情况下更是。 这种情况下，我们读取大量数据（99497条已接近1/10数据总量），由于随机性变大，故读取时间会变大。因此索引在查询一些小数据量数据的情况下效果明显，或者索引带来的优化已经完全超过了数据读取时间。 关于上面，我们可以看下有索引和无索引SQL执行情况。 有索引情况下: 无索引情况下: 可以看到它们明显的差距就在返回数据这儿了，有索引情况下读取数据浪费了太多时间。 关于索引带来的优化已经完全超过了数据读取时间这个，我们可以看下这个例子。 我们在添加一条姓名索引。 我们查询5班所有姓张的童鞋。 有索引情况下: 无索引情况下: 明显的有索引的完爆无索引查询时间啦，这里就不在具体分析了。 我们可以看下这儿的like，可以直接看下面的图。 明显得到的结论就是当有like查询时，%或者_在前面的是无法使用当前字段索引的（如果有），会进行全表扫描，这一点需要注意。 SQL优化我们接下来分析一些SQL的优化。 1.我想查询生日在1998年至2010年之间的所有学生数量。（生日已建立索引index_3） 可以这样，如下图。 Explain分析仅为index级别，虽然用到了索引index_3。 如何优化下呢？ 我们可以这样处理： 通过Explain分析type也达到了range级别。 PS： 通过上面例子，我们知道我们应该尽量减少对字段的计算操作，以便使其可以正常利用索引，也就是尽量使条件数据与字段数据去匹配，而不是对字段数据进行操作后去匹配条件数据。 2.我想查询生日在98年后且是5班的且姓张的童鞋。 有生日索引index_3，姓名索引index_2，班级索引index_1。 运行上述SQL后我们看到type级别为range，还可以，但是却只用到了index_2索引，时间还可以0.006s。 还有好一点的方法吗？ 我们试着删除上面三个索引，创建一个联合索引。如下： 进行测试。 可以看到时间有提高（反复测试是比单索引要好的），type级别还是range。 PS：联合索引只有在一些特定的情况下能发挥很好的作用，一般单索引可以满足大多需求，所以联合索引的创建是需要慎重的。 联合索引有一些特点需要注意，以免被坑。对于上面的联合索引，index_1(stu_name, stu_birthday, stu_class)，遵循最左原则。 最左原则可以看下面7个例子。 例子1:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01'; Explain分析结果如下： 例子2:1EXPLAIN select * from student s where s.stu_name like '张%'; Explain分析结果如下： 例子3:1EXPLAIN select * from student s where s.stu_class = '5'; Explain分析结果如下： 例子4:1EXPLAIN select * from student s where s.stu_class = '5' and s.stu_birthday &gt;= '1998-01-01'; Explain分析结果如下： 例子5:1EXPLAIN select * from student s where s.stu_class = '5' and s.stu_name like '张%'; Explain分析结果如下： 例子6:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01' and s.stu_name like '张%'; Explain分析结果如下： 例子7:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01' and s.stu_name like '张%' and s.stu_class = '5'; Explain分析结果如下： 可以看到，可以使用索引的查询有 （stu_name）（stu_name, stu_birthday）（stu_name, stu_birthday, stu_class）（stu_name, stu_class），也就是必须有stu_name这个条件。这就是联合索引的最左原则。 3.我们统计姓名相同的学生的学生名及数量。1select s.stu_name,count(1) from student s GROUP BY s.stu_name HAVING count(1)&gt;1 ORDER BY s.stu_name; 这儿就不在贴图了，当stu_name有索引时，查询效率会有明显提升。 在有索引Explain分析后，我们可以看到，Mysql认为这个查询需要检索全部数据（rows 99万）。 这是Mysql认为要检索的行数，实际由于stu_name有索引，是没有检索这么多的。 因为查询语句中出现了group by，对于某张表，正常情况的的认知肯定是检索完全表后才能进行分组归类。所以，在一些经常被group by 或者 order by 的字段上，数据量较大的情况下，且该字段比较多样性（比如性别这种字段顶多有男、女、未知三种，加不加索引都可以），向表中常用的创建时间、修改时间等字段，还是有必要添加索引的。 4.查询所有学生生日，性别，班级相同的学生数量。 我们假设建立了index_1(stu_birthday)索引，index_2(stu_sex)索引，index_3(stu_class)索引。 你可能会想到如下SQL。1select count(1) from student a,(select stu_birthday,stu_class,stu_sex from student) b where a.stu_birthday = b.stu_birthday and a.stu_class=b.stu_class and a.stu_sex=b.stu_sex; 嗯，这个SQL是不正确的，而且耗时无法忍受。 其实不用Explain，单独看它就可以知道它会扫描全表，无论有无索引。而且我们统计学生数量，如果有2个以上情况相同呢？显然上面SQL的3个等于条件会出现问题。 其实我们可以根据分组数量来处理这个问题，如下：1select count(1) from student s ,(select a.stu_birthday,a.stu_sex,a.stu_class from student a GROUP BY a.stu_birthday,a.stu_sex,a.stu_class HAVING count(1)&gt;1) b where s.stu_birthday=b.stu_birthday and s.stu_sex=b.stu_sex and s.stu_class=b.stu_class; 通过学生生日，性别，班级相同，他们的数量要大于1，从而进行数量统计。 耗时在8s左右。我们Explain分析下： 先从id为2的看起，生成了一个临时表a，但一个索引也没用到，向上看它用了学生生日，性别，班级这三个临时表的值做了auto_key（可以理解为临时索引），然后对s表全表扫描得到结果。 那我们的三个索引岂不是创建的是毫无意义的…… 我们可以将它的auto_key变成一个联合索引，index_4(stu_birthday, stu_sex, stu_class)。 这时候查询时间减少到5s左右。 可以看到使用到了index_4，且Extra字段原来使用Using temporary现在变为了Using index。 总结好了，基本上就这些内容，其实也有想写一个多表联查复杂的SQL语句的优化，但是无奈例子比较难找（自己不想再建新表，再插入数据，再想例子），在这儿就不在举例了。 其实实际中SQL语句的编写，复杂的还是比较少的（毕竟有程序结合，一个不行可以拆开嘛）。 这一节主要的内容其实是讲Explain这个关键字的，实际中写完复杂SQL语句可以用它分析下SQL执行情况。 上面也看到了有些索引不是建了就会减少查询时间的，索引的选择还是要认真对待，处理好能提升不少查询效率，另外索引创建也应找到问题的关键点。比如该字段经常被group by，order by ，该字段业务中经常使用等等。 使用Explain可以帮我们分析到索引的利用情况。 另外，Mysql除BTree索引外，还有一种Hash索引。 BTree索引具有范围查找和前缀查找的能力，一个N节点的BTree，检索一条记录时间复杂度为O(LogN)，相当于二分查找。 Hash索引只能做等值查找，但查找时间复杂度为O(1)。 如果值的差异性大，并且以等值查找（=、 &lt;、&gt;、in）为主，Hash索引是更好的选择。如果值的差异性相对较差，并且以范围查找为主，BTree是更好的选择。 一般使用BTree索引的场合较多。 结语结束之前，我们可以看看索引的一些失效场景。 索引列查询出现 is not null 情况，会进行全表扫描不走索引。 列类型是字符串，数据需要引号引起来，否则会进行全表扫描不走索引。 模糊查询like的各种情况，不再详述。 联合索引的各种情况，不再详述。 or的情况，当用or连接条件时，应保证条件都有索引才能使用索引，否则索引无效。 可以看到由于stu_name没有索引，故进行了全表扫描。 stu_sex和stu_birthday均有索引，使用了索引。 如果MySQL觉得全表扫描要比使用索引要快，它也不会使用索引。 如下：由于stu_sex=0 或者 stu_class =1 数据很多，且都要返回，故没有使用它们的索引。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>Explain</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的2018年度总结]]></title>
    <url>%2Fblog%2F%E6%88%91%E7%9A%842018%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%9320181216%2F</url>
    <content type="text"><![CDATA[我的2018年度总结本来计划月底写的，可是啊，看到有时力不从心的自己，还是决定提前写了吧。 写这个文章，算是对自己今年一整年的总结吧。 怎么说呢？ 先整体上总结一下吧，感觉自己比17年状态要好，懂得了时间的宝贵吧，自制力上也有些许提高。 语无伦次了呢…… 哎，谁让我是个偏内向的男孩啊。啊啊啊啊啊啊啊，老男人，哈哈。 不知道3、5年后，在看到这篇文章，会想到些什么呢？ 如果文章还在的话…… 不扯那么多矫情的话了…… 我的2018年度总结开篇吧。 先说下自己的一些目标及愿望吧，我比较喜欢列清单的方式。 主要也是由于制定的比较晚吧，可能目标/愿望较少，不过也基本上能代表2018年的一些自己主要的计划/想法了。 总的来说一些目标也太模糊，一些目标也很随意，毕竟第一年制定这么个清单。 比如坚持锻炼，可能那时定个坚持锻炼7天，坚持15天……一点点来，自己就不会太迷茫，开始就感觉没戏一样。 而且总感觉对自己下不去狠心，惩罚写的很潦草，奖励写的不现实。 PS：所以这儿就不贴上面给大家看啦，哈哈。 PS：而且还有一些比较个人隐私的目标和愿望没写到上面。 PS：如果你看到这篇文章，也可以自己制定个计划List，不一定要一年，一个月，半年也是可以的。有的时候越长远，反而越不容易实现。 人都是有惰性的嘛。 总会越来越好的。 2018年的自己，怎么说呢？ 也自信也迷茫，也努力也彷徨，也欢喜也哀伤，也年轻也沧桑。 感觉自己也是蛮幸运的吧。 也受到过不少负面的影响。 有时甚至无聊到想探寻人类的本质…… 有人说，出生那一刻，人命就定了，曾经年少轻狂，以为自己可以拯救世界，却发现世界根本不在乎你…… 哈哈，越长大越“认命”吗？ 罗曼·罗兰有句话说的好啊。 世界上只有一种真正的英雄主义，那就是在认清生活的真相后依然热爱生活。 对于人生，我觉得，或许会认命，但绝不会认输。 好吧，我自己这都是写了啥啊…… 哎，跑题了…… 就这样吧，就这样吧。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper Java客户端Curator]]></title>
    <url>%2Fblog%2FZookeeper-Java%E5%AE%A2%E6%88%B7%E7%AB%AFCurator20181215%2F</url>
    <content type="text"><![CDATA[前言我们今天来讲讲Zookeeper Java客户端的一些使用吧。 之前我们说到Zookeeper的安装及简单使用，要介绍它的一些应用场景，要明白它的应用场景，要先理解它客户端的一些操作方法。 Zookeeper的Java客户端，最常使用的便是Apache Curator了，它是Netflix公司开源的一个Zookeeper客户端，与Zookeeper提供的原生客户端ZooKeeper相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量，而且Curator的功能更加强大。 正文要使用Curator客户端，需要下面的两个依赖。 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; curator-recipes包一般能满足我们的需要，要是封装更简便的底层功能的话，curator-framework包必不可少。 创建并启动客户端使用程序创建一个客户端client并启动（连接到Zookeeper）。 Builder模式创建一个客户端。 1234567CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder() .connectString(zkConfig.getConnectString()) .namespace(zkConfig.getNameSpace()) .retryPolicy(new ExponentialBackoffRetry(zkConfig.getRetryInterval(), Integer.MAX_VALUE)) .connectionTimeoutMs(zkConfig.getConnectTimeoutMs()) .sessionTimeoutMs(zkConfig.getSessionTimeoutMs()); client = builder.build(); 客户端启动。 1client.start(); 简单说下连接时的参数: connectString：服务器列表，逗号隔开（host1:port1,host2:port2……） namespace：命名空间，可以用来进行业务区分。 retryPolicy：重试策略,有以下4种重试策略，也可以自己实现重试策略（实现RetryPolicy接口）。RetryOneTime：重试一次。 RetryNTimes：重试N次（需要传入重试间隔参数sleepMsBetweenRetries，及尝试次数n），它继承了抽象类SleepingRetry（每休眠一段时间重试一次）。 RetryForever：一直重试（需要传入重试间隔retryIntervalMs参数）。 BoundedExponentialBackoffRetry：重试次数固定，但每次重试的时间间隔会不断变大（如果一直连不上），需要传入初始等待重试时间baseSleepTimeMs，重试次数maxRetries，及最大等待重试时间maxSleepTimeMs 参数，这个类继承ExponentialBackoffRetry（它又继承SleepingRetry）抽象类。 sessionTimeoutMs：会话超时时间，单位毫秒，默认60000ms。 connectionTimeoutMs：连接创建超时时间，单位毫秒，默认60000ms。 创建数据节点。Zookeeper节点有4种，上篇文章已介绍。 创建持久化节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path, data); 使用create方法，creatingParentsIfNeeded这个方法保证如果有父节点也会一起创建，这在原生客户端是无法实现的。CreateMode 有4种，跟Zookeeper的节点类型对应。forPath方法可以认为最终操作，path表示节点路径，data表示节点数据。data是byte数组，其它类型的数据应转换为byte数组。 注：如果不设置withMode方法，默认创建持久化节点，不设置data，节点默认内容为空。 如下： 1client.create().forPath(path); 创建顺序节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(path, data); 创建临时节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(path, data); 创建临时顺序节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(path, data); 设置节点数据1client.setData().forPath(path, bytes); 直接调用setData方法即可，返回一个Stat（节点信息类）。 获取节点数据1client.getData().forPath(path); 使用getData方法，返回byte数组。 获取子节点1client.getChildren().forPath(path); 使用getChildren方法，返回一个子节点List &lt;String&gt; 列表，数据为各个子节点名称。 删除节点1client.delete().guaranteed().forPath(path); 使用delete方法，guaranteed方法可以保证一定删除。如果某个节点删除失败，会抛出异常，但是如果使用了guaranteed，它会在后台继续进行删除直到删除成功。 删除节点（包括子节点）1client.delete().guaranteed().deletingChildrenIfNeeded().forPath(path); deletingChildrenIfNeeded方法可以保证如果有子节点的话一并删除，原生client是无法实现此功能的（需要我们写方法处理）。 判断节点是否存在1234567891011public boolean checkNodeExist(String path) &#123; boolean exits = false; try &#123; Stat stat = client.checkExists().forPath(path); if (stat != null) &#123; exits = true; &#125; &#125; catch (Exception e) &#123; &#125; return exits;&#125; 使用checkExists方法，最终返回一个Stat，如果Stat为空就说明不存在。 PS：由此我们可以创建一个 createOrUpdate方法，无节点时创建，有节点时更新内容。 1234567891011public void createOrUpdateNode(String path, byte[] data) &#123; try &#123; if(checkNodeExist(path))&#123; setData(path,data); &#125;else&#123; createNode(path, data); &#125; &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125;&#125; 异步处理上面的操作方法，都可以使用异步进行处理的，主要使用了inBackground方法。 如下： 1client.create().inBackground().forPath(path, data); 该方法全参函数如下，且重载了多个方法。 1public T inBackground(BackgroundCallback callback, Object context, Executor executor); BackgroundCallback callback：异步回调函数，处理完成后会回调此函数进行某些逻辑。 Object context：上下文对象。 Executor executor：异步处理的线程，不指定的话将使用内部默认线程处理。 我们可以看下BackgroundCallback 方法 会有两个参数。 123public interface BackgroundCallback &#123; void processResult(CuratorFramework var1, CuratorEvent var2) throws Exception;&#125; 第二个参数CuratorEvent里面包含了此次处理结果的所有信息，包括节点信息等。 123456789101112public interface CuratorEvent &#123; CuratorEventType getType(); int getResultCode(); String getPath(); Object getContext(); Stat getStat(); byte[] getData(); String getName(); List&lt;String&gt; getChildren(); List&lt;ACL&gt; getACLList(); WatchedEvent getWatchedEvent();&#125; CuratorEventType表示事件类型，表示此次操作的事件类型。可以看到它与CuratorFramework里的方法是一一对应的。 getResultCode返回处理结果码。可以在这个枚举里查看各个状态码。 添加watcher12client.getData().usingWatcher(watcher).forPath(path);client.getChildren().usingWatcher(watcher).forPath(path); 使用usingWatcher结合getData或者getChildren方法可以为指定节点或者子节点添加watcher。 Watcher可以为CuratorWatcher或者Zookeeper自带的Watcher。它们有一个event参数。 可以拿到Zookeeper的状态 KeeperState和 事件类型 EventType，从而进行某些必要的操作。 KeeperState枚举和EventType枚举如下图。 事务支持Zookeeper一些操作是支持事务的。 主要用到的方法有inTransaction、and、commit等方法。举例如下： 123456789101112131415161718192021222324252627282930/** * 开启事务 */public CuratorTransaction startTransaction() &#123; return client.inTransaction();&#125;/** * 事务中添加create操作 */public CuratorTransactionFinal addCreateToTransaction(CuratorTransaction transaction, String path) throws Exception &#123; return transaction.create().forPath(path, new byte[0]).and();&#125;/** * 事务中添加delete操作 */public CuratorTransactionFinal addDeleteToTransaction(CuratorTransaction transaction, String path) throws Exception &#123; return transaction.delete().forPath(path).and();&#125;/** * 事务中添加seData操作 */public CuratorTransactionFinal addSetDataToTransaction(CuratorTransaction transaction, String path, byte[] data) throws Exception &#123; return transaction.setData().forPath(path, data).and();&#125;/** * 提交事务 */public Collection&lt;CuratorTransactionResult&gt; commitTransaction(CuratorTransactionFinal transaction) throws Exception &#123; return transaction.commit();&#125; 检查连接情况1client.getZookeeperClient().isConnected(); 关闭连接1client.close(); 提升对Curator客户端有简单理解后，我们把它进行简单功能的封装。 PS：Curator的强大之处在于其增强功能部分，我们会在后面结合Zookeeper应用讨论。 创建项目framework-zookeeper，搭建如下结构： 接口ZKClient，里面有一些Zookeeper客户端的协议，大致如下： 12345678910public interface ZKClient &#123; void start();//启动 boolean isConnected();//连接情况 void close();//关闭 void createNode(String path, byte[] data);//创建永久节点 void createOrUpdateNode(String path, byte[] data); //创建或者更新节点 void createEphemeralNode(String path, byte[] data); String createSequenceNode(String path) ; ......&#125; CuratorZKClient是Curator对接口ZKClient的实现，BaseZKClient是原生客户端对接口ZKClient的实现。 我们来写下CuratorZKClient的一些关键代码。 12345678910111213public class CuratorZKClient implements ZKClient &#123; //zk客户端 private CuratorFramework client; //关闭状态 private volatile boolean closed = false; //zk配置 private ZKConfig zkConfig; //连接状态监听 private Set&lt;ConnectionListener&gt; connectionListeners = new CopyOnWriteArraySet&lt;&gt;(); //节点变化监听及相关Watcher private final ConcurrentMap&lt;String, ConcurrentMap&lt;NodeListener, CuratorWatcher&gt;&gt; nodeListeners = new ConcurrentHashMap&lt;&gt;();......&#125; 提供一个连接监听接口，以便我们可以监听Zookeeper的连接状态并且执行某些操作。 123public interface ConnectionListener &#123; void stateChanged(ZKConstants.ConnectState state);&#125; 及节点变化接口，监测节点变化进行某些操作。 12345public interface NodeListener &#123; void nodeChanged(String path, List&lt;String&gt; nodes);//节点改变 void nodeDelete(String path);//节点删除 void dataChanged(String path, byte[] data);//节点数据改变&#125; ZKConfig是Zookeeper客户端连接的配置，属性值可以配置在properties等配置文件里。 12345678public class ZKConfig &#123; private String connectString; private String nameSpace; private int retryInterval = 1000; private int connectTimeoutMs = 60000; private int sessionTimeoutMs = 60000; ......&#125; CuratorZKClient 里实现接口的start方法是，部分代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940...... public void start() &#123; ....... CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder() .connectString(zkConfig.getConnectString()) .namespace(zkConfig.getNameSpace()) //重试指定的次数, 且每一次重试之间停顿的时间逐渐增加 .retryPolicy(new ExponentialBackoffRetry(zkConfig.getRetryInterval(), Integer.MAX_VALUE)) .connectionTimeoutMs(zkConfig.getConnectTimeoutMs()) .sessionTimeoutMs(zkConfig.getSessionTimeoutMs()); client = builder.build(); //添加Zookeeper状态监听 client.getConnectionStateListenable().addListener(new ConnectionStateListener() &#123; @Override public void stateChanged(CuratorFramework client, ConnectionState zkConnectionState) &#123; ZKConstants.ConnectState state = toConnectionListenerState(zkConnectionState); if (state != null) &#123; for(ConnectionListener connectionListener : connectionListeners) &#123; connectionListener.stateChanged(state); &#125; &#125; &#125; private ZKConstants.ConnectState toConnectionListenerState(ConnectionState zkConnectionState) &#123; switch (zkConnectionState) &#123; case LOST: return ZKConstants.ConnectState.DISCONNECTED; case SUSPENDED: return ZKConstants.ConnectState.DISCONNECTED; case CONNECTED: return ZKConstants.ConnectState.CONNECTED; case RECONNECTED: return ZKConstants.ConnectState.RECONNECTED; default: return null; &#125; &#125; &#125;); client.start(); &#125;..... 这样我们暴露了Zookeeper的连接状态监听接口，以后想监听它的连接状态进行某些操作，直接实现接口，并通过addConnectionListener添加进来即可。 12345678@Overridepublic void addConnectionListener(ConnectionListener listener) &#123; connectionListeners.add(listener); &#125;@Overridepublic void removeConnectionListener(ConnectionListener listener) &#123; connectionListeners.remove(listener);&#125; 其它的方法，比如createNode、deleteNode等，我们拿到client后，按照上面讲述的各个操作便可以写出代码，这里不再赘述。以下是createNode的例子。 12345678910@Overridepublic void createNode(String path, byte[] data) &#123; try &#123; client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path, data); &#125; catch (KeeperException.NodeExistsException e) &#123; log.warn(String.format("create node is exist:%s", path)); &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125;&#125; 再说一下需要实现的NodeListener方法，节点发生变化，主要通过watcher通知。实现一个watcher。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758private class CuratorWatcherImpl implements CuratorWatcher &#123; private volatile NodeListener listener; public CuratorWatcherImpl(NodeListener listener) &#123; this.listener = listener; &#125; public void unWatch() &#123; this.listener = null; &#125; @Override public void process(WatchedEvent event) throws Exception &#123; if (listener != null) &#123; log.debug(event.getPath() + " with event " + event.getType()); switch (event.getType()) &#123; //节点数据变化，调用listener指定方法 case NodeDataChanged: try &#123; byte[] data = client.getData().usingWatcher(this).forPath(event.getPath()); log.debug(event.getPath() + " data after change: " + new String(data)); listener.dataChanged(event.getPath(), data); &#125; catch (Exception e) &#123; log.warn(e.getMessage(), e); &#125; break; //节点删除 case NodeDeleted: //节点创建 case NodeCreated: log.error(event.getPath()); //自己点改变 case NodeChildrenChanged: try &#123; if (event.getType().equals(Watcher.Event.EventType.NodeDeleted)) &#123; listener.nodeDelete(event.getPath()); &#125; else &#123; List&lt;String&gt; nodes = getNodes(event.getPath()); if (nodes != null) &#123; client.getChildren().usingWatcher(this).forPath(event.getPath()); &#125; //监控子节点数据变化 //for(String node : nodes) &#123; // client.getData().usingWatcher(this).forPath( ZKPathMgr.joinPath(event.getPath(),node)); //&#125; log.debug(event.getPath() + " nodes after change: " + nodes); listener.nodeChanged(event.getPath(), nodes); &#125; &#125; catch (KeeperException.NoNodeException e) &#123; log.warn(e.getMessage()); &#125; catch (Exception e) &#123; log.warn(e.getMessage(), e); &#125; break; case None: default: break; &#125; &#125; &#125; &#125; 然后实现NodeListener的添加移除。 123456789101112131415161718192021222324252627282930313233343536@Override public void addNodeListener(String path, NodeListener listener) &#123; ConcurrentMap&lt;NodeListener, CuratorWatcher&gt; listeners = nodeListeners.get(path); if (listeners == null) &#123; nodeListeners.putIfAbsent(path, new ConcurrentHashMap&lt;NodeListener, CuratorWatcher&gt;()); listeners = nodeListeners.get(path); &#125; CuratorWatcher watcher = listeners.get(listener); if (watcher == null) &#123; listeners.putIfAbsent(listener, new CuratorWatcherImpl(listener)); watcher = listeners.get(listener); &#125; addChildrenCuratorWatcher(path, watcher); &#125; @Override public void removeNodeListener(String path, NodeListener listener) &#123; ConcurrentMap&lt;NodeListener, CuratorWatcher&gt; listeners = nodeListeners.get(path); if (listeners != null) &#123; CuratorWatcher watcher = listeners.remove(listener); if (watcher != null) &#123; ((CuratorWatcherImpl) watcher).unWatch(); &#125; &#125; &#125; //添加目录watcher private void addChildrenCuratorWatcher(final String path, CuratorWatcher watcher) &#123; try &#123; client.getData().usingWatcher(watcher).forPath(path); client.getChildren().usingWatcher(watcher).forPath(path); &#125; catch (KeeperException.NoNodeException e) &#123; log.warn(String.format("add watcher node not exist:%s", path)); &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125; &#125; 可以看到主要是是维护一个ConcurrentHashMap，listener为key，watcher为value，节点有变化，通知到listener。 好。到这里基本上一个Zookeeper工具客户端就OK了，BaseZKClient的实现与CuratorZKClient类似，有兴趣的可以自己看看。 测试我们测试下我们的代码。 12345678910111213141516171819public static void main(String[] args) throws Exception&#123; ZKConfig config = new ZKConfig(); config.setConnectString("127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"); config.setConnectTimeoutMs(60000); config.setNameSpace("zwt"); config.setRetryInterval(1000); config.setSessionTimeoutMs(60000); ZKClient client = new CuratorZKClient(config); client.addConnectionListener(new ConnectionListener() &#123; @Override public void stateChanged(ZKConstants.ConnectState state) &#123; System.out.println("ZKState state "+ state.name()); &#125; &#125;); client.start(); client.createNode("/mytest","Hello World"); System.out.println(new String(client.getData("/mytest"),"UTF-8")); client.close();&#125; 运行可以看到输出： 我们使用命令行也可以看到我们新增的test节点及其属性。 PS：可以看到nameSpace 业务命名空间相当于新增一个根节点以区分不同业务，避免节点冲突等作用。 我们在client启动后添加watcher。 12345678910111213141516171819202122......client.start();client.addNodeListener("/mytest", new NodeListener() &#123; @Override public void nodeChanged(String path, List&lt;String&gt; nodes) &#123; System.out.println(path+" node changed"); &#125; @Override public void nodeDelete(String path) &#123; System.out.println(path+" node delete"); &#125; @Override public void dataChanged(String path, byte[] data) &#123; System.out.println(path+" data changed "+ data); &#125;&#125;);System.out.println(new String(client.getData("/mytest"),"UTF-8"));client.setData("/mytest","World");System.out.println(new String(client.getData("/mytest"),"UTF-8"));client.createNode("/mytest/test");client.deleteNodeWithChildren("/mytest");...... 继续测试，结果如下。 到这里，我们基本把客户端操作的基本说完了。关于其它一些Zookeeper客户端，这里就不在过多介绍了，有兴趣的可以继续实现ZKClient接口去完成。 PS：BaseZKClient类是我写的一个原生Zookeeper客户端的集成工具，但有些小问题未处理。 总结通过使用Zookeeper客户端的一些例子，更对Zookeeper有了更深入的了解。 下面的文章我们将结合Curator的一些高级功能及Zookeeper的一些应用来了解Zookeeper的强大之处。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>Curator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper初探]]></title>
    <url>%2Fblog%2FZookeeper%E5%88%9D%E6%8E%A220181208%2F</url>
    <content type="text"><![CDATA[前言前不久研究了下Zookeeper，感觉挺不错的，这几天把成果总结下，方便记忆。 今天先简单介绍下它的安装使用和一些特性吧。 ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 正文Zookeeper的安装与配置首先需要下载安装包。 Zookeeper官网地址 点击这里 我们可以通过镜像的方式将压缩包下载下来。 解压到合适目录，进入到zookeeper目录下的conf目录，需要新建zoo.cfg文件，可以看到里面有一个zoo_sample.cfg文件，可以作为我们的配置参考。 我们来看下它里面的一些常用参数： tickTime=2000 指Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。单位毫秒 initLimit=10 此配置表示，允许 follower（相对于 leader 而言）连接并同步到 leader 的初始化连接时间，它以 tickTime 的倍数来表示。当超过设置倍数的 tickTime 时间，则连接失败。 syncLimit=5 此配置表示，leader 与 follower 之间发送消息，请求和应答时间长度。如果 follower 在设置的时间内不能与leader 进行通信，那么此 follower 将被丢弃。 dataDir=D:/zookeeper1/zookeeper/data 数据存储快照的地址 dataLogDir=D:/zookeeper1/zookeeper/log 数据库事务日志地址，如果不设置此项，默认dataDir的地址 clientPort=2181 供客户端连接的端口，对外提供的端口 server.10=127.0.0.1:2888:3888 集群模式下需要的配置（单机不需要） server.A = B:C:D A：表示这是第几号服务器 B：服务器的 IP 地址 C：集群内机器通讯使用（Leader监听此端口） D：一旦集群中的 Leader 服务器挂了，需要一个端口重新进行选举，选出一个新的 Leader Zookeeper的启动和使用看完上面配置后，我们先配置单机zookeeper对其简单使用。 进入到zookeeper bin目录下，我们可以看到一些脚本，分为cmd脚本和sh脚本。 zkCleanup 清理Zookeeper历史数据脚本，包括日志文件和快照数据文件 zkCli Zookeeper的客户端脚本 zkEnv 设置Zookeeper的环境变量脚本 zkServer Zookeeper服务器的启动、停止、和重启脚本 启动ZookeeperWindows操作： 进入到bin目录下，执行如下命令： zkServer 可以启动zookeeper服务。（需要注意windows下dataDir路径支持/和\\，但使用\会出现问题。） 下图是我公司电脑（Windows）上的zookeeper启动图。 Linux/Mac/Unix操作： 进入到bin目录下，执行如下命令： ./zkServer.sh start 可以启动zookeeper服务。 停止ZookeeperWindows操作： 直接可以Ctrl+C结束命令。 Linux/Mac/Unix操作： ./zkServer.sh stop PS:杀进程也可以结束，不推荐 连接ZookeeperWindows操作： 进入到bin目录下，执行如下命令： zkCli -server localhost:2181 可以连接到zookeeper服务。 下图是Windows电脑上连接到zookeeper服务的图。 Linux/Mac/Unix操作： 进入到bin目录下，执行如下命令： ./zkCli.sh -server localhost:2181 可以连接到zookeeper服务。 Zookeeper常用命令在zookeeper客户端，使用help可以查看可以使用的命令。 我们说几个比较常用的命令。 创建节点： create [-s] [-e] path data acl 使用此命令可以创建一个zookeeper节点。 其中，-s或-e分别指定节点特性，顺序或临时节点，若不指定，则表示持久节点；acl用来进行权限控制。 create -s /zk-test test 表示创建顺序节点zk-test。 可以看到创建的zk-test节点后面添加了一串数字以示区别顺序。 create -e /zk-temp test 表示创建临时节点。临时节点在客户端断开连接后会自动删除。 可以看到创建临时节点重新登陆后节点已被删除。 create /zk-permanent test 创建永久节点zk-permanent。 读取节点： ls path [watch] 可以列出节点下面的所有子节点。 get path [watch] 可以获取根节点数据内容和属性信息。 ls2 path [watch] 列出节点信息。 如下图： 更新节点： set path data [version] 我们将zk-permanent节点下的数据test变为test1。 可以看到dataVersion也发生了变化，用来表示已经进行了更新。 删除节点： delete path [version] 我们删除刚才创建的zk-permanent节点。 可以看到节点已经被删除。 PS：若删除节点存在子节点，那么无法删除该节点，必须先删除子节点，再删除父节点。 Zookeeper部署模式Zookeeper的部署模式一般分为3种，单机模式、伪集群模式、集群模式。 单机模式 单个Zookeeper服务，如上面所述。 伪集群模式 所谓伪集群, 是指在单台机器中启动多个zookeeper进程, 并组成一个集群。 以3个zookeeper服务进程为例。 需要有3个zookeeper项目。 三个项目配置特殊的地方： zookeeper1配置：1234567dataDir=D:/zookeeper1/zookeeper/datadataLogDir=D:/zookeeper1/zookeeper/logclientPort=2181server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 1 zookeeper2配置：1234567dataDir=D:/zookeeper2/zookeeper/datadataLogDir=D:/zookeeper2/zookeeper/logclientPort=2182server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 2 zookeeper3配置：1234567dataDir=D:/zookeeper3/zookeeper/datadataLogDir=D:/zookeeper3/zookeeper/logclientPort=2183server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 3 在同一台机器上，3者的端口号不能相同，不然出现端口冲突。 集群模式 集群模式下配置和伪集群相似。 服务器1配置：123456789tickTime=2000initLimit=10syncLimit=5dataDir=D:/zookeeper1/zookeeper/datadataLogDir=D:/zookeeper1/zookeeper/logclientPort=2181server.1=10.1.100.1:2888:3888server.2=10.1.100.2:2888:3888server.3=10.1.100.3:2888:3888 由于不在一个服务器上，故服务器2，服务器3也如上面的配置即可，它们3个唯一不同的地方是dataDir文件夹里面的myid文件，代表着服务器各自的编号。 Zookeeper特性由上面内容，我们汇总说下Zookeeper的一些性质。 类文件系统（数据节点） Zookeeper有类似于Unix文件系统的节点。它可以自由创建、修改和删除子目录，这些目录项被称为znode，znode可以存储数据。 Zookeeper有4种类型的znode： 1. PERSISTENT 持久化目录节点，客户端与zookeeper断开连接后，该节点依旧存在 2. PERSISTENT_SEQUENTIAL 持久化顺序编号目录节点，客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 3. EPHEMERAL 临时目录节点，客户端与zookeeper断开连接后，该节点被删除 4. EPHEMERAL_SEQUENTIAL 临时顺序编号目录节点，客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 通知机制 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。这是非常重要的一条性质。 高可用性 zookeeper的高可用性通过zookeeper集群实现。 Zookeeper一般有三种集群角色： Leader（领导者） Follower（跟随者） Observer（观察者） 一个 ZooKeeper 集群同一时刻只会有一个 Leader，其他都是 Follower 或 Observer。 ZooKeeper 默认只有 Leader 和 Follower 两种角色，没有 Observer 角色。为了使用 Observer 模式，在任何想变成Observer的节点的配置文件中加入 peerType=observer 并在所有 server 的配置文件中，配置成 observer 模式的 server 的那行配置追加 :observer即可。 ZooKeeper 集群的所有机器通过一个 Leader 选举过程来选定一台机器作为Leader，Leader服务器为客户端提供读和写服务。Follower 和 Observer 都能提供读服务，不能提供写服务。两者唯一的区别在于，Observer机器不参与 Leader 选举过程，也不参与写操作的过半写成功策略，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。 我们简单来看下： 我在自己的Mac机器上创建了4个zookeeper服务并启动（伪集群），其中一个是Observer，另三个其中一个是Leader，其它两个Follower。（3台机器中谁是Leader是选举出来的，一般情况下先启动的会成为Leader）。 配置如下(关键配置)：12345678910111213141516171819202122232425262728# zookeeper1的配置，myid文件里值为1clientPort=2181server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper2的配置，myid文件里值为2clientPort=2182server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper3的配置，myid文件里值为3clientPort=2183server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper4的配置，myid文件里值为4clientPort=2184peerType=observerserver.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer 启动后查看它们的角色： 可以看到zookeeper2是leader，我们停掉zookeeper2. 可以看到leader变成了zookeeper3. 我们继续把zookeeper3停掉。 可以发现zookeeper集群已经不能正常运行了。 zookeeper在执行增删改操作时，只要有半数以上服务器通过，就可以成功执行（不包括observer服务器） 半数以上投票通过：可以这样理解，客户端的增删改操作无论访问到了哪台zookeeper服务器，最终都会被转发给leader服务器，再由leader服务器分给zookeeper集群中所有follower服务器去投票（投票指的是在内存中做增删改操作），半数投票通过就被认为操作可执行（commit），否则不可执行。 由于在增删改操作中需要半数以上服务器通过，来分析以下情况。 2台服务器，至少2台正常运行才行，正常运行1台服务器都不允许挂掉。 3台服务器，至少2台正常运行才行，正常运行可以允许1台服务器挂掉。 4台服务器，至少3台正常运行才行，正常运行可以允许1台服务器挂掉。 ...... 故上面例子当挂掉两台服务器时，zookeeper服务已经不能正常运行了。 由上面可以知道，2n-1台服务器和2n台服务器都最多允许n-1台服务器挂掉（n&gt;=2）。 一般为了节省资源，环境基本配置奇数2n-1台服务器（不包括observer）。 Zookeeper应用场景Zookeeper的应用场景还是很多的，如： 1.命名服务 2.分布式协调服务/通知 3.数据发布与订阅（配置中心） 4.Master选举 5.分布式锁 等。 我会在后面结合例子研究zookeeper的一些应用场景。 结语通过对Zookeeper的一些学习，明白了Zookeeper的一些特点用途，了解了一些简单操作，还是蛮不错的一次学习过程。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成RocketMQ]]></title>
    <url>%2Fblog%2FSpringBoot%E9%9B%86%E6%88%90RocketMQ20181203%2F</url>
    <content type="text"><![CDATA[前言在开始之前，大家可以先了解RocketMQ的一些特性。 RocketMQ简介 今天我们接上之前说的，对RocketMQ进行简单使用。主要的也是讲如何在SpringBoot项目中使用RocketMQ。 环境RocketMQ安装我们先在RocketMQ官网上下载最新的MQ版本并进行安装。 可以通过镜像进行下载。 将压缩包解压并放在一个指定文件夹下。（这里要注意的是文件夹路径中尽量不要有空格，像Program Files这种，有可能导致mq无法正常启动） RocketMQ启动通过命令行进入到bin目录下，使用 mqnamesrv -n localhost:9876 （windows）可以启动mq的namesrv。如下图： 使用 mqbroker -n localhost:9876 （windows）可以启动mqbroker。如下图： 注意：上图表示RocketMQ的namesrv和broker启动成功，RocketMQ若正常使用应保证namesrv和broker均启动成功。 与Java集成使用主要依赖于rocketmq-client的jar包，在与SpringBoot进行集成时，应当引入该jar包。 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;$&#123;version.rocketmq&#125;&lt;/version&gt;&lt;/dependency&gt; 例子我们简单使用下该jar包创建消费者和生产者进行消费，来了解下它们的一些参数。 生产者： 12345678910111213141516171819public class ProducerTest &#123; public static void main(String[] args) throws Exception &#123; //producerGroup 生产者组名称 DefaultMQProducer producer = new DefaultMQProducer("producer1"); //设置NamesrvAddr producer.setNamesrvAddr("127.0.0.1:9876"); //设置自动创建Topic producer.setCreateTopicKey("AUTO_CREATE_TOPIC_KEY"); //调用start()方法启动一个producer实例 producer.start(); System.out.println("Producer started"); Message message=new Message(); message.setTopic("Test"); message.setTags("123"); message.setBody(new String("Hello").getBytes()); SendResult result=producer.send(message); System.out.println(result); &#125;&#125; 消费者： 123456789101112131415161718192021222324252627282930313233public class ConsumerTest &#123; public static void main(String[] args) throws Exception&#123; //需要consumerGroup DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("consumer1"); //设置NameServer consumer.setNamesrvAddr("127.0.0.1:9876"); //消费策略 //CONSUME_FROM_LAST_OFFSET 默认策略，从该队列最尾开始消费，即跳过历史消息 //CONSUME_FROM_FIRST_OFFSET 从队列最开始开始消费，即历史消息（还储存在broker的）全部消费一遍 //CONSUME_FROM_TIMESTAMP 从某个时间点开始消费，和setConsumeTimestamp()配合使用，默认是半个小时以前 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET); //Topic和Tag，*代表全部的Tag consumer.subscribe("Test", "*"); //设置一个Listener，主要进行消息的逻辑处理 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; msgs.forEach((a)-&gt;&#123; System.out.println(new String(a.getBody())); &#125;); //返回消费状态 //CONSUME_SUCCESS 消费成功 //RECONSUME_LATER 消费失败，需要稍后重新消费 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); //启动 consumer.start(); System.out.println("Consumer started"); &#125;&#125; 运行这两个类进行测试。 生产者结果： 消费者结果： 备注：如果启动中出现异常com.alibaba.rocketmq.client.exception.MQClientException: No route info of this topic, TestTopic可能是没有开启AUTO_CREATE_TOPIC，我们可以在启动broker的时候加上该参数，mqbroker -n localhost:9876&amp;autoCreateTopicEnable=true 可以保证使用时自动创建Topic。 生产等环境一般需要什么Topic就配置什么，不会开启这个参数让程序自动创建。 简要说明我们对上面例子里的一些参数等做些说明，以便于我们可以更好的封装功能。 可与之前RocketMQ的简介结合理解。 DefaultMQProducer部分参数 producerGroup：生产者组名称。 namesrvAddr：生产者NameSrvAddr的服务地址。 createTopicKey：可以创建指定的Topic。AUTO_CREATE_TOPIC_KEY为自动创建Topic。 其它参数略。 发送Message时，需要设置Message的Topic和Tag，并能收到发送状态结果。 DefaultMQPushConsumer部分参数 consumerGroup：消费者组名称。 namesrvAddr：消费者NameSrvAddr的服务地址。 ConsumeFromWhere：消费策略。 ---&gt; CONSUME_FROM_LAST_OFFSET 默认策略，从该队列最尾开始消费，即跳过历史消息 ---&gt; CONSUME_FROM_FIRST_OFFSET 从队列最开始开始消费，即历史消息（还储存在broker的）全部消费一遍 ---&gt; CONSUME_FROM_TIMESTAMP 从某个时间点开始消费，和setConsumeTimestamp()配合使用，默认是半个小时以前 subscribe方法：使消费者订阅指定的Topic和Tag。 registerMessageListener方法：注册消费者监听，用于消费消息，这里面也是我们业务逻辑的主要内容。有顺序消费和并行消费两种模式。 分别需要实现MessageListenerOrderly接口和MessageListenerConcurrently接口。 我们的例子是并行消费处理的。 小试牛刀每次处理时都要这样写会有很多的与业务无关的代码，也不美观。 我们对其进行必要封装，之前集成了SpringBoot的一个自己封装的starter插件，今天我们把RocketMQ也与SpringBoot集成下。 A 首先新建SpringBoot项目，引入RocketMQ 的jar依赖，不在详述。 然后在项目下创建必要的package。 producer：用来存放我们构建producer的类的包。 consumer：用来存放我们构建consumer的类的包。 listener:用来存放我们构建listener的类的包 factory:用来构建生产者和消费者群组的包。 config：存放SpringBoot配置类的包。 autoware:存放启动配置生效的类的包。 annotation：用来存放注解的包。 先说下简单思路吧。 首先这个生产者和消费者是可以有多个的，然后我们怎么管理它们？生产者可以发送顺序消息和并发消息，消费者可以处理顺序消息和并发消息，同时我们可能有两种业务要使用同一个Listener，如何解耦呢？ 关于管理：我们可以管理生产者和消费者的一个集合来解决。 关于解耦：可以提供一个接口，业务类实现这个接口拿到Message，进行处理。那如何知道这个业务类需要哪个listener呢？自然需要customerId或者listenerId。 好了开始工作。 B 先从配置入手。 下面是生产者和消费者的配置Bean。 由于可以配置多个生产者或者消费者，故使用List处理它们。部分代码如下： C 然后从消费者和生产者的提供入手。也是比较简单的，主要是根据参数生成一个生产者或者消费者，然后暴露一些方法，如start，stop等方法。 提供生产者的类，部分代码如下。 备注：生产环境一般不设置AUTO_CREATE_TOPIC_KEY，需要什么Topic要手动创建加入管理。 创建消费者的类，部分代码如下。 我们认为两个consumerId相等则获取的是一个Consumer，因此需要重写equals方法。 D 创建消费者监听，为处理消息提供一个接口。 使用抽象类部分实现这个接口。 这一步的目的是由于不同的业务逻辑可能用到一个监听，这样可以两个业务逻辑写到两个不同的类中，只需实现IProcessor。 写两个并行处理监听和顺序处理监听，对其进行实现。 并行监听： 1234567891011121314151617181920public class ConcurrentlyRocketMQMessageListener extends AbstractRocketMQMessageListener implements MessageListenerConcurrently &#123; private static final transient Logger logger = LoggerFactory.getLogger(ConcurrentlyRocketMQMessageListener.class); @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; logger.debug("ConcurrentlyRocketMQMessageListener receive message begin,length:&#123;&#125;" , msgs.size()); for(MessageExt msg:msgs ) &#123; for (IProcessor processor : processorList) &#123; try &#123; // 处理消息 process(processor, msg); &#125; catch (Exception ex) &#123; logger.error("ConcurrentlyRocketMQMessageListener error",ex); return ConsumeConcurrentlyStatus.RECONSUME_LATER; &#125; &#125; &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;&#125; 顺序监听： 12345678910111213141516171819public class OrderlyRocketMQMessageListener extends AbstractRocketMQMessageListener implements MessageListenerOrderly &#123; private static final transient Logger logger = LoggerFactory.getLogger(OrderlyRocketMQMessageListener.class); @Override public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; logger.debug("OrderlyRocketMQMessageListener receive message begin,length:&#123;&#125;" , msgs.size()); for(MessageExt msg:msgs ) &#123; for (IProcessor processor : processorList) &#123; try &#123; process(processor, msg); &#125; catch (Exception ex) &#123; logger.error("OrderlyRocketMQMessageListener error",ex); return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT; &#125; &#125; &#125; return ConsumeOrderlyStatus.SUCCESS; &#125;&#125; 然后需要对实现IProcesser接口的类添加如下注解，用来查看调用的哪个Listener。 1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface RocketMQProcessor &#123; /** * 该程序归哪个消费者监听处理 * @return */ String consumerId() default "";&#125; E 好，我们开始构建factory。 使用ConcurrentHashMap存储生产者和消费者集合。 1234//用于存放生产者的map组private static Map&lt;String, RocketMQMessageProducer&gt; producers=new ConcurrentHashMap&lt;&gt;();//用于存放消费者的map组private static Map&lt;String, RocketMQMessageConsumer&gt; consumers=new ConcurrentHashMap&lt;&gt;(); 创建生产者： 1234567891011121314151617181920212223242526272829/** * 创建一个生产者 * @param configuration * @return */public RocketMQMessageProducer createProducer(MqProducerConfiguration configuration)&#123; //如果map里存在这个实例，直接返回 if(producers.get(configuration.getProducerId())!=null)&#123; return producers.get(configuration.getProducerId()); &#125; //创建一个生产者 RocketMQMessageProducer producer=new RocketMQMessageProducer(configuration.getGroupName(), configuration.getNamesrvAddr()); if(configuration.getSendMsgTimeout()!=null)&#123; producer.setSendMsgTimeout(configuration.getSendMsgTimeout()); &#125; if(configuration.getMaxMessageSize()!=null)&#123; producer.setMaxMessageSize(configuration.getMaxMessageSize()); &#125; try &#123; //启动生产者并放入map进行管理 producer.start(); producers.put(configuration.getProducerId(), producer); logger.info("MqProducer start success "+configuration.toString()); &#125; catch (MQClientException e) &#123; logger.error("MqProducer start error "+configuration.toString(),e); throw new RuntimeException(e); &#125; return producer;&#125; 创建消费者： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 创建一个消费者 * @param mqConsumerConfiguration * @return */public RocketMQMessageConsumer createConsumer(MqConsumerConfiguration mqConsumerConfiguration, List&lt;IProcessor&gt; list) &#123; //如果map里存在，直接返回 if (consumers.get(mqConsumerConfiguration.getConsumerId()) != null) &#123; return consumers.get(mqConsumerConfiguration.getConsumerId()); &#125; try &#123; RocketMQMessageConsumer consumer = new RocketMQMessageConsumer(mqConsumerConfiguration.getConsumerId(), mqConsumerConfiguration.getGroupName(), mqConsumerConfiguration.getNamesrvAddr()); consumer.subscribe(mqConsumerConfiguration.getTopicAndTagMap()); //设置消费者其它参数 if(!CollectionUtils.isEmpty(mqConsumerConfiguration.getOptions()))&#123; String consumeFromWhere = mqConsumerConfiguration.getOptions().get("consumeFromWhere"); String consumeThreadMin = mqConsumerConfiguration.getOptions().get("consumeThreadMin"); String consumeThreadMax = mqConsumerConfiguration.getOptions().get("consumeThreadMax"); String pullThresholdForQueue = mqConsumerConfiguration.getOptions().get("pullThresholdForQueue"); String consumeMessageBatchMaxSize = mqConsumerConfiguration.getOptions().get("consumeMessageBatchMaxSize"); String pullBatchSize = mqConsumerConfiguration.getOptions().get("pullBatchSize"); String pullInterval = mqConsumerConfiguration.getOptions().get("pullInterval"); if (StringUtils.isNotBlank(consumeFromWhere)) &#123; if (StringUtils.equals(consumeFromWhere, "CONSUME_FROM_LAST_OFFSET")) &#123; consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET); &#125; else if (StringUtils.equals(consumeFromWhere, "CONSUME_FROM_FIRST_OFFSET")) &#123; consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); &#125; &#125; if (StringUtils.isNotBlank(consumeThreadMin)) &#123; consumer.setConsumeThreadMin(Integer.parseInt(consumeThreadMin)); &#125; if (StringUtils.isNotBlank(consumeThreadMax)) &#123; consumer.setConsumeThreadMax(Integer.parseInt(consumeThreadMax)); &#125; if (StringUtils.isNotBlank(pullThresholdForQueue)) &#123; consumer.setPullThresholdForQueue(Integer.parseInt(pullThresholdForQueue)); &#125; if (StringUtils.isNotBlank(consumeMessageBatchMaxSize)) &#123; consumer.setConsumeMessageBatchMaxSize(Integer.parseInt(consumeMessageBatchMaxSize)); &#125; if (StringUtils.isNotBlank(pullBatchSize)) &#123; consumer.setPullBatchSize(Integer.parseInt(pullBatchSize)); &#125; if (StringUtils.isNotBlank(pullInterval)) &#123; consumer.setPullInterval(Integer.parseInt(pullInterval)); &#125; &#125; //设置消费者监听 if(mqConsumerConfiguration.isOrderly())&#123; OrderlyRocketMQMessageListener orderlyRocketMQMessageListener=new OrderlyRocketMQMessageListener(); orderlyRocketMQMessageListener.setProcessorList(list); consumer.registerMessageListener(orderlyRocketMQMessageListener); &#125;else&#123; ConcurrentlyRocketMQMessageListener concurrentlyRocketMQMessageListener=new ConcurrentlyRocketMQMessageListener(); concurrentlyRocketMQMessageListener.setProcessorList(list); consumer.registerMessageListener(concurrentlyRocketMQMessageListener); &#125; consumer.start(); consumers.put(mqConsumerConfiguration.getConsumerId(), consumer); logger.info("MqConsumer start success "+mqConsumerConfiguration.toString()); logger.info("MqConsumer processors size "+list.size()); return consumer; &#125; catch (Exception e) &#123; logger.error("MqConsumer start error", e); throw new RuntimeException(e); &#125;&#125; 它们的其它方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344 /** * 获取一个生产者 * @param producerId * @return */ public RocketMQMessageProducer getProducer(String producerId)&#123; if(producers.get(producerId)!=null)&#123; return producers.get(producerId); &#125; return null; &#125; /** * 停止某个生产者 * @param producerId */ public void stopProducer(String producerId)&#123; if(producers.get(producerId)!=null)&#123; producers.get(producerId).shutdown(); producers.remove(producerId); logger.info("MqProducer "+producerId+" is shutdown!"); &#125; &#125;/** * 获取一个消费者 * @param customerId * @return */ public RocketMQMessageConsumer getConsumer(String customerId)&#123; if(consumers.get(customerId)!=null)&#123; return consumers.get(customerId); &#125; return null; &#125; /** * 停止某个消费者 * @param customerId */ public void stopConsumer(String customerId)&#123; if(consumers.get(customerId)!=null)&#123; consumers.get(customerId).shutdown(); consumers.remove(customerId); logger.info("MqConsumer "+customerId+" is shutdown!"); &#125; &#125; F 然后到了我们关键的自动配置部分了。 让这个类实现ApplicationContextAware可以拿到applicationContext。 同时生成一个FactoryBean。 主要方法： 12345678910111213141516171819202122232425262728293031323334353637@Bean@ConditionalOnMissingBean@ConditionalOnProperty(prefix = "spring.rocketmq.config", name = "enabled", havingValue = "true")RocketMQFactory rocketMQFactory()&#123; RocketMQFactory rocketMQFactory=new RocketMQFactory(); //处理生产者 if(mqConfigurations.getMqProducerConfigurations()!=null&amp;&amp;mqConfigurations.getMqProducerConfigurations().size()&gt;0)&#123; mqConfigurations.getMqProducerConfigurations().forEach(producerConfiguration -&gt;&#123; rocketMQFactory.createProducer(producerConfiguration); &#125;); &#125; if(mqConfigurations.getMqConsumerConfigurations()!=null&amp;&amp;mqConfigurations.getMqConsumerConfigurations().size()&gt;0)&#123; //处理消费者 mqConfigurations.getMqConsumerConfigurations().forEach(consumerConfiguration-&gt;&#123; //处理Processor final Map&lt;String, Object&gt; annotationMap = applicationContext.getBeansWithAnnotation(RocketMQProcessor.class); List&lt;IProcessor&gt; list = new ArrayList&lt;&gt;(); if(annotationMap!=null)&#123; annotationMap.forEach((key,value)-&gt;&#123; RocketMQProcessor annotation = value.getClass().getAnnotation(RocketMQProcessor.class); if(consumerConfiguration.getConsumerId().equals(annotation.consumerId()))&#123; try&#123; list.add((IProcessor) value); &#125;catch (Exception e)&#123; throw new RuntimeException(e); &#125; &#125; &#125;); &#125; rocketMQFactory.createConsumer(consumerConfiguration,list); &#125;); &#125; return rocketMQFactory;&#125; 总的来说就是拿到配置生产生产者组，生成消费者组。 在生成消费者的时候需要注册监听，一个监听可以有很多业务类，通过注解拿到业务类，放到处理器列表里，再把该监听注册到指定的customerId上。 G 配置spring.factories。 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.zwt.rocketmqspringboot.autoware.RocketMqAutoConfig H 然后我们添加application.properties进行测试。 12345678910111213141516171819202122232425262728## 是否启用RocketMQspring.rocketmq.config.enabled=true## Producer1配置spring.rocketmq.config.mq-producer-configurations[0].group-name=rocketmq-producerspring.rocketmq.config.mq-producer-configurations[0].namesrv-addr=localhost:9876spring.rocketmq.config.mq-producer-configurations[0].producer-id=001## Consumer1配置spring.rocketmq.config.mq-consumer-configurations[0].namesrv-addr=localhost:9876spring.rocketmq.config.mq-consumer-configurations[0].group-name=rocketmq-consumerspring.rocketmq.config.mq-consumer-configurations[0].consumer-id=001spring.rocketmq.config.mq-consumer-configurations[0].topic-and-tag-map.123=123spring.rocketmq.config.mq-consumer-configurations[0].orderly=falseserver.port=8001## Producer2配置spring.rocketmq.config.mq-producer-configurations[1].group-name=rocketmq-producer1spring.rocketmq.config.mq-producer-configurations[1].namesrv-addr=localhost:9876spring.rocketmq.config.mq-producer-configurations[1].producer-id=002## Consumer2配置spring.rocketmq.config.mq-consumer-configurations[1].namesrv-addr=localhost:9876spring.rocketmq.config.mq-consumer-configurations[1].group-name=rocketmq-consumer1spring.rocketmq.config.mq-consumer-configurations[1].consumer-id=002spring.rocketmq.config.mq-consumer-configurations[1].topic-and-tag-map.1234=1234spring.rocketmq.config.mq-consumer-configurations[1].orderly=false 编写一些测试类进行测试。 123456789@Service@RocketMQProcessor(consumerId = "001")public class TestConsumer implements IProcessor &#123; private static final Logger logger = LoggerFactory.getLogger(TestConsumer.class); @Override public void handleMessage(MessageExt msg) throws Exception &#123; System.out.println(new String(msg.getBody())+"TestConsumer"); &#125;&#125; 123456789101112131415161718@Controllerpublic class TestProducer &#123; @Autowired RocketMQFactory rocketMQFactory; @RequestMapping("/test") public String doSomething() throws Exception&#123; Message message=new Message(); message.setTopic("123"); message.setTags("123"); message.setBody(new String("Hello World").getBytes()); SendResult result=rocketMQFactory.getProducer("001").sendMessage(message); System.out.println(result); return result.toString(); &#125;&#125; 可以看到正常运行。 实战我们按照上篇文章那样把它封装成jar包。 完成后进行测试。 新建一个test的SpringBoot项目。作为消费者。 引入我们的包。 PS：这儿没有命名为xxxx-spring-boot-starter的形式。 创建一个消费者。 再新建一个test1的SpringBoot项目。作为生产者。 引入我们的包并进行配置。 启动test消费者，同时使用test1生产者发送一条消息。 可以看到。 生产者发送成功： 消费者处理成功： 总结通过对RocketMQ的集成封装使用，更好地学会了如何使用RocketMQ，及对其的更多理解。 消息中间件在我们软件开发中具有重要作用，应当好好理解。 如果觉得properties配置太繁琐可以改用yml配置，会更简介好看些。 如下： 1234567891011121314151617181920212223spring: rocketmq: config: enabled: true # 是否启用RocketMQ mq-producer-configurations: # 生产者配置 - producerId: TestProducer1 # 生产者1的id groupName: producer1 # 生产者1的组名称 namesrvAddr: 127.0.0.1:9876 # 生产者1的namesrvAddr - producerId: TestProducer2 # 生产者2的id groupName: producer2 # 生产者2的组名称 namesrvAddr: 127.0.0.1:9876 # 生产者2的namesrvAddr mq-consumer-configurations: # 消费者配置 - consumerId: TestConsumer1 # 消费者1的id groupName: consumer1 # 消费者1的组名称 namesrvAddr: 127.0.0.1:9876 # 消费者1的namesrvAddr topicAndTagMap: &#123; TestTopic1:TestTag1 &#125; # 消费者1监听的topic和tag orderly: false # 消费者1是否顺序消费消息 - consumerId: TestConsumer2 # 消费者2的id groupName: consumer2 # 消费者2的组名称 namesrvAddr: 127.0.0.1:9876 # 消费者2的namesrvAddr topicAndTagMap: &#123; TestTopic2:TestTag2 &#125; # 消费者2监听的topic和tag orderly: true # 消费者2是否顺序消费消息 代码地址：rocketmq-spring-boot]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建一个自定义的spring-boot-starter]]></title>
    <url>%2Fblog%2F%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84spring-boot-starter20181129%2F</url>
    <content type="text"><![CDATA[前言现在很多项目都使用SpringBoot构建了，不知道大家在使用SpringBoot项目时有没有注意到它的一些jar包的引用。如下图： 我们引入相关starter后，在application.properties或者application.yml里面进行相关配置，便可以尽情使用相关功能了，十分方便。 今天我们构建一个自己的spring-boot-starter，来体验下spring-boot的魅力和乐趣。 正文我之前有一篇文章是ElasticSearch的介绍并且对它的Java客户端封装了下，那个例子中我们最后的配置文件叫property/es-config.properties 不知道大家还有没有印象。要使用那个jar包，我们需要配置一个es-config.properties文件，显然，我想去掉这个文件，改为application.properties的那种配置，和各种spring-boot-starter一样。 好了，我们开始吧。 创建项目首先，我们需要创建一个名为 elasticsearch-spring-boot-starter 的SpringBoot项目（除springboot基础外不需要其他dependencies）。 这儿有一点要注意的，就是项目（后面生成jar包）命名。 备注：Spring官方的starter命名一般会采用 spring-boot-starter-xxxx的形式，而官方建议非官方的starter命名可以遵循 xxxx-spring-boot-starter的形式。 引入依赖然后我们引入我们要编写的starter的一些依赖。（这里对以前封装的ElasticSearch 客户端jar包进行继续优化封装） 备注：dependency里的optional表示可选依赖，因为spring-boot父项目有日志的相关jar，我们不必再次引用。 这样配置的优点大致如下： 节约空间（不用两个jar包重复引用） 避免license许可问题（两个jar版本不同可能出现问题） 避免jar包出现冲突等问题（两个jar版本不同可能出现问题） 构建（改造）项目然后我们把原来的项目改造下。 我们移除了配置解析的一些方法及类，改用自动配置。 使用@ConfigurationProperties注解，指定要进行属性配置的Bean。 使用该注解，需要引入下面的jar包。 这个jar包的@ConfigurationProperties注解可以帮我们把application.properties的属性值映射到Bean里。 这个jar的作用是编译时生成 spring-configuration-metadata.json ，此文件主要给IDE使用，可以进行友好的提示。 我们还需要创建一个自动配置类，名为ElasticSearchStarterAutoConfigure，如下： 123456789101112131415@Configuration@ConditionalOnClass(ElasticSearchClientFactory.class)@EnableConfigurationProperties(ElasticSearchConfiguration.class)public class ElasticSearchStarterAutoConfigure &#123; @Autowired private ElasticSearchConfiguration properties; @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = "spring.elasticserch.zwt", name = "enabled", havingValue = "true") ElasticSearchUtil elasticSearchUtil ()&#123; ElasticSearchClientFactory elasticSearchClientFactory=new ElasticSearchClientFactory(properties); return new ElasticSearchUtil(elasticSearchClientFactory); &#125;&#125; @Configuration注解，我们可以看到它被标识为 @Component注解，可以认为它最终也会成为spring容器里的一个Bean。 @ConditionalOnClass 当指定class存在时进行Bean创建。 @EnableConfigurationProperties 为指定class启用属性配置。 @ConditionalOnMissingBean 当bean不存在时进行bean创建。 @ConditionalOnProperty 用属性对bean进行配置创建。它里面可以enabled可以控制是否启用我们的配置。 然后在resources/META-INF 文件夹下新建spring.factories文件，添加如下内容，用于Spring加载自动配置类。 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.zwt.elasticsearchspringbootstarter.factory.ElasticSearchStarterAutoConfigure 配置完成后，在application.properties里，就可以看到我们的自定义属性啦。 测试创建测试类进行测试。 application.properties配置如下： 1234## es地址spring.elasticserch.zwt.address=127.0.0.1:9300## 是否启用es客户端spring.elasticserch.zwt.enabled=true 可以看到我们的日志输出。 当我们把spring.elasticserch.zwt.enabled设置为false时，在运行可以看到bean创建失败了，即未注入属性配置bean。 提升我们把项目用maven打包发布下（clean install），可以看到大约30多M……. 原因是它里面有原始的springboot项目，还内置了tomcat等，我们如果想把该工具jar包引入到另一个springboot项目里，这显然是不合理的。 我们还需要对其进行优化。 我们删除无用的Application class（springboot启动使用，显然我们封装jar包不是为了封装web项目），application.properties（当我们把我们工具jar引入到新的springboot项目里，里面已经有了application.properties）及test类包。 然后处理下pom文件。 在pom文件里，spring-boot-starter这个依赖我们是不需要打到包里的。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 将其添加属性。 1&lt;optional&gt;true&lt;/optional&gt; 同时build插件做下调整（原来用的springboot的插件）。 build插件基本固定的，代码可以不用看。 12345678910111213141516171819202122232425262728293031&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;attach&gt;true&lt;/attach&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 再次编译，这时候我们就可以看到jar包大小只有10几Kb啦。 应用我们随便创建一个新的springboot项目。引入我们封装的jar包。 配置application.properties文件 1234## es地址spring.elasticserch.zwt.address=127.0.0.1:9300## 是否启用es客户端spring.elasticserch.zwt.enabled=true 新建个测试类，对我们的jar包进行测试。可以看到正常运行。 总结通过一步步改造，我们成功创建了一个spring-boot-starter。 通过实践，我们也能看到springboot基于注解简化代码的配置。也是蛮不错的一次工具封装体验。 补充：上面说的@ConfigurationProperties注解也可以定义自己的配置文件。 如果我们想使用原来的es-config.properties则需要进行如下配置即可。 PS：SpringBoot官方已经有了对elasticsearch client的支持，建议使用官方相关支持工具。 自己实践时，可以简单的写一个HelloWorld去完成一个简单的spring-boot-starter的封装。 相关代码： https://github.com/JavaZWT/framework-base/tree/master/elasticsearch-spring-boot-starter]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>spring-boot-starter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口管理平台YAPI]]></title>
    <url>%2Fblog%2F%E6%8E%A5%E5%8F%A3%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0YAPI20181125%2F</url>
    <content type="text"><![CDATA[前言有没有为维护接口文档而烦恼过？ 作为一个良好的开发人员，为使用者提供准确良好的接口文档也是开发的一部分。 也便于测试创建测试用例。 接口管理有很多种方式，很多是提供接口文档供使用者参考，也有一些是通过平台管理。 今天介绍一款非常好用的工具，YAPI，其主要功能是一款优秀的API接口管理平台。旨在为开发、产品、测试人员提供更优雅的接口管理服务。 我在使用这个API管理平台时，感觉还蛮不错的，特此介绍下。 官网地址：https://yapi.ymfe.org/ 我们搭建内网环境，把这款接口管理平台在内网部署下，并且简单使用下。 正文要搭建内网环境，首先要准备以下几个软件。 nodejs （必须） mongodb （必须） git （非必须） git主要是从GitHub上 clone项目的，如果采用可视化安装或者直接下载压缩包是用不到的。 没有以上软件的需要下载安装。 nodejs下载 mongodb下载 安装不做过多介绍，自己参考一些资料即可完成。 对于YAPI服务，我们采用可视化部署，执行命令： 1npm install -g yapi-cli --registry https://registry.npm.taobao.org 注：其他部署方式，详情可以查看官方文档 https://yapi.ymfe.org/devops/index.html 这儿不做讨论。 安装成功后启动 yapi server，执行命令： 1yapi server 通过浏览器打开页面 http://0.0.0.0:9090/ 会看到如下界面： 注意：需要启动mongdb数据库。 mongodb启动备注(路径未加入PATH):12cd /Users/zhangwentong/Applications/mongodb/mongodb-osx-x86_64-4.0.4/bin./mongod --dbpath /Users/zhangwentong/Applications/mongodb/data/db 部署引导完成后。我们进入到vendors 文件夹，启动YAPI服务。如下命令： 12cd my-yapi/vendors/node server/app.js 说明我们安装完成并启动成功了。浏览器打开图上地址。 使用管理员账号登陆，初始密码为 ymfe.org 其他人可以进行注册后登陆。 登陆成功后如下图： 我们可以在个人中心修改密码，这儿就不做过多介绍了。看一看它的主要功能吧。 首先我们创建一个分组。 然后我们在该分组里创建一个项目。 上图的成员列表里可以添加成员，把一起开发的开发者拉进来。 分组动态展示我们分组的一些操作等。 我们可以看到项目里内容如下： 接口主要就是用来管理我们的接口的界面，比如后台提供某些接口，全部放在里面，供其它开发人员查看使用。 动态代表着这个项目里的接口等改动动态，方便人员查看。 成员管理可以管理这个项目的成员，指定组长，开发者，外部接口使用者（访客）等。 然后我们可以添加一些模块接口分类。如下，不同接口放在不同模块下。 继续，我们新增加一个接口来看看吧。 新增OK后对接口进行修改。 当我们接口参数定义好，或者说后台接口完成后（可以测试时），我们应该把接口状态改为已完成。 一个写好的接口demo如下： 同时动态里也可以看到我们对于接口的改动。 同时YAPI还支持测试接口，mock，wiki等，类似于Postman等工具。 这儿就不过多介绍了，对于我们开发人员，最主要的应该属对接口文档的维护了，也就是刚才介绍的部分。 结语今天说到这里也差不多啦，YAPI还可以控制人员的权限、给指定人员发送邮件、设置是否可以注册等其它功能，有兴趣的可以摸索。 我们对YAPI的使用就是用其对接口文档进行管理。它是易用的简便的。 在项目开发过程中，在公司服务器上搭建一套环境，对组成员接口文档维护也是受益的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>YAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 新的日期API]]></title>
    <url>%2Fblog%2FJava-8-%E6%96%B0%E7%9A%84%E6%97%A5%E6%9C%9FAPI20181122%2F</url>
    <content type="text"><![CDATA[前言Java 8 中引入了新的一套日期API，相对于之前的Date或者Calendar类，这套API更好的解决了日期和时间的问题。 我们来简单看下。 正文LocalDateTime,LocalDate,LocalTime开始使用日期API时，最先碰到的也是这三个类。 他们的静态工厂of方法可以创建日期实例。如下代码： 1234567//1. 第一部分//创建一个日期LocalDate date=LocalDate.of(2018,11,22);//时分秒LocalTime time=LocalTime.of(13,45,20);//LocalDateTime对象LocalDateTime dateTime=LocalDateTime.of(2018,Month.NOVEMBER,22,13,45,20); 是不是很简单。 要注意这三个类都是final的，即不可被改变的。 LocalDate 表示年月日，LocalTime表示时分秒，LocalDateTime表示年月日时分秒。 他们三个之间的转换也是比较容易的。如下： 1234567891011//2. 第二部分//使用localDate和localTime构造一个LocalDateTimeLocalDateTime dateTime1=LocalDateTime.of(date,time);//使用LocalDate构造一个LocalDateTimeLocalDateTime dateTime2=date.atStartOfDay();//这一天的00:00:00LocalDateTime dateTime3=date.atTime(LocalTime.of(12,12,12));//指定这一天的时间//使用LocalTime构造LocalDateTimeLocalDateTime dateTime4=time.atDate(LocalDate.of(2018,11,22));//指定日期//通过LocalDateTime获取LocalDate和LocalTimeLocalDate date1=dateTime.toLocalDate();LocalTime time1=dateTime.toLocalTime(); 对于固定的日期，我们可以获取它的时间信息，也是比较容易的。如具体年月日的数值，等等。 123456789101112131415161718192021//3. 第三部分//获取日期年份int year=dateTime.getYear();int year1=dateTime.get(ChronoField.YEAR);//获取日期月份Month month=dateTime.getMonth();int month1=month.getValue();int month2=dateTime.get(ChronoField.MONTH_OF_YEAR);//获取当月第几天int day=dateTime.getDayOfMonth();int day1=dateTime.get(ChronoField.DAY_OF_MONTH);//获取星期几DayOfWeek dow=dateTime.getDayOfWeek();//获取该月有几天int len=date.lengthOfMonth();//获取小时数int hour=dateTime.getHour();//获取分钟int minute=dateTime.getMinute();//获取秒数int second=dateTime.getSecond(); 他们还有一些常用的方法，如时间和字符串时间之间的转化、判断闰年、获取当前时间信息、时间的比较、时间的加减天数（年数等）等方法。 相比于我们单独封装处理Date，或者Calendar类，更简便和安全了。 1234567891011121314151617181920212223242526272829//4. 第四部分//是不是闰年boolean leap=date.isLeapYear();//获取当前时间信息LocalDateTime localDateTimeNow=LocalDateTime.now();LocalDate localDateNow=LocalDate.now();LocalTime localTimeNow=LocalTime.now();//字符串转时间LocalDate localDate1=LocalDate.parse("2014-03-18");LocalTime localTime1=LocalTime.parse("13:45:20");LocalDateTime localDateTime1=LocalDateTime.parse("2018/11/22 11:22:33", DateTimeFormatter.ofPattern("yyyy/MM/dd HH:mm:ss"));//时间转换为字符串String localDateStr=localDate1.toString();String localTimeStr=localTime1.format(DateTimeFormatter.ofPattern("HH:mm:ss"));String localDateTimeStr=localDateTime1.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"));//当前时间减10天LocalDateTime localDateTime2=localDateTime1.minusDays(10);//当前时间加1年LocalDateTime localDateTime3=localDateTime1.plusYears(1);//当前时间加1个月LocalDateTime localDateTime4=localDateTime1.minus(-1, ChronoUnit.MONTHS);//当前时间加1个月LocalDateTime localDateTime5=localDateTime1.plus(1, ChronoUnit.MONTHS);//更改日期时间，返回新的对象，原对象不会变化LocalDate localDate=date1.with(ChronoField.MONTH_OF_YEAR,9);//日期时间比较boolean flag=localDateTime2.isAfter(localDateTime3);boolean flag1=localDateTime2.isBefore(localDateTime3);boolean flag2=localDateTime2.isEqual(localDateTime3); 这里面对于时间的加减（plus，minus方法），赋值（with方法）都会生成新的LocalDateTime对象，不会对原来的对象做修改。 Period、Duration 类这两个类都可以表示日期时间的差值，Period的话表示年月日，如两个时间差1年或者-1个月，Duration 的话用来表示天时分秒，比如它可以表示两个时间差相差34.5s这样的数据。 123456789//5. 第五部分//计算两个时间差Duration d1=Duration.between(localDateTime2,localDateTime3);long days1=d1.toDays();Duration d2=Duration.between(localDateTime4,localDateTime5);long hours=d2.toHours();//计算相差时间，结果10天Period period=Period.between(LocalDate.of(2014,3,8),LocalDate.of(2014,3,18));int days2=period.getDays(); Instant和 ZoneId从计算机的角度来看，建模时间最自然的格式是表示一个持续时间段上某个点的单一大整型数。这也是新的 java.time.Instant 类对时间建模的方式，基本上它是以Unix元年时间（传统的设定为UTC时区1970年1月1日午夜时分）开始所经历的秒数进行计算。而每个ZoneId由该地区的ID标识。地区ID都为“{区域}/{城市}”的格式，这些地区集合的设定都由英特网编号分配机构（IANA）的时区数据库提供。 看下它们的用法。 123456789101112131415161718//6. 第六部分//机器时间//以下均表示3s时间Instant.ofEpochSecond(3);Instant.ofEpochSecond(3,0);Instant.ofEpochSecond(2,1000000);Instant.ofEpochSecond(4,-1000000);//当前时间的时间戳Instant.now();//localDateTime转换为instantInstant instantFormDateTime=localDateTime1.toInstant(ZoneOffset.UTC);//获取本地区的zoneIdZoneId romeZone= TimeZone.getDefault().toZoneId();//instant转localDateTimeInstant instant1=Instant.now();LocalDateTime timeFromInstant=LocalDateTime.ofInstant(instant1,romeZone);//将时区设置为欧洲罗马城市。ZoneId romeZone1 = ZoneId.of("Europe/Rome"); 一般传统的Date和LocalDate之间的转换会用到它们（Instant，ZoneId）。 其他在一些项目中，应用Date还是很多的。我们想使用新的日期API，又不太想改动源代码。可以写一些转换的工具类，或者使用Java8的一些日期API对Date进行处理。 如Date转化为LocalDateTime，LocalDateTime转换为Date等。 12345678910111213141516171819202122232425/** * Java8 LocalDateTime对象转Date对象 * @param localDateTime * @return */public static Date localDateTime2Date(LocalDateTime localDateTime)&#123; //获取zoneId ZoneId zone = ZoneId.systemDefault(); //将localDateTime转换为Instant对象 Instant instant = localDateTime.atZone(zone).toInstant(); return Date.from(instant);&#125;/** * Date对象 转 Java8 LocalDateTime对象 * @param date * @return */public static LocalDateTime date2LocalDateTime(Date date)&#123; //根据date拿到Instant Instant instant = date.toInstant(); ZoneId zone = ZoneId.systemDefault(); //转换为LocalDateTime return LocalDateTime.ofInstant(instant, zone);&#125; 将字符串日期格式化为LocalDateTime，或者将LocalDateTime转换为字符串时间。 12345678910111213141516171819/** * 将日期格式化为指定的格式 * @param date * @return */ public static String localDateTime2String(Date date)&#123; LocalDateTime localDateTime=date2LocalDateTime(date); return DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss").format(localDateTime); &#125; /** * 将string时间格式转化为LocalDateTime * @param string * @return */ public static LocalDateTime string2LocalDateTime(String string)&#123; DateTimeFormatter df=DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); return LocalDateTime.parse(string,df); &#125; 有了这些比较基础的方法，我们某些日期便可以使用Java8处理。比如项目使用Date，在判断闰年，或者时间的加减等，可以封装Java8工具类。 12345678910111213141516171819202122232425262728293031/** * 获取两个日期的时间间隔 * @param date1 * @param date2 * @return */public static long diffDays(Date date1,Date date2)&#123; LocalDateTime localDateTime1=date2LocalDateTime(date1); LocalDateTime localDateTime2=date2LocalDateTime(date2); return Duration.between(localDateTime1,localDateTime2).toDays();&#125;/** * 当期日期增加多少天后的日期，负数为减少多少天 * @param date * @param days * @return */public static Date addDays(Date date,int days)&#123; LocalDateTime localDateTime1=date2LocalDateTime(date); LocalDateTime localDateTime2=localDateTime1.minusDays(days); return localDateTime2Date(localDateTime2);&#125;/** * 判断是否是闰年 * @param date * @return */public static boolean isLeapYear(Date date)&#123; LocalDateTime localDateTime=date2LocalDateTime(date); return localDateTime.toLocalDate().isLeapYear();&#125; 这样对于Date的处理，均使用LocalDateTime处理，虽然整个项目把Date改造成LocalDateTime较困难，但是工具类相当于黑匣子，这样慢慢使用LocalDateTime去处理Date，也是蛮不错的一次体验。 总结Java8 新的日期API比较优秀的地方是更直观了，使用更简洁。而且不用担心变量污染问题，想想一般的Date，对日期进行操作，如果不小心没有创建新对象，会把传入的Date改变掉，是很不安全的。 而且它也没有时间从1900年起那种莫名其妙的限制，而且获取到的月份值是1-12，不是0-11，也是符合自然的。 其实Java 8 的日期API不单单完成了对于Java日期的优化，其更多的方法更像是一种工具API，如判断是不是闰年，求两日期之差等等常用方法，都被封装在了Java8的新的日期API里面了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码的优雅]]></title>
    <url>%2Fblog%2F%E4%BB%A3%E7%A0%81%E7%9A%84%E4%BC%98%E9%9B%8520181117%2F</url>
    <content type="text"><![CDATA[前言很早之前就想写这么一篇文章，苦于准备不足和时间有限吧，进度一拖再拖。 来到新公司差不多3个多月了，让我感到或者说受益比较大的可以说就是公司的代码风格吧。 估计也是由于有架构师的缘故，大家的代码质量感觉都比较高。 扯远了…… 今天结合实际聊一下关于如何让代码变得规范、优雅吧。 正文遵守规范这一点或许可以说很笼统，目前大家除了遵守公司制定的一些规范，针对于Java我还比较建议的是阅读下阿里的《阿里巴巴Java开发手册》，H5前端的可以看看Github 上的这个规范http://alloyteam.github.io/CodeGuide/ ，也是蛮不错的。 必要的封装当有一些重复，或者有共性的代码时，我们可以对它们进行包装。小到一些特殊的判断，大到常用的设计模式。 如下面的代码，实现相同的功能，明显第二种要好些。 这些必要的封装不仅可以减少一些额外的工作量，还能提高代码可阅读性。 一些工具包，如apache.commons 已经封装了一些通用校验，我们是可以直接拿来使用的。 学会占位符的使用这一点打印日志比较常见到。如下： 可以看到两者的输出是一样的，但是第一种是方便我们进行程序源码阅读的。 除了String.format里的%s占位符，log4j里的{}占位符也可以实现这种作用。 枚举和常量的使用正确使用一些枚举和常量可以提高代码的阅读性。 比如我们创建一个水果枚举。 进行某些方法，这些方法更容易被阅读和维护。 异常的正确使用或许你对异常不感冒，觉得有异常还要try catch是一件很费劲的事情，也影响代码美观，怎么会使代码优雅呢？ 我们可以举个例子看一下异常的一种好玩的用法。 比如有一个业务逻辑，需要通过分布式接口获取其它数据，比如分三步： 这种情况下每个Res我们需要判断成功，才能获取数据。 即retCode为0000时。其它为失败。 这时候我们可以创建异常类。用于对retCode和retInfo进行接收。 如果上面步骤中获取失败，直接上抛。 最后可以使用try catch统一处理返回。 还有一些异常的用法，在这里就不一一说明了。 必要时尝试使用多线程可以在必要时使用多线程来提高程序运行效率。 对于上面的例子，查询可用余额，查询提现次数，查询银行卡信息相互之间是不影响的（他们的结果只取决于userId或者customerId的值）。我们可以使用Future获取结果，减少程序运行时间。 使用多线程分别去请求获取结果，最后在进行处理。 结语规范的代码不仅自己赏心悦目，别人看后也有所启发，养成编写良好代码的习惯，对自己、对别人都是有所帮助的。 其实最重要的还是多学、多练、多思考吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[和朋友的一次关于数组问题讨论]]></title>
    <url>%2Fblog%2F%E5%92%8C%E6%9C%8B%E5%8F%8B%E7%9A%84%E4%B8%80%E6%AC%A1%E6%95%B0%E7%BB%84%E9%97%AE%E9%A2%98%E8%AE%A8%E8%AE%BA20181107%2F</url>
    <content type="text"><![CDATA[前言前两天下班和一朋友一起，被问到如下问题：如何移除一个数组中某些符合条件（或不符合条件）的元素形成新的数组，要求数组长度也应该变化（这是我后面加的）。 这两天研究下了这个问题，感觉比较有意思，也从中学到了一些其它该注意的东西。 特此来分享一下。 如有问题欢迎批评指正。 正文对于上面的问题，最简单也最应该想到的一个应该是借助List这个工具类。 是的，我写下大致如下代码： （1）使用List帮助类实现,借助remove方法优点：代码简洁，便于理解，不易出错 缺点：数组和List转换耗时，效率不算太高，使用List remove方法时应注意线程安全问题，如果是基本数据类型，需要转换为包装类，拆箱装箱也是影响效率的一个因素。 代码如下： 123456789101112131415/** * 过滤形成新的数组（转换为List，通过List里面remove(安全的)移除元素） * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray1(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //asList生成的是一个List的内部类，无法调用add remove delete方法会抛出异常 List&lt;T&gt; sourceList= new ArrayList&lt;&gt;(Arrays.asList(sourceArray)); //使用Java8 函数式接口，移除不符合条件的元素 sourceList.removeIf(predicate); return sourceList.toArray((T[]) Array.newInstance(type,0));&#125; 注： Predicate predicate 相当于一个判断条件（lambda表达式，Java8语法），具体的问题也可以写明，就是那种for list 在加上 if 条件判断的，这儿我就不啰嗦了。Java 8里面对于集合类，新增了 removeIf方法，我们可以看下它，其实就是我们上面说的那个。 关于这种写法，有需要注意的几点： for-each，for, iterator 这三种对于List的循环，for，iterator是可以遍历并进行remove操作的，但是for-each是不可以的。有兴趣的可以研究研究，这不是我们的重点。 看到我上面使用的泛型T，其实使用Object[]也是可以的，要注意一个问题，泛型T是不能包含int,long等基本数据类型的，使用的话只能转化为它们的包装类。 Object[]是没有限制的，但是Object[]转换为Integer[]或者int[]或者其他不能直接转换，如下写法是错误的，会出现问题。 12Object [] objects=new Object[10];Integer [] a=(Integer[])objects; 正确的转换方法应该循环里面的元素，并对它们强转添加进数组；或者使用下面的方法。123Object [] objects=new Object[10];//Integer [] a=(Integer[])objects;Integer [] a=(Integer[])Array.newInstance(Integer.class,objects.length); 当我们把数组转换成List的时候，为了不想循环遍历添加，可能会想到使用Arrays.asList(T t) 这个方法，这个方法生成的List是一个Arrays里面的一个内部类ArrayList。 看下这个内部类你会发现它没有对remove、add等方法的实现，也就是继承自AbstractList。 123public E remove(int index) &#123; throw new UnsupportedOperationException();&#125; 也就是执行它们时会抛出异常，如果想使用remove方法，应该把它转为具体实现，如ArrayList。 1List&lt;T&gt; sourceList= new ArrayList&lt;&gt;(Arrays.asList(sourceArray)); 为什么它要使用内部类处理这个方法呢？也是比较有趣的，我这儿还没做深入研究，有时间研究下。 (2)使用两次循环实现。原理是第一次数组循环查找符合条件（或不符合条件）的个数count，后面在创建一个指定长度（原数组总长度-count）的数组，然后在遍历循环一遍原数组，将符合条件（或不符合条件）的元素添加进新的数组。 优点：简单直接，易于理解，基本数据类型数组的处理应该比List方法有优势。 缺点：两次循环应该比较耗时，对于长Array应该显现的明显。 代码如下： 12345678910111213141516171819202122232425262728293031323334/** * 过滤形成新的数组（两次循环查找符合条件的，移动过去） * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray2(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; int count=0; for (T t:sourceArray)&#123; if(predicate.test(t))&#123; count++; &#125; &#125; //都不符合条件 if(count==0)&#123; return Arrays.copyOf(sourceArray,sourceArray.length); &#125; //都符合条件 if(count==sourceArray.length)&#123; return (T[]) Array.newInstance(type,0); &#125; T [] targetArray=(T[]) Array.newInstance(type,sourceArray.length-count); int index=0; for (T t:sourceArray)&#123; if(!predicate.test(t))&#123; targetArray[index]=t; index++; &#125; &#125; return targetArray;&#125; 注意：还有一种情况，如果过滤的数据（符合条件或者不符合条件的）出现次数较低，我们是不是可以考虑一次拿出一整段进行处理。当然，如果频率较高，比如在一堆自然数中取偶数，明显奇数偶数出现频率相近，都为50%，那么我们可能用上面这种方法效率也很好。 (3)预先设置等长数组，而后截取得到目标数组对比上一种方法，这是一种空间换时间的做法。 开始时创建一个和原数组相同大小的数组，遍历后把元素放进去，最后将数组截短。这种方法仅仅循环一次。 代码如下： 123456789101112131415161718192021222324252627/** * 对比第二种方法，这属于空间换时间的做法 * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray3(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //直接创建一个空的一样长的数组 T[] tempArray=(T[]) Array.newInstance(type,sourceArray.length); //不符合条件的数量 int count=0; for (T t:sourceArray)&#123; //拿到不符合过滤条件的，一个个赋值给新数组 if(!predicate.test(t))&#123; tempArray[count]=t; count++; &#125; &#125; //最后这个数组长度&lt;=原数组长度 //特殊处理下 if(count==0)&#123; return (T[]) Array.newInstance(type,0); &#125; return Arrays.copyOf(tempArray,count);&#125; (4)借助List的toArray方法借助List实现循环一次把符合条件的放到里面，再把List强转成数组。也是不错的实现方法。 代码如下： 123456789101112131415161718/** * 借助list拿到符合条件的，在强转成数组 * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray4(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //记录符合条件的元素下标 List&lt;T&gt; targetList=new ArrayList&lt;&gt;(); for (T t:sourceArray)&#123; if(!predicate.test(t))&#123; targetList.add(t); &#125; &#125; return targetList.toArray((T[]) Array.newInstance(type,0));&#125; 无论上面哪种方法，其底层都使用了System.arraycopy方法。 12345678910/*** 数组复制核心方法* @param src 原数组* @param srcPos 原数组要复制的起始位置下标* @param dest 目标数组* @param destPos 目标数组的起始位置下标* @param length 要复制的长度* @return*/public static native void arraycopy(Object src, int srcPos,Object dest, int destPos,int length); (5)Java8串行处理方式Java8中使用Stream操作集合工具类来对其进行处理。分为串行和并行两种方式。先来看看串行。 代码如下： 1234567891011121314/** * Java8 串行流语法(收集符合条件的) * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray5(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; return Arrays.stream(sourceArray).sequential(). filter(predicate). collect(Collectors.toList()). toArray((T[]) Array.newInstance(type,0));&#125; 其底层也是元素的循环遍历。 （6）Java8并行处理方式我们应该知道，无论方法怎样，至少应该遍历数组一次以判断该元素是否符合条件。当数据量较大时，这儿会成为方法运行时间的瓶颈，由于List家族中ArrayList是有序的，我们可以使用多线程对它进行分割，每段进行遍历筛选结果，最后再把结果合起来。 并行流就是利用分支/合并框架实现的，使用了多线程。当数组数据量较大时效率是明显的。 Java8的相关API已经封装好，我们可以直接使用。 代码如下： 1234567891011121314/** * Java8 并行流语法(收集符合条件的) * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray6(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; return Arrays.stream(sourceArray).parallel(). filter(predicate). collect(Collectors.toList()). toArray((T[]) Array.newInstance(type,0));&#125; (7)自己动手，丰衣足食我们其实是可以借助多线程自己实现一个相似工具类的。 可以使用分支/合并框架自己实现一个多线程的处理。 关于这一块，我有一篇文章 一道Java试题引发的思考中有具体例子及测试。 大家可以看下，数据量大的情况下并行效率还是比较明显的。 我这儿对这个例子就不在验证了。 注意：使用并行流（或者说多线程）要注意的点。 首先是数据量，数据量的主要意义就在于单线程处理的耗时（处理数据的时间）已经超过了多线程耗时（数据处理时间+拆分数据时间+合并结果时间），这一点是比较难把控的。其次一点是要确定这些数据可以使用多线程处理，不会产生意外的情况，比如我们这个问题，我想删除两个数相差1的所有元素，剩下的元素生成一个新的数组，多线程显然不易解决这种问题，或者解决起来较复杂。 测试测试的话今天就省了，（3）、（4）、（5）都是不错的写法，（6）话具体问题具体分析，（7）的话有想法的可以试试，（1），（2）不推荐。 因为数据量大小，数据类型都对方法有些影响。 比如较短的原数组，基本数据类型，（3）方法效率很快的，对比（4）、（5）是没有数据拆箱操作的。换成长数组，引用数据类型，（6）可能效率就高了。 结语开始写这篇文章的时候脑子不好使，根本没想到Java8的Stream，失误。我甚至一开始想的都是些可能不安全（（1）方法），或者比较繁琐（（2）方法）的方法，过了一天晾了晾脑子就好使多了。 在处理时，因为数组底层操作都是基于System.arraycopy嘛，我想到是不是循环一次记录符合条件（不符合条件）的元素下标（可用List记录），然后建立一个目标数组，使用System.arraycopy一段一段的将数据copy进去。 无奈才疏学浅，想了半天使用System.arraycopy时的两个起始位置，copy长度始终弄错了，仔细想了下，估计这种方法效率也不怎么高。 哈哈，于是就没写。 大家有什么好的、耳目一新的方法也可以说出来交流交流。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ简介]]></title>
    <url>%2Fblog%2FRocketMQ%E7%AE%80%E4%BB%8B20181103%2F</url>
    <content type="text"><![CDATA[前言 为什么要学习RocketMQ？ RocketMQ是一个队列模型的的消息中间件，具有高性能、高可靠、高实时、分布式等特点。 其主要特点如下： 生产者、消费者、队列都可以分布式。 能够保证严格的消息顺序。 提供丰富的消息拉取模式。 高效的订阅者水平扩展能力。 实时的消息订阅机制。 亿级消息堆积能力。 较少的依赖。 RocketMQ的基本原理就是生产者（Producer）向一些队列轮流发送消息，队列集合称为Topic，消费者（Consumer）如果做广播消费，则一个消费者实例消费这个Topic对应的所有队列，如果做集群消费，则多个消费者实例平均消费这个topic对应的队列集合。 消息中间件收发消息的典型模型如下图： 好吧，我承认前言太官方了。我们来看下正文关于RocketMQ的一些特点吧。 正文RocketMQ术语。先介绍RocketMQ的几个术语。 Producer 消息生产者，生产者的作用就是将消息发送到Broker（MQ），生产者本身既可以产生消息。也可以对外提供接口，由外部应用来调用接口，再由生产者将收到的消息发送到 Broker（MQ）。 Producer Group 生产者组，一般来说多个发送同一类消息的生产者称之为一个生产者组。 Consumer 消息消费者，消费 Broker（MQ） 上的消息的应用程序就是消费者，消息的处理取决于业务。 Consumer Group 消费者组，和生产者类似，消费同一类消息的多个 consumer 实例组成一个消费者组。 Topic Topic 是一种消息的逻辑分类，比如说你有支付订单类的消息，也有奖品抽取类的消息，那么就需要进行分类，一个是支付订单 Topic 存放支付订单相关的消息，一个是奖品抽取 Topic 存储抽取奖品结果相关的消息。 Message Message 是消息的载体。一个 Message 必须指定 topic。Message 还有一个可选的 tag 设置，以便消费者可以基于 tag 进行过滤消息。也可以添加额外的键值对等。 Tag 标签可以被认为是对 Topic 进一步细化。一般在相同业务模块中通过引入标签来标记不同用途的消息。 Broker Broker 是 RocketMQ系统的主要角色。Broker 接收来自生产者的消息，储存以及为消费者拉取消息的请求做好准备。 Name Server Name Server 为 producer 和 consumer 提供路由信息。 对于上面的几个概念的理解，可以从一封邮件的发送入手，认为发信人是Producer，收信人为Consumer，Message，Topic和Tag分别指信的内容，信的分类规则等，Broker就相当于邮局了。 RocketMQ集群架构再来看一下RocketMQ常用的物理部署结构。（生产环境常用的RocketMQ集群架构） RocketMQ的集群特点如下： Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server。 Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。要注意Producer并不能和 Broker Slave建立连接。 Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 Broker Master和 Broker Slave之间会进行数据同步，即Data Sync。 数据复制主要有两种： ①同步复制 SYNC_MASTER master 和 slave 都写成功后返回成功状态。好处是如果master出故障，slave上有全部备份，容易恢复。缺点是增大延迟，降低吞吐量。 ②异步复制 ASYNC_MASTER 只要 master 写成功就返回成功状态。好处是低延迟、高吞吐，缺点是如果 master 出故障，数据没有写入 slave，就会有丢失。 master或者 slave在返回处理成功消息时，需要将数据持久化（写入磁盘）。称之为MQ的刷盘。 刷盘策略也有两种： ①同步刷盘 SYNC_FLUSH 返回成功状态时，消息已经被写入磁盘。 消息写入内存后，立即通知刷盘线程，刷盘完成后，返回消息写成功的状态。 ②异步刷盘 ASYNC_FLUSH 返回成功状态时，消息只是被写入内存，写操作返回快，吞吐量达，当内存里的消息积累到一定程度时，统一出发写磁盘动作，快速写入。 RocketMQ部署模式根据上面的描述，我们也大致可以知道RocketMQ通常有3种部署模式。 单master部署 简单来说就是环境上就一台RocketMQ，一般自己开发研究学习会用这种模式。这种部署明显的缺点就是MQ宕机后系统就会挂掉。所以不用于测试或生产环境。 多master部署 就是环境上有多台RocketMQ，都充当master的角色。这种方式部署，如果有一台MQ挂掉或者重启，并不会对整个应用系统造成影响。但是该节点MQ宕机期间，未消费的MQ消息是无法被消费的，只有等该节点故障排除后才能恢复正常。如果无法恢复，则会造成RocketMQ消息的永久丢失。 这种方式，如果是使用的异步刷盘，则故障发生并恢复正常后可能丢失部分消息（写入内存没来得及写入磁盘的消息）；如果同步刷盘，则不会出现此情况，但同步刷盘会对MQ的响应速度造成一些影响。 多master 多slave部署 根据刷盘策略和数据复制策略。我们知道这种部署方式基本有四种组合策略。同步复制同步刷盘明显是效率最差但最安全的，异步复制异步刷盘明显是效率最好但最不安全的。 一般情况下，我们比较常用的是异步刷盘+同步复制的模式。这样即使master 宕机，同步复制的slave也能保证把消息写入磁盘。同时使用异步刷盘策略，因为写入磁盘本身是应用系统中耗时的一个操作，先写入内存，随后写入磁盘，可以保证MQ的响应速度。 结语这一篇文章简单介绍了RocketMQ的一些性质和特点，了解这些特点，才能更好的掌握RocketMQ，后面我们会搭建一个RocketMQ服务进行下学习，并对我们的调用代码做些优化，封装为自己的一个小小工具类。 回答下最开始的问题，因为我们公司用到了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring Transaction 注解不生效bug引发的思考]]></title>
    <url>%2Fblog%2F%E4%B8%80%E4%B8%AASpring-Transaction-%E6%B3%A8%E8%A7%A3%E4%B8%8D%E7%94%9F%E6%95%88bug%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%8320181103%2F</url>
    <content type="text"><![CDATA[前言某日，在项目测试代码过程中，发现一个问题，对于一个方法A（无事务），调用B方法（有事务），当A，B方法在同一个类中的时候，在B方法上的事务注解是不生效的！ 同事说将B方法写到新的Service类中就可以解决，遂试之，确实得以解决。但不解其原理，问同事、查资料均感觉不如意。故分析了下Spring 事务的部分源码。有所见解，特此记录。 下图就是我描述的这种情况，B事务不生效的问题。 1.测试类 2.实现类 我们经过测试可以发现，当insert方法有事务、但被该实现类内部方法doInsert调用后，即使insert方法出现异常，该方法的数据库操作也不会回滚。 3.数据没有回滚，已经入库 正文要理解研究这种情况，我们先来简单说下Spring 的注解方式的事务实现机制。 事务的一些基础我在一篇文章中有介绍 https://www.sakuratears.top/blog/Spring-Transactional%E6%B3%A8%E8%A7%A320181013/ 不懂得可以先大致看看。 Spring注解方式的事务实现机制在应用系统调用声明@Transactional 的目标方法时，Spring Framework 默认使用 AOP 代理，在代码运行时生成一个代理对象，根据@Transactional 的属性配置信息，这个代理对象决定该声明@Transactional 的目标方法是否由拦截器 TransactionInterceptor 来使用拦截，在 TransactionInterceptor 拦截时，会在在目标方法开始执行之前创建并加入事务，并执行目标方法的逻辑, 最后根据执行情况是否出现异常，利用抽象事务管理器AbstractPlatformTransactionManager 操作数据源 DataSource 提交或回滚事务, 如图。 Spring AOP 代理有 CglibAopProxy 和 JdkDynamicAopProxy 两种，上图以 CglibAopProxy 为例，对于 CglibAopProxy，需要调用其内部类的 DynamicAdvisedInterceptor 的 intercept 方法。对于 JdkDynamicAopProxy，需要调用其 invoke 方法。当然我们也可以使用AspectJ的方式实现AOP代理，这儿不做过多介绍。 事务管理的框架是由抽象事务管理器 AbstractPlatformTransactionManager 来提供的，而具体的底层事务处理实现，由 PlatformTransactionManager 的具体实现类来实现，如事务管理器 DataSourceTransactionManager。不同的事务管理器管理不同的数据资源 DataSource，比如 DataSourceTransactionManager 管理 JDBC 的 Connection。 PlatformTransactionManager，AbstractPlatformTransactionManager 及具体实现类关系图如下。 一次正常事务调试出现问题，debug是比较好的解决方法。我们大致跟下SpringTransaction的使用过程。先从正确流程入手吧。如下：直接将事务注解加在doInsert方法上，明显，这种情况下出现异常事务会回滚。我们debug下事务大致的回滚过程。 DefaultAopProxyFactory里的createAopProxy方法可以拿到看到该方法具体使用的哪种代理。 可以看到我们这个类使用了Cglib代理。使用了Cglib代理，上面讲到 对于 CglibAopProxy，需要调用其内部类的 DynamicAdvisedInterceptor 的 intercept 方法。我们继续断点跟踪下。 一步步进行，如事务图所示，进入了TransactionInterceptor的invoke方法，并执行invokeWithinTransaction方法。 继续跟踪。来到了TransactionAspectSupport，这是spring事务处理的关键类，谨记。 会进行事务的创建，createTransactionIfNecessary getTransaction方法会开启一个事务。 根据上面debug看到的事务管理器是DataSourceTransactionManager， 执行getTransaction会调用它的doBeigin方法。 可以看到把自动提交设置成了false，并且暂时保存了原来的自动提交属性状态。 而后可以看到他将当前事务信息绑定在了ThreadLocal里了。 执行我们添加事务注解的方法，抛出了异常被捕获。 执行completeTransactionAfterThrowing方法，我们的异常正好是这个异常（或者其父类）。 然后执行回滚操作，最终到达下图所示方法（DataSourceTransactionManager的doRollback） 回滚具体代码不在介绍，我们可以看到在回滚时它把原来数据库的自动提交属性改了过来。 最后他会把本次事务状态清除，相当于保存上一次的事务状态。 注意： 在spring启动时获取事务注解时我们可以看到下图。 这个说明Spring AOP 事务注解只能作用于public 方法。 关于事务回滚rollbackFor条件的问题，我们可以看到下图。 当我们事务注解配置具体的回滚条件，如rollbackFor = Exception.class，只要是Exception.class或者其子类，都可以实现事务回滚。它会通过RuleBasedTransactionAttribute.class这个类去校验抛出的异常是否符合条件。进而判断是否需要回滚。 但是当我们不声明rollbackFor 条件时，这儿应该注意一点。它会使用默认的条件，而不是不处理异常。主要由DefaultTransactionAttribute.class 里的rollbackOn方法实现。我们看下这个方法，可以发现，他只会处理RuntimeException和Error。也就是说，如果我们一个方法有事务，但抛出了非RuntimeException（如检查时异常等），且事务没有声明rollbackFor回滚条件，那么，它是不会触发事务回滚的。这一点要注意。 上图调用RuleBasedTransactionAttribute.class的方法，回滚规则为空，使用父类rollbackOn方法。 通过调试可以轻松看到这一情况，这儿不在做过多赘述。 异常事务调试我们来看下事务不成功的情况。就是题目开始的问题。 Spring刚启动时，会扫描需要进行代理的类，生成代理对象，在AdvisedSupport.class类中，把类中的方法缓存起来。 首先查询该方法是不是需要拦截（是不是有事务注解） 在TransactionAttributeSourcePointcut.class 类里的matches方法，查询事务注解情况。 查到了就缓存起来了。 当doInset方法进入时，同样的逻辑。也会缓存起来，但是cached是值为null。 该类的其他方法也会被缓存，没有事务注解的都放为null。 开始执行doInsert方法时，进入CglibAopProxy的intercept方法。 可以看到尝试拿缓存，但缓存的值为空。 尝试获取一下，显然也是没有值的。所以这时候认为不需要进行事务。事务链为空。 就直接执行了方法doInsert。并不会开启事务。（不为空的话会创建一个CglibMethodInvocation并开启事务执行方法，如上面开始的情况）。 当生成一个动态代理对象后，对这个对象引用中方法的调用就是对代理的调用，而这个代理能够代理所有跟特定方法调用相关的拦截器。不过，一旦调用最终抵达了目标对象 （此处为TransactionalTestImpl类的引用），任何对自身的调用例如insert将对this引用进行调用而非代理。这一点意义重大， 它意味着自我调用将不会导致和方法调用关联的通知得到执行的机会。 如果需要insert的事务生效，一种典型的方法就将方法insert放到新的类中，这便很好理解了。因为新的类会生成新的动态代理对象，调用源从而获得通知。 如果我非要在本类中实现通知呢？ 那我们就需要直接获取代理对象调用insert方法了。如下图。 要实现这个功能，需要开启Spring AspectJ支持，我使用的Springboot，启动类上加入如下注解，并引入如下依赖。 这个pom文件你进去可以看到就是引用了AspectJ 的相关jar包。 这个时候我们在测试一下，就会发现事务生效了。 在CglibAopProxy中可以看到如下代码，可以明白开启后它把代理对象绑定到ThreadLocal上等待insert方法执行的通知。 当然，如果这两个方法上都存在事务，它也会进行判断处理，也就是事务的传播属性，他们主要通过AbstractPlatformTransactionManager这个类（这个类也很重要）的getTransaction方法和handleExistingTransaction方法来进行事务传播属性的处理。这儿不做过多讲解，自己看看逻辑处理即可。 getTransaction部分代码： handleExistingTransaction部分代码： 结语总的来说，通过一个问题，我们大致看了下Spring Transactional注解的实现过程。并分析了产生这种问题的原因，通过有效的手段来进行验证。还是蛮不错的一次体验。 下面总结下： A：在Spring中，一个类中无事务注解的方法A调用有事务注解的方法B，默认情况下B出现异常事务是不会进行回滚的。 解决方法： 将B写到一个新的方法中。（原理上是生成不同类的动态代理对象，实际中比较常用的一种手段，但需要管理一个新的类） 如果业务（情形等）允许，可以将事务移动到A上，或者B的事务不动，给A也加一个事务。（根据具体情况讨论，有时候效果很好，有时候不适宜，使用此种方法可能影响程序效率或者产生莫名其妙的bug，慎用） 启用增强型事务，引入AspectJ。（不太常用的一种手段，但如果项目中本来已经引入了AspectJ并且开启了增强型事务管理，何乐而不为呢？） B：研究过程中发现的其他应该注意的坑。 事务注解应当作用在public方法上，需要注意。 如果不设置事务回滚条件（rollbackFor参数为空），它能捕获RuntimeException及其子类 和 Error及其子类 出现的异常情况并回滚，其他异常是无法捕获并回滚的。如IOException（检查型异常）等。 事务的传播属性的几个应该了解，不能乱用，虽然我们可能就用到过或者就用到了Propagation.REQUIRED ，但不代表其它不会用到。 C：本次研究学习我们应该理解掌握的。 Spring事务的处理过程。（Spring AOP的体现，应用反射和动态代理） 事务的一些性质。（事务的传播属性、事务的四大特性等） 其它一些需要学习的地方。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何以并发方式在同一个流上执行多种操作]]></title>
    <url>%2Fblog%2F%E5%A6%82%E4%BD%95%E4%BB%A5%E5%B9%B6%E5%8F%91%E6%96%B9%E5%BC%8F%E5%9C%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E6%B5%81%E4%B8%8A%E6%89%A7%E8%A1%8C%E5%A4%9A%E7%A7%8D%E6%93%8D%E4%BD%9C20181027%2F</url>
    <content type="text"><![CDATA[前言Java 8中，流有一个非常大的局限性，使用时，对它操作一次仅能得到一个处理结果。当流进行终端操作后，如果你在试图遍历它，就会出现异常。 1java.lang.IllegalStateException: stream has already been operated upon or closed 虽然流就是如此设计的，但是我们有时候就希望可以通过流获取多个结果。或者说，你希望一次性向流中传入多个Lambda表达式。 为了达到这一目标，我们应该需要一个fork类型的方法，对每个复制的流应用不同的函数。理想情况下，这些操作也应该支持并行去拿到运算结果。 这一特性在Java 8中是没有的，不过我们可以利用一个通用API，即Spliterator，尤其是它的延迟绑定能力，结合BlockingQueues和Futures来实现这一特性。 正文复制流要达到此效果，我们首先应该创建一个StreamForker，它会对原始的流进行封装，在此基础上在执行各种操作。我们来看下代码。 12345678910111213141516public class StreamForker&lt;T&gt; &#123; private final Stream&lt;T&gt; stream; private final Map&lt;Object, Function&lt;Stream&lt;T&gt;,?&gt;&gt; forks=new HashMap&lt;&gt;(); public StreamForker(Stream&lt;T&gt; stream) &#123; this.stream = stream; &#125; public StreamForker&lt;T&gt; fork(Object key,Function&lt;Stream&lt;T&gt;,?&gt; f)&#123; //使用一个键对流上的函数进行索引 forks.put(key,f); //返回this从而保证多次顺畅的调用fork方法 return this; &#125; public Results getResults()&#123; //TODO &#125;&#125; fork方法接受两个参数。 Function:对流进行处理，转变成这些操作结果的类型。 key: 通过它拿到结果，这些结果被放到内部的一个Map中。 fork方法需要返回自身，这样可以复制多个操作构成流水线。 如图： 上图不难理解。 而由fork方法添加的操作如何执行呢，就是通过getResults方法的调用触发，该方法返回一个Results接口的实现。接口定义如下： 123public interface Results &#123; public &lt;R&gt; R get(Object key);&#125; 实现Results接口我们使用ForkingStreamConsumer实现Results接口。 123456789public Results getResults()&#123; ForkingStreamConsumer&lt;T&gt; consumer=build(); try&#123; stream.sequential().forEach(consumer); &#125;finally &#123; consumer.finish(); &#125; return consumer;&#125; ForkingStreamConsumer同时实现了Results和Consumer接口。其主要任务就是来处理流元素，将他们分发到多个BlockingQuenes中处理，BlockingQuenes的数量和通过fork方法提交的操作数是一致的。这里的getResults的实现，流应该是顺序处理的，否则，forEach后元素的顺序就会变化。finish方法用来表明队列中没有更多要处理的元素了。build方法主要用于创建ForkingStreamConsumer。代码如下： 123456789101112131415161718private ForkingStreamConsumer&lt;T&gt; build()&#123; //创建由队列组成的列表，每一个队列对应一个操作 List&lt;BlockingQueue&lt;T&gt;&gt; queues=new ArrayList&lt;&gt;(); //建立用于标识操作的键与包含操作结果的Future之间的映射关系 Map&lt;Object,Future&lt;?&gt;&gt; actions= forks.entrySet().stream().reduce( new HashMap&lt;Object,Future&lt;?&gt;&gt;(), (map,e)-&gt;&#123; map.put(e.getKey(),getOperationResult(queues,e.getValue())); return map; &#125;, (m1,m2)-&gt;&#123; m1.putAll(m2); return m1; &#125; ); return new ForkingStreamConsumer&lt;&gt;(queues,actions);&#125; 可以看到，我们先创建了BlockingQuenes列表。接着创建了一个Map，Map的键就是用来标识不同操作的键，值包含着Future里。最终BlockingQuenes和Map会被传递给ForkingStreamConsumer的构造函数。每个Future通过关键方法getOperationResult创建。 来看看getOperationResult的实现。 1234567891011private Future&lt;?&gt; getOperationResult(List&lt;BlockingQueue&lt;T&gt;&gt; queues,Function&lt;Stream&lt;T&gt;,?&gt; f)&#123; //创建一个队列，并将其添加到队列的列表中 BlockingQueue&lt;T&gt; queue=new LinkedBlockingDeque&lt;&gt;(); queues.add(queue); //创建一个Spliterator，遍历队列中的元素 Spliterator&lt;T&gt; spliterator=new BlockingQueueSpliterator&lt;&gt;(queue); //创建一个流，将Spliterator作为数据源 Stream&lt;T&gt; source= StreamSupport.stream(spliterator,false); //创建一个Future对象，以异步方式计算在流上执行特定函数的结果 return CompletableFuture.supplyAsync(()-&gt;f.apply(source));&#125; 该方法创建一个新的BlockingQuene，并将其添加到队列列表。队列会被传递给一个新的BlockingQueueSpliterator对象，后者是一个延迟绑定的Spliterator。然后我们创建一个顺序流对Spliterator进行遍历，最终创建一个Future收集结果。 开发ForkingStreamConsumer代码如下： 1234567891011121314151617181920212223242526public class ForkingStreamConsumer&lt;T&gt; implements Consumer&lt;T&gt;,Results &#123; public static final Object END_OF_STREAM=new Object(); private final List&lt;BlockingQueue&lt;T&gt;&gt; queues; private final Map&lt;Object, Future&lt;?&gt;&gt; actions; public ForkingStreamConsumer(List&lt;BlockingQueue&lt;T&gt;&gt; queues, Map&lt;Object, Future&lt;?&gt;&gt; actions) &#123; this.queues = queues; this.actions = actions; &#125; @Override public &lt;R&gt; R get(Object key) &#123; try &#123; return ((Future&lt;R&gt;)actions.get(key)).get(); &#125;catch(Exception e)&#123; throw new RuntimeException(e); &#125; &#125; @Override public void accept(T t) &#123; //将流中遍历的元素添加到所有的队列中 queues.forEach(q-&gt;q.add(t)); &#125; void finish()&#123; //将最后一个元素添加到队列中，表明该流已经结束 accept((T)END_OF_STREAM); &#125;&#125; 这个类同时实现了Consumer接口和Results接口。 Consumer接口要求实现accept方法，每当ForkingStreamConsumer接受流中的一个元素，它就会将元素添加到所有BlockingQuenes中当所有元素都添加到所有队列后，finish方法将最后一个元素添加到所有队列。处理时碰上这个元素表明后面没有元素要处理了。 Results接口需要实现get方法。一旦处理结束，get方法会获取Map中由键索引的Future，解析到结果后返回。 每有一个操作，就会对应一个BlockingQueueSpliterator。我们来看下BlockingQueueSpliterator的实现。 开发BlockingQueueSpliterator12345678910111213141516171819202122232425262728293031323334public class BlockingQueueSpliterator&lt;T&gt; implements Spliterator&lt;T&gt; &#123; private final BlockingQueue&lt;T&gt; q; public BlockingQueueSpliterator(BlockingQueue&lt;T&gt; q) &#123; this.q = q; &#125; @Override public boolean tryAdvance(Consumer&lt;? super T&gt; action) &#123; T t; while (true)&#123; try &#123; t=q.take(); break; &#125;catch(InterruptedException e)&#123; &#125; &#125; if(t!=ForkingStreamConsumer.END_OF_STREAM)&#123; action.accept(t); return true; &#125; return false; &#125; @Override public Spliterator&lt;T&gt; trySplit() &#123; return null; &#125; @Override public long estimateSize() &#123; return 0; &#125; @Override public int characteristics() &#123; return 0; &#125;&#125; 可以看到该Spliterator未定义任何切割流的策略，仅仅利用了流的延迟绑定能力。也没有实现trySplit方法。由于我们的操作数是不确定的，故estimateSize不能提供任何有意义的数字，返回0.也没有体现Spliterator的特性，故characteristics返回0. 仅仅实现了tryAdvance方法，它从BlockingQueue中取得原始流元素，进一步传给Consumer对象。当返回true时表明还有元素要处理，直到发现最后一个元素时终止。 以上基本上是在一个流上执行多种操作的代码。 我们下面来检测一下正确性。 测试编写测试类。如下数据： 123456789101112131415161718public static void main(String[] args) &#123; //生成1到1000的数组 List&lt;Integer&gt; list1=IntStream.rangeClosed(1,1000).filter(n-&gt;n%2==0).boxed().collect(Collectors.toList()); List&lt;Integer&gt; list2=IntStream.rangeClosed(1,1000).filter(n-&gt;n%5==0).boxed().collect(Collectors.toList()); //同时对list1数据求和，统计list1数据数量，统计list1和list2相同元素，统计list1和list2相同元素的最大值和最小值 Results results=new StreamForker&lt;Integer&gt;(list1.stream()) .fork("sum",s-&gt;s.mapToInt(Integer::intValue).sum()) .fork("count",s-&gt;s.count()) .fork("list3",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).collect(Collectors.toList())) .fork("max",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).max(Comparator.naturalOrder())) .fork("min",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).min(Comparator.naturalOrder())) .getResults(); System.out.println("sum="+results.get("sum")); System.out.println("count="+results.get("count")); System.out.println("max="+((Optional) results.get("max")).get()); System.out.println("min="+((Optional)results.get("min")).get()); ((List&lt;Integer&gt;)results.get("list3")).stream().forEach(System.out::println);&#125; 输出结果： 可以看到，使用了一个流，通过我们实现的方法进行了多次终端操作返回正确结果。 性能问题这是我们用一个流实现多种终端操作的方式，当然这并不意味着会比普通的写法效率高，如果对于上述问题，我们可以分个构建若干个流进行一一实现。 这种一个流进行多个终端操作的情况使用，一定是生成流比较耗费资源性能时才会用到，比如操作一个较大文件时生成的字符流，我们想统计字数，检查某些单词出现的次数，统计行数等等操作，重复生成流显然是耗费资源的。这种情况可以考虑使用这种一个流进行多个终端操作的实现。 当然，具体到具体问题优化，建议认真分析两者的资源消耗。这是比较稳妥的做法。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[封装一个属于自己的Redis API]]></title>
    <url>%2Fblog%2F%E5%B0%81%E8%A3%85%E4%B8%80%E4%B8%AA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84Redis-API20181027%2F</url>
    <content type="text"><![CDATA[前言Redis作为一款强大的key-value型数据库，其应用是十分广泛的。在Java语言中，常用来与Redis数据库建立连接用到的是Jedis Pool连接池。 今天我们来简单了解下它们然后实现一个可移植的操作Redis的API。 正文知识准备我们知道Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。并提供了一系列的命令操作这些数据类型。 Jedis相当于对这些操作进行了代码封装，及提供了一些其它常用操作。 我们先来了解下Jedis的连接池配置参数。 commons-pool2 有一个配置类GenericObjectPoolConfig里面的通用参数设置如下： 参数 说明 默认值 备注 maxTotal 说明一个pool最多可以有多少个Jedis实例 8 -1表示不限制 maxIdle 一个pool最多可以有多少个空闲的Jedis实例 8 minIdle 一个pool最少有多少个空闲的Jedis实例 0 可以看到它继承BaseObjectPoolConfig。我们可以看到BaseObjectPoolConfig的参数如下。 部分参数意义如下： 参数 说明 默认值 备注 lifo pool中的idle列表是双端队列，设定是否last in first out true maxWaitMillis 当active数量为max时,等待的时长 -1L(代表一直等) 配合blockWhenExhausted使用 blockWhenExhausted 当active数量为max时，是否阻塞等待一段时间 true testOnCreate 创建实例时有效性检测 false testOnReturn 归还实例时有效性检测 false testOnBorrow 借出实例时有效性检测 false 思路分析首先Redis连接池属性我们应当放置在配置文件里，解析并获得，连接池最好设计成单例的，每次不用在初始化过多连接资源。同时Redis有单机模式和集群模式区分，这两种模式我们也应该区分开来。单机模式下，可以选择多个database，集群模式下只能选择database0.集群模式下，如果redis地址过多，我们如何分开呢？ 我们可以考虑如下样式：address =127.0.0.1:6379;127.0.0.1:6380 每个redis地址用分号分隔，解析配置时把每个解析到并建立连接。 当然，最后完成JedisPool的创建后，我们应该编写工具类对一些常用操作方法进行封装，便于我们使用。 代码我们根据上述思路，构造了如下图所示的小项目。 其中： RedisException是用来统一处理程序过程中的异常的类。 JedisFactory可以认为是一个JedisPool工厂，用来提供单机模式的连接池或者集群模式的连接池。 RedisConfiguration是与配置文件对应的配置类，用于存放配置的数据。 RedisConstants用来放置一些项目中用到的常量。 RedisUtil工具类接口，提供了多种操作Redis的方法。 RedisSingleUtil工具接口的单机模式实现。 RedisClusterUtil工具接口的集群模式实现。 redis-config.properties Redis的配置文件存放 JedisFactory和RedisUtill为主要类。我们看下他们的具体实现。 JedisFactory的主要代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697private volatile JedisPool jedisPool; private volatile JedisCluster jedisCluster; private RedisConfiguration redisConfig; private Pattern addRessPattern = Pattern.compile("^.+[:]\\d&#123;1,5&#125;\\s*(;.+[:]\\d&#123;1,5&#125;\\s*)*[;]?\\s*$"); public JedisFactory(final RedisConfiguration redisConfiguration)&#123; this.redisConfig=redisConfiguration; &#125; public JedisPool getJedisPool()&#123; if(jedisPool==null)&#123; synchronized (JedisFactory.class)&#123; if(jedisPool==null)&#123; init(); &#125; &#125; &#125; return jedisPool; &#125; public JedisCluster getJedisCluster()&#123; if(jedisCluster==null)&#123; synchronized (JedisFactory.class)&#123; if(jedisCluster==null)&#123; init(); &#125; &#125; &#125; return jedisCluster; &#125; public void init()&#123; logger.info("JedisFactory init start..."); try&#123; if(StringUtils.isNotBlank(redisConfig.getLocalPropertiesPath()))&#123; fillData(); &#125; logger.info("redis config is: &#123;&#125;.", redisConfig.toString()); Set&lt;HostAndPort&gt; hostAndPortSet = this.parseHostAndPort(redisConfig.getAddress()); GenericObjectPoolConfig genericObjectPoolConfig = new GenericObjectPoolConfig(); genericObjectPoolConfig.setMaxWaitMillis(redisConfig.getMaxWaitMillis()); genericObjectPoolConfig.setMaxTotal(redisConfig.getMaxTotal()); genericObjectPoolConfig.setMinIdle(redisConfig.getMinIdle()); genericObjectPoolConfig.setMaxIdle(redisConfig.getMaxIdle()); if(redisConfig.getMode()== RedisConstants.REDIS_MODE_SINGLE)&#123; HostAndPort hostAndPort=(HostAndPort)hostAndPortSet.toArray()[0]; jedisPool=new JedisPool(genericObjectPoolConfig, hostAndPort.getHost(), hostAndPort.getPort(), redisConfig.getTimeout(), null,redisConfig.getDatabase()); logger.info("jedisPool init is finished"); &#125;else&#123; if(redisConfig.getDatabase()!=0)&#123; logger.warn("当前配置的database为："+redisConfig.getDatabase()+",集群模式下不能选择database，只能使用database0"); &#125; jedisCluster = new JedisCluster(hostAndPortSet, redisConfig.getTimeout(), redisConfig.getMaxRedirections(), genericObjectPoolConfig); logger.info("jedisCluster init is finished"); &#125; &#125;catch(Exception ex)&#123; throw new RedisException(ex); &#125; &#125; private void fillData() throws Exception &#123; Properties localProperties = PropertiesUtils.loadLocalProperties(redisConfig.getLocalPropertiesPath()); String address=localProperties.getProperty("address", ""); if (StringUtils.isBlank(address)) &#123; throw new RedisException("error:redis config address is blank!"); &#125; // 设置初始值 long maxWaitMillis=Long.parseLong(localProperties.getProperty("maxWaitMillis", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_WAIT_MILLIS))); int maxTotal=Integer.parseInt(localProperties.getProperty("maxTotal", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_TOTAL))); int minIdle=Integer.parseInt(localProperties.getProperty("minIdle", String.valueOf(GenericObjectPoolConfig.DEFAULT_MIN_IDLE))); int maxIdle=Integer.parseInt(localProperties.getProperty("maxIdle", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_IDLE))); int timeout=Integer.parseInt((localProperties.getProperty("timeout", "2000"))); int maxRedirections=Integer.parseInt((localProperties.getProperty("maxRedirections", "6"))); int database=Integer.parseInt((localProperties.getProperty("database", "0"))); //1单机模式，2集群模式 int mode=Integer.parseInt((localProperties.getProperty("mode", String.valueOf(RedisConstants.REDIS_MODE_SINGLE)))); redisConfig.setAddress(address); redisConfig.setMaxWaitMillis(maxWaitMillis); redisConfig.setMaxTotal(maxTotal); redisConfig.setMinIdle(minIdle); redisConfig.setMaxIdle(maxIdle); redisConfig.setTimeout(timeout); redisConfig.setMaxRedirections(maxRedirections); redisConfig.setDatabase(database); redisConfig.setMode(mode); &#125; //部分代码略 对于RedisUtil接口，应有两个实现，单机和集群的，这里为了简化代码，只简单列举了一个方法。 1234public interface RedisUtil &#123; String setString(String key, String value); //部分代码略&#125; 单机模式的实现： 123456789101112public class RedisSingleUtil implements RedisUtil&#123; @Override public String setString(String key, String value) &#123; Jedis jedis = this.getResource(); try&#123; return jedis.set(key, value); &#125;finally&#123; this.closeResource(jedis); &#125; &#125; //部分代码略&#125; 集群模式的实现： 12345678public class RedisClusterUtil implements RedisUtil&#123; @Override public String setString(String key, String value) &#123; JedisCluster cluster=getResource(); return getResource().set(key, value); &#125; //部分代码略&#125; 其他方法及实现不在赘述，有兴趣的可以在 https://github.com/javazwt/framework-base 上查看相关代码。 测试1234567public class RedisTest &#123; private static RedisSingleUtil redisSingleUtil=new RedisSingleUtil(); public static void main(String[] args) &#123; redisSingleUtil.setString("str","123"); redisSingleUtil.getString("str"); &#125;&#125; 可以检测我们的正确性。 结束语经过封装后，我们可以把该工具类使用在任何项目上，提高开发效率，降低项目耦合性，同时对Redis有了更深入的认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[封装构建基于ES的Java API包]]></title>
    <url>%2Fblog%2F%E5%B0%81%E8%A3%85%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8EES%E7%9A%84Java-API%E5%8C%8520181027%2F</url>
    <content type="text"><![CDATA[前言前些日子，elastic公司成功上市了。旗下有一款优秀的开源搜索引擎ElasticSearch。 ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。 好了，扯远了，回归正题。今天我们使用Java对它的一些API进行封装，以使其API更具备灵活性。 完成对其简单的使用是比较简单的，我们今天把我们的一些调用代码封装，以便可以在各个项目中自由使用。这才是我们的根本目的。 今天，我们来构建一个工具包类。 环境准备安装ElasticSearch，主要就是为了测试我们的代码，对于有现成环境的，如开发环境，测试环境上的ElasticSearch，这一步可以忽略。 首先请安装ElasticSearch，这里就不介绍了，我这里是安装的ElasticSearch最新版，6.4.0版本。https://www.elastic.co/downloads/elasticsearch 。安装成功后可以将其添加到环境变量中，然后启动。启动成功浏览器访问http://localhost:9200/ 会看到json信息。 安装Kibana，Kibana是ElasticSearch的可视化工具，可以方便的查看ElasticSearch及其运行状态。https://www.elastic.co/products/kibana 。我安装的也是最新版本，6.4.0. 安装成功后加入环境变量，然后启动，访问http://localhost:5601/ 可以看到Kibana界面。 JDK版本为1.8 以上都为准备工作。 架构构建org.elasticsearch.client.transport jar包已经有相关关于ES API的操作。 我们把它封装为专用jar包，建议使用Maven构建。如下： Maven项目首先，我们先建一个名叫 framework-es的Maven项目。如下。 建好各个package。 exception里面定义我们的异常。 factory里面用来生成esclient。 util里面放一些工具类。 vo里面可以放查询对象等。 property为配置文件。 test里面存放测试类。 各个package的名字可以自己定义，描述清目的即可。如下图。 添加依赖添加依赖，我的pom.xml依赖如下。 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.31&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 思路我的思路是构建一个单例的ESClient，用于与ES建立连接，以后增删改查等逻辑均使用此Client，保证资源的高效利用。这个也应当支持ES集群，有多个ES服务端也应当支持。而且地址应为可配置的。 话不多说，上代码，主要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class ElasticSearchClientFactory &#123; private final static Logger logger = LoggerFactory.getLogger(ElasticSearchClientFactory.class); private volatile TransportClient esClient; //ES配置 private ElasticSearchConfiguration esConfig; //校验多个ES地址的正则 private Pattern addRessPattern = Pattern.compile("^.+[:]\\d&#123;1,5&#125;\\s*(;.+[:]\\d&#123;1,5&#125;\\s*)*[;]?\\s*$"); public ElasticSearchClientFactory(final ElasticSearchConfiguration elasticSearchConfiguration)&#123; this.esConfig=elasticSearchConfiguration; &#125; /** * 获取一个单例的ESClient * @return */ public Client getEsClient()&#123; if(esClient==null)&#123; synchronized (ElasticSearchClientFactory.class)&#123; if(esClient==null)&#123; logger.info("ElasticSearchClientFactory init start..."); try&#123; if(StringUtils.isNotBlank(esConfig.getLocalPropertiesPath()))&#123; //获取ES配置信息 fillData(); &#125; logger.info("ESConfig is:&#123;&#125;",esConfig.toString()); //多个ES地址解析 List&lt;HostAndPort&gt; hostAndPortList = this.parseHostAndPortList(esConfig.getAddress()); TransportAddress [] transportAddress=new TransportAddress[hostAndPortList.size()]; for (int i = 0; i &lt; hostAndPortList.size(); i++) &#123; transportAddress[i] = new TransportAddress(InetAddress.getByName(hostAndPortList.get(i).getIp()),hostAndPortList.get(i).getPort()); &#125; //节点名 String nodeName=esConfig.getNodeName()+ UUID.randomUUID(); String clusterName=esConfig.getClusterName(); Settings.Builder settingsBuilder = Settings.builder(); settingsBuilder.put("node.name", nodeName); if(StringUtils.isNotBlank(clusterName))&#123; settingsBuilder.put("cluster.name", clusterName); &#125; settingsBuilder.put("client.transport.sniff", true); Settings settings = settingsBuilder.build(); TransportClient client = new PreBuiltTransportClient(settings); //创建ESClient esClient = client.addTransportAddresses(transportAddress); logger.info("EalsticSearchClientFactory init is finished"); &#125;catch(Exception e)&#123; logger.error("EalsticSearchClientFactory create failed",e); throw new ElasticSearchException("EalsticSearchClientFactory create faile",e); &#125; &#125; &#125; &#125; return esClient; &#125; //其他代码略&#125; 以上代码用于生成一个单例的ESClient类。 123public class ElasticSearchConstants &#123; public final static String DEFAULT_PROPERTIES_PATH="property/es-config.properties";&#125; 同时默认配置文件为property/es-config.properties可手动进行配置，且支持多个地址，地址写法为 127.0.0.1:9200;127.0.0.2:9300 这样，中间应使用;分割。 我们对外提供一个工具类，供使用者进行对数据的操作。如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class ElasticSearchUtil &#123; private final static Logger logger = LoggerFactory.getLogger(ElasticSearchUtil.class); private static final SerializerFeature[] featuresWithNullValue=&#123;SerializerFeature.WriteMapNullValue, SerializerFeature.WriteNullBooleanAsFalse, SerializerFeature.WriteNullListAsEmpty, SerializerFeature.WriteNullNumberAsZero, SerializerFeature.WriteNullStringAsEmpty&#125;; private ElasticSearchClientFactory elasticSearchClientFactory; public void setElasticSearchClientFactory(ElasticSearchClientFactory elasticSearchClientFactory) &#123; this.elasticSearchClientFactory = elasticSearchClientFactory; &#125; public ElasticSearchUtil(ElasticSearchClientFactory elasticSearchClientFactory)&#123; this.elasticSearchClientFactory=elasticSearchClientFactory; &#125; public ElasticSearchUtil(String localPropertiesPath)&#123; ElasticSearchConfiguration esConfig=new ElasticSearchConfiguration(); esConfig.setLocalPropertiesPath(localPropertiesPath); this.elasticSearchClientFactory=new ElasticSearchClientFactory(esConfig); &#125; public ElasticSearchUtil() &#123; ElasticSearchConfiguration esConfig = new ElasticSearchConfiguration(); esConfig.setLocalPropertiesPath(ElasticSearchConstants.DEFAULT_PROPERTIES_PATH); this.elasticSearchClientFactory = new ElasticSearchClientFactory(esConfig); &#125; public ElasticSearchUtil(ElasticSearchConfiguration esConfig) &#123; this.elasticSearchClientFactory = new ElasticSearchClientFactory(esConfig); &#125; public Client getEsClient() &#123; return elasticSearchClientFactory.getEsClient(); &#125; /** * 创建索引 * @param index * @param type * @param id * @param isOnlyCreate * @param jsonString * @return */ protected boolean createDocument(String index,String type,String id,boolean isOnlyCreate,String jsonString)&#123; IndexResponse indexResponse; if(StringUtils.isBlank(id))&#123; indexResponse=elasticSearchClientFactory.getEsClient().prepareIndex(index,type).setCreate(isOnlyCreate).setSource(jsonString, XContentType.JSON).get(); &#125;else&#123; indexResponse=elasticSearchClientFactory.getEsClient().prepareIndex(index,type,id).setCreate(isOnlyCreate).setSource(jsonString,XContentType.JSON).get(); &#125; if(logger.isDebugEnabled())&#123; String _index=indexResponse.getIndex(); String _type=indexResponse.getType(); String _id=indexResponse.getId(); long _version = indexResponse.getVersion(); boolean created = RestStatus.CREATED.equals(indexResponse.status()); logger.debug(String.format("createDocument index:%s,type:%s,id:%s,version:%s,created:%s", _index, _type, _id, _version, created)); &#125; return RestStatus.CREATED.equals(indexResponse.status()); &#125; //其他代码略&#125; 创建好客户端后，我们可以用它执行增删改查，我们在封装一个ESUtil类，用于执行该系列操作，暴露一些特定增删改查接口。如上代码。 注：其他代码略。 这样，完成代码后，工程项目如下图所示。 ElasticSearchException 为自定义异常类。 ElasticSearchClientFactory为主要方法，用来构建一个单例的ESClient ElasticSearchConfiguration为ES配置类 ElasticSearchConstants为常量类，里面存放配置文件的路径 HostAndPort为地址和端口的一个辅助Bean ElasticSearchUtil为主要方法，用于对外提供服务（CRUD） DocumentVo为辅助Bean es-config.properties为配置文件 当我们把项目打包成jar包时，配置文件可以不用打包，这样引入其他项目后，在其他项目里配置配置文件即可。 测试若ES的安装路径D:\Program Files\elasticsearch-6.4.0\bin成功配置到环境变量后，在命令行输入elasticsearch便可启动服务。 若Kibana的安装路径D:\Program Files\kibana-6.4.0-windows-x86_64成功配置到环境变量后，在命令行输入kibana便可启动服务。 访问http://127.0.0.1:5601/ 看到可视化界面。 编写测试类。 12345678910public class ESTest &#123; private static ElasticSearchUtil esUtil=new ElasticSearchUtil(); private static String index="user_index"; private static String type="user_type"; public static void main(String[] args) &#123; JSONObject js=new JSONObject(); js.put("1","2"); esUtil.insertDocument(index,type,"123",js); &#125;&#125; 查看Kibana界面可以看到我们的数据被添加进来了。 结论关于ElasticSearch及Kibana的具体使用我们以后再讲。 我们平时开发中，也可以创建类似的工具包，提高代码使用率，实现软件的高内聚低耦合，同时也是提高自己。 部分未展示代码请见我的GitHub地址: https://github.com/javazwt/]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 Spliterator接口]]></title>
    <url>%2Fblog%2FJava8-Spliterator%E6%8E%A5%E5%8F%A320181027%2F</url>
    <content type="text"><![CDATA[前言Spliterator 是Java8中加入的一个新接口，是“可分迭代器”（splitable iterator）的意思。它也是用来遍历数据源中的元素的，但它是为并行执行而设计的。 其接口主要代码如下： 123456Public interface Spliterator&lt;T&gt;&#123; Boolean tryAdvance(Consumer &lt;? super T&gt;) action; Spliterator&lt;T&gt; trySplit(); Long estimateSize(); Int characteristics();&#125; T 是Spliterator要遍历的元素类型。 tryAdvance 方法的行为类似于普通的Iterator，因为它会按顺序一个一个使用Spliterator中的元素，并且如果还有其他元素要遍历就返回true。 trySplit 是专为Spliterator接口而设计的，因为它可以把一些元素划出去分给第二个Spliterator，让他们两个并行处理。 estimateSize方法估计还剩多少元素需要遍历，因为即使不那么精确，快速算出来的值也有助于让拆分均匀点。 注：将Stream流拆分成多个部分的算法是个递归过程，第一步第一个Spliterator调用trySplit，生成两个Spliterator，第二步这两个Spliterator调用trySplit，生成4个Spliterator，直到调用Spliterator的trySplit 方法后返回null，表示这部分Spliterator不能在分割。 这个拆分过程也受Spliterator本身特性的影响，而特性是通过characteristics方法声明的。 我们来简单看一下它的特性的常用值。 特性 含义 ORDERED 按元素的既定顺序遍历和划分 DISTINCT 对于任一遍历过的元素x，y,x.equals(y)返回false SORTED 遍历元素按照一个预定义顺序排序 SIZED Spliterator由一个已知大小的数据源建立，estimateSize会返回准确值 NONNULL 保证遍历元素不会为空 IMMUTABLE Spliterator的数据源不能被修改，（不能 添加、删除、修改任何元素） CONCURRENT Spliterator的数据源可以被其他线程同时修改而无需同步 SUBSIZED 该Spliterator和从它拆分出来的Spliterator都是SIZED的 例子为什么我们需要了解这个类，有的时候甚至要实现这个类呢？ 我们来看一个例子。 对于下面一个String，我想统计下单词数量。 1static final String WORD="Hello World Happy EveryDay Good good study day day up let us study Spliterator"; 我们需要创建一个counter来累计流中字符，以及在counter中把它们结合起来的逻辑，如下： 12345678910111213141516171819202122232425public class WordCounter &#123; private final int counter; private final boolean lastSpace; public WordCounter(int counter, boolean lastSpace) &#123; this.counter = counter; this.lastSpace = lastSpace; &#125; //遍历一个个的Character public WordCounter accumulate(Character c)&#123; if(Character.isWhitespace(c))&#123; return lastSpace ? this : new WordCounter(counter,true); &#125;else&#123; //上一个字符是空格，而当前遍历的字符不是空格时，将单词计数器加一 return lastSpace ? new WordCounter(counter+1,false):this; &#125; &#125; //合并两个WordCounter，将其计数器加起来 public WordCounter combine(WordCounter wordCounter)&#123; return new WordCounter(counter+wordCounter.counter,wordCounter.lastSpace); &#125; public int getCounter()&#123; return counter; &#125;&#125; 这时候，我们在书写一个规约Character流统计单词个数就很简单了。 1234public static int countWords(Stream&lt;Character&gt; stream)&#123; WordCounter wordCounter=stream.reduce(new WordCounter(0,true),WordCounter::accumulate,WordCounter::combine); return wordCounter.getCounter();&#125; 12Stream&lt;Character&gt; stream= IntStream.range(0,WORD.length()).mapToObj(WORD::charAt);System.out.println(countWords(stream)); 输出14。结果是正确的。 现在我们让他在并行流上进行工作： 1Stream&lt;Character&gt; stream= IntStream.range(0,WORD.length()).mapToObj(WORD::charAt).parallel(); 结果输出26。显然这是不正确的。一脸懵逼。 为什么会出现这种情况呢？ 因为在并行流进行Spliterator分割时，把一个单词拆分成两部分了，导致结果变大。这显然不是我们想看到的。 实践我们要处理这种情况，就要指定分割原则，不要让程序把整个单词切开。 因此我们需要编写自己的Spliterator才能让上述问题在并行流下工作。如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class WordCounterSpliterator implements Spliterator&lt;Character&gt; &#123; private final String string; private int currentChar=0; public WordCounterSpliterator(String string) &#123; this.string = string; &#125; @Override public boolean tryAdvance(Consumer&lt;? super Character&gt; action) &#123; //处理当前字符 action.accept(string.charAt(currentChar++)); //如果还有字符要处理，返回true return currentChar&lt;string.length();&#125; @Override public Spliterator&lt;Character&gt; trySplit() &#123; int currentSize=string.length()-currentChar; //返回null表示要处理的String已经足够小 if(currentSize&lt;10)&#123; return null; &#125; //将试探拆分位置设定到要解析的文字中间 for(int splitPos=currentSize/2+currentChar;splitPos&lt;string.length();splitPos++)&#123; //如果是空格就开始拆分，不是空格将拆分位置前进直到下一个空格 if(Character.isWhitespace(string.charAt(splitPos)))&#123; //创建一个新的WordCounterSpliterator来解析String从开始到拆分位置的部分 Spliterator&lt;Character&gt; spliterator= new WordCounterSpliterator(string.substring(currentChar,splitPos)); //将这个WordCounterSpliterator的起始位置设为拆分位置 currentChar=splitPos; return spliterator; &#125; &#125; return null; &#125; @Override public long estimateSize() &#123; return string.length()-currentChar; &#125; @Override public int characteristics() &#123; return ORDERED+SIZED+SUBSIZED+NONNULL+IMMUTABLE; &#125;&#125; tryAdvance方法把String中当前位置的Character传给了Consumer，并让位置加一。作为参数传递的内部类Consumer，在遍历流时将要处理的Character传递给要执行的函数。如果新的指针位置小于String总长度，说明没有遍历完，返回true继续遍历。 trySplit方法，首先我们设置了一个拆分下限——10个Character，实际应用中我们应尽量提高这个长度避免生成太多的任务。如果长度小于这个数，就返回空无需继续拆分。否则就把试探拆分位置放到要解析的String块中间，但不能直接使用此位置，应该看看是不是空格，如果是就拆分，如果不是，就向前找，找到空格进行拆分，避免把一个单词拆成两份。 estimatedSize方法返回的是这个Spliterator解析的String的总长度和当前遍历位置的差值。 characteristic方法告诉这个框架是ORDERED（String的每个Character的默认顺序），SIZED（estimatedSize方法返回值是精确的），SUBSIZED（trySplit分出来的Spliterator大小也是固定的），NONNULL（String里面的Character不可能为null），IMMUTABLE（String本身就不可变化）。 下面我们测试一下我们的WordCounterSpliterator 。 123Spliterator&lt;Character&gt; spliterator=new WordCounterSpliterator(WORD);Stream&lt;Character&gt; stream= StreamSupport.stream(spliterator,true);System.out.println(countWords(stream)); 可以看到输出结果为14. 结论可以看到，并行流不是所有情况都适用的，有些情况要定制自己的Spliterator才能使并行流正常工作。这个例子或许运行效率并行比不上串行，但是在大数据下，比如分析一个文本文件中的单词数量，就能明显看到并行带来的速度优势了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[和前同事闲聊小记]]></title>
    <url>%2Fblog%2F%E5%92%8C%E5%89%8D%E5%90%8C%E4%BA%8B%E9%97%B2%E8%81%8A%E5%B0%8F%E8%AE%B020181027%2F</url>
    <content type="text"><![CDATA[前言这是一篇技术问题讨论文章~~~ 最近和前同事（朋友）聊天，帮他分析了这么一个需求，蛮有意思的，特来分享下其代码。 和他聊了很久，算是大致上听懂了他的需求，总结下他的需求： 假设有若干方法（任务）(A,B,C,D,E…….)，对于每个方法（任务），同一时间只能有一个用户（线程）访问，现一些用户需要执行若干任务（如用户1执行（ABC），用户2执行（BEF），用户3执行（ACEF）….），对每个用户，用户的任务不要求执行顺序，如何尽可能的提高程序运行的效率。 分析其实这之中比较重要的一点是每个方法同一时间只允许一个用户访问，我们如果给每个方法加上synchronized关键字呢？我从这方面入手没有想到太好的方法。我用了一种方法，就是从队列入手，对每个方法，可以设置一个等待队列，如果有任务正在执行该方法，则将其他的访问该方法的线程挂起，当然，对于每个用户，应该开多个线程去异步执行各个方法。 于是我想到了定长线程池，允许一个线程执行，其他线程进入后会放到等待队列中。对于方法，每有一个方法（任务），就创建一个定长线程池。应当在初始化时就将数据设置好。 代码以下是我的一些实现代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class DoTask &#123; private static Map&lt;String,ExecutorService&gt; poolMap=new HashMap&lt;&gt;(); static&#123; //获取指定class的public方法 Method[] methods=ThingsMethod.class.getDeclaredMethods(); //有多少个方法就创建多少个线程池,一个方法指定一个定长线程池 for(Method method:methods)&#123; ExecutorService pool= Executors.newFixedThreadPool(1); poolMap.put(method.getName(),pool); &#125; &#125; //list里面假设是要执行的任务，变化的 public void doTask(List&lt;String&gt; list)&#123; for(String taskStr:list)&#123; final String id="Thread"+Thread.currentThread().getId(); switch (taskStr)&#123; case "A": try&#123; executeMethod("doSomethingA",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "B": try&#123; executeMethod("doSomethingB",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "C": try&#123; executeMethod("doSomethingC",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "D": try&#123; executeMethod("doSomethingD",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; default: break; &#125; &#125; &#125; public void executeMethod(String methodName,String id)&#123; poolMap.get(methodName).execute(()-&gt;&#123; try&#123; Method method=ThingsMethod.class.getMethod(methodName,String.class); method.invoke(ThingsMethod.class.newInstance(),id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ThingsMethod &#123; public boolean doSomethingA(String id)&#123; System.out.println("线程"+id+"做A事情开始-----&gt;"); //假设做A事情花费1s try&#123; Thread.sleep(1000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做A事情结束-----&gt;"); return true; &#125; public boolean doSomethingB(String id)&#123; System.out.println("线程"+id+"做B事情开始-----&gt;"); //假设做B事情花费3s try&#123; Thread.sleep(3000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做B事情结束-----&gt;"); return true; &#125; public boolean doSomethingC(String id)&#123; System.out.println("线程"+id+"做C事情开始-----&gt;"); //假设做C事情花费2s try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做C事情结束-----&gt;"); return true; &#125; public boolean doSomethingD(String id)&#123; System.out.println("线程"+id+"做D事情开始-----&gt;"); //假设做D事情花费5s try&#123; Thread.sleep(5000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做D事情结束-----&gt;"); return true; &#125;&#125; 编写测试类对其进行测试。 123456789101112131415161718192021222324252627282930313233343536public class MyTest implements Runnable&#123; @Override public void run() &#123; //随机假设我们的任务是这样的 List&lt;String&gt; s=new ArrayList&lt;&gt;(); String [] strings=new String[]&#123;"A","B","C","D"&#125;; int l=(int)Math.ceil(Math.random()*strings.length); for(int i=0;i&lt;l;i++)&#123; s.add(strings[i]); &#125; //获取结果 System.out.println(); System.out.print("Thread"+Thread.currentThread().getId()+"随机生成的任务--》"); for(int i=0;i&lt;s.size();i++)&#123; System.out.print(s.get(i)); &#125; System.out.println(); DoTask doTask=new DoTask(); doTask.doTask(s); &#125; public static void main(String[] args) &#123; MyTest test = new MyTest(); Thread t1 = new Thread(test); Thread t2 = new Thread(test); Thread t3 = new Thread(test); Thread t4 = new Thread(test); Thread t5 = new Thread(test); Thread t6 = new Thread(test); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); t6.start(); &#125;&#125; 输出结果： 12345678910111213141516171819202122232425262728293031323334353637Thread11随机生成的任务--》AThread13随机生成的任务--》AThread15随机生成的任务--》ABThread14随机生成的任务--》ABCDThread12随机生成的任务--》ABCDThread16随机生成的任务--》A线程Thread16做A事情开始-----&gt;线程Thread14做C事情开始-----&gt;线程Thread12做B事情开始-----&gt;线程Thread12做D事情开始-----&gt;线程Thread16做A事情结束-----&gt;线程Thread13做A事情开始-----&gt;线程Thread14做C事情结束-----&gt;线程Thread12做C事情开始-----&gt;线程Thread13做A事情结束-----&gt;线程Thread12做A事情开始-----&gt;线程Thread12做B事情结束-----&gt;线程Thread14做B事情开始-----&gt;线程Thread12做A事情结束-----&gt;线程Thread14做A事情开始-----&gt;线程Thread12做C事情结束-----&gt;线程Thread14做A事情结束-----&gt;线程Thread11做A事情开始-----&gt;线程Thread12做D事情结束-----&gt;线程Thread14做D事情开始-----&gt;线程Thread11做A事情结束-----&gt;线程Thread15做A事情开始-----&gt;线程Thread14做B事情结束-----&gt;线程Thread15做B事情开始-----&gt;线程Thread15做A事情结束-----&gt;线程Thread15做B事情结束-----&gt;线程Thread14做D事情结束-----&gt; 可以看到运行结果是正确的。 讨论我还想到一种就是对各个方法（任务）添加synchronized关键字，这些方法对用户共享，这样每当有用户请求时，开线程池执行各个方法。如下： 123456789101112131415161718192021222324252627282930public class DoTask &#123; private static ThingsMethod method=new ThingsMethod(); public void doTask(List&lt;String&gt; list)&#123; for(String taskStr:list)&#123; ExecutorService executor= Executors.newCachedThreadPool(); executor.execute(()-&gt;&#123; final String id="Thread"+Thread.currentThread().getId(); doSomething(taskStr,id,method); &#125;); &#125; &#125;public void doSomething(String taskStr,String id,ThingsMethod method)&#123; switch (taskStr)&#123; case "A": method.doSomethingA(id); break; case "B": method.doSomethingA(id); break; case "C": method.doSomethingA(id); break; case "D": method.doSomethingA(id); break; default: break; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ThingsMethod &#123; public synchronized boolean doSomethingA(String id)&#123; System.out.println("线程"+id+"做A事情开始-----&gt;"); //假设做A事情花费1s try&#123; Thread.sleep(1000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做A事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingB(String id)&#123; System.out.println("线程"+id+"做B事情开始-----&gt;"); //假设做B事情花费3s try&#123; Thread.sleep(3000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做B事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingC(String id)&#123; System.out.println("线程"+id+"做C事情开始-----&gt;"); //假设做C事情花费2s try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做C事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingD(String id)&#123; System.out.println("线程"+id+"做D事情开始-----&gt;"); //假设做D事情花费5s try&#123; Thread.sleep(5000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做D事情结束-----&gt;"); return true; &#125;&#125; 由于在主线程上开启了多线程，所以这种方法观看日志比较不方便。这种方法的正确性也还没来得及验证。 如果大家有什么好的其他想法也可以讨论交流。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java8 Stream API简介]]></title>
    <url>%2Fblog%2FJava8-Stream-API%E7%AE%80%E4%BB%8B20181027%2F</url>
    <content type="text"><![CDATA[前言今天来简单讲下Java8新增的Stream API。 Stream是Java8新增的对于集合框架操作的一个类，我们也称之为流操作。使用Stream，可以简化很多重复代码，使用并行流时有时还能提高程序运行效率。我们来看下。 正文要把一个集合，List、Set或其他集合类转换为一个流，需要调用stream方法（转换为串行流）或者parallelStream方法（转换为并行流）。 Stream的常用方法如下表格介绍： 常用方法 说明 参数 举例 filter(Predicate&lt;? super T&gt; predicate) 过滤，返回一个匹配给定布尔表达式的数据流 一个返回值为boolean的lambda表达式 例1 map(Function&lt;? super T, ? extends R&gt; mapper) 匹配，对流的每个元素都应用给定的函数以获得新流 任一符合条件的lambda表达式 例2 mapToInt(ToIntFunction&lt;? super T&gt; mapper) 将流数据转换为IntStream流 流元素转为int的lambda表达式 例3 mapToLong(ToLongFunction&lt;? super T&gt; mapper) 将流数据转换为LongStream流 流元素转为long的lambda表达式 略 mapToDouble(ToDoubleFunction&lt;? super T&gt; mapper) 将流数据转换为DoubleStream流 流元素转为Double的lambda表达式 略 flatMap(Function&lt;? super T, ? extends Stream&lt;? extends R&gt;&gt; mapper) 返回一个对流的每个元素都应用给定的函数的新流。这个入参lambda包含有过滤流，此方法类似于平常的双重循环 一个包含过滤流的lambda表达式 例4 flatMapToInt(Function&lt;? super T, ? extends IntStream&gt; mapper) 参考mapToInt 参考mapToInt 略 flatMapToLong(Function&lt;? super T, ? extends LongStream&gt; mapper) 参考mapToLong 参考mapToLong 略 flatMapToDouble(Function&lt;? super T, ? extends DoubleStream&gt; mapper) 参考mapToDouble 参考mapToDouble 略 distinct() 去除流中的重复元素 无参数 例5 sorted() 对流中元素进行排序，使用自然顺序。 无参数 例6 sorted(Comparator&lt;? super T&gt; comparator) 对流中的元素进行排序，按照指定的规则 流中元素的比较规则 例7 peek(Consumer&lt;? super T&gt; action) 截断流后进行某些操作 指定的操作，无返回值lambda 例8 limit(long maxSize) 获取流中的前n个元素的流 限定值 例9 skip(long n) 获取从n开始后面元素的流，不包括n，如果n超出流长度，返回一个空流 起始长度 例10 forEach(Consumer&lt;? super T&gt; action) 循环流中的每个元素，执行指定的lambda表达式 无返回值的lambda表达式 例11 forEachOrdered(Consumer&lt;? super T&gt; action) 按照默认顺序循环流中的元素，执行指定的lambda表达式 无返回值的lambda表达式 略 toArray() 返回流中元素组成的数组 无参数 例12 toArray(IntFunction&lt;A[]&gt; generator) 将流中元素转换为指定类型的数组，数组类型必须是流元素的本身或者超类 略 略 reduce(T identity, BinaryOperator&lt;T&gt; accumulator) 归约，将流中的元素按照指定表达式归并。 起始值（默认值），指定归约公式 例13 reduce(BinaryOperator&lt;T&gt; accumulator) 同上，不同的是这个没有设置起始值，故结果可能为空，返回的是Optional对象 指定的归约公式 略 reduce(U identity,BiFunction&lt;U,? super T,U&gt; accumulator,BinaryOperator&lt;U&gt; combiner) 归约全参方法，可以指定初始值，归约方法，归约方式 略 例14 collect(Collector&lt;? super T, A, R&gt; collector) 收集器，比较常用的是toList，将流元素变为集合 略 略 collect(Supplier&lt;R&gt; supplier,BiConsumer&lt;R, ? super T&gt; accumulator,BiConsumer&lt;R, R&gt; combiner) 收集器，可以指定收集规则，收集方法等 略 略 min(Comparator&lt;? super T&gt; comparator) 根据指定规则获取流中最小值 排序规则lambda表达式 例15 max(Comparator&lt;? super T&gt; comparator) 根据指定规则获取流中最大值 排序规则lambda表达式 例15 count() 统计流中元素数量 无参数 例16 anyMatch(Predicate&lt;? super T&gt; predicate) 查找流中是否有元素符合条件，一旦查到，就返回true 条件的lambda表达式 例17 allMatch(Predicate&lt;? super T&gt; predicate) 查找流中的元素是否全部匹配指定表达式，全部符合才返回true 条件的lambda表达式 例18 noneMatch(Predicate&lt;? super T&gt; predicate) 查找流中的元素是否全部不匹配指定表达式，全部不匹配才返回true 条件的lambda表达式 略 findFirst() 查找流的第一个元素，如果流为空，返回空（Optional的空） 无参数 略 findAny() 随便从流中拿一个元素，如果流为空，返回空（Optional的空） 无参数 略 empty() 静态方法，返回一个空流 略 例19 of(T t) 静态方法，生成一个元素的流 略 例19 of(T… values) 静态方法，生成若干元素的流 略 例19 iterate(final T seed, final UnaryOperator&lt;T&gt; f) 静态方法，生成一个有规律的流 起始值，指定的lambda表达式 例19 generate(Supplier&lt;T&gt; s) 静态方法，生成指定规则的数据流，流数据的生成完全取决于lambda表达式 生成流数据的lambda表达式 例19 concat(Stream&lt;? extends T&gt; a, Stream&lt;? extends T&gt; b) 静态方法，将两个数据流合并，两个流的类型须一致 两个数据流 例19 例子举例：假设有一个1-100的List集合list1,50-150的List集合list2. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//例1：需要一个1-100的偶数List集合。list1.stream().filter(n-&gt;n%2==0).collect(Collectors.toList());//例2：获取list1各元素加上100，再取基数的List集合。list1.stream().map(n-&gt;n+100).map(n-&gt;n%2!=0).collect(Collectors.toList());//例3：将list1的流转换为IntStream。list1.stream().mapToInt(n-&gt;n.intValue());//例4：list1和list2的交集集合。list1.stream().flatMap(i-&gt;list2.stream().map(j-&gt;i==j)).collect(Collectors.toList());//例5：list1和list2的并集集合。list1.addAll(list2);list1.stream().distinct();//例6：对list1进行排序。list1.stream().sorted().collect(Collectors.toList());//例7：对list1进行倒序排序。list1.stream().sorted(Comparator.reverseOrder()).collect(Collectors.toList());//例8：截断查看流的每一步得到的流元素。list1.stream().map(n-&gt;n+100).peek(System.out::println).map(n-&gt;n%2!=0).peek(System.out::println).collect(Collectors.toList());//例9：取list1的前10个元素。list1.stream().limit(10).collect(Collectors.toList());//例10：取list1的后10个元素。list1.stream().skip(list1.size()-10).collect(Collectors.toList());//例11：输出list1的各个元素。list1.stream().forEach(System.out::println);//例12：将list1元素转为数组。list1.stream().toArray();//例13：对list1元素求和，对list2元素求差。list1.stream().reduce(0,(a,b)-&gt;a+b);list2.stream().reduce(0,(a,b)-&gt;a-b);//例14：对list1按照最大值到最小值求和。list1.stream().reduce(0,(a,b)-&gt;a+b, BinaryOperator.maxBy(Comparator.naturalOrder()));//例15：取list1的最大最小值list1.stream().max(Comparator.naturalOrder());list1.stream().min(Comparator.naturalOrder());//例16：求list1的元素个数list1.stream().count();//例17：查询list1中有没有17的倍数（除了17）。list1.stream().anyMatch(n-&gt;n!=17&amp;&amp;n%17==0);//例18：查询list1中所有元素是不是都是2的倍数。list1.stream().allMatch(n-&gt;n%2==0);//例19：Stream.empty();//空流Stream.of(1);//生成一个元素的流Stream.of(1,2,3,4);//生成若干元素的流Stream.iterate(1,n-&gt;2*n);//生成指定规则的数据流Stream.generate(()-&gt;Math.random());//生成指定规则的数据流Stream.concat(list1.stream(),list2.stream());//将两个数据流合并 结语Java8 Stream API的一些常用方法基本如上，Stream API为操作集合类提供了更方便快捷的方式，值得我们掌握。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你该相信什么]]></title>
    <url>%2Fblog%2F%E4%BD%A0%E8%AF%A5%E7%9B%B8%E4%BF%A1%E4%BB%80%E4%B9%8820181027%2F</url>
    <content type="text"><![CDATA[前言写这篇文章，或是怀念过去的自己，或是憧憬自己未来。可是啊，无论如何，要活好当下。 正文工作总的来说差不多四年了，或许已从职场菜鸟变化为职场老手了。 可是，岁月的变迁确无法掩饰我这颗浮躁的心，哎，想想自己，确实很浮躁啊。 曾经，想静心，于是开始练字，练了很长一段时间，可是后面确不知不觉的放弃了，至今剩下的字帖和钢笔仍静静地躺在我的桌子上。 后来，心血来潮搭建了属于自己的博客，然后创建了微信公众号，平时写些文章来总结自己和工作，我希望自己可以坚持下去，也相信自己可以坚持下去。 世上大多数人是平凡的，可是，要有一颗不甘平凡的心。 最怕一生碌碌无为，还安慰平凡难能可贵。 任何事都不应成为懒惰的借口。 不诉困苦于环境，不寄期望于运气，不服挫折于命运。 还记得那个年少的自己吗？或许他变了，可是他还在，他一直陪着你。 相信自己。做自己的英雄。 努力吧，骚年！ 加油！ 总结好了，喝饱鸡汤了。 努力去“搬砖”吧！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 接口]]></title>
    <url>%2Fblog%2FJava8-%E6%8E%A5%E5%8F%A320181014%2F</url>
    <content type="text"><![CDATA[前言Java 8已经推出相当长一段时间了，其中，接口部分有一些变化。我们来研究下它。 问题我们知道，对于一个接口，如果我们声明好后，如果再想对其增加新的方法是困难的，因为我们要改变所有其实现类，也就是每个实现类都要对其新方法进行实现。如下图： 这显然是不现实的，如果我们直接把方法写在实现类里，接口中没有此方法，就破坏了我们的多态性。 对于某些已经发布的应用，无论哪种做法都是比较繁重且不被推荐的。 接口默认实现还好，Java大神们已经意识到了这个问题，于是在Java8中，引入了对接口的默认方法实现。 什么是默认方法实现呢？ 简单来说，就是允许接口定义默认方法，在接口中需要有方法体的具体实现，实现类默认继承该方法（如果不重写默认方法的话）。同时为区分默认方法，默认方法在接口中采用default关键字标明。如下图： 这样，如果我们新增一个接口方法（对于已经发布的接口），可以使用默认实现，就不会出现我们上述的问题。 思考你一定会说，这和抽象类有什么区别呢？ 当然还是有区别的，Java8以后，接口和抽象类的几点总结如下： 新的问题接口引入了默认方法后，就会有新的问题，好在Java已经替我们解决了，我们来看下。 情况一123456789101112131415public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B extends A&#123; default void doSomething()&#123; System.out.println("Interface B"); &#125;&#125;public class C implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 结果：输出 Interface B 情况二1234567public class D implements A &#123;&#125;public class C extends D implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 结果：输出 Interface B 情况三如果D是这样呢？ 12345public class D implements A &#123; public void doSomething()&#123; System.out.println("Class D"); &#125;&#125; 结果：输出Class D 如果D不对doSomething提供实现（D为抽象的类），则C需要为doSomething提供实现。 情况四如果B接口不在继承A接口。如下： 12345678910public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B&#123; default void doSomething()&#123; System.out.println("Interface B"); &#125;&#125; 那么我们C类必须为doSomething提供实现，当然我们可以具体制定使用哪个接口的doSomething方法，如下： 123456public class C implements A, B &#123; @Override public void doSomething() &#123; B.super.doSomething(); &#125;&#125; 情况五如果两个函数不一样但差距很小呢？如下： 123456789101112131415public interface A &#123; default int doSomething()&#123; return 100; &#125;&#125;public interface B&#123; default long doSomething()&#123; return 200; &#125;&#125;public class C implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 在IDEA里我们可以看到，类C是无法编译的，这是不被允许的。 情况六1234567891011121314public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B extends A&#123;&#125;public interface C extends A&#123;&#125;public class D implements B,C &#123; public static void main(String[] args) &#123; new D().doSomething(); &#125;&#125; 输出结果Interface A 因为只有A声明了一个默认方法，这个接口是D的父接口，故输出Interface A。如果B也提供了一个默认方法，签名和A一致，那么编译器会选择B的默认方法，如果B添加一个相同签名的抽象方法，则D需要为其提供实现，如果B,C都有相同签名的默认方法doSomething，则会出现冲突，需要我们为doSomething提供实现或者指定使用B，C中的哪个方法。 结论解决问题的三条规则： 如果一个类使用相同的函数签名从多个地方（比如另一个类或者接口）继承了方法，通过三条规则可进行判断。 类中的方法优先级最高。类或者父类中声明的方法优先级高于任何声明为默认方法的优先级。 如果无法依据第一条判断，那么子接口的优先级更高：函数签名相同时，优先选择拥有最具体实现的默认方法的接口，及如果B继承了A，那么B就比A更加具体。 最后，如果还是无法判断，继承了多个接口的类必须通过显式覆盖和调用期望的方法，显式地选择使用哪一个默认方法的实现。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Callable原理分析]]></title>
    <url>%2Fblog%2FCallable%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%9020181014%2F</url>
    <content type="text"><![CDATA[前言今天我们来分析实现多线程的一种方式，实现Callable接口。这种方式有种特殊的地方，就是可以拿到线程返回值。具体怎么实现的呢？我们来研究下。 栗子我们先举个简单的栗子来看下Callable接口吧。要首先明白，线程池提交实现Callable接口的线程后会返回一个Future对象，这个对象里包含程序的运行结果。 123456789101112public class CallableTest &#123; public static void main(String[] args) throws ExecutionException, InterruptedException,TimeoutException&#123; //创建一个线程池 ExecutorService executor = Executors.newCachedThreadPool(); Future&lt;String&gt; future = executor.submit(()-&gt; &#123; //System.out.println("CallableTest"); TimeUnit.SECONDS.sleep(5); return "CallableTest"; &#125;); System.out.println(future.get()); &#125;&#125; 可以看到我们可以通过future.get()拿到结果”CallableTest”。我们也可以设置指定时间后拿到结果，如指定6s后拿到结果。 1System.out.println(future.get(4,TimeUnit.SECONDS)); 可以看到也拿到了返回结果，如果我们设置4s拿到结果，小于程序运行时间5s，可以看到它抛出了超时异常。java.util.concurrent.TimeoutException。 原理是不是很神奇？ 我们来研究下Callable接口获取返回值的原理。 我们先来看看ExecutorService的submit方法，它接受一个Callable对象，返回一个Future对象。 1&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); 及它的实现。AbstractExecutorService的submit方法。 12345678910public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125;protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable);&#125;void execute(Runnable command); ThreadPoolExecutor的execute是对其实现。 可以看到创建了一个FutureTask对象并执行。 FutureTask对象实现了Runable接口，我们来看下它。 看一下它的run方法。 1234567891011121314151617181920212223242526272829303132333435public void run() &#123; if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //拿到结果设置ran为true result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; //异常设置结果为空ran为false并设置异常 result = null; ran = false; setException(ex); &#125; //ran为true时放入结果 if (ran) set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; 1234567public V get() throws InterruptedException, ExecutionException &#123; int s = state; //会一直挂起知道处理业务的线程完成，唤醒等待线程 if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125; 我们调用get方法时，他先查询线程状态，如果未完成，就调用awaitDone方法。 1234567891011121314151617181920212223242526272829303132333435363738private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; //如果设置了超时时间就获取截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; /循环监视线程的状态 for (;;) &#123; if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; else if (s == COMPLETING) // cannot time out yet Thread.yield(); else if (q == null) q = new WaitNode(); else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); else if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; else LockSupport.park(this); &#125; &#125; 带有超时时间的get，到达时间后，会判断线程状态，如果未完成，抛出超时异常。 12345678910public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; if (unit == null) throw new NullPointerException(); int s = state; if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s);&#125; 总结因此，带有返回值得异步线程基本上可以这样理解。 由于有返回值，如果未设置等待时间，会等线程执行完成后返回，基本类似同步。其原理是线程运行后，如果未完成，会放入等待队列。直到线程状态变化（完成或者异常等）。如果设置了等待时间，则到时间后无论线程状态是否完成都会返回线程状态。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道Java试题引发的思考]]></title>
    <url>%2Fblog%2F%E4%B8%80%E9%81%93Java%E8%AF%95%E9%A2%98%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%8320181013%2F</url>
    <content type="text"><![CDATA[前言无意中看到了自己入门Java时的一本Java基础书，看到了其中一个小结的习题，颇为简单。求一个数组所有数据的和。 分析那时入门Java，看到以前自己在书上写下的for循环。大致如下： 1234567public static Long sum(int [] numbers)&#123; long result=0L; for(int i=0;i&lt;numbers.length;i++)&#123; result+=numbers[i]; &#125; return result;&#125; 现已入行Java颇有时日，遂想到当n逐渐变大时，其执行效率会逐渐降低的。 近日对多线程有些许研究，故想到可以将数据平均分为2份，3份或者多份，每份进行和值，最后相加得到结果。也是一个不错的想法。 好在Java里有这么一个框架，名字叫分支/合并框架。我们来配合例子研究下。 分支/合并框架分支/合并框架的目的是以递归的方式将可以并行的任务分为更小的任务，然后将每个子任务的结果合并起来生成整体的结果。它是ExecutorService接口的一个实现，它把子任务分配给线程池（ForkJoinPool）中的工作线程。那我们如何来定义任务和子任务呢？ 要把任务提交到ForkJoinPool，必须创建RecursiveTask&lt; R &gt;的一个子类，其中R是并行化任务产生的结果类型。它其中有唯一一个抽象方法compute，用来定义将任务拆分成子任务的逻辑，以及无法再拆分或不方便拆分时，生成单个子任务结果的逻辑。 伪代码如下： 1234567if(任务足够小或者不可分)&#123; 顺序执行该任务&#125;else&#123; 将任务分为两个子任务 递归调用本方法，拆分每个子任务，等待所有子任务完成 合并每个子任务结果&#125; 那如何定义一个任务是否可以在拆分呢？ 一般来说没有明确的标准决定一个任务是否可以在拆分，但是有几种试探方法可以帮助我们做出决定，分支/合并框架采用了一种称为工作窃取的技术来解决这个问题。每个线程的子任务都保存在一个双向链式队列里，每完成一个任务，就从队列头部获取一个任务，如果有线程先执行完成，它不会“闲”下来，而是去未完成的队列尾部“窃取”一个任务进行完成，直至所有子任务完成返回结果。 实践我们用分支/合并框架对数组数据进行并行求和。 代码如下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ForkJoinSumCalculator extends RecursiveTask&lt;Long&gt; &#123; //要求和的数组 private final int[] numbers; //子任务处理的数组的起始位置 private final int start; //子任务处理的数组的终止位置 private final int end; //不再将任务划分的子任务数组大小 public static final long THRESHOLD=10000; public ForkJoinSumCalculator(int[] numbers)&#123; this(numbers,0,numbers.length); &#125; private ForkJoinSumCalculator(int[] numbers,int start,int end)&#123; this.numbers=numbers; this.start=start; this.end=end; &#125; @Override protected Long compute() &#123; int length=end-start; //小于等于阈值，计算结果 if(length&lt;=THRESHOLD)&#123; return computeSequentially(); &#125; //创建一个子任务来为数组的前一半求和 ForkJoinSumCalculator leftTask=new ForkJoinSumCalculator(numbers,start,start+length/2); //利用另一个ForkJoinPool线程异步执行新创建的子任务 leftTask.fork(); //创建一个任务为数组的后一半求和 ForkJoinSumCalculator rightTask=new ForkJoinSumCalculator(numbers,start+length/2,end); //同步执行第二个子任务，有可能允许进一步递归划分 Long rightResult=rightTask.compute(); //读取第一个子任务的结果，没有完成就等待 Long leftResult=leftTask.join(); //合并结果 return rightResult+leftResult; &#125; //子任务数组求和 private long computeSequentially()&#123; long sum=0; for(int i=start;i&lt;end;i++)&#123; sum+=numbers[i]; &#125; return sum; &#125;&#125; 这样，我们在编写一个方法并行对数组求和就很简单了。 1234public static long forkJoinSum(int [] numbers)&#123; ForkJoinTask&lt;Long&gt; task=new ForkJoinSumCalculator(numbers); return new ForkJoinPool().invoke(task);&#125; 我们可以写一个测试方法，测试这两种方法的执行效率。 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) &#123; //构建一个数组 int [] numbers=new int[100000000]; for(int i=0;i&lt;numbers.length;i++)&#123; numbers[i]=(int)(Math.random() * Integer.MAX_VALUE); &#125; //分支/合并框架计算执行速度 long fastest=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); forkJoinSum(numbers); long duration=(System.nanoTime()-start); if(duration&lt;fastest)&#123; fastest=duration; &#125; &#125; System.out.println("分支/合并最快执行速度为"+fastest+"ns"); //普通方法计算执行速度 long fastest1=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); sum(numbers); long duration=(System.nanoTime()-start); if(duration&lt;fastest1)&#123; fastest1=duration; &#125; &#125; System.out.println("普通算法最快执行速度为"+fastest1+"ns"); &#125; 输出如下： 12分支/合并最快执行速度为25894038ns普通算法最快执行速度为38811709ns 可以看到速度是有明显提升的。 其他源数据问题这个计算的数组之所以随机，是因为我之前测试了1-n的和计算，发现for循环居然比分支/合并框架快！！我加大了n值也是如此，所以对于这种比较特殊的计算，Java虚拟机或者编译器对它们的计算做了优化，因此用这些数据测试时，可能得不到我们想要的结果，这也是并发处理比较难的地方。有的时候我们多线程处理的代码可能还没有单线程快，或者快是快了，但结果错了。 拥抱Java8看到上面我们为了求个和冒着出错的风险写了一个sum类，是不是心里退却了？不过啊，Java 8已经给我们提供了类似的功能啦。 它就是全新的Stream API，这个我们有时间在介绍。先看看Stream API对于这个是如何求和的。 12Arrays.stream(numbers).sum();Arrays.stream(numbers).parallel().sum(); 是不是很简单？？ 第一个为串行求和，即单线程，第二个为并行求和。 我们来测试下他们的效率。 1234567891011121314151617181920212223242526//Stream API 串行方法计算执行速度 long fastest2=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); //sum(numbers); Arrays.stream(numbers).sum(); long duration=(System.nanoTime()-start); if(duration&lt;fastest2)&#123; fastest2=duration; &#125; &#125; System.out.println("Stream API 串行 最快执行速度为"+fastest2+"ns"); //Stream API 并行方法计算执行速度 long fastest3=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); //sum(numbers); Arrays.stream(numbers).parallel().sum(); long duration=(System.nanoTime()-start); if(duration&lt;fastest3)&#123; fastest3=duration; &#125; &#125; System.out.println("Stream API 并行 最快执行速度为"+fastest3+"ns"); 输出如下结果： 1234分支/合并最快执行速度为25316712ns普通算法最快执行速度为38812671nsStream API 串行 最快执行速度为36572646nsStream API 并行 最快执行速度为24291637ns 可以看到，并行情况下时间与刚才写的分支/合并框架相近。 其实：并行流背后使用的基础框架就是分支/合并框架。 这只是最简单的求和例子，遇到实际问题，可能使用Stream流比普通实现简单快速，因此，Stream是值得我们学习的。 结论学习就是不断思考不断进步的过程，有的时候看看自己之前写的代码，应该少一些“我去，写的什么玩意，原来？”之类的抱怨，多一些深入的思考及优化。 相关代码均已上传Github ： https://github.com/JavaZWT/framework-base]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Transactional注解]]></title>
    <url>%2Fblog%2FSpring-Transactional%E6%B3%A8%E8%A7%A320181013%2F</url>
    <content type="text"><![CDATA[前言使用@Transactional 注解有一段时间了，今天来对它进行下总结。 再说这个之前先说下事务。 事务的基本要素（ACID） 原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 说明Transactional注解的主要参数如下： 下面来简单介绍下它的参数： value：一般用来配置指定的事务管理器。 propagation：事务的传播属性，有七种。见枚举Propagation，默认REQUIRED。 1234567891011121314//支持当前事务，如果不存在就创建一个REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED),//支持当前事务，如果不存在就以非事务的方式运行SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS),//支持当前事务，不存在就抛出异常MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY),//创建一个新事务，如果当前有事务就暂停当前事务REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW),//以非事务方式运行，如果当前有事务就暂停当前事务NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED),//以非事务的方式运行，如果当前有事务就抛出异常NEVER(TransactionDefinition.PROPAGATION_NEVER),//如果当前有事务，就加入当前事务。NESTED(TransactionDefinition.PROPAGATION_NESTED); isolation：隔离级别。事务的隔离级别有4种。我们看一下Isolation枚举类。 12345678910//使用默认的隔离级别，取决于底层数据库的默认隔离级别DEFAULT(TransactionDefinition.ISOLATION_DEFAULT),//读未提交，这种隔离级别最低，会出现脏读，不可重复读，虚读（幻读）等情况。一般不用。READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED),//读已提交，这种隔离级别可以防止脏读的产生，但是无法避免不可重复读和虚读（幻读）的出现。READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED),//可重复读，这种隔离级别可以防止脏读、不可重复读的出现，但是无法避免虚读（幻读）的产生。REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ),//串行化，这种隔离级别最高，可以避免脏读、不可重复读和虚读（幻读）的产生。SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。 Mysql默认隔离级别为可重复读。 关于事务的并发问题 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 timeout : 事务的超时时间，默认为-1，即事务直到完成都不会超时。 readOnly：是否只读，一般用来标识。拥有这个标识，写入操作不一定会失败，取决于数据库系统。 rollbackFor：事务回滚条件。参数为继承Throwable的class类。 rollbackForClassName：事务回滚条件。可以接受String数组。 noRollbackFor： 事务不会回滚条件。 noRollbackForClassName：事务不会回滚条件。 原理Spring的Transaction注解是如何实现并生效的呢？我们来探究下。 我们首先来看看SpringTransactionAnnotationParser这个类，这个是解析transaction注解的类。调用AnnotationUtils类的getAnnotation方法拿到注解信息。 可以看到AnnotationTransactionAttributeSource这个类调用了parseTransactionAnnotation方法。 我们来看下AbstractFallbackTransactionAttributeSource这个类的computeTransactionAttribute方法。调用了上图的findTransactionAttribute方法。getTransactionAttribute方法，由于获取注解信息耗时，故spring先从缓存里获取注解事务信息，查不到在用程序获取。 TransactionAspectSupport这个类的createTransactionIfNecessary方法，调用getTransactionAttribute方法。 我们可以看到AbstractTransactionAspect.aj文件使用了createTransactionIfNecessary方法。这个应用了aspectj。Before指在事务开始之前获取注解信息。After throwing 指当方法抛出异常后的执行动作。这里一般会进行回滚操作。After returning指当方法返回前的执行动作。这里一般会提交事务。After 指当事务完成后的动作。这里会清空当前事务注解信息。 结论事务的隔离级别和传播属性都是我们应该掌握和学习的，对于Spring的@Transactional 注解，我们不但要会使用，而且应该深入去理解它的实现原理。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道面试题引发的思考（递归）]]></title>
    <url>%2Fblog%2F%E4%B8%80%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%8320181003%2F</url>
    <content type="text"><![CDATA[前言某日，去某互联网公司面试，被问到了如下一道面试题： 题目很简单，有一队人，已知第一个人8岁，后一个人比前一个人大两岁，以此类推，问第8个人多少岁？第N个人多少岁。 我拿过笔和纸，不假思索的写下了如下答案： 1234567static int getAge(int n)&#123; if(n==1)&#123; return 8; &#125;else&#123; return getAge(n-1)+2; &#125;&#125; 面试官又问我还有什么需要注意的吗？我说要注意参数不能小于1. 显然，面试官不是很满意，又问我如果N很大会怎么样？ 我想了想，说，会出现OOM异常吧，或者超了int的范围。 感觉他还是不满意。 思考自那过去一段时间，我自认为答得没有问题，也答到了点上（只可惜面试没过~O(∩_∩)O哈哈~）。 最近又想到了这个问题，决定研究研究。 实践我用自己的方法，进行了数据测试。当然我把为了测试效果明显，我加大了测试值。 123public static void main(String[] args) &#123; System.out.println(getAge(1000000));&#125; 结果使我惊讶，不是OOM异常，而是堆栈异常。 1Exception in thread "main" java.lang.StackOverflowError 后面我了解到，递归调用，可以假想成一个函数调用另一个函数，而每个函数相当于占用一个栈帧，这些栈帧以先进后出的方式排列起来形成栈。如下图： 这样，函数会追寻到栈顶，拿到getAge(1)的值后逐渐返回。如下。 1234getAge(1)=8getAge(2)=getAge(1)+2=10getAge(3)=getAge(2)+2=12getAge(4)=getAge(3)+2=14 可以知道，如果堆栈深度不够的话，就会出现异常。 我们上图所示的异常就是这个原因。 提升改进一在研究这个问题时，我发现了一种递归，尾递归。 如下所示： 1234567static int getAge1(int n,int result)&#123; if(n==1)&#123; return result; &#125;else&#123; return getAge1(n-1,result+2); &#125;&#125; 这种递归我们可以看到，带了一个参数result，当他运行到n==1时，直接返回了result，不用在一层层回退进行计算。如下： 12345 getAge(4,8)= getAge(3,8+2)= getAge(2,8+2+2)= getAge(1,8+2+2+2)= 14 一些编译器发现这些函数可以在一个栈帧里进行完成，就会复用栈帧，优化代码。 可惜的是，到目前为止，JAVA在HotSpot（Oracle的JVM）上使用时，并不支持尾递归优化。据说IBM的JVM支持尾递归优化，有兴趣的童鞋可以试下。 所以这种方案的测试结果肯定也是StackOverflowError啦。 改进二循环解决: 那时脑子笨，现在想想，这道题用循环也是可以解决的，且不用担心堆栈溢出问题。如下： While循环： 1234567static int getAge2(int n,int result,int step)&#123; while(n&gt;1)&#123; result+=step; n--; &#125; return result;&#125; For循环: 123456static int getAge3(int start,int end,int firstValue,int step)&#123; for(int i=start;i&lt;end;i++)&#123; firstValue+=step; &#125; return firstValue;&#125; 且速度也提高了不少。 改进三这么有规律的数据，当然乘法就可以解决，如下： 123static int getAge4(int start,int end,int firstValue,int step)&#123; return firstValue+(end-start)*step;&#125; 以上例子没有对入参做校验处理，结合实际场景，其实需要处理的，在此略掉了。 总结JDK源码中很少有递归，因为递归不能被优化，当数据过大时，很容易出现堆栈溢出，我们也应该慎用，基本能有递归解决的问题也可以用循环解决。 可以看出，对于一个问题，实现的方案可能不止一种。 看到自己想了及总结了多种方法，感觉很诧异，面试时却只想到了递归一种，而且实现很不好。还是自己基础不够扎实，应该多学多看多练。 其他上面都说要多学多看多练啦….. 这是我测试IBM JDK对于尾递归（getAge1方法）的结果。 说明IBM JDK确实对尾递归有优化，我又测试了getAge方法（普通递归），结果和Oracle JDK一样，抛出 java.lang.StackOverflowError 异常。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于注解的通用数据验证]]></title>
    <url>%2Fblog%2F%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E7%9A%84%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E9%AA%8C%E8%AF%8120180924%2F</url>
    <content type="text"><![CDATA[前言在项目开发交互过程中，难免会遇到一些数据校验。以校验客户端发送数据的合法性，对于一些非空校验，我们也许可以使用@NonNull，@NotNull 等注解，可是对于一些常规的，如手机号，身份证等等的校验，我们就还要判断处理每个请求的参数的合法性。 但是合法性的判断是难以避免的，我们是否可以精简工作量、提高工作效率呢。 思考我们或许应该从@NonNull @NotNull等其他注解那里受到些启发。 我们或许可以结合正则表达式及注解对某些通用数据进行验证。 注解可以设置参数，我们可以设置参数为校验规则，通过枚举列举出来，同时也应该允许用户自定义正则等校验。 我们知道，注解有三种类型 RetentionPolicy.SOURCE RetentionPolicy.CLASS RetentionPolicy.RUNTIME。 SOURCE主要用于编译之前，编译过程中会被丢弃如@Override注解。 CLASS主要用于编译，运行时会被丢弃。 RUNTIME在源码，编译，运行时始终会存在。 可以利用反射，拿到具有特定注解的bean，并处理。所以我们定义的注解应该是RUNTIME类型。同时声明注作用范围为FIELD及PARAMETER。 实践定义注解1234567891011121314151617181920/** * 数据验证注解 */@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.FIELD,ElementType.PARAMETER&#125;)public @interface DataValid &#123; //是否可以为空 boolean nullable() default false; //提供几种常用的正则验证 RegexType regexType() default RegexType.NONE; //自定义正则验证 String regexExpression() default ""; //参数或者字段描述 String description() default "";&#125; 定义如上注解，nullable用来校验参数是否可空，默认不可以为空，false。 同时提供几种通用的正则校验，用枚举列出，如手机号码校验，身份证信息校验等等。 同时如果没有规定的正则表达式，可以让用户自定义自己的正则表达式。 另增加描述字段，用来说明这个paramer的用途。 定义常用正则枚举12345678910111213141516/** * 正则类型枚举 */public enum RegexType &#123; NONE, SPECIALCHAR, CHINESE, EMAIL, IP, NUMBER, NUMBERORNIL, PHONENUMBER, ID; &#125; 列出几种常用枚举。非空，特殊字符，中文，邮箱，IP，数字等等 枚举规则定义了枚举，要定义它们的具体对应的方法，以便后续调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164/** * 常用正则表达式 */public class RegexUtils &#123; /** * 判断是否是正确的IP地址 * * @param ip * @return boolean true,通过，false，没通过 */ public static boolean isIp(String ip) &#123; if (null == ip || "".equals(ip)) return false; String regex = "^(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|[1-9])\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)$"; return ip.matches(regex); &#125; /** * 判断是否是正确的邮箱地址 * * @param email * @return boolean true,通过，false，没通过 */ public static boolean isEmail(String email) &#123; if (null == email || "".equals(email)) return false; String regex = "\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*"; return email.matches(regex); &#125; /** * 判断是否含有中文，仅适合中国汉字，不包括标点 * @param text * @return boolean true,通过，false，没通过 */ public static boolean isChinese(String text) &#123; if (null == text || "".equals(text)) return false; Pattern p = Pattern.compile("[\u4e00-\u9fa5]"); Matcher m = p.matcher(text); return m.find(); &#125; /** * 判断是否正整数 * * @param number * 数字 * @return boolean true,通过，false，没通过 */ public static boolean isNumber(String number) &#123; if (null == number || "".equals(number)) return false; String regex = "[0-9]*"; return number.matches(regex); &#125; /** * 判断是否正整数(可以为空) * * @param number * 数字 * @return boolean true,通过，false，没通过 */ public static boolean isNumberOrNil(String number) &#123; if(null == number) return true; if ("".equals(number.trim())) return true; String regex = "[0-9]*"; return number.matches(regex); &#125; /** * 判断几位小数(正数) * * @param decimal * 数字 * @param count * 小数位数 * @return boolean true,通过，false，没通过 */ public static boolean isDecimal(String decimal, int count) &#123; if (null == decimal || "".equals(decimal)) return false; String regex = "^(-)?(([1-9]&#123;1&#125;\\d*)|([0]&#123;1&#125;))(\\.(\\d)&#123;" + count + "&#125;)?$"; return decimal.matches(regex); &#125; /** * 判断是否是手机号码 * * @param phoneNumber * 手机号码 * @return boolean true,通过，false，没通过 */ public static boolean isPhoneNumber(String phoneNumber) &#123; if (null == phoneNumber || "".equals(phoneNumber)) return false; String regex = "^1[3|4|5|6|7|8|9][0-9]\\d&#123;8&#125;$"; return phoneNumber.matches(regex); &#125; /** * 判断身份证号格式正确性 * * @param ID * 身份证号 * @return boolean true,通过，false，没通过 */ public static boolean isID(String ID) &#123; if (null == ID || "".equals(ID)) return false; String regex = "^(\\d&#123;14&#125;[0-9a-zA-Z])|(\\d&#123;17&#125;[0-9a-zA-Z])$"; return ID.matches(regex); &#125; /** * 判断是否含有特殊字符 * * @param text * @return boolean true,通过，false，没通过 */ public static boolean hasSpecialChar(String text) &#123; if (null == text || "".equals(text)) return false; if (text.replaceAll("[a-z]*[A-Z]*\\d*-*_*\\s*", "").length() == 0) &#123; // 如果不包含特殊字符 return true; &#125; return false; &#125; /** * 适应CJK（中日韩）字符集，部分中日韩的字是一样的 */ public static boolean isChinese2(String strName) &#123; char[] ch = strName.toCharArray(); for (int i = 0; i &lt; ch.length; i++) &#123; char c = ch[i]; if (isChinese(c)) &#123; return true; &#125; &#125; return false; &#125; private static boolean isChinese(char c) &#123; Character.UnicodeBlock ub = Character.UnicodeBlock.of(c); if (ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_COMPATIBILITY_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_A || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_B || ub == Character.UnicodeBlock.CJK_SYMBOLS_AND_PUNCTUATION || ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS || ub == Character.UnicodeBlock.GENERAL_PUNCTUATION) &#123; return true; &#125; return false; &#125;&#125; 实现及调用基本数据都定义及处理好了，我们应该建立注解与方法之间的关联，RUNTIME类型的注解在程序运行时也会被保留，我们可以利用反射，拿到具体注解参数信息，进行相关处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113/** * 注解解析Service */public class ValidateService &#123; /*private static DataValid dataValid;*/ public ValidateService() &#123; super(); &#125; /** * 解析入口 * @param object * @throws Exception */ public static void valid(Object object) throws Exception&#123; //获取object的类型 Class&lt;? extends Object&gt; clazz=object.getClass(); //获取该类型声明的成员 Field[] fields=clazz.getDeclaredFields(); //遍历属性 for(Field field:fields)&#123; //对于private私有化的成员变量，通过setAccessible来修改器访问权限 field.setAccessible(true); validate(field,object); //重新设置会私有权限 field.setAccessible(false); &#125; &#125; public static void validate(Field field,Object object) throws Exception&#123; String description = null; Object value = null; DataValid dataValid = null; //获取对象的成员的注解信息 dataValid=field.getAnnotation(DataValid.class); value=field.get(object); if(dataValid==null)return; description=dataValid.description().equals("")?field.getName():dataValid.description(); /*************注解解析工作开始******************/ if(!dataValid.nullable() &amp;&amp; dataValid.regexType() != RegexType.NUMBERORNIL)&#123; if(value==null|| StringUtils.isBlank(value.toString()))&#123; throw new Exception(description+"不能为空"); &#125; &#125; if(dataValid.regexType()!=RegexType.NONE)&#123; switch (dataValid.regexType()) &#123; case NONE: break; case SPECIALCHAR: if(RegexUtils.hasSpecialChar(value.toString()))&#123; throw new Exception(description+"不能含有特殊字符"); &#125; break; case CHINESE: if(RegexUtils.isChinese2(value.toString()))&#123; throw new Exception(description+"不能含有中文字符"); &#125; break; case EMAIL: if(!RegexUtils.isEmail(value.toString()))&#123; throw new Exception(description+"邮箱地址格式不正确"); &#125; break; case IP: if(!RegexUtils.isIp(value.toString()))&#123; throw new Exception(description+"IP地址格式不正确"); &#125; break; case NUMBER: if(!RegexUtils.isNumber(value.toString()))&#123; throw new Exception(description+"不是数字"); &#125; break; case NUMBERORNIL: if(value == null)&#123; break; &#125; if(!RegexUtils.isNumberOrNil(value.toString()))&#123; throw new Exception(description+"格式不正确"); &#125; break; case PHONENUMBER: if(!RegexUtils.isPhoneNumber(value.toString()))&#123; throw new Exception("手机号格式不正确"); &#125; break; case ID: if(!RegexUtils.isID(value.toString()))&#123; throw new Exception("身份证号格式不正确"); &#125; break; default: break; &#125; &#125; if(!dataValid.regexExpression().equals(""))&#123; if(value.toString().matches(dataValid.regexExpression()))&#123; throw new Exception(description+"格式不正确"); &#125; &#125; /*************注解解析工作结束******************/ &#125;&#125; 如上代码。 当然，到具体业务层，应该调用这个Service的valid方法去校验参数。 结论可以看到，经过这样，我们可以把一些常用的校验通过这种方式封装，大大简化代码量，使业务层更注重业务。 这种也可以添加自己的通用类型，灵活性很强。 这个小小的简单工具最主要的就是利用了Java的反射机制。 以上。 今天就到这里啦，中秋节快乐～～]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK里那些有趣的代码（1）]]></title>
    <url>%2Fblog%2FJDK%E9%87%8C%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%881%EF%BC%8920180916%2F</url>
    <content type="text"><![CDATA[前言今天我们来看一下JDK里一些好玩的代码。我们来分析下。 在Integer源码里，我们可以看到这样一段代码： 12345678final static int [] sizeTable = &#123; 9, 99, 999, 9999, 99999, 999999, 9999999,99999999, 999999999, Integer.MAX_VALUE &#125;;// Requires positive xstatic int stringSize1(int x) &#123; for (int i=0; ; i++) if (x &lt;= sizeTable[i]) return i+1;&#125; 这段代码是在干啥？？？ 我们仔细读下，其实就会发现他其实是计算传入的int型x的位数，要求x为正数。 嗯，不难理解。 他为什么要这么写呢？ 思考对于我们来讲，当拿到一个int型正整数，如何用程序算出它的位数呢？ 一般不假思索的可能想到String的length方法计算。如下： 123static int myStringSize1(int x)&#123; return String.valueOf(x).length();&#125; 嗯，代码量很少。。。。。。 还有吗？ 对，还可以用除法，除以10，计算位数。 12345678static int myStringSize2(int x)&#123; int num=1; while(x&gt;10)&#123; x=x/10; num++; &#125; return num;&#125; 嗯。。。。不错不错～ 当然能用除法解决的基本上也可以用乘法解决，int的最大位数为10位，所以可以乘以10，判断大小。 12345678910// Requires positive xstatic int stringSize2(int x) &#123; int p = 10; for (int i=1; i&lt;11; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 10;&#125; 好吧，目前我就想到了除JDK以外的这三种方法。 我们测试一下我们的代码正确性。 12345678910public static void main(String[] args) &#123; int [] x=new int[]&#123;5,10,333,6666,77777,123456,1234567,87654321,999999999,1111111111,Integer.MAX_VALUE&#125;; for(int i=0;i&lt;x.length;i++)&#123; int a1=Test.stringSize1(x[i]); int a2=Test.stringSize2(x[i]); int a3=Test.myStringSize1(x[i]); int a4=Test.myStringSize2(x[i]); System.out.println(a1+"---"+a2+"---"+a3+"---"+a4); &#125;&#125; 运行结果： 结果还是木有问题哒。 提升JDK的这种写法有什么好处呢？？？ 可以看到，JDK的方法把一部分数据计算变成了数据比较，相当于优化吧。 那它的执行效率怎么样呢？ 我们写的方法的效率又如何呢？ 我们来测试下吧！ 实践我们可以制造一个随机的定长int数组，看看它们执行耗时，同时统计若干组数据，进行比较。 话不多说，直接写代码并记录到Excel里供分析。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public static void main(String[] args) throws Exception&#123; List&lt;List&lt;Long&gt;&gt; rowList= Lists.newArrayList(); List&lt;String&gt; titleList=Lists.newArrayList(); titleList.add("JDK方法"); titleList.add("乘法"); titleList.add("String方法"); titleList.add("除法"); for(int s=0;s&lt;50;s++)&#123; List&lt;Long&gt; cellList=Lists.newArrayList(); int [] xArrays=new int [10000]; for(int i=0;i&lt;xArrays.length;i++)&#123; xArrays[i]=1 + (int)(Math.random()*Integer.MAX_VALUE); &#125; //System.out.println("当前S值为"+s); long start1=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; stringSize1(xArrays[i]); &#125; long end1=System.nanoTime(); long time1=(end1-start1)/1000; System.out.println("JDK方法耗时---》"+time1+"ms"); cellList.add(time1); long start4=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; stringSize2(xArrays[i]); &#125; long end4=System.nanoTime(); long time4=(end4-start4)/1000; System.out.println("乘法耗时---》"+time4+"ms"); cellList.add(time4); long start2=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; myStringSize1(xArrays[i]); &#125; long end2=System.nanoTime(); long time2=(end2-start2)/1000; System.out.println("String方法耗时---》"+time2+"ms"); cellList.add(time2); long start3=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; myStringSize2(xArrays[i]); &#125; long end3=System.nanoTime(); long time3=(end3-start3)/1000; System.out.println("除法耗时---》"+time3+"ms"); cellList.add(time3); rowList.add(cellList); &#125; WriteExcelUtil.writeExecl(titleList,rowList,"/Users/zhangwentong/Desktop/workbook.xlsx"); &#125; 我记录了50组数据，每组里面每个计算位数的方法执行10000次，得到如下结果。 把它绘制成折线图。 可以看到，String的length方法效率是最差的。。。。 我们其实看一下String.valueOf(x).length()这个源码，就知道为什么这么慢了。 然后除法也稍微逊色一点。。。 乘法和JDK的效率都可以说不错。。。。。但JDK方法其实要好一点。。。。 而且开始创建的sizeTable是占据较少空间，但却降低了CPU的计算次数（乘法需要每次乘以十在比较计算）。 当然，因为int的位数只有10位，可以写一个sizeTable，当数据量大时，比如long，最大19位，写一个19个数的sizeTable？ 一堆9？？？ 哈哈，我们可以看看Long里面的计算位数的方法。 12345678910// Requires positive xstatic int stringSize(long x) &#123; long p = 10; for (int i=1; i&lt;19; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 19;&#125; 人家当然用的乘法啦。。。。。 结论我们可以看到，我们最容易想到的String.length方法确是效率最低的。。。。 JDK源码里一些方法会被大量调用，当然要做到最好的优化啦。。。 多读些源码，深入思考，多加练习，有助于提升自己。 今天就到这儿吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）线上某应用的FULLGC分析]]></title>
    <url>%2Fblog%2F%E8%BD%AC-%E7%BA%BF%E4%B8%8A%E6%9F%90%E5%BA%94%E7%94%A8%E7%9A%84FULLGC%E5%88%86%E6%9E%9020180831%2F</url>
    <content type="text"><![CDATA[原文链接：http://jm.taobao.org/2016/03/23/3782/ 这2天，排除线上某应用启动内存变化频繁的问题时，额外发现了一个fullgc的问题，分享给大家。 过程如下：抽了台线上机器，想看下这段时间机器的gc情况，发现里面有好几个FullGc的日志： T23:23:02.009+0800: 21860.015: [Full GC 21860.015: [CMS: 2361237K-&amp;gt;1111804K(4718592K), 4.9917540 secs] 2532961K-&amp;gt;1111804K(5190464K), [CMS Perm : 17397K-&amp;gt;17240K(131072K)], 4.9918770 secs] [Times: user=4.96 sys=0.03, real=4.99 secs] JVM参数设置如下： -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=60 参数的意思是：在旧区到60％的时候，会触发一次cmsgc，应该出现如下日志： T20:10:37.803+0800: 3246087.559: [CMS-concurrent-mark-start] T20:10:38.463+0800: 3246088.220: [CMS-concurrent-mark: 0.661/0.661 secs] [Times: user=3.17 sys=0.56, real=0.66 secs] T20:10:38.463+0800: 3246088.220: [CMS-concurrent-preclean-start] T20:10:38.552+0800: 3246088.309: [CMS-concurrent-preclean: 0.069/0.089 secs] [Times: user=0.14 sys=0.04, real=0.09 secs]_&lt;/span&gt; T20:10:38.552+0800: 3246088.309: [CMS-concurrent-abortable-preclean-start] 而现在日志里面都是old区到2.3G（50%）的时候，就会触发一次FullGc，而且gc日志里面没有一次正常的cmsgc，现在是什么原因在半路截胡了？ 开始怀疑JVM参数是否设置生效，通过jinfo进行查看： jinfo -flag UseCMSInitiatingOccupancyOnly 20195 jinfo -flag CMSInitiatingOccupancyFraction 20195 一切正常。 出现Fullgc，当时我想可能的原因有以下几个情况： cmsgc失败导致（GC日志中没有相关cmsgc失败的日志） JMAP -histo：现场（人为执行肯定不是） 大对象分配时，空间不够导致（当时还剩下50%内存，并且如果大对象分配，gc日志里面是会有如下WARN的） 内存碎片导致？（由于系统会经常分配一些大数组，这个会加剧碎片化） 第四点是最可能的原因了。于是，接下来怎么验证是否是它导致的呢？加上PrintGCReason，先打印出fullgc的原因, 命令如下： /java/bin/jinfo -flag +PrintGCReason 第二天，查看日志，如下： GC Cause: Heap Inspection Initiated GC T16:16:01.880+0800: 687439.886: [Full GC 687439.886: [CMS: 2362138K-&amp;gt;1180717K(4718592K), 5.6573690 secs] 2700275K-&amp;gt;1180717K(5190464K), [C MS Perm : 17531K-&amp;gt;17488K(131072K)], 5.6574950 secs] [Times: user=5.59 sys=0.06, real=5.65 secs] GC原因：堆检查启动GC，FullGc的原因是这个，看不明白，咨询过后，说这个很可能是因为JAMP -hist继：活导致的FullGc。 那如果是这样，就有可能是有脚本或者定时任务，也可能是什么其他东西，去执行了这个命令，反正据我了解的cs没有做这事。接下来就是找这个“凶手”了，这事情没做过，没啥头绪，看进程也看不出什么，想grep所有脚本，懒癌又发作了，还是先去群里咨询下有啥简单又省力的办法吧，一下搞定： [ ~]$ crontab -l */1 * * * * /home/bin/config-monitor.sh &amp;gt;&amp;gt; /home/logs/config-monitor.log 2&amp;gt;&amp;amp;1 [logs]$ cat /home/bin/config-monitor.sh |grep &quot;jmap&quot; jmaplog=&quot;/home/jmap.log&quot;; if (count == 3) { / run jmap print &quot;run jmap command : /java/bin/jmap -histo:live &quot;pid&quot; |head -n 20&quot;; system(&quot;/java/bin/jmap -histo:live &quot;pid&quot; |head -n 20&quot;)&amp;gt;jmaplog; print &quot;#######Server has recovered after running jmap######&quot;; 有个定时任务跑一个叫config-monitor.sh的脚本，里面做的事情，基本就是监视内存各个区的比例，超过一定比例，就通过jamp -histo：现场触发下fullgc，防止溢出===》这个定时任务是cs以前遗留下来的，一直没发现，后续就是评估是否去掉这个定时任务，整个过程告一段落。 总结： 问题可能出现的原因，要尽快动手去验证，不要只停留在思考的层面; 出现fullgc的时候，可以通过加上PrintGCReason，查看具体GC原因。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）为什么 String hashCode 方法选择数字31作为乘子]]></title>
    <url>%2Fblog%2F%E4%B8%BA%E4%BB%80%E4%B9%88-String-hashCode-%E6%96%B9%E6%B3%95%E9%80%89%E6%8B%A9%E6%95%B0%E5%AD%9731%E4%BD%9C%E4%B8%BA%E4%B9%98%E5%AD%9020180829%2F</url>
    <content type="text"><![CDATA[原文地址：https://segmentfault.com/a/1190000010799123 背景某天，我在写代码的时候，无意中点开了 String hashCode 方法。然后大致看了一下 hashCode 的实现，发现并不是很复杂。但是我从源码中发现了一个奇怪的数字，也就是本文的主角31。这个数字居然不是用常量声明的，所以没法从字面意思上推断这个数字的用途。后来带着疑问和好奇心，到网上去找资料查询一下。在看完资料后，默默的感叹了一句，原来是这样啊。那么到底是哪样呢？在接下来章节里，请大家带着好奇心和我揭开数字31的用途之谜。 选择数字31的原因在详细说明 String hashCode 方法选择数字31的作为乘子的原因之前，我们先来看看 String hashCode 方法是怎样实现的，如下： 123456789101112public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; 上面的代码就是 String hashCode 方法的实现，是不是很简单。实际上 hashCode 方法核心的计算逻辑只有三行，也就是代码中的 for 循环。我们可以由上面的 for 循环推导出一个计算公式，hashCode 方法注释中已经给出。如下： s[0]31^(n-1) + s131^(n-2) + … + s[n-1] 这里说明一下，上面的 s 数组即源码中的 val 数组，是 String 内部维护的一个 char 类型数组。这里我来简单推导一下这个公式： 假设 n=3 i=0 -&gt; h = 31 * 0 + val[0] i=1 -&gt; h = 31 * (31 * 0 + val[0]) + val[1] i=2 -&gt; h = 31 * (31 * (31 * 0 + val[0]) + val[1]) + val[2] h = 31*31*31*0 + 31*31*val[0] + 31*val[1] + val[2] h = 31^(n-1)*val[0] + 31^(n-2)*val[1] + val[2] 上面的公式包括公式的推导并不是本文的重点，大家了解了解即可。接下来来说说本文的重点，即选择31的理由。从网上的资料来看，一般有如下两个原因： 第一、31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。另外一些相近的质数，比如37、41、43等等，也都是不错的选择。那么为啥偏偏选中了31呢？请看第二个原因。 第二、31可以被 JVM 优化，31 * i = (i &lt;&lt; 5) - i。 上面两个原因中，第一个需要解释一下，第二个比较简单，就不说了。下面我来解释第一个理由。一般在设计哈希算法时，会选择一个特殊的质数。至于为啥选择质数，我想应该是可以降低哈希算法的冲突率。至于原因，这个就要问数学家了，我几乎可以忽略的数学水平解释不了这个原因。上面说到，31是一个不大不小的质数，是优选乘子。那为啥同是质数的2和101（或者更大的质数）就不是优选乘子呢，分析如下。 这里先分析质数2。首先，假设 n = 6，然后把质数2和 n 带入上面的计算公式。并仅计算公式中次数最高的那一项，结果是2^5 = 32，是不是很小。所以这里可以断定，当字符串长度不是很长时，用质数2做为乘子算出的哈希值，数值不会很大。也就是说，哈希值会分布在一个较小的数值区间内，分布性不佳，最终可能会导致冲突率上升。 上面说了，质数2做为乘子会导致哈希值分布在一个较小区间内，那么如果用一个较大的大质数101会产生什么样的结果呢？根据上面的分析，我想大家应该可以猜出结果了。就是不用再担心哈希值会分布在一个小的区间内了，因为101^5 = 10,510,100,501。但是要注意的是，这个计算结果太大了。如果用 int 类型表示哈希值，结果会溢出，最终导致数值信息丢失。尽管数值信息丢失并不一定会导致冲突率上升，但是我们暂且先认为质数101（或者更大的质数）也不是很好的选择。最后，我们再来看看质数31的计算结果： 31^5 = 28629151，结果值相对于32和10,510,100,501来说。是不是很nice，不大不小。 上面用了比较简陋的数学手段证明了数字31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。接下来我会用详细的实验来验证上面的结论，不过在验证前，我们先看看 Stack Overflow 上关于这个问题的讨论，Why does Java’s hashCode() in String use 31 as a multiplier?。其中排名第一的答案引用了《Effective Java》中的一段话，这里也引用一下： The value 31 was chosen because it is an odd prime. If it were even and the multiplication overflowed, information would be lost, as multiplication by 2 is equivalent to shifting. The advantage of using a prime is less clear, but it is traditional. A nice property of 31 is that the multiplication can be replaced by a shift and a subtraction for better performance: 31 * i == (i &lt;&lt; 5) - i. Modern VMs do this sort of optimization automatically. 简单翻译一下： 选择数字31是因为它是一个奇质数，如果选择一个偶数会在乘法运算中产生溢出，导致数值信息丢失，因为乘二相当于移位运算。选择质数的优势并不是特别的明显，但这是一个传统。同时，数字31有一个很好的特性，即乘法运算可以被移位和减法运算取代，来获取更好的性能：31 * i == (i &lt;&lt; 5) - i，现代的 Java 虚拟机可以自动的完成这个优化。 排名第二的答案设这样说的： As Goodrich and Tamassia point out, If you take over 50,000 English words (formed as the union of the word lists provided in two variants of Unix), using the constants 31, 33, 37, 39, and 41 will produce less than 7 collisions in each case. Knowing this, it should come as no surprise that many Java implementations choose one of these constants. 这段话也翻译一下： 正如 Goodrich 和 Tamassia 指出的那样，如果你对超过 50,000 个英文单词（由两个不同版本的 Unix 字典合并而成）进行 hash code 运算，并使用常数 31, 33, 37, 39 和 41 作为乘子，每个常数算出的哈希值冲突数都小于7个，所以在上面几个常数中，常数 31 被 Java 实现所选用也就不足为奇了。 上面的两个答案完美的解释了 Java 源码中选用数字 31 的原因。接下来，我将针对第二个答案就行验证，请大家继续往下看。 实验及数据可视化本节，我将使用不同的数字作为乘子，对超过23万个英文单词进行哈希运算，并计算哈希算法的冲突率。同时，我也将针对不同乘子算出的哈希值分布情况进行可视化处理，让大家可以直观的看到数据分布情况。本次实验所使用的数据是 Unix/Linux 平台中的英文字典文件，文件路径为 /usr/share/dict/words。 哈希值冲突率计算计算哈希算法冲突率并不难，比如可以一次性将所有单词的 hash code 算出，并放入 Set 中去除重复值。之后拿单词数减去 set.size() 即可得出冲突数，有了冲突数，冲突率就可以算出来了。当然，如果使用 JDK8 提供的流式计算 API，则可更方便算出，代码片段如下： 123456789101112131415161718192021222324252627public static Integer hashCode(String str, Integer multiplier) &#123; int hash = 0; for (int i = 0; i &lt; str.length(); i++) &#123; hash = multiplier * hash + str.charAt(i); &#125; return hash;&#125; /** * 计算 hash code 冲突率，顺便分析一下 hash code 最大值和最小值，并输出 * @param multiplier * @param hashs */public static void calculateConflictRate(Integer multiplier, List&lt;Integer&gt; hashs) &#123; Comparator&lt;Integer&gt; cp = (x, y) -&gt; x &gt; y ? 1 : (x &lt; y ? -1 : 0); int maxHash = hashs.stream().max(cp).get(); int minHash = hashs.stream().min(cp).get(); // 计算冲突数及冲突率 int uniqueHashNum = (int) hashs.stream().distinct().count(); int conflictNum = hashs.size() - uniqueHashNum; double conflictRate = (conflictNum * 1.0) / hashs.size(); System.out.println(String.format("multiplier=%4d, minHash=%11d, maxHash=%10d, conflictNum=%6d, conflictRate=%.4f%%", multiplier, minHash, maxHash, conflictNum, conflictRate * 100));&#125; 结果如下： 从上图可以看出，使用较小的质数做为乘子时，冲突率会很高。尤其是质数2，冲突率达到了 55.14%。同时我们注意观察质数2作为乘子时，哈希值的分布情况。可以看得出来，哈希值分布并不是很广，仅仅分布在了整个哈希空间的正半轴部分，即 0 ~ 2^31-1。而负半轴 -2^31 ~ -1，则无分布。这也证明了我们上面断言，即质数2作为乘子时，对于短字符串，生成的哈希值分布性不佳。然后再来看看我们之前所说的 31、37、41 这三个不大不小的质数，表现都不错，冲突数都低于7个。而质数 101 和 199 表现的也很不错，冲突率很低，这也说明哈希值溢出并不一定会导致冲突率上升。但是这两个家伙一言不合就溢出，我们认为他们不是哈希算法的优选乘子。最后我们再来看看 32 和 36 这两个偶数的表现，结果并不好，尤其是 32，冲突率超过了了50%。尽管 36 表现的要好一点，不过和 31，37相比，冲突率还是比较高的。当然并非所有的偶数作为乘子时，冲突率都会比较高，大家有兴趣可以自己验证。 哈希值分布可视化上一节分析了不同数字作为乘子时的冲突率情况，这一节来分析一下不同数字作为乘子时，哈希值的分布情况。在详细分析之前，我先说说哈希值可视化的过程。我原本是打算将所有的哈希值用一维散点图进行可视化，但是后来找了一圈，也没找到合适的画图工具。加之后来想了想，一维散点图可能不合适做哈希值可视化，因为这里有超过23万个哈希值。也就意味着会在图上显示超过23万个散点，如果不出意外的话，这23万个散点会聚集的很密，有可能会变成一个大黑块，就失去了可视化的意义了。所以这里选择了另一种可视化效果更好的图表，也就是 excel 中的平滑曲线的二维散点图（下面简称散点曲线图）。当然这里同样没有把23万散点都显示在图表上，太多了。所以在实际绘图过程中，我将哈希空间等分成了64个子区间，并统计每个区间内的哈希值数量。最后将分区编号做为X轴，哈希值数量为Y轴，就绘制出了我想要的二维散点曲线图了。这里举个例子说明一下吧，以第0分区为例。第0分区数值区间是[-2147483648, -2080374784)，我们统计落在该数值区间内哈希值的数量，得到 &lt;分区编号, 哈希值数量&gt; 数值对，这样就可以绘图了。分区代码如下： 1234567891011121314151617181920212223242526 /** * 将整个哈希空间等分成64份，统计每个空间内的哈希值数量 * @param hashs */public static Map&lt;Integer, Integer&gt; partition(List&lt;Integer&gt; hashs) &#123; // step = 2^32 / 64 = 2^26 final int step = 67108864; List&lt;Integer&gt; nums = new ArrayList&lt;&gt;(); Map&lt;Integer, Integer&gt; statistics = new LinkedHashMap&lt;&gt;(); int start = 0; for (long i = Integer.MIN_VALUE; i &lt;= Integer.MAX_VALUE; i += step) &#123; final long min = i; final long max = min + step; int num = (int) hashs.parallelStream() .filter(x -&gt; x &gt;= min &amp;&amp; x &lt; max).count(); statistics.put(start++, num); nums.add(num); &#125; // 为了防止计算出错，这里验证一下 int hashNum = nums.stream().reduce((x, y) -&gt; x + y).get(); assert hashNum == hashs.size(); return statistics;&#125; 本文中的哈希值是用整形表示的，整形的数值区间是 [-2147483648, 2147483647]，区间大小为 2^32。所以这里可以将区间等分成64个子区间，每个自子区间大小为 2^26。详细的分区对照表如下： 分区编号 分区下限 分区上限 分区编号 分区下限 分区上限 0 -2147483648 -2080374784 32 0 67108864 1 -2080374784 -2013265920 33 67108864 134217728 2 -2013265920 -1946157056 34 134217728 201326592 3 -1946157056 -1879048192 35 201326592 268435456 4 -1879048192 -1811939328 36 268435456 335544320 5 -1811939328 -1744830464 37 335544320 402653184 6 -1744830464 -1677721600 38 402653184 469762048 7 -1677721600 -1610612736 39 469762048 536870912 8 -1610612736 -1543503872 40 536870912 603979776 9 -1543503872 -1476395008 41 603979776 671088640 10 -1476395008 -1409286144 42 671088640 738197504 11 -1409286144 -1342177280 43 738197504 805306368 12 -1342177280 -1275068416 44 805306368 872415232 13 -1275068416 -1207959552 45 872415232 939524096 14 -1207959552 -1140850688 46 939524096 1006632960 15 -1140850688 -1073741824 47 1006632960 1073741824 16 -1073741824 -1006632960 48 1073741824 1140850688 17 -1006632960 -939524096 49 1140850688 1207959552 18 -939524096 -872415232 50 1207959552 1275068416 19 -872415232 -805306368 51 1275068416 1342177280 20 -805306368 -738197504 52 1342177280 1409286144 21 -738197504 -671088640 53 1409286144 1476395008 22 -671088640 -603979776 54 1476395008 1543503872 23 -603979776 -536870912 55 1543503872 1610612736 24 -536870912 -469762048 56 1610612736 1677721600 25 -469762048 -402653184 57 1677721600 1744830464 26 -402653184 -335544320 58 1744830464 1811939328 27 -335544320 -268435456 59 1811939328 1879048192 28 -268435456 -201326592 60 1879048192 1946157056 29 -201326592 -134217728 61 1946157056 2013265920 30 -134217728 -67108864 62 2013265920 2080374784 31 -67108864 0 63 2080374784 2147483648 接下来，让我们对照上面的分区表，对数字2、3、17、31、101的散点曲线图进行简单的分析。先从数字2开始，数字2对于的散点曲线图如下： 上面的图还是很一幕了然的，乘子2算出的哈希值几乎全部落在第32分区，也就是 [0, 67108864)数值区间内，落在其他区间内的哈希值数量几乎可以忽略不计。这也就不难解释为什么数字2作为乘子时，算出哈希值的冲突率如此之高的原因了。所以这样的哈希算法要它有何用啊，拖出去斩了吧。接下来看看数字3作为乘子时的表现： 3作为乘子时，算出的哈希值分布情况和2很像，只不过稍微好了那么一点点。从图中可以看出绝大部分的哈希值最终都落在了第32分区里，哈希值的分布性很差。这个也没啥用，拖出去枪毙5分钟吧。在看看数字17的情况怎么样： 数字17作为乘子时的表现，明显比上面两个数字好点了。虽然哈希值在第32分区和第34分区有一定的聚集，但是相比较上面2和3，情况明显好好了很多。除此之外，17作为乘子算出的哈希值在其他区也均有分布，且较为均匀，还算是一个不错的乘子吧。 接下来来看看我们本文的主角31了，31作为乘子算出的哈希值在第33分区有一定的小聚集。不过相比于数字17，主角31的表现又好了一些。首先是哈希值的聚集程度没有17那么严重，其次哈希值在其他区分布的情况也要好于17。总之，选31，准没错啊。 最后再来看看大质数101的表现，不难看出，质数101作为乘子时，算出的哈希值分布情况要好于主角31，有点喧宾夺主的意思。不过不可否认的是，质数101的作为乘子时，哈希值的分布性确实更加均匀。所以如果不在意质数101容易导致数据信息丢失问题，或许其是一个更好的选择。 写在最后经过上面的分析与实践，我想大家应该明白了 String hashCode 方法中选择使用数字31作为乘子的原因了。 本文为转载，原文地址：https://segmentfault.com/a/1190000010799123]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReentrantLock那些事]]></title>
    <url>%2Fblog%2F%E8%B0%88%E8%B0%88ReentrantLock20180826%2F</url>
    <content type="text"><![CDATA[前言在说ReentrantLock之前，我们先说说并发吧。 在JDK1.5之前，并发处理常用的关键字synchronized。使用synchronized关键字，锁的获取和释放是隐式的，synchronized主要通过系统的monitorenter指令实现的。 那时候synchronized可以称为重量级锁，执行效率不是很高。 而Doug Lea编写的util.concurrent 包被纳入JSR-166标准。这里面就包含了ReentrantLock。 ReentrantLock为编写并发提供了更多选择。 使用ReentrantLock的通常用法如下：123456789101112public class X &#123; private final ReentrantLock lock = new ReentrantLock(); public void m() &#123; lock.lock(); try &#123; //TODO &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 原理ReentrantLock主要是通过AbstractQueuedSynchronizer实现的，是一个重入锁，即一个线程加锁后仍然可以获得锁，不会出现自己阻塞自己的情况。 UML图我们看一下它们的UML图。 可以看到ReentrantLock实现了Lock接口。 锁类型ReentrantLock的两种锁类型，公平锁和非公平锁。 源码分析我们先来看下ReentrantLock的构造方法。 123456public ReentrantLock() &#123; sync = new NonfairSync();&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 可以看到默认无参构造方法为非公平锁实现。如果想定义公平锁实现，可以传入true来控制。 它的lock方法： 12345678public void lock() &#123; sync.acquire(1); &#125; public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; 公平锁和非公平锁各有自己的实现方式。我们来看下他们的tryAcquire方法。 非公平锁源码： 123456789101112131415161718192021222324static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125;final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125; 可以看到，非公平锁首先判断AQS（AbstractQueuedSynchronizer）中的state是否为0，0表示没有线程持有该锁，当前线程就尝试获取锁。 如果不是0，那在判断是不是当前线程持有该锁，如果是，就会增加state，改变state状态。（因此ReentranLock支持重入）。 公平锁源码： 1234567891011121314151617181920212223static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; @ReservedStackAccess protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 1234567public final boolean hasQueuedPredecessors() &#123; Node t = tail; Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 公平锁的tryAcquire方法，可以看到，相比非公平锁，多了hasQueuedPredecessors方法，这个方法是判断队列中是否有其他线程，如果没有，线程才会尝试获取锁，如果有，会先把锁分配给队列的线程，因此称为公平锁。 这儿可以看到，非公平锁的效率比公平锁要高。 这是tryAcquire方法，如果尝试获取锁失败了呢？ 那就会执行acquireQueued(addWaiter(Node.EXCLUSIVE), arg)方法啦。 我们先来看一下addWaiter方法。 12345678910111213141516private Node addWaiter(Node mode) &#123; Node node = new Node(mode); for (;;) &#123; Node oldTail = tail; if (oldTail != null) &#123; node.setPrevRelaxed(oldTail); if (compareAndSetTail(oldTail, node)) &#123; oldTail.next = node; return node; &#125; &#125; else &#123; initializeSyncQueue(); &#125; &#125;&#125; 可以看到，这个方法会把线程添加到队列尾，同时，for(;;)循环保证添加成功，直到return出去。 添加后，调用acquireQueued方法，这个方法为挂起等待线程。 看下该方法源码： 1234567891011121314151617181920212223final boolean acquireQueued(final Node node, int arg) &#123; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125;&#125;private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 可以看到，如果节点为头节点，就尝试获取一次锁，如果成功，就返回。 否则判断该线程是否需要挂起，如果需要的化就调用parkAndCheckInterrupt挂起。 调用LockSupport.park方法挂起线程，直到被唤醒。 selfInterrupt方法： 12345678910111213141516static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125;public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); b.interrupt(this); return; &#125; &#125; interrupt0();&#125; 调用interrupt方法，中断正在执行的线程（如果不是当前线程的话）。 释放锁unlock方法： 公平锁和非公平锁释放锁的方法是一样的。 123456789101112131415161718192021222324public void unlock() &#123; sync.release(1); &#125; public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; 可以看到首先会判断当前线程是否是获得锁的线程，如果是重入锁需要将state减完才算是完全释放锁。 释放后调用unparkSuccessor唤起挂起线程。 总结 非公平锁的效率是比公平锁要高的。 ReentranLock支持重入，因为增加了对自身线程的处理，通过state可以控制。 解锁操作应放到finally块里，避免使用锁时出现资源无法释放的问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[越努力越幸运]]></title>
    <url>%2Fblog%2F%E8%B6%8A%E5%8A%AA%E5%8A%9B%E8%B6%8A%E5%B9%B8%E8%BF%9020180825%2F</url>
    <content type="text"><![CDATA[前言感觉最近自己浮躁的很，不知为什么。 准备写些东西源自3个月前了，那时候想自己记录些知识，一方面便于加深记忆，一方面不让自己太“闲”。 或许心态本来就不太正，浮浮躁躁，有的时候写点东西感觉就是为了应付内心那个自己。 兴趣与工作从事Java开发已经3-4年了，也算是个半路出家的吧，总应该比别人多付出些努力。 人如何克服惰性？ 关键是培养自己的兴趣爱好吧。 3-4年工作期间，换了2、3家公司，每次面试的时候，总感觉自己欠点什么，说不出来的感觉。大概是基础比较薄弱吧。 回想起自己曾经从采矿专业转到软件开发，培训的时间，那时候是为了什么？那时候不是对软件的兴趣吗？ 初心呢？初衷呢？ 自己也渴望成为一个大牛，无论是架构师、全栈还是资深大咖。 阅读与写作曾说写作可以平复心态，阅读可以使人优秀。 是的呀，无论是阅读或者写作，都可以使人思考。 思考是什么、为什么、怎么办。 提升任何的辉煌与成功都是一步步慢慢实现的。 越努力越幸运。 创造性的思维也是必须的。 时刻牢记你想要的是什么。 结语兄台，就和我一起干了这碗鸡汤吧！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList、LinkedList和Vector分析]]></title>
    <url>%2Fblog%2FArrayList%E5%92%8CVector%E5%88%86%E6%9E%9020180814%2F</url>
    <content type="text"><![CDATA[前言List家族常用的类有3个，ArrayList、LinkedList、Vector。 ArrayList和Vector的底层是基于数组实现的，LinkedList的底层是基于链表实现的。 三者的比较如下： ArrayList和LinkedList不是线程安全的，Vector是线程安全的。 对于随机访问（get和set），ArrayList的性能要优于LinkedList。 对于add和remove操作，LinkedList和ArrayList性能差距不是很大。 三者均实现了Collection接口。 分析UML图三者与其他类的继承实现关系UML图如下。 ArrayList： LinkedList： Vector： 源码分析核心实现ArrayList是基于数组实现。 12transient Object[] elementData;private int size; LinkedList是基于链表实现。 123transient int size = 0;transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; Vector是基于数组实现。 123protected Object[] elementData;protected int elementCount;protected int capacityIncrement; 核心参数ArrayList 可以设置初始大小（由于数组实现的原因），不设置默认为10。 LinkedList不需要设置参数（由于使用链表实现，无界）。 Vector不仅可以设置初始大小，还可以设置容量增幅。 1234567891011121314151617181920212223//ArrayList传参构造函数 public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125; &#125; //LinkedList构造函数 public LinkedList() &#123; &#125; //Vector传参构造函数 public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement; &#125; 关于初始化大小和扩容机制下面讲。 核心方法我们主要分析get、set、add、remove这几个方法。对于ArrayList和Vector，还要分析扩容方法。 get、set方法ArrayList get，set方法： 12345678910public E get(int index) &#123; Objects.checkIndex(index, size); return elementData(index);&#125;public E set(int index, E element) &#123; Objects.checkIndex(index, size); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; Vector get，set方法： 1234567891011121314public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125;public synchronized E set(int index, E element) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 二者的逻辑一样，get方法判断是否下标越界，不越界返回index下的数值。set方法判断是否越界，不越界将新值放到指定下标上。它俩的区别在与synchronized关键字，正好说明了Vector是线程安全的。 LinkedList get，set方法： 1234567891011121314151617181920212223242526public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;public E set(int index, E element) &#123; checkElementIndex(index); Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 可以看到LinkedList的get方法会先检查是否越界，不越界返回指定下标node的item值。set方法也是先检查越界情况，不越界将该点的node的item赋为新值。取node指定位置上的值时要循环遍历，所以对于随机的get，set，ArrayList的性能要优于LinkedList的。 add、remove方法ArrayList add、remove方法： 1234567891011121314151617181920212223242526272829303132333435363738public boolean add(E e) &#123; modCount++; add(e, elementData, size); return true;&#125;private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) elementData = grow(); elementData[s] = e; size = s + 1;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); System.arraycopy(elementData, index, elementData, index + 1, s - index); elementData[index] = element; size = s + 1;&#125;public E remove(int index) &#123; Objects.checkIndex(index, size); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; Vector add、remove方法： 12345678910111213141516171819202122232425public synchronized boolean add(E e) &#123; modCount++; add(e, elementData, elementCount); return true;&#125;private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) elementData = grow(); elementData[s] = e; elementCount = s + 1;&#125;public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue;&#125; 它们里面有add、remove方法不止一个，我们只拿一个来举例。 可以看到，Vector和ArrayList十分相近了，除了synchronized关键字。 add方法当elementData.length和elementCount相等时（容量满），会执行扩容操作，并将元素放到指定位置。 remove方法先判断下标是否越界，不越界会删除指定位置的元素，并且将数组重新拷贝合并。 同时它们有一个计数器modCount，在HashMap那边已经讲过，是用来fast-fail的，当多个线程同时操作，modCount不一致，就会抛出异常。 LinkedList的add、remove方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125;E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 可以看到，LinkedList的add方法开始也会校验指针位置，然后如果在末尾，就在链表最后面添加节点，否则就插入到链表指定位置上。 remove方法校验指针位置后，会删除指定位置上的node。 上面可以看到，对于add和remove，ArrayList数组要进行扩容或者删除部分长度，执行Sysetm.arraycopy方法，这是要消耗一些性能的，对于LinkedList，不需要维护容量问题，但是每次新增或者删除时，都会创建或删除一个Node对象，也是要消耗一些性能的。 扩容方法对于ArrayList或者Vector，扩容方法如下： ArrayList 扩容方法：1234567891011121314151617181920212223private static final int DEFAULT_CAPACITY = 10; private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; Vector扩容方法： 12345678910111213141516171819202122232425public Vector() &#123; this(10); &#125; private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt;= 0) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 上面代码可以看到，对于ArrayList，如果不传入初始容量，默认为10。容量达到最值，执行扩容，每次扩容 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 默认原容量的1.5倍。 Vector，如果不传入初始容量和自增容量，默认初始容量也为10.扩容时执行int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ?capacityIncrement : oldCapacity); 默认为原容量的2倍。 两者的最大值容量均为Integer.MAX_VALUE. LinkedList由于是链表实现，没有容量限制。无需扩容。 代码我们从代码的角度比较下ArrayList和LinkedList，Vector。 我们构建一个有200W数据的ArrayList和LinkedList。 1234567891011121314151617181920212223242526public static void main(String[] args)&#123; List&lt;Integer&gt; list = new ArrayList(); //List&lt;Integer&gt; list = new LinkedList&lt;Integer&gt;(); //Vector&lt;Integer&gt; list=new Vector&lt;&gt;(); for (int i = 0; i &lt; 2000000; i++) &#123; list.add(i); &#125; Integer tmp; long start=System.currentTimeMillis() ; //ForEach for(Integer s:list)&#123; tmp=s; &#125; System.out.println("foreach spend:"+(System.currentTimeMillis()-start)); start = System.currentTimeMillis(); for(Iterator&lt;Integer&gt; it = list.iterator(); it.hasNext();)&#123; tmp=it.next(); &#125; System.out.println("Iterator spend;"+(System.currentTimeMillis()-start)); start=System.currentTimeMillis(); int size=list.size(); for(int i=0;i&lt;size;i++)&#123; tmp=list.get(i); &#125; System.out.println("for spend;"+(System.currentTimeMillis()-start));&#125; 某一次的结果： 数组 forEach遍历 Iterator遍历 for遍历 ArrayList 9ms 9ms 12ms LinkedList 17ms 16ms ??? Vector 44ms 55ms 41ms 由于for循环遍历是随机访问，故LinkedList在数据量很大的情况下时间消耗会很长，基本不能接受。由于Vector线程安全，synchronized，故其整体效率会比ArrayList低些。在实际开发中，应用的ArrayList还是比较多的。 结语以上就是对ArrayList、LinkedList、Vector的全部分析。对于不同的应用场景，合理的选择List的类型也是至关重要的。三种List都是比较基础的知识，应当学习和掌握。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StringBuffer和StringBuilder]]></title>
    <url>%2Fblog%2FStringBuffer%E5%92%8CStringBuilder20180813%2F</url>
    <content type="text"><![CDATA[前言我们知道，对于一般大量频繁的String操作，我们不建议也不应该直接用String进行相加操作，而我们应借助StringBuffer或者StringBuilder来实现。 StringBuffer是线程安全的，而StringBuilder是线程不安全的。 由此看来，StringBuilder对String的操作快，不安全，适合单线程；StringBuilder对String的操作较StringBuilder慢，安全，适合多线程和单线程。 我们今天分析一下二者的源码。 分析class定义两者的class定义。 1234567891011//StringBuffer public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125;//StringBuilderpublic final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125; 二者的UML图如下： 可以看到二者均继承AbstractStringBuilder类，且都实现了Serializable和CharSequence接口。即二者分别是AbstractStringBuilder类的安全和不安全的一种实现。 构造器我们先来分析下StringBuffer。 12345678910public StringBuffer() &#123; super(16);&#125;public StringBuffer(int capacity) &#123; super(capacity);&#125;public StringBuffer(String str) &#123; super(str.length() + 16); append(str);&#125; 可以看到，当我们new StringBuffer时，如果什么也不传，默认赋予16数组长度，如果传入一个String，则长度为String.length()+16。 append方法在看一下append方法。关键字synchronized 对该方法进行了加锁，保证安全，toStringCache 赋值为空。然后调用AbstractStringBuilder的append方法。 StringBuffer append方法。123456@Overridepublic synchronized StringBuffer append(String str) &#123; toStringCache = null; super.append(str); return this;&#125; AbstractStringBuilder里的方法。 123456789101112131415161718192021222324252627public AbstractStringBuilder append(String str) &#123; if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this;&#125;private AbstractStringBuilder appendNull() &#123; int c = count; ensureCapacityInternal(c + 4); final char[] value = this.value; value[c++] = 'n'; value[c++] = 'u'; value[c++] = 'l'; value[c++] = 'l'; count = c; return this;&#125;private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) &#123; value = Arrays.copyOf(value, newCapacity(minimumCapacity)); &#125;&#125; String里的getChars方法。 123456789101112public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) &#123; if (srcBegin &lt; 0) &#123; throw new StringIndexOutOfBoundsException(srcBegin); &#125; if (srcEnd &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(srcEnd); &#125; if (srcBegin &gt; srcEnd) &#123; throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); &#125; System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin);&#125; 对于这段代码的理解。追加一个Str时，如果是null，则调用appendNull方法，在后面直接加一个null字符串。如果不为空，拿到字符串长度，进行容量扩容为当前容量+str的长度，调用String的getChars方法，将字符串数组加在后面，这最后是个char数组。 通过上图可以看到 AbstractStringBuilder是基于char数组实现的，count用于统计当前长度。 toString方法我们看一下toString方法。可以发现他把字符串数组先放到了缓存数组，然后在返回一个String。当StringBuffer变化时，如append，则直接把toStringCache 赋值为空。 1234567@Overridepublic synchronized String toString() &#123; if (toStringCache == null) &#123; toStringCache = Arrays.copyOfRange(value, 0, count); &#125; return new String(toStringCache, true);&#125; insert方法我们再来分析一下StringBuffer的insert，由于重载方法较多，我们只分析insert String的代码。 StringBuffer insert方法。123456@Overridepublic synchronized StringBuffer insert(int offset, String str) &#123; toStringCache = null; super.insert(offset, str); return this;&#125; AbstractStringBuilder里的insert方法。 123456789101112public AbstractStringBuilder insert(int offset, String str) &#123; if ((offset &lt; 0) || (offset &gt; length())) throw new StringIndexOutOfBoundsException(offset); if (str == null) str = "null"; int len = str.length(); ensureCapacityInternal(count + len); System.arraycopy(value, offset, value, offset + len, count - offset); str.getChars(value, offset); count += len; return this;&#125; 可以看到与append大致相同，就是调用System.arraycopy的时候插入的位置发生了变化。 其他方法（delete，replace等）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//StringBuffer @Override public synchronized StringBuffer delete(int start, int end) &#123; toStringCache = null; super.delete(start, end); return this; &#125; //AbstractStringBuilder public AbstractStringBuilder delete(int start, int end) &#123; if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (end &gt; count) end = count; if (start &gt; end) throw new StringIndexOutOfBoundsException(); int len = end - start; if (len &gt; 0) &#123; System.arraycopy(value, start+len, value, start, count-end); count -= len; &#125; return this; &#125; //StringBuffer @Override public synchronized StringBuffer replace(int start, int end, String str) &#123; toStringCache = null; super.replace(start, end, str); return this; &#125; //AbstractStringBuilder public AbstractStringBuilder replace(int start, int end, String str) &#123; if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (start &gt; count) throw new StringIndexOutOfBoundsException("start &gt; length()"); if (start &gt; end) throw new StringIndexOutOfBoundsException("start &gt; end"); if (end &gt; count) end = count; int len = str.length(); int newCount = count + len - (end - start); ensureCapacityInternal(newCount); System.arraycopy(value, end, value, start + len, count - end); str.getChars(value, start); count = newCount; return this; &#125; 可以看到他们均使用了System.arraycopy方法。这儿不再一一赘述。 其他我们再看一下StringBuilder的源码。发现它与StringBuffer差别很小。 不同点： 增删改操作上没有synchronized关键字。 没有private transient char[] toStringCache;的定义。 第一点正好验证了StringBuilder不是线程安全的，第二点StringBuffer中toStringCache的引入是为了在多线程并发下读取写入数据起到一定的缓存缓冲作用。 结论我们可以看到，无论StringBuilder还是StringBuffer，都是AbstractStringBuilder的实现类。 AbstractStringBuilder对字符串的操作，实质是将它存储在一个char数组中，这样减小了内存开销。我们知道，如果使用String连加，会创造大量String对象，GC来不及回收，导致OOM异常或内存开销增大。StringBuffer和StringBuilder对String的操作完美的解决了这个问题，且相当于提供了操作字符串更加直观的方法（如insert，delete，append等）。这是值得我们借鉴和学习的。 以上就是StringBuffer和StringBuilder的源码分析。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer源码解析]]></title>
    <url>%2Fblog%2FInteger%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%9020180811%2F</url>
    <content type="text"><![CDATA[前言今天我们来分析一下Integer源码。 Integer是八种包装类里面的比较常用的一种。那在使用时有什么注意及学习的地方呢？ 让我们一起来看一下 分析Integer是包java.lang下的一个类。 123public final class Integer extends Number implements Comparable&lt;Integer&gt; &#123;//code&#125; 其被定义成final类型，继承Number类实现Comparable接口。 123@Native public static final int MIN_VALUE = 0x80000000;@Native public static final int MAX_VALUE = 0x7fffffff;public static final Class&lt;Integer&gt; TYPE = (Class&lt;Integer&gt;) Class.getPrimitiveClass("int"); 可以看出，其定义了Integer的最大值为2^31-1，最小值为-2^31。Integer的基本数据类型为int。 我们来看一下Integer的toString方法，是比较有趣的。 12345678public static String toString(int i) &#123; if (i == Integer.MIN_VALUE) return "-2147483648"; int size = (i &lt; 0) ? stringSize(-i) + 1 : stringSize(i); char[] buf = new char[size]; getChars(i, size, buf); return new String(buf, true);&#125; 方法中使用到了stringSize函数，就是求这个Integer数的长度，我们来看看他是如何实现的。 123456789final static int [] sizeTable = &#123; 9, 99, 999, 9999, 99999, 999999, 9999999, 99999999, 999999999, Integer.MAX_VALUE &#125;;// Requires positive xstatic int stringSize(int x) &#123; for (int i=0; ; i++) if (x &lt;= sizeTable[i]) return i+1;&#125; 可以看到这段代码在计算Integer数长度时，构建了一个一维数组，然后拿x与数组每个值进行比较。而未使用我们经常说的除法或乘法计算长度。我们可以看下源码里的注释。 1234567891011121314151617// I use the "invariant division by multiplication" trick to// accelerate Integer.toString. In particular we want to// avoid division by 10.//// The "trick" has roughly the same performance characteristics// as the "classic" Integer.toString code on a non-JIT VM.// The trick avoids .rem and .div calls but has a longer code// path and is thus dominated by dispatch overhead. In the// JIT case the dispatch overhead doesn't exist and the// "trick" is considerably faster than the classic code.//// TODO-FIXME: convert (x * 52429) into the equiv shift-add// sequence.//// RE: Division by Invariant Integers using Multiplication// T Gralund, P Montgomery// ACM PLDI 1994 我们知道计算机在计算除法效率要比加减乘法低。所以为了避免除法，提高计算效率，采用此种方法。 正好我们可以看看Long的toString方法里的stringSize方法。 12345678910// Requires positive xstatic int stringSize(long x) &#123; long p = 10; for (int i=1; i&lt;19; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 19;&#125; 可以看到使用了乘法。你或许会问为什么没有像Integer那样构建一个数组去比较？额，如果要构造数组，那要构造一个19位的数组，里面有1-19位的数，代码写起来很多很臃肿吧，而且构造好的数组会长期放在内存中，我们知道，在实际应用中，Integer的使用频率要比Long高多了，长期让Long里面的一个数组占据内存空间也不太合理。以上是我个人见解。 我们再来看一下parseInt方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public static int parseInt(String s, int radix) throws NumberFormatException&#123; /* * WARNING: This method may be invoked early during VM initialization * before IntegerCache is initialized. Care must be taken to not use * the valueOf method. */ if (s == null) &#123; throw new NumberFormatException("null"); &#125; if (radix &lt; Character.MIN_RADIX) &#123; throw new NumberFormatException("radix " + radix + " less than Character.MIN_RADIX"); &#125; if (radix &gt; Character.MAX_RADIX) &#123; throw new NumberFormatException("radix " + radix + " greater than Character.MAX_RADIX"); &#125; int result = 0; boolean negative = false; int i = 0, len = s.length(); int limit = -Integer.MAX_VALUE; int multmin; int digit; if (len &gt; 0) &#123; char firstChar = s.charAt(0); if (firstChar &lt; '0') &#123; // Possible leading "+" or "-" if (firstChar == '-') &#123; negative = true; limit = Integer.MIN_VALUE; &#125; else if (firstChar != '+') throw NumberFormatException.forInputString(s); if (len == 1) // Cannot have lone "+" or "-" throw NumberFormatException.forInputString(s); i++; &#125; multmin = limit / radix; while (i &lt; len) &#123; // Accumulating negatively avoids surprises near MAX_VALUE digit = Character.digit(s.charAt(i++),radix); if (digit &lt; 0) &#123; throw NumberFormatException.forInputString(s); &#125; if (result &lt; multmin) &#123; throw NumberFormatException.forInputString(s); &#125; result *= radix; if (result &lt; limit + digit) &#123; throw NumberFormatException.forInputString(s); &#125; result -= digit; &#125; &#125; else &#123; throw NumberFormatException.forInputString(s); &#125; return negative ? result : -result;&#125; 我们可以看到此方法首先进行异常处理，然后判断传入String是否有正负号，然后截取位数，使用乘法，用减法得到int值，然后判断正负并返回结果。 我们再来看下Integer的内部类IntegerCache。 这儿可以说是一个坑，也是比较有意思的地方。 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 很容易理解这段代码，初始化Integer后，IntegerCache会缓存[-128,127]之间的数据，这个区间的上限可以配置，取决于java.lang.Integer.IntegerCache.high这个属性，这个属性在VM参数里为-XX:AutoBoxCacheMax=2000进行设置调整或者VM里设置-Djava.lang.Integer.IntegerCache.high=2000。所以Integer在初始化完成后会缓存[-128,max]之间的数据。 并且我们可以看到valueOf方法。 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 可以看到valueOf方法，在cache范围内，返回的是缓存的值，是相同的对象，不在cache范围内，才会新建Integer。 由于有了Integer缓存，我们可以测试以下代码。 12345678Integer a=1;Integer b=1;Integer c=new Integer(1);Integer d=1000;Integer e=1000;System.out.println(a==b);System.out.println(b==c);System.out.println(d==e); 可以看到结果为true,false,false。 这样，我们在比较Integer时，如果仅仅比较值相等，建议使用equals方法比较。 我们可以看下Integer的equals方法。 123456public boolean equals(Object obj) &#123; if (obj instanceof Integer) &#123; return value == ((Integer)obj).intValue(); &#125; return false;&#125; 可以看到它会先判断类型是否符合，然后进行拆箱比较操作。 同样，在Long，Byte，Short，我们也可以看到缓存，其缓存数据长度均是-128到127。 123456789101112131415161718192021222324252627282930313233//Long private static class LongCache &#123; private LongCache()&#123;&#125; static final Long cache[] = new Long[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Long(i - 128); &#125; &#125; //Byte private static class ByteCache &#123; private ByteCache()&#123;&#125; static final Byte cache[] = new Byte[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Byte((byte)(i - 128)); &#125; &#125; //Short private static class ShortCache &#123; private ShortCache()&#123;&#125; static final Short cache[] = new Short[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Short((short)(i - 128)); &#125; &#125; 关于缓存的意义：在该范围内数据比较常用，添加缓存提高性能。不用每次都新建，浪费系统资源。 同时根据Integer的hashCode方法，我们可以看到，Integer的hashCode返回本身的int值。 1234@Overridepublic int hashCode() &#123; return Integer.hashCode(value);&#125; 结论以上就是Integer的源码分析，可以看到，对于偏底层的一些调用频繁的类，Java都做了很多方面的优化。包括从性能及内存开销等诸多方面。是值得我们学习和理解的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 数据结构之链表]]></title>
    <url>%2Fblog%2FJava-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A820180805%2F</url>
    <content type="text"><![CDATA[前言链表是一种物理存储单元上非连续、非顺序的存储结构，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储数据元素的数据域，另一个是存储下一个结点地址的指针域。 由于链表不必按照顺序存储，故在插入数据时可以达到O(1)的复杂度，但是查找的时候就需要遍历，时间复杂度为O(n)。 分类链表根据实现方式一般有三种分类：单向链表、循环链表、双向链表。 单向链表单向链表指的是链表中的元素的指向只能指向链表中的下一个元素或者为空，元素之间不能相互指向。也就是一种线性链表。 图示： 普通单向链表用Java代码实现一普通的单向链表。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class SingleLinkList &#123; private int size;//链表节点的个数 private Node head;//头节点 public SingleLinkList()&#123; size = 0; head = null; &#125; //链表的每个节点类 private class Node&#123; private Object data;//每个节点的数据 private Node next;//每个节点指向下一个节点的连接 public Node(Object data)&#123; this.data = data; &#125; &#125; //在链表头添加元素 public Object addHead(Object obj)&#123; Node newHead = new Node(obj); if(size == 0)&#123; head = newHead; &#125;else&#123; newHead.next = head; head = newHead; &#125; size++; return obj; &#125; //在链表头删除元素 public Object deleteHead()&#123; Object obj = head.data; head = head.next; size--; return obj; &#125; //查找指定元素，找到了返回节点Node，找不到返回null public Node find(Object obj)&#123; Node current = head; int tempSize = size; while(tempSize &gt; 0)&#123; if(obj.equals(current.data))&#123; return current; &#125;else&#123; current = current.next; &#125; tempSize--; &#125; return null; &#125; //删除指定的元素，删除成功返回true public boolean delete(Object value)&#123; if(size == 0)&#123; return false; &#125; Node current = head; Node previous = head; while(current.data != value)&#123; if(current.next == null)&#123; return false; &#125;else&#123; previous = current; current = current.next; &#125; &#125; //如果删除的节点是第一个节点 if(current == head)&#123; head = current.next; size--; &#125;else&#123;//删除的节点不是第一个节点 previous.next = current.next; size--; &#125; return true; &#125; //判断链表是否为空 public boolean isEmpty()&#123; return (size == 0); &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 栈具有先进后出的原则，所以单向链表可以用来实现栈。Java代码如下： 12345678910111213141516171819202122232425262728293031public class StackSingleLinkList &#123; public class StackSingleLink &#123; private SingleLinkList link; public StackSingleLink()&#123; link = new SingleLinkList(); &#125; //添加元素 public void push(Object obj)&#123; link.addHead(obj); &#125; //移除栈顶元素 public Object pop()&#123; Object obj = link.deleteHead(); return obj; &#125; //判断是否为空 public boolean isEmpty()&#123; return link.isEmpty(); &#125; //打印栈内元素信息 public String display()&#123; return link.display(); &#125; &#125;&#125; 我们可以看出，如果对链表的最后一个元素进行操作，需要遍历到链表尾部，在进行操作，十分消耗资源。 双端链表还有一种单向链表称为双端链表。这种链表有一个特点，即在链表内添加了对链表尾部的引用。这使得链表可以方便的操作尾部元素。 Java代码实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class DoublePointLinkList &#123; private Node head;//头节点 private Node tail;//尾节点 private int size;//节点的个数 private class Node&#123; private Object data; private Node next; public Node(Object data)&#123; this.data = data; &#125; &#125; public DoublePointLinkList()&#123; size = 0; head = null; tail = null; &#125; //链表头新增节点 public void addHead(Object data)&#123; Node node = new Node(data); if(size == 0)&#123;//如果链表为空，那么头节点和尾节点都是该新增节点 head = node; tail = node; size++; &#125;else&#123; node.next = head; head = node; size++; &#125; &#125; //链表尾新增节点 public void addTail(Object data)&#123; Node node = new Node(data); if(size == 0)&#123;//如果链表为空，那么头节点和尾节点都是该新增节点 head = node; tail = node; size++; &#125;else&#123; tail.next = node; tail = node; size++; &#125; &#125; //删除头部节点，成功返回true，失败返回false public boolean deleteHead()&#123; if(size == 0)&#123;//当前链表节点数为0 return false; &#125; if(head.next == null)&#123;//当前链表节点数为1 head = null; tail = null; &#125;else&#123; head = head.next; &#125; size--; return true; &#125; //判断是否为空 public boolean isEmpty()&#123; return (size ==0); &#125; //获得链表的节点个数 public int getSize()&#123; return size; &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 双端链表可以用来实现队列，相关实现如下： 123456789101112131415161718192021222324252627public class QueueLinkList &#123; private DoublePointLinkList dp; public QueueLinkList()&#123; dp = new DoublePointLinkList(); &#125; public void insert(Object data)&#123; dp.addTail(data); &#125; public void delete()&#123; dp.deleteHead(); &#125; public boolean isEmpty()&#123; return dp.isEmpty(); &#125; public int getSize()&#123; return dp.getSize(); &#125; public String display()&#123; return dp.display(); &#125;&#125; 有序链表上面所说的单链表数据都是无序的，我们可以构建一个有序的单向链表。即有序链表。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class OrderLinkList &#123; private Node head; private int size; private class Node&#123; private int data; private Node next; public Node(int data)&#123; this.data = data; &#125; &#125; public OrderLinkList()&#123; size=0; head = null; &#125; //插入节点，并按照从小打到的顺序排列 public void insert(int value)&#123; Node node = new Node(value); Node pre = null; Node current = head; while(current != null &amp;&amp; value &gt; current.data)&#123; pre = current; current = current.next; &#125; if(pre == null)&#123; head = node; head.next = current; &#125;else&#123; pre.next = node; node.next = current; &#125; size++; &#125; //删除头节点 public void deleteHead()&#123; head = head.next; size--; &#125; //判断是否为空 public boolean isEmpty()&#123; return (size ==0); &#125; //获取长度 public int getSize() &#123; return size; &#125; public String display()&#123; StringBuilder sb=new StringBuilder(); Node current = head; while(current != null)&#123; sb.append(current.data+" "); current = current.next; &#125; return sb.toString(); &#125;&#125; 对于有序链表，可以看出，插入或删除某一项最多需要O(n)的时间复杂度（遍历），但如果我们每次只删除最小值，且对插入没有过高要求的话，有序链表是一个不错的选择，比如优先级队列就可以利用有序链表实现。 比如我们插入int数并以最小值为优先级，每次取最小的int值的队列。 123456789101112131415161718192021222324252627public class QueueOrderLinkList &#123; private OrderLinkList dp; public QueueOrderLinkList()&#123; dp = new OrderLinkList(); &#125; public void insert(int data)&#123; dp.insert(data); &#125; public void delete()&#123; dp.deleteHead(); &#125; public int getSize() &#123; return dp.getSize(); &#125; public boolean isEmpty() &#123; return dp.isEmpty(); &#125; public String display()&#123; return dp.display(); &#125;&#125; 单向链表的用途可以说是十分广泛的。 双向链表双向链表即是这样一个有序的结点序列，每个链表元素既有指向下一个元素的指针，又有指向前一个元素的指针，其中每个结点都有两种指针，即left和right。left指针指向左边结点，right指针指向右边结点。所以双向链表是可以从两个方向进行遍历的。 图示： 双向链表的Java实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class DoubleWayLinkList &#123; private Node head;//表示链表头 private Node tail;//表示链表尾 private int size;//表示链表的节点个数 private class Node&#123; private Object data; private Node next; private Node prev; public Node(Object data)&#123; this.data = data; &#125; &#125; public DoubleWayLinkList()&#123; size = 0; head = null; tail = null; &#125; //在链表头增加节点 public void addHead(Object value)&#123; Node newNode = new Node(value); if(size == 0)&#123; head = newNode; tail = newNode; size++; &#125;else&#123; head.prev = newNode; newNode.next = head; head = newNode; size++; &#125; &#125; //在链表尾增加节点 public void addTail(Object value)&#123; Node newNode = new Node(value); if(size == 0)&#123; head = newNode; tail = newNode; size++; &#125;else&#123; newNode.prev = tail; tail.next = newNode; tail = newNode; size++; &#125; &#125; //删除链表头 public Node deleteHead()&#123; Node temp = head; if(size != 0)&#123; head = head.next; head.prev = null; size--; &#125; return temp; &#125; //删除链表尾 public Node deleteTail()&#123; Node temp = tail; if(size != 0)&#123; tail = tail.prev; tail.next = null; size--; &#125; return temp; &#125; //获得链表的节点个数 public int getSize()&#123; return size; &#125; //判断链表是否为空 public boolean isEmpty()&#123; return (size == 0); &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 使用双向链表可以构建双端队列。在这儿就不上代码了，和之前的队列构造类似。 循环链表循环链表指的是在单向链表和双向链表的基础上，将两种链表的最后一个结点指向第一个结点从而实现循环。 图示： 循环链表的Java实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class CircleLinkList&lt;T&gt; &#123; // 链表的每个节点类 private class Node&lt;T&gt; &#123; private Object data;// 每个节点的数据 private Node&lt;T&gt; next;// 每个节点指向下一个节点的连接 public Node(Object data) &#123; this.data = data; &#125; &#125; Node&lt;T&gt; head, tail; Node&lt;T&gt; p; int size = 0; public CircleLinkList() &#123; this.head = null; tail = head; p = head; &#125; public int length() &#123; return size; &#125; /** * 添加节点 * * @param data */ public void add(T data) &#123; Node node = new Node&lt;T&gt;(data); if (head == null) &#123; head = node; tail = head; p = head; size++; &#125; else &#123; node.next = head; head = node; tail.next = head; p = head; size++; &#125; &#125; /** * 得到数据 * * @param index * @return */ public T get(int index) &#123; int i = 0; p = head; while (i != index &amp;&amp; p != tail) &#123; i++; p = p.next; &#125; return (T) p.data; &#125; /** * @return */ public boolean isEmpty() &#123; if (head != null) return false; else return true; &#125;&#125; 同样，使用循环链表可以实现循环队列。 总结链表作为数据结构的一部分，应用是十分广泛的，我们上面说明了几种链表在不同情况下的应用，链表是我们应当学会掌握和使用的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一款基于JQuery和bootstrap的单页面WebApp框架]]></title>
    <url>%2Fblog%2F%E4%B8%80%E6%AC%BE%E5%9F%BA%E4%BA%8EJQuery%E5%92%8Cbootstrap%E7%9A%84%E5%8D%95%E9%A1%B5%E9%9D%A2WebApp%E6%A1%86%E6%9E%B620180730%2F</url>
    <content type="text"><![CDATA[前言现在React.js，Vue.js等MVVM框架逐渐火了起来，今天，先暂时不说这些。 我在之前呆过一家公司，他们前端封装了一套基于JQuery和bootstrap的单页面WebApp框架，虽然技术栈方面可能比较落后了，且是基于CSS，Html，JavaScript分层的。与现在的模块化开发有些不同。但是它的一些封装思想是值得我们学习的。 而且这个框架，让我这个前端小白（我是后端开发）对前端JS产生了很大兴趣，特此写下这篇文章来分析和纪念这个框架。 框架主页：https://github.com/JavaZWT/JQueryApp 简介package.json这款框架也是基于Node.js搭建的，主要是用了gulp，还包含了css,html,js压缩，js校验等包。其环境启动包依赖如下图： 在项目的package.json里可以看到。 关于为什么引用这些插件的说明： js，img，png等的压缩，可大幅度减小生成的资源包体积，在生成APK后用户当然是希望安装包要小越好。如果gulp到服务器WebRoot目录下，当用户访问项目资源时，体积小的静态资源更能减小用户流量开支，提高系统响应速度，增强用户体验。 创建一个前端webserver服务器，相当于提供了一个前端APK环境（可以这么理解），大部分bug在浏览器上就可以复现和解决，减少了打包到apk里的繁琐流程，提高开发效率。 根据以上介绍，大家应该知道node_modules,gulpfile.js,package.json,package-lock.json都不会被打到前台资源包中。 package.json里的依赖，都可以通过npm install （同级目录执行）的方式安装，安装后就会生成node_modules文件夹。 安装启动前端环境主要有以下三个步骤。 npm install -g gulp 安装全局gulp，使gulp可以在任何目录下使用，和java环境变量差不多。 npm install 安装环境依赖，需要到有package.json的文件夹下执行。 gulp 启动前端webserver gulpfile.js再来说下gulpfile.js 光有插件也不行啊，这个文件就是把插件们拿来用的。 关于这个js，我们简单说下配置及使用。 参数配置envConfig env：启动模式，有三种，DEV，DEBUG，PRO。DEV和DEBUG模式下不会压缩js和图片，节省时间，便于调试。 distPath: 生成的前端资源文件路径，有的gulpfile.js里面没有它，其他是分开的，那个可以生成发布在webapp目录下的资源文件。 webserverConfig 服务器启动配置参数 port ：默认打开的端口号 open：默认代开的文件名 host：默认打开的地址，不配置的话默认localhost livereload:即时刷新，主要用来检测文件变化。 项目架构图 架构说明： public 前端项目文件夹。 css 用于存放页面css样式的文件夹，理论上里面的文件都会被放入生成的前端资源包。 html 用于放置前端页面的文件夹，理论上里面的文件都会被放入生成的前端资源包。 js 用于放置前端js的文件夹，理论上里面的文件都会被放入生成的前端资源包。 node_modules npm install 后生成的文件夹，是前端资源包的打包环境及在Web浏览器上查看及debug App提供支持。 resources 用于放置前端静态资源的文件夹，如图片，字体，静态json等，理论上里面的文件都会被放入前端资源包。 gulpfile.js 构建前端环境，生成前端资源包的主要文件。 index.html 前端主要页面，所有资源理论上都会被按需加载进来。 package-lock.json npm install 之后生成的文件，存放着一些npm之后的包的信息。 package.json 构建前端环境，生成前端资源包的主要文件，里面放置着构建环境及生成资源包所需的依赖。 分析分析这个框架，我们先从index.html入手，单页面的操作实现主要是将目标html移到这个index.html里实现的。 可以看下index.html里面的几个主要id，如上图。 index.html引入了相关js及css。 我们看下api.js和app.js 项目启动后会调用到此方法，详见index.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function loadViewport()&#123; var path; if(location.hash.indexOf("?")&gt;-1)&#123; path = location.hash.substr(0,location.hash.indexOf("?")); &#125;else&#123; path = location.hash; &#125; path = (path || '#' + api.route.default.path).slice(1); //如果没有配置路径则不做操作 if(!api.route[path])&#123; api.globalLoading.hide(); console.log('[APP] 没有查询到路径:'+ path + ',不进行加载'); &#125;else if(api.curr.clean)&#123; switch(api.curr.clean.length)&#123; case 0://如果不包含参数 则认为这是一个同步 直接执行完成后 加载页面 api.curr.clean(); load(); break; case 1://如果只接收一个参数,则传入回调函数 等待页面完成后自行处理 api.curr.clean(load); break; case 2://如果接收两个参数 则分别是 要跳转页面的hash,回调函数 api.curr.clean(path,load); break; default://如果接收大于2个参数 则只传入三个 分别是 要跳转页面的hash,要跳转页面的配置在route中的数据,回调函数 api.curr.clean(path,api.route[path],load); &#125; &#125;else&#123; load(); &#125; function load()&#123; api.globalLoading.show(); console.log('[APP] 开始加载页面['+path+']'); api.loadPath(path,function(data)&#123; console.log('[APP] 加载页面&lt;p&gt;['+path+']&lt;/p&gt;完成'); api.plugins.header(data,path); //如果页面没有设置 则默认隐藏全局loading if(!data || !data.selfHideLoading) api.globalLoading.hide(); &#125;); &#125; &#125; api.loadViewport = loadViewport; /** * 当浏览器的hash发生变化时进行页面的加载 */ window.onhashchange = loadViewport; 其原理就是利用location.hash的？后半段，进行地址跳转，加载在route里配置的相关html及js，加载之前移除原来的html和js，以实现页面跳转。 方法及使用api.js1、route,modal 新增一个页面，可以添加在route里，新增一个模态框，可以添加在modal里。 2、api.globalLoading 全局loading，有两个方法: api.globalLoading.show(); api.globalLoading.hide(); 一般应用在ajax发送前后，当需要等待操作处理时，也可以直接调用。 3、api.globalShadow 全局shadow，是一个z-index为2000的透明遮罩层。 有两个方法： api.globalShadow.hide(); api.globalShadow.show(); 一般调用模态框时会被加载，自己写一些插件集成在里面时可以自己控制处理下。 4、api.modal.exec 调用模态框时会使用此方法，由于模态框和页面不太一样，模态框关闭后再打开可以保证数据不用重新加载。 5、api.loadJS 框架中加载js用到的方法，分为串行加载和并行加载，各有利弊。 6、api.loadModal modal加载方法，刚才提到的那个是这一个的进一步封装，这里面的方法显示了modal是如何被加载的，这里应注意，modal默认只会被加载一次。 7、api.loadPath 页面跳转实现的主要方法，这个方法制定跳转path后，会在route里寻找，若没有此页面，则不会跳转。 8、loadViewport 刚才提到了，这个可以控制页面hash路由跳转。也是页面跳转的主要方法。 app.js1、app.alert 消息提示框方法，只有一个确认键。 2、app.confirm 消息提示框，有确认和取消两个按钮。 3、app.ajax 基于JQuery ajax后自己封装的一套ajax，增加里请求头签名等等。 cordova.js自己封装的一个cordova工具，根据设备类型不同（Android，iOS）调用不同的cordova插件。 GlobalValue.js全局变量控制插件，通过sessionStorage存储变量。 总结还有很多部分没有介绍到，但框架的关键部分都介绍了，要想清楚明白，可以自己在框架上写个页面或者完善个插件来试试。加深对它的理解。 虽然在现在前端框架日新月异的今天，虚拟dom流行的今天，感觉这个框架已经过时了，但是思想永不过时，框架设计上的一些思想还是值得我们学习和借鉴的。 关于这个框架有什么问题欢迎留言，也可以直接在GitHub上提问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Stack类简介]]></title>
    <url>%2Fblog%2FJava-Stack%E7%B1%BB%E7%AE%80%E4%BB%8B20180726%2F</url>
    <content type="text"><![CDATA[前言栈（Stack）是限定只能在一段进行插入和删除操作的线性表。 进行插入和删除操作的一端称为“栈顶”（top），另一端称为“栈底”（bottom）。 栈的插入操作称为“入栈”(push)，栈的删除 操作称为“出栈”（pop）。 栈具有后进先出（LIFO），先进后出(FILO)的特性。 Stack类Java工具包下的Stack类继承于Vector，由此可见Stack底层是由数组实现的。 Stack和Collection的关系如下图： 我们来看下Stack的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package java.util;publicclass Stack&lt;E&gt; extends Vector&lt;E&gt; &#123; /** * 构造函数 */ public Stack() &#123; &#125; /** * 将一个元素压入栈顶 */ public E push(E item) &#123; addElement(item); return item; &#125; /** * 取出栈顶的一个元素，并删除 */ public synchronized E pop() &#123; E obj; int len = size(); obj = peek(); removeElementAt(len - 1); return obj; &#125; /** * 取出栈顶元素，不删除 */ public synchronized E peek() &#123; int len = size(); if (len == 0) throw new EmptyStackException(); return elementAt(len - 1); &#125; /** * 判断栈是不是空 */ public boolean empty() &#123; return size() == 0; &#125; /** * 查找元素在栈的位置，没有返回-1 */ public synchronized int search(Object o) &#123; int i = lastIndexOf(o); if (i &gt;= 0) &#123; return size() - i; &#125; return -1; &#125; private static final long serialVersionUID = 1224463164541339165L;&#125; 根据源码，可以发现Stack的方法调用了Vector类的方法，实现了线程安全。 我们主要看一下Vector里的下面三个方法： 12345678910111213141516171819202122232425262728293031323334//添加一个元素 public synchronized void addElement(E obj) &#123; modCount++; //确认容量，不够会扩容 ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = obj; &#125; //移除指定位置的元素 public synchronized void removeElementAt(int index) &#123; modCount++; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt;= " + elementCount); &#125; else if (index &lt; 0) &#123; throw new ArrayIndexOutOfBoundsException(index); &#125; int j = elementCount - index - 1; if (j &gt; 0) &#123; System.arraycopy(elementData, index + 1, elementData, index, j); &#125; elementCount--; //赋空，便于垃圾回收 elementData[elementCount] = null; &#125; //找出在指定位置的元素 public synchronized E elementAt(int index) &#123; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt;= " + elementCount); &#125; return elementData(index); &#125; 关联方法如下： 1234567891011121314151617181920212223242526272829// private void ensureCapacityHelper(int minCapacity) &#123; 如果长度超了就扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; //扩容方法 private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; //新的容量定义：如果有容量增量且大于0，取增量，否则执行2倍扩容 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); //扩容后容量比传入容量还小，就取传入容量。 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //新容量比数组最大长度还大 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //计算容量 newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); &#125; //计算容量 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 实践我们如何用数组实现自己的一个stack呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Stack &#123; //栈元素组 private Object[] members; //指针 private int size; //自定义初始栈长度 public Stack(int initCapacity) throws Exception&#123; if(initCapacity&lt;=0) &#123; throw new Exception(); &#125; this.members=new Object[initCapacity]; &#125; //默认栈长度为10 public Stack() &#123; this.members=new Object[10]; &#125; //元素入栈 public synchronized void push(Object o)&#123; ensureCapacity(size+1); members[size++]=o; &#125; //元素出栈 public synchronized Object pop() throws Exception&#123; if(size&lt;=0) &#123; throw new Exception(); &#125; return members[--size]; &#125; //查看栈顶元素 public synchronized Object peek() throws Exception&#123; if(size&lt;=0) &#123; throw new Exception(); &#125; return members[size-1]; &#125; //确认容量 private synchronized void ensureCapacity(int minCapacity) &#123; //size+1比数组长度要长，扩容 if(minCapacity-members.length&gt;0) &#123; int oldCapacity = members.length; Object oldMembers=members; //扩容到二倍 int newCapacity = 2 * oldCapacity ; //扩容后还不够或者超过int最大值，就直接赋值size+1 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; members=new Object[newCapacity]; //拷贝数组 System.arraycopy(oldMembers, 0, members, 0, size); oldMembers=null; &#125; &#125;&#125; 以上代码就是一个简易的Stack的实现方式。 代码见： https://github.com/JavaZWT/sakuratears 总结Stack类在编程过程中用到的不是很多，但是计算机栈内存机制遵循先进后出原则，学习Stack类，可以帮助我们加深对程序及数据结构的理解。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java正则表达式使用]]></title>
    <url>%2Fblog%2FJava%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BD%BF%E7%94%A820180721%2F</url>
    <content type="text"><![CDATA[前言1.什么是正则表达式？ 正则表达式是一种可以用于模式匹配和替换的强有力的工具。 2.为什么要使用正则表达式？有什么优点缺点？ 正则表达式在参数校验、参数匹配方面可以发挥重大作用。 优点：更少的代码，更高的效率。 缺点：需要一定时间的学习及记忆，不然理解晦涩。 正则表达式符号及其含义下表是正则表达式的所有符号及含义。 常用的符号.表示任意字符\s空格字符(空格键, tab, 换行, 换页, 回车)\S非空格字符([^\s])\d 一个数字，(相当于[0-9] )\D一个非数字的字符，(相当于[^0-9] )\w一个单词字符(word character) (相当于 [a-zA-Z_0-9] )\W一个非单词的字符，[^\w]^一行的开始$一行的结尾\b一个单词的边界 \B一个非单词的边界\G前一个匹配的结束 []匹配方括号内的一个字符例如:[abc] 表示字符 a，b，c 中的任意一个(与 a|b|c 相同) [a-zA-Z]表示从a到z或A到Z当中的任意一个字符表示次数的符号*重复零次或更多次例如:a* 匹配零个或者多个 a+重复一次或更多次例如:a+ 匹配一个或者多个 a?重复零次或一次例如:a? 匹配零个或一个 a{n}重复n次例如:a{4}匹配4个a {n,}重复 n 次或更多次例如:a{4,} 匹配至少 4 个 a{n,m}重复n到m次例如:a{4,10} 匹配 4~10 个 a 正则表达式实例java 正则表达式通过 java.util.regex 包下的 Pattern 类与 Matcher 类实现: 正则表达式使用总共分三步: 构造一个模式. Pattern p=Pattern.compile(“[a-z]*”); 建造一个匹配器 Matcher m = p.matcher(str); 进行判断，得到结果 boolean b = m.matches(); Matcher 类提供三个匹配操作方法,三个方法均返回 boolean 类型,当匹配到 时返回 true,没匹配到则返回 false m.matches() matches()对整个字符串进行匹配,只有整个字符串都匹配了才返回 true m.lookingAt() lookingAt()对前面的字符串进行匹配,只有匹配到的字符串在最前面才返 回 true m.find() find()对字符串进行匹配,匹配到的字符串可以在任何位置 Matcher 类的其他方法: int groupcount() 返回此匹配器模式中的捕获组数。 String replaceAll(String replacement) 用给定的 replacement 全部替代匹配的 部分 String repalceFirst(String replacement) 用给定的 replacement 替代第一次匹 配的部分 appendReplacement(StringBuffer sb,String replacement) 根 据 模 式 用 replacement 替换相应内容,并将匹配的结果添加到 sb 当前位置之后 StringBuffer appendTail(StringBuffer sb) 将输入序列中匹配之后的末尾字串 添加到 sb 当前位置之后. group(n) 0 代表永远都是匹配整个表达式的字符串的那部分 n&lt;&gt;0 时代表 第 n 组匹配的部分 正则表达式的用法 字符匹配 1234Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(str); // 操作的字符串boolean b = m.matches(); //返回是否匹配的结果 System.out.println(b); 分割字符串 12Pattern pattern = Pattern.compile(expression); //正则表达式String[] strs = pattern.split(str); //操作字符串 得到返回的字符串数组 替换字符串 123Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串String s = m.replaceAll(str); //替换后的字符串 查找替换指定字符串 12345678910Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串 StringBuffer sb = new StringBuffer();int i = 0;while (m.find()) &#123; m.appendReplacement(sb, str); i++; //字符串出现次数&#125;m.appendTail(sb);//从截取点将后面的字符串接上 String s = sb.toString(); 查找输出字符串 1234567Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串while (m.find()) &#123; //m.start() 返回匹配到的子字符串在字符串中的索引位置. //m.end()返回匹配到的子字符串的最后一个字符在字符串中的索引位置. //m.group()返回匹配到的子字符串&#125; 常用的正则表达式 手机号码校验 12345 public static boolean isPhone(String str) &#123; Pattern p = Pattern.compile("^[1][3,5,7,8]+\\d&#123;9&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 邮箱地址校验 12345 public static boolean isEmail(String str) &#123; Pattern p = Pattern.compile("^[a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\\.[a-zA-Z0-9-]+)*\\.[a-zA-Z0-9]&#123;2,6&#125;$"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; IP地址校验 12345 public static boolean isIPv4(String str) &#123; Pattern p = Pattern.compile("\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 是不是汉字 12345 public static boolean isChinese(String str) &#123; Pattern p = Pattern.compile("^[\\u4e00-\\u9fa5]+$"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; QQ号校验 12345 public static boolean isQQ(String str)&#123; Pattern p = Pattern.compile("[1-9][0-9]&#123;4,13&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 身份证号校验 1234567891011public static boolean isIDNumber(String IDNumber) &#123; if (IDNumber == null || "".equals(IDNumber)) &#123; return false; &#125; // 定义判别用户身份证号的正则表达式（15位或者18位，最后一位可以为字母） String regularExpression = "(^[1-9]\\d&#123;5&#125;(18|19|20)\\d&#123;2&#125;((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d&#123;3&#125;[0-9Xx]$)|" + "(^[1-9]\\d&#123;5&#125;\\d&#123;2&#125;((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d&#123;3&#125;$)"; boolean matches = IDNumber.matches(regularExpression); //详细校验略 return matches;&#125; 邮编校验 12345 public static boolean isMail(String str) &#123; Pattern p = Pattern.compile("[1-9]\\d&#123;5&#125;(?!\\d)"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 其他 其他常用校验不在一一说明。 可以通过 https://github.com/JavaZWT/sakuratears 获取。 结语如果我们能够熟练掌握正则表达式，一定会对我们的开发提供很大的帮助。 正则表达式整体上相对于代码来说是比较晦涩的，用好它需要一定的功力及学习能力。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（2）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%EF%BC%882%EF%BC%8920180717%2F</url>
    <content type="text"><![CDATA[前言关于之前的部分请参考我的《搭建自己的Hexo博客（1）》这篇文章。 创建Github账户我们把博客发布到Github上。 首先申请Github账号：https://github.com/ 新建一个Repository 红框内为项目名称，应写成 yourname.github.io 这种形式。 如我注册的用户名是JavaZWT，则项目名应写为 JavaZWT.github.io 新建好后，我们开始发布我们的博客。 博客发布进入到原来的blog文件夹。 我们要记住以下指令： 博客本地调试启动：hexo s 博客部署文件生成：hexo g 博客发布：hexo d 博客部署文件夹清空： hexo clean hexo g 后，要发布的博客文件会生成在public文件夹下。 hexo d 会把这个文件夹下的文件deploy到Github上。 _config.yml配置文件里，指明自己的Github地址上新建的yourname.github.io的项目。 12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:JavaZWT/JavaZWT.github.io.git 正常的发布流程： 新写了一篇博文 通过hexo s本地调试通过后 hexo clean 清空public文件夹及db文件 hexo g 重新生成部署文件 hexo d 发布到Github 当我们发布成功后，通过 https://yourname.github.io/ 便可以在外网访问自己的blog啦。 添加域名成功访问之后，我们觉得这个网址不好，想换个自己喜欢的域名怎么办？ 这里我使用了阿里云申请了域名。 首先登陆阿里云，没有账号请注册申请。 找到域名与网站（万网）菜单。 找到自己喜欢的域名进行购买。 得到域名后，我们需要配置域名解析。把域名指向我们的博客。 在域名解析列表里我们新增两条记录，如下图红框部分。 其中记录值可以写 https://yourname.github.io/ 相当于把这个域名指向了https://yourname.github.io/这个域名。 或者我们可以查到 https://yourname.github.io/ 这个域名的IP，直接将新的域名指向这个IP即可。 配置好后等一段时间，我们访问新的域名，就可以跳转到我们的博客啦。 其他优化添加音乐播放功能找到 /themes/next/layout/_custom/sidebar.swig 文件 登陆网易云音乐，对于一首歌，生成外链播放器。 将上述代码复制到sidebar.swig文件里。 生成如下图所示效果。 添加博客编写功能写博客时是不是很不方便？ 我们安装hexo-admin插件吧。这个插件可以是写博客更快捷方便。 npm install hexo-admin –save 安装好后，启动调试博客 hexo s 输入 http://localhost:4000/admin/ 是不是看到进入了博客管理菜单？ 通过这个插件可以方便的管理和书写博客。 压缩博客博客里有一些图片或者css，js过大怎么办，这样会导致博客加载缓慢。 我们在发布博客时可以先对其进行压缩。 在blog目录下新建gulpfile.js 有如下内容： 123456789101112131415161718192021222324252627282930313233343536373839404142var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');var imagemin = require('gulp-imagemin');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public/images 目录 imagesgulp.task('images', function() &#123; return gulp.src(['./public/**/*.png', './public/**/*.jpg', './public/**/*.gif', './public/**/*.svg']) .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html', 'minify-css', 'minify-js', 'images']); 同时在package.json里添加这几个包。 &quot;gulp-minify-css&quot;: &quot;^1.2.4&quot;, &quot;gulp-uglify&quot;: &quot;^3.0.0&quot;, &quot;gulp-htmlmin&quot;: &quot;^4.0.0&quot;, &quot;gulp-htmlclean&quot;: &quot;^2.7.22&quot;, &quot;gulp&quot;: &quot;^3.9.1&quot;, &quot;gulp-imagemin&quot;: &quot;^4.1.0&quot;, 添加完后执行 npm install。 或者直接一个个的 npm install 包名 –save 也是可以的。 弄好后，在hexo g 指令后进行 压缩操作，执行 gulp 命令，可对public文件夹下的资源进行压缩。 结语一个博客的基本搭建到这里基本就结束了，如果想提高性能及速度，可以自己申请个服务器，或者使用CDN进行加速，基本都涉及到一定的费用。 博客还有一些比较小的美化的地方就不一一说明，大家可以自己尝试，自己动手，把自己的博客搭建的更漂亮。 毕竟，搭建博客这件事我们应本着学习，兴趣，快乐的本质去出发，自己摸索自己研究一些比较好玩的东西，这样才是做这件事的意义。 附上我的博客地址： https://www.sakuratears.top/ 大家可以参考下，有什么问题可以随时联系我，一定会为大家解答。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（1）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A220180715%2F</url>
    <content type="text"><![CDATA[前言我的Hexo博客很早就建好了，但一直没时间把搭建博客的步骤和经验好好总结。今天，我把在搭建博客中注意到的点及经验总结下来。一方面与大家分享，另一方面也加深自己的记忆，总结经验。 环境安装Node.js及npm要搭建Hexo博客，首先需要安装Node.js环境，去Node.js官网下载Node并安装。 NodeJs官网：https://nodejs.org/ 在命令行界面输入node -v 查询版本号。 输入npm -v 可查询npm版本号。 安装Hexo使用npm指令安装hexo npm install hexo-cli -g 安装成功后输入hexo -v可查询hexo版本信息 博客初始化安装好hexo后，我们找到一个自己文件夹，通过指令进入该文件夹。 使用指令： ①hexo init blog ②cd blog ③npm install ④hexo s 会看到命令行输出如下信息 打开浏览器，输入地址便可以看到blog搭建成功了。 美化博客虽然blog搭建好了，但是总感觉不是很好看。 推荐一款hexo主题Next，其有很多种blog主题供我们选择。 安装Next主题首先cd到刚刚建好的blog目录下，执行命令： git clone https://github.com/iissnan/hexo-theme-next themes/next 如果没有安装git，那么安装下吧。 请去git官网下载git： https://git-scm.com/downloads 如果命令执行成功，在blog下的themes包下，我们可以看到next文件夹，那就是刚刚下好的next主题啦。themes包下那个landscape包，就是hexo自带的默认主题。 如果不成功，那么我们可以直接下载next主题，https://github.com/iissnan/hexo-theme-next/releases，下载source code，然后把文件解压，重命名为next放到themes包下就可以了啦。 完成后，在blog包下，有个_config.yml文件，打开它，里面配置为theme: next即可加载next主题。 Hexo的_config.yml文件上文提到的这个配置文件是Hexo的核心配置文件，我们来说下这个文件吧 请先对yml语法有一定了解，本文不做说明。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: SakuraTears的博客 //博客的标题subtitle: 越努力越幸运 //博客简介description: 越努力越幸运 //博客描述keywords: 'Java,努力,随笔,学习,SakuraTears,博客,生活' //博客关键字author: SakuraTears //博客作者language: zh-Hans //博客语言timezone: //时区avatar: /uploads/avatar.jpg //头像#google_analytics: your-analytics-id //谷歌统计，开启的话请输入自己的id#baidu_analytics: your-analytics-id //百度统计，开启的话请输入自己的id# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://www.sakuratears.top //自己的网站地址root: / //根目录permalink: blog/:title:year:month:day/ permalink_defaults:# Directorysource_dir: source //源码位置，请默认public_dir: public //生成的文件位置，请默认tag_dir: tags //标签文件位置，请默认archive_dir: archives //归档文件位置，请默认category_dir: categories //分类文件位置，请默认code_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new posts //新文章默认名字default_layout: post //默认样式titlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: //高亮 enable: true //启用 line_number: true //linenumber启用 auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 3 //每页三篇文章 order_by: -date //按日期排序 # Category &amp; Tagdefault_category: uncategorized //按默认策略分类category_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD //日期时间格式time_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10 //归档每页10篇文章pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/plugins: hexo-generate-feed## Themes: https://hexo.io/themes/theme: next //hexo主题# hexo-admin authentificationadmin: username: sakuratears password_hash: $2a$10$HhGu7w5fYH6e7wmb2tXta.FvJvGWlzosdUgImJxGhxU0K3YAkXIkm secret: what's your name?# Live2D //live2D插件，后面介绍## https://github.com/xiazeyu/live2d-widget.js## https://l2dwidget.js.org/docs/class/src/index.js~L2Dwidget.html#instance-method-initlive2d: model: scale: 1 hHeadPos: 0.5 vHeadPos: 0.618 display: superSample: 2 width: 150 height: 300 position: right hOffset: 0 vOffset: -20 mobile: show: false scale: 0.1 motion: true react: opacityDefault: 0.7 opacityOnHover: 0.2##local_search //搜索插件search: path: search.xml field: post format: html limit: 10000# Deployment //部署## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:JavaZWT/JavaZWT.github.io.git Next主题的_config.yml文件在next包下，也有一个_config.yml文件，这个文件是配置next主题的，也简单介绍下。 12345678910menu: //next主题菜单 home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap commonweal: /404/ || heartbeat support: /support/ || support 1scheme: Pisces //主题类型 我选的Pisces主题。 12345social: //社交信息 GitHub: https://github.com/JavaZWT || github Email: mailto:971258230@qq.com || envelope QQ: http://wpa.qq.com/msgrd?v=3&amp;uin=971258230&amp;site=qq&amp;menu=yes || qq CSDN: https://blog.csdn.net/JavaZWT || copyright 上面都是一些基本配置，下面我们对blog进行更多的美化。 添加动态旋转头像打开/themes/next/source/css/_common/components/sidebar/sidebar-author.styl文件 将代码替换为： 1234567891011121314151617181920212223242526272829.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; border-radius: 60%; transition: 2.5s all; &#125;.site-author-image:hover &#123; transform: rotate(360deg);&#125;.site-author-name &#123; margin: $site-author-name-margin; text-align: $site-author-name-align; color: $site-author-name-color; font-weight: $site-author-name-weight;&#125;.site-description &#123; margin-top: $site-description-margin-top; text-align: $site-description-align; font-size: $site-description-font-size; color: $site-description-color;&#125; 后重新发布，可以看到头像可以旋转啦。 添加本地搜索插件安装hexo-generator-searchdb： npm install hexo-generator-searchdb –save 同时在hexo的_config.yml里配置如下参数： 123456##local_searchsearch: path: search.xml field: post format: html limit: 10000 Next的_config.yml配置如下： 1234567local_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 重新发布，可以看到新增了搜索功能。 更换背景图片找到 /themes/next/source/css/_custom/custom.styl文件 加入如下代码： 123456789body &#123; background:url("/images/background.jpg"); background-attachment: fixed; // 不随屏幕滚动而滚动 background-repeat: no-repeat; // 如果背景图不够屏幕大小则重复铺，改为no-repeat则表示不重复铺 background-size: 100%; // 等比例铺满屏幕&#125;.main-inner &#123; opacity: 0.8;&#125; 重新发布，可以看到背景图片变化了。 添加live2D功能安装live2D插件hexo-helper-live2d npm install hexo-helper-live2d –save Hexo的 _config.yml文件里配置如下参数 12345678910111213141516171819live2d: model: scale: 1 hHeadPos: 0.5 vHeadPos: 0.618 display: superSample: 2 width: 150 height: 300 position: right hOffset: 0 vOffset: -20 mobile: show: false scale: 0.1 motion: true react: opacityDefault: 0.7 opacityOnHover: 0.2 重新发布，便看到萌萌的二次元人物出现啦。 添加评论插件这里用的Valine评论插件，Valine插件需要leancloud支持，请先注册leancloud，注册成功拿到id和key。 12345678910valine: enable: true appid: //你的id appkey: //你的key notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: (*^_^*)看了这么多，可能你有话想说 # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 配置成功后，重新发布便能看到文章可以评论啦。 开启访客统计当我们注册好leancloud后。 1234leancloud_visitors: enable: true app_id: //你的id app_key: //你的key 找到代码，填入自己的id和key。 重新发布，在页面上可以看到博客的访客数量。 开启字数统计安装字数统计插件：hexo-wordcount npm install hexo-wordcount –save Next配置文件如下参数开启： 123456post_wordcount: item_text: true wordcount: true min2read: true totalcount: true separated_meta: true 重新发布便拥有了字数统计功能。 续文章还没写完，后面会继续更新。 欢迎大家关注我的博客 https://www.sakuratears.top/]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java通过反射获取class信息]]></title>
    <url>%2Fblog%2FJava%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96class%E4%BF%A1%E6%81%AF20180714%2F</url>
    <content type="text"><![CDATA[前言在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为反射机制。 我们知道，Java语言不是动态语言，但是他却有非常突出的动态相关机制，反射机制。 代码我们可以通过反射机制获取一个class的相关信息。 1. 利用Javassist获取class信息 Javassist是一个动态类库，可以用来检查、”动态”修改以及创建 Java类。其功能与jdk自带的反射功能类似，但比反射功能更强大。 123456789101112131415161718192021222324252627282930313233343536373839404142public static void getClassInfoByJavassist(Class clazz) &#123; ClassPool classPool = ClassPool.getDefault(); classPool.insertClassPath(new ClassClassPath(clazz)); try &#123; System.out.println("class--&gt;"+clazz.getName()); CtClass ctClass = classPool.get(clazz.getName()); //获取常量信息 CtField[] fields=ctClass.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].getConstantValue()+"--&gt;type="+fields[i].getType().getName()); &#125; //获取方法信息 CtMethod[] ctMethods = ctClass.getMethods(); for (CtMethod ctMethod : ctMethods) &#123; //排除equals，hash，toString等方法 if (!clazz.getName().equals(ctMethod.getDeclaringClass().getName())) &#123; continue; &#125; MethodInfo methodInfo = ctMethod.getMethodInfo(); CodeAttribute codeAttribute = methodInfo.getCodeAttribute(); LocalVariableAttribute attr = (LocalVariableAttribute) codeAttribute.getAttribute(LocalVariableAttribute.tag); if (attr == null) &#123; // exception &#125; String[] paramNames = new String[ctMethod.getParameterTypes().length]; TreeMap&lt;Integer, String&gt; sortMap = new TreeMap&lt;Integer, String&gt;(); for (int i = 0; i &lt; attr.tableLength(); i++) sortMap.put(attr.index(i), attr.variableName(i)); int pos = Modifier.isStatic(ctMethod.getModifiers()) ? 0 : 1; paramNames = Arrays.copyOfRange(sortMap.values().toArray(new String[0]), pos, paramNames.length + pos); CtClass[] types=ctMethod.getParameterTypes(); for(int i=0;i&lt;paramNames.length;i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+ctMethod.getName()+"--&gt;isStatic="+Modifier.isStatic(ctMethod.getModifiers())+"--&gt;paramsType="+types[i].getName()+"--&gt;paramsName="+paramNames[i]); &#125; &#125; &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 我们新建一个Demo.class 1234567891011121314151617181920 public class Demo &#123; public static String getName(String str) &#123; String s="123"; return str+s; &#125; public Integer doSomething(String str,double a,Map&lt;String, String&gt; map,List&lt;String&gt; list) &#123; //doSomething Integer i=0; return i; &#125; private static final int NUM=1; private static String s="1234"; static &#123; s="5678"; &#125;&#125; 调用方法 getClassInfoByJavassist(Demo.class)，输出class信息。 12345678class--&gt;com.zwt.reflect.Demoobject=NUM--&gt;value=1--&gt;type=intobject=s--&gt;value=null--&gt;type=java.lang.Stringclass=Demo--&gt;method=getName--&gt;isStatic=true--&gt;paramsType=java.lang.String--&gt;paramsName=strclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.lang.String--&gt;paramsName=strclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=double--&gt;paramsName=aclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.util.Map--&gt;paramsName=mapclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.util.List--&gt;paramsName=list 2. 通过spring里的LocalVariableTableParameterNameDiscoverer获取paramsName，使用jdk自带reflect反射类获取class其他信息 在jdk1.8以下java版本中，根据jdk自带reflect包，可以拿到大部分class信息，唯一拿不到的是参数name，我们可以借助spring包里的LocalVariableTableParameterNameDiscoverer去获取paramsName。 1234567891011121314151617181920212223public static void getClassInfoBySpringAndReflect(Class clazz) &#123; try &#123; LocalVariableTableParameterNameDiscoverer u = new LocalVariableTableParameterNameDiscoverer(); Method[] methods =clazz.getDeclaredMethods(); Field[] fields=clazz.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; //设置成可以access的，否则get(clazz)报错，无法读取private属性 fields[i].setAccessible(true); System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].get(clazz)+"--&gt;type="+fields[i].getType().getName()); &#125; for(Method method:methods) &#123; //使用spring LocalVariableTableParameterNameDiscoverer 获取paramsName String[] params = u.getParameterNames(method); Class&lt;?&gt; [] classType=method.getParameterTypes(); for (int i = 0; i &lt; params.length; i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+method.getName()+"--&gt;isStatic="+Modifier.isStatic(method.getModifiers())+"--&gt;paramsType="+classType[i].getName()+"--&gt;paramsName="+params[i]); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 调用getClassInfoBySpringAndReflect(Demo.class)，也可以拿到class信息。 3.使用jdk1.8及以上java版本获取class信息 若jdk版本较高，完全可以不用借助第三方jar包而获取class信息。 jdk1.8以上，添加了Parameter类，可以获取参数paramsName. 123456789101112131415161718192021public static void getClassInfoByJava8AndReflect(Class clazz)&#123; try &#123; Method[] methods = clazz.getDeclaredMethods(); Field[] fields=clazz.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; //设置成可以access的，否则get(clazz)报错，无法读取private属性 fields[i].setAccessible(true); System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].get(clazz)+"--&gt;type="+fields[i].getType().getName()); &#125; for (Method method : methods) &#123; //使用jdk1.8Parameter获取paramsNames Parameter[] params = method.getParameters(); Class&lt;?&gt; [] classType=method.getParameterTypes(); for (int i = 0; i &lt; params.length; i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+method.getName()+"--&gt;isStatic="+Modifier.isStatic(method.getModifiers())+"--&gt;paramsType="+classType[i].getName()+"--&gt;paramsName="+params[i]); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 调用getClassInfoByJava8AndReflect(Demo.class)，获取class信息。 其他 反射优点： 合理的使用反射机制可以有效降低代码冗余及代码量。并且可以让应用程序实现一些几乎不可能做到的事情。反射属于Java语言里比较高级的一个特性。 反射缺点： 如果不合理的使用反射，可能降低系统性能。 而且非常重要的一点，我们看如上代码，有一句fields[i].setAccessible(true); 这是在设置私有属性可以访问，显然，这破坏了代码的抽象性，而且可能导致安全问题的产生。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据传输安全性保证]]></title>
    <url>%2Fblog%2F%E6%B5%85%E8%B0%88HTTP%E4%B8%8EHTTPS20180707%2F</url>
    <content type="text"><![CDATA[前言数据传输过程中，安全性十分重要，尤其是对于一些敏感的用户信息，其安全性保证更应当被重视。 现代软件开发中，无论是基于敏捷式的软件开发，或者基于前后端分离的软件开发，都离不开提供对外接口，而请求这些接口数据的安全性应当得到保证。 方法保证传输数据的安全性，主要有以下方法： 数据加密 数据加密一直是保密数据的重要部分，常见的加密算法有可逆加密算法和不可逆加密算法，可逆加密算法又分为对称加密算法和非对称加密算法。 比如一个系统的登陆操作，客户输入用户名登陆，如果不进行任何保障措施，用户名和密码明文传输，被不法分子截获数据后，显然是不安全的。如果我们这时对密码进行不可逆加密，如md5，对用户名进行可逆加密，如des，这时候在截获数据时，得到的将是一串密文，显然，即使要破解，也需要相当时间。 但这样，有一个明显问题，就是接口吞吐量下降，明显，加密情况下，由于需要解密数据，接口的响应速度会下降。 可能，对于一些非重要数据，我们这样牺牲系统性能换取来的安全可能有些过了。 数据签名 数据签名又是什么呢？它和数据加密的区别呢？ 数据签名，相当于对传输的数据，进行一些不可逆加密算法后，如md5，生成一段签名字符串sign。 比如上述列子中，登陆操作中如果还要传输IP，地点等等数据，这些数据明显没那么重要，这时可以对全部传输数据进行签名，生成sign，将其传入后端，后端用同样算法及密钥计算比较sign，如果一致认为数据正确，直接拿到IP，地点等数据（不用解密，相对于解密各个信息，理论上所有信息计算签名要节省时间），不一致则认为被修改过，返回错误信息。 session，token机制 session（cookie）和token机制的出现是为了校验用户状态的。 比如不法分子知道了我们的后台接口，恶意伪造大量数据攻击，即使这些数据不正确，而服务器每次都需要校验这些数据的正确性，显然带来大量性能消耗。 我们当然可以进行一些优化操作，如对于同一个IP，短时间大量请求则封掉该IP一段时间，但这不是太合理的。 设想，如果用户登陆后，保存状态，只有登陆的用户可以访问这些接口，每次请求到来，均先校验用户登陆状态，对于session，如果没有sessionid或者sessionid错误或者过期则直接返回登陆界面。对于token，与session同理，没有token或者token错误或者过期的直接返回登陆页面。 这样，我们开始校验token或者session，就可以拒绝大量伪造请求。 Https（数字证书机制） 上面，无论数据加密还是签名，我们发现最重要的就是加密方法和加密密钥。 对于两台服务器交互，可能不用太担心，但是如果是webapp或者原生app，不法分子反编译前端代码后，就有可能拿到加密方法和加密key，怎么办呢？ 这就属于Https要解决的事情，下篇文章会介绍https，这儿先简单说下： 在加密算法中，有一种叫做非对称加密的算法，有公钥和私钥组成，他有个特点：公钥加密的数据，只有私钥能解密；私钥加密的数据，只有公钥能解密。 https就是需要让客户端与服务器端安全地协商出一个对称加密算法。剩下的就是通信时双方使用这个对称加密算法进行加密解密。 ①客户端启动，发送请求到服务端，服务端通过非对称加密算法（如RSA）生成公钥pubkey1和私钥prikey1。 ②服务端将公钥pubkey1发给客户端，客户端用自己的非对称加密算法也生成一套公钥pubkey2和私钥prikey2，并将公钥pubkey2通过pubkey1加密后返回服务端。 ③服务端用私钥prikey1解密后拿到pubkey2，并将确定好的未来交互的对称加密算法和密钥通过pubkey2加密，返回客户端。 ④客户端用私钥pubkey2解密数据，拿到服务器给定的加密算法和密钥，双方开始用其数据通信。 这样仍有一个问题，如何证明公钥pubkey1加密的这串数字是客户端来的，即证明他就是他。。。 这就是https的数字证书，相当于网络中心的部分，证明他就是他。数字证书就是来干这个的。 其他 安全性保证还有其他可以自己操作的方法。 如对于两台稳定的服务器交互，直接进行IP校验或许比token，session机制更好更方便。及一些其他的操作，如同一IP短时间大量错误报文，可以将其暂时拉入黑名单。等等。 其他数据安全一直是数据交互的永恒话题，保证数据安全，肯定是需要消耗一定的系统性能的。如何选择合适的安全保证机制，是每一个软件开发工程师应该了解和掌握的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UTF-8和GBK等编码格式转换问题]]></title>
    <url>%2Fblog%2FUTF-8%E5%92%8CGBK%E7%AD%89%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2%E9%97%AE%E9%A2%9820180621%2F</url>
    <content type="text"><![CDATA[前言在开发过程中，我们可能会遇到Java各种编码格式之间的转换问题。下面我们来研究下UTF-8和GBK等编码格式之间的相互转化。 实践在进行编码转换时，我们用ISO-8859-1编码来接受和保存数据，并转换为相应编码。 为什么采用ISO-8859-1编码作为中间转存方案呢？ 下面我们通过程序验证： 通过ISO-8859-1转存： 123456789101112131415161718192021public static void test(String str1,String encode) throws UnsupportedEncodingException &#123; System.out.println("字符串："+str1); //将str转为原编码字节流 byte[] byteArray1=str1.getBytes(encode); System.out.println(byteArray1.length); //转换为一个ISO-8859-1的字符串 String str2=new String(byteArray1,"ISO-8859-1"); System.out.println("转成ISO-8859-1："+str2); //转回为byte数组 byte[] byteArray2=str2.getBytes("ISO-8859-1"); System.out.println(byteArray2.length); //重新用目标编码格式编码 String str3=new String(byteArray2,encode); System.out.println("字符串："+str3); &#125; public static void main(String[] args) throws UnsupportedEncodingException &#123; String str1="你好"; String str2="你好呀"; test(str1,"UTF-8"); test(str2,"UTF-8"); &#125; 运行结果： 字符串：你好 6 转成ISO-8859-1：ä½ å¥½ 6 字符串：你好 字符串：你好呀 9 转成ISO-8859-1：ä½ å¥½å 9 字符串：你好呀 通过GBK转存： 123456789101112131415161718192021 public static void test(String str1,String encode) throws UnsupportedEncodingException &#123; System.out.println("字符串："+str1); //将str转为原编码字节流 byte[] byteArray1=str1.getBytes(encode); System.out.println(byteArray1.length); //转换为一个GBK的字符串 String str2=new String(byteArray1,"GBK"); System.out.println("转成GBK："+str2); //转回为byte数组 byte[] byteArray2=str2.getBytes("GBK"); System.out.println(byteArray2.length); //重新用目标编码格式编码 String str3=new String(byteArray2,encode); System.out.println("字符串："+str3); &#125; public static void main(String[] args) throws UnsupportedEncodingException &#123; String str1="你好"; String str2="你好呀"; test(str1,"UTF-8"); test(str2,"UTF-8");&#125; 运行结果： 字符串：你好 6 转成GBK：浣犲ソ 6 字符串：你好 字符串：你好呀 9 转成GBK：浣犲ソ鍛� 9 字符串：你好�? 可以看到，当用GBK暂存UTF-8编码字符串时，字符串汉字出现了乱码。 为什么会这样？ 分析我们新增一个方法，将byte数组打印出来： 123456789public static void printHex(byte[] byteArray) &#123; StringBuffer sb = new StringBuffer(); for (byte b : byteArray) &#123; sb.append(Integer.toHexString((b &gt;&gt; 4) &amp; 0xF)); sb.append(Integer.toHexString(b &amp; 0xF)); sb.append(" "); &#125; System.out.println(sb.toString());&#125;; 这样上面两个的运行结果分别如下：ISO-8859-1: 字符串：你好 e4 bd a0 e5 a5 bd 转成ISO-8859-1：ä½ å¥½ e4 bd a0 e5 a5 bd 字符串：你好 字符串：你好呀 e4 bd a0 e5 a5 bd e5 91 80 转成ISO-8859-1：ä½ å¥½å e4 bd a0 e5 a5 bd e5 91 80 字符串：你好呀 GBK: 字符串：你好 e4 bd a0 e5 a5 bd 转成GBK：浣犲ソ e4 bd a0 e5 a5 bd 字符串：你好 字符串：你好呀 e4 bd a0 e5 a5 bd e5 91 80 转成GBK：浣犲ソ鍛� e4 bd a0 e5 a5 bd e5 91 3f 字符串：你好�? 可以看到，UTF-8转换为GBK在转换回来时，最后的80变成了3f，为什么会这样？ 我们使用”你好呀” 三个字来分析，它的UTF-8 的字节流为： [e4 bd a0] [e5 a5 bd] [e5 91 80] 我们按照三个字节一组分组，用GBK处理，因为GBK是双字节编码，如下按照两两一组进行分组： [e4 bd] [a0 e5] [a5 bd] [e5 91] [80 ?] 不够了，怎么办？它把 0x8d当做一个未知字符，用一个半角Ascii字符的 “？” 代替，变成了： [e4 bd] [a0 e5] [a5 bd] [e5 91] \3f 数据被破坏了。 为什么 ISO-8859-1 没问题呢？ 因为 ISO-8859-1 是单字节编码，因此它的分组方案是： [e4] [bd] [a0] [e5] [a5] [bd] [e5] [91] [80] 因此中间不做任何操作，因此数据没有变化。 问题你也许会问到，比如将“你好呀”三个字先由UTF-8转为ISO-8859-1，再由ISO-8859-1转为GBK，结果也是乱码啊，不是和下面的代码一样么，性质上？12String isoFont = new String(chinese.getBytes("UTF-8"),"ISO-8859-1");String gbkFont = new String(isoFont.getBytes("ISO-8859-1"),"GBK"); 1String gbkFont = new String(chinese.getBytes("UTF-8"),"GBK"); 两者的性质确实是一样的。 那与上面说的不矛盾吗？ 不矛盾。上面的代码，第一步你指定了字符串编码格式为UTF-8，第二步你将其转换为GBK，肯定会乱码。可以认为你拿一个UTF-8的字符串去转GBK字符串，其实在程序里这种写法本身是错误的！ 我们来看下面一段代码： 12345678910111213141516public static void test2() throws UnsupportedEncodingException &#123; String chinese = "你好呀"; //GBK 测试 String gbkChinese = new String(chinese.getBytes("GBK"),"ISO-8859-1"); System.out.println(gbkChinese); printHex(gbkChinese.getBytes("ISO-8859-1")); String gbkTest = new String(gbkChinese.getBytes("ISO-8859-1"),"GBK"); System.out.println(gbkTest); //UTF-8测试 String utf8Chinese = new String(chinese.getBytes("UTF-8"),"ISO-8859-1"); System.out.println(utf8Chinese); printHex(utf8Chinese.getBytes("ISO-8859-1")); String utfTest = new String(utf8Chinese.getBytes("ISO-8859-1"),"UTF-8"); System.out.println(utfTest); &#125; 输出结果： ÄãºÃÑ½ c4 e3 ba c3 d1 bd 你好呀 ä½ å¥½å e4 bd a0 e5 a5 bd e5 91 80 你好呀 可以看到， GBK分组 ：[c4 e3]–&gt;你 [ba c3]–&gt;好 [d1 bd]–&gt;呀 UTF-8分组：[e4 bd a0]–&gt;你 [e5 a5 bd]–&gt;好 [e5 91 80]–&gt;呀 字符串“你好呀”在GBK编码和UTF-8编码里生成的byte数据流是不一样的。 结论所以如何正确将两种编码格式数据进行转换？ 注意：这儿的转换指的是这样，比如一个GBK编码文件，里面有“你好呀”字符串，写入到UTF-8编码文件里仍然是“你好呀”。 我们新建一个GBK编码文件，里面有你好呀，三个字符，同时将三个字用UTF-8，写入到另一个文件里。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Test2 &#123; public static void main(String[] args) throws Exception &#123; String line = readInFile("/Users/zhangwentong/junrongdai/gbk.txt", "GBK"); System.out.println(line); writeInFile("/Users/zhangwentong/junrongdai/utf8.txt", line, "UTF-8"); &#125; public static String readInFile(String fileName, String charset) &#123; File gbkfile = new File(fileName); String line = ""; FileInputStream gbkIO = null; InputStreamReader gbkISR = null; BufferedReader br = null; try &#123; gbkIO = new FileInputStream(gbkfile); gbkISR = new InputStreamReader(gbkIO, charset); br = new BufferedReader(gbkISR); String rline = ""; while ((rline = br.readLine()) != null) &#123; line += rline; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if(br!=null) fos.close(); if(gbkISR!=null) gbkISR.close(); if(gbkIO!=null) gbkIO.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return line; &#125; public static void writeInFile(String fileName, String content, String charset) &#123; File f = new File(fileName); FileOutputStream fos = null; try &#123; if (!f.exists()) &#123; f.createNewFile(); &#125; fos = new FileOutputStream(f); fos.write(content.getBytes(charset)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 可以测试下上段代码，GBK文字被转为了UTF-8文字。反过来一个UTF-8文件写入到GBK也是可以实现的。 所以，在读取和写入文字时，指定文字的编码格式，再进行读取和写入操作，便不会有乱码的产生。否则读取和写入时会按照执行操作的class文件的编码格式进行写入和读取。 结语欢迎光临我的博客 https://www.sakuratears.top 我的GitHub地址 https://github.com/javazwt]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap实现原理]]></title>
    <url>%2Fblog%2FHashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%8620180616%2F</url>
    <content type="text"><![CDATA[前言今天我们来聊聊HashMap。 Java集合类里的HashMap，实现Map接口，是个非线程安全的类。HashMap允许key和value有null值，且循环遍历为无序的，HashMap底层主要是通过数组+链表实现的，同时JDK8引入红黑树优化，提高HashMap的性能。 要了解HashMap，我们可以从几方面下手。 分析我们先来看下HashMap的两个参数： initialCapacity：初始容量，默认16 loadFactor：负载因子，默认0.75 现在我们执行了下面一段代码，根据代码来分析HashMap： 12Map&lt;String,String&gt; map=new HashMap&lt;&gt;();map.put("0","0"); 创建一个HashMap，其会初始化以下数据： 123456static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64; DEFAULT_INITIAL_CAPACITY: 初始容量，也就是默认会创建 16 个箱子，箱子的个数不能太多或太少。如果太少，很容易触发扩容，如果太多，遍历哈希表会比较慢。 MAXIMUM_CAPACITY: 哈希表最大容量，一般情况下只要内存够用，哈希表不会出现问题。 DEFAULT_LOAD_FACTOR: 默认的负载因子。因此初始情况下，当键值对的数量大于 16 * 0.75 = 12 时，就会触发扩容。 TREEIFY_THRESHOLD: 如果哈希函数不合理，即使扩容也无法减少箱子中链表的长度，因此处理方案是当链表太长时，转换成红黑树。这个值表示当某个箱子中，链表长度大于 8 时，有可能会转化成树。 UNTREEIFY_THRESHOLD: 在哈希表扩容时，如果发现链表长度小于 6，则会由树重新退化为链表。 MIN_TREEIFY_CAPACITY: 在转变成树之前，还会有一次判断，只有键值对数量大于 64 才会发生转换。这是为了避免在哈希表建立初期，多个键值对恰好被放入了同一个链表中而导致不必要的转化。 根据HashMap源码，可以看到源码中有两个static final class Node &lt; K,V &gt; 和 TreeNode &lt; K,V &gt;分别为链表和红黑树链表。 本文不对红黑树链表的实现做过多分析。 我们来看下HashMap的put方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//put方法，调用putVal方法 public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; //HashMap放值方法 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //构建一个链表数组tab，链表p，长度n，索引i Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //把table的值赋给tab，如果tab是空或者长度为0 if ((tab = table) == null || (n = tab.length) == 0) //调用resize方法，并获得tab长度 n = (tab = resize()).length; //计算索引并获得tab索引下的值，如果为空直接将值添加 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果key值相同，直接替换value值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果key不相同，判断p是不是TreeNode，是的话就执行红黑树放入值操作 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; //如果key上没值就放入普通链表 p.next = newNode(hash, key, value, null); //如果链表长度超了8 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //尝试将链表转化为红黑树（不一定会转化） treeifyBin(tab, hash); break; &#125; //如果key上有值就覆盖掉value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果节点value不为空，即key上有值，把这个值返回去 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //记录改变次数（fast-fail机制） ++modCount; //如果长度超过当前，就进行扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 我们再来看下，resize方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081final Node&lt;K,V&gt;[] resize() &#123; //获取旧的tab Node&lt;K,V&gt;[] oldTab = table; //旧的容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; //旧的阀值 int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; //如果容量超了MAXIMUM_CAPACITY，最大阀值定为Integer.MAX_VALUE if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //如果新容量赋值后小于MAXIMUM_CAPACITY并且旧容量不小于初始值 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果定义了初始容量 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults //否则为初始化，所有均为默认值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //为新的阀值赋值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap的get方法 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; //调用getNode方法 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;//获取HashMap Value值final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //先判断链表第一个值是不是结果 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //否则循环链表找值 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 我们看一下hash获取方法： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 代码的意思是：如果Key值为null，返回0；如果Key值不为空，返回原hash值和原hash值无符号右移16位的值按位异或的结果。可以看到当key=null时，hash为0. 问题 为什么HashMap的初始长度默认16，负载因子默认0.75，且长度建议取2的倍数？ &ensp;&ensp;&ensp;&ensp; 通过以上的代码我们可以知道这两个值主要影响的threshold的大小，这个值的数值是当前桶数组需不需要扩容的边界大小，我们都知道桶数组如果扩容，会申请内存空间，然后把原桶中的元素复制进新的桶数组中，这是一个比较耗时的过程。既然这样，那为何不把这两个值都设置大一些呢，threshold是两个数的乘积，设置的大些不就减小了扩容次数吗？ &ensp;&ensp;&ensp;&ensp; 原因是这样的，如果桶初始化桶数组设置太大，就会浪费内存空间，16是一个折中的大小，既不会像1，2，3那样放几个元素就扩容，也不会像几千几万那样可以只会利用一点点空间从而造成大量的浪费。 &ensp;&ensp;&ensp;&ensp; 加载因子设置为0.75而不是1，是因为设置过大，桶中键值对碰撞的几率就会越大，同一个桶位置可能会存放好几个value值，这样就会增加搜索的时间，性能下降，设置过小也不合适，如果是0.1，那么10个桶，threshold为1，你放两个键值对就要扩容，太浪费空间了。 &ensp;&ensp;&ensp;&ensp; HashMap为了存取高效，要尽量较少碰撞，就是要尽量把数据分配均匀，每个链表长度大致相同，这个实现就在把数据存到哪个链表中的算法；这个算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算，源码中做了优化hash&amp;(length-1)，hash%length==hash&amp;(length-1)的前提是length是2的n次方；为什么这样能均匀分布减少碰撞呢？&ensp;&ensp;&ensp;&ensp; 2的n次方实际就是1后面n个0，2的n次方-1 实际就是n个1；例如长度为9时候，3&amp;(9-1)=0 2&amp;(9-1)=0 ，都在0上，碰撞了；例如长度为8时候，3&amp;(8-1)=3 2&amp;(8-1)=2 ，不同位置上，不碰撞； HashMap的fast-fail机制？ &ensp;&ensp;&ensp;&ensp; 在put方法里，我们注意到一个参数，modCount，每当HashMap改变，modCount都会改变,在HashMap的remove，clear等对HashMap的变动操作中，都可以看到对此参数的操作，同时我们还能看到如下代码： 12if (modCount != expectedModCount) throw new ConcurrentModificationException(); &ensp;&ensp;&ensp;&ensp; 这就是HashMap的fast-fail机制，HashMap会记录Map的改变次数，如果多个线程操作HashMap，会导致modCount和expectedModCount不一致，就会抛出ConcurrentModificationException异常，说明你的代码里有多线程对HashMap的操作，这是不被允许的，这样也可以快速检索程序错误，但是我们不能指望HashMap的fast-fail机制来检索错误，我们更应该在编码中注意HashMap是线程不安全的，多线程情况可以考虑ConcurrentHashMap,Hashtable或者使用Collections.synchronizedMap(map)等操作。 深度分析 关于hash桶索引计算函数 i=(n-1)&amp;hash 好处。 &ensp;&ensp;&ensp;&ensp; 观察HashMap将元素放入tab操作如下图，n在1处被赋值为tab.length，在2处，有一个关键算法，i=(n-1)&amp;hash,i为tab下标，这样做有什么好处呢？ （1）保证不会发生数组越界 &ensp;&ensp;&ensp;&ensp; 首先我们要知道的是，在HashMap，数组的长度按规定是2的幂。因此，数组的长度的二进制形式是：10000…000, 1后面有偶数个0。 那么，length - 1 的二进制形式就是01111…111, 0后面有偶数个1。最高位是0, 和hash值相“与”，结果值一定不会比数组的长度值大，因此也就不会发生数组越界。 （2）保证元素尽可能的均匀分布 &ensp;&ensp;&ensp;&ensp; 由上边的分析可知，length若是一个偶数，length - 1一定是一个奇数。假设现在数组的长度length为16，减去1后length - 1就是15，15对应的二进制是：1111。现在假设有两个元素需要插入，一个哈希值是8，二进制是1000，一个哈希值是9，二进制是1001。和1111“与”运算后，结果分别是1000和1001，它们被分配在了数组的不同位置，这样，哈希的分布非常均匀。那么，如果数组长度是奇数呢？减去1后length - 1就是偶数了，偶数对应的二进制最低位一定是 0，例如14二进制1110。对上面两个数子分别“与”运算，得到1000和1000。结果都是一样的值。那么，哈希值8和9的元素都被存储在数组同一个index位置的链表中。在操作的时候，链表中的元素越多，效率越低，因为要不停的对链表循环比较。 关于”扰动函数”，(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16) ？ &ensp;&ensp;&ensp;&ensp; 在对数据进行hash计算时，可以看到，不仅仅是取了数据的hashCode，而是将hashCode和hashCode无符号右移16位的值进行异或运算。 &ensp;&ensp;&ensp;&ensp; 我们知道，key.hashCode返回一个int值，这个值一般比hash桶数组长度要大，比如一个长度为16的hash桶，放入String abc (hashCode为96354)，直接进行桶索引计算，i=(n-1)&amp;hashCode 可以得出(15&amp;96354)=2,索引值为2，如果是abcd，计算（15&amp;2987074）=2，索引值也为2。 &ensp;&ensp;&ensp;&ensp; 可以看出，即使hashCode散列再离散，计算索引值时低位才是主要影响原因，而特征较大的高位（96354和2987074高位特征较大）根本不参与运算，这样hash冲突也会较高。而右移16位（32位的一半，int最大32位），正好为32位一半，这样可以把前16位认为高位，后16位认为低位，然后进行异或操作，高16位的信息被变相保存了下来，增大了随机性。 &ensp;&ensp;&ensp;&ensp; 可以看出这样操作后abc的下标为3（二进制11），abcd的下标为15（二进制1111）。 &ensp;&ensp;&ensp;&ensp; Peter Lawrey有一篇关于hash冲突率比较的文章《An introduction to optimising a hashing strategy》，大家可以看看。 &ensp;&ensp;&ensp;&ensp; https://www.javacodegeeks.com/2015/09/an-introduction-to-optimising-a-hashing-strategy.html 为什么引入负载因子这个概念？ &ensp;&ensp;&ensp;&ensp; 负载因子的引入，可以来说是时间复杂度和空间复杂度的折中。（大数据统计下）负载因子越低，一般认为空间开销越大，查询时间开销越低（hash碰撞低），大量hash数组，少量链表；负载因子越高，一般认为空间开销越低，查询时间开销越高（hash碰撞高），少量hash桶数组，大量链表。负载因子的引入恰可以增加HashMap不同场景使用的灵活性。 结语欢迎光临我的博客 https://www.sakuratears.top 我的GitHub地址 https://github.com/javazwt]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池]]></title>
    <url>%2Fblog%2FJava%E7%BA%BF%E7%A8%8B%E6%B1%A020180610%2F</url>
    <content type="text"><![CDATA[前言Java中，创建线程的方式一般有三种方法： 继承Thread类创建线程 实现Runnable接口创建线程 使用Callable和Future创建线程 关于三种创建方法本文不再赘述。 可以看出，以上创建线程的方式，都缺乏对线程的管理，我们设想，如果线程在调用过程中使用了某一资源，当该资源处理缓慢或异常时，可能产生大量线程等待的情况，严重时可能造成OOM异常。 针对以上情况，应该对创建线程进行管理，这样线程池便产生了，好在在jdk1.5时，Doug Lea大神已经帮我们实现了这些功能，它们均在java.util.concurrent包下。建议大家想学习多线程，把该包下的源码理解，一定对多线程会有更深入的理解。 本文重点讲述线程池，会对以下这几个类（接口）进行重点讲解。 Executor，ExecutorService，Executors，AbstractExecutorService，ThreadPoolExecutor 线程池的创建我们先来简单说下线程池的使用： 缓存型线程池 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 Executors.newCachedThreadPool 简单使用： 123456789101112131415ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; final int index = i; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); &#125; &#125;); &#125; 定长线程池 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 Executors.newFixedThreadPool 简单使用： 123456789101112131415ExecutorService fixedThreadPool = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 定时及周期性任务性线程池 创建一个定长线程池，支持定时及周期性任务执行。 Executors.newScheduledThreadPool 简单使用： 1234567ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id="+Thread.currentThread().getId()+";5s后,每2s执行一次"); &#125; &#125;, 5, 2, TimeUnit.SECONDS); 单线程型线程池 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 Executors.newSingleThreadExecutor 简单使用： 123456789101112131415ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("Thread id="+Thread.currentThread().getId()+";index="+index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 自定义线程池 创建一个自定义线程池，以优化线程池。 根据Executors源码，可以看出： 12345678910111213141516171819 public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 线程池的实现使用ThreadPoolExecutor这个类实现的。这个类全参参数有以下几个： 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ① corePoolSize：核心线程池大小 取值范围 0-Integer.MaxValue ② maximumPoolSize：最大线程池大小 取值范围 0-Integer.MaxValue ③ keepAliveTime：线程空闲时存活时间 ④ unit：线程空闲时存活时间单位 ⑤ workQueue：工作队列类型，线程队列类型 队列分类： 直接提交策略：SynchronousQueue，其无法设置队列长度，所有线程均直接提交给线程池。 无界队列：LinkedBlockingQueue，如果默认不设置初始长度，这个队列是无界的，可缓存大量等待线程。 有界队列：ArrayBlockingQueue，必须设置初始长度，线程池满，且达到队列最大长度后执行拒绝策略。 ⑥ threadFactory：线程工厂 ⑦ handler：线程池饱和后的拒绝策略 ThreadPoolExecutor定义了四种，我们也可以自己定义： ThreadPoolExecutor.AbortPolicy：拒绝该任务并抛出异常 ThreadPoolExecutor.CallerRunsPolicy：直接在 execute 方法的调用线程中运行被拒绝的任务；如果执行程序已关闭，则会丢弃该任务 ThreadPoolExecutor.DiscardOldestPolicy：直接丢弃正在执行的任务，并执行该任务 ThreadPoolExecutor.DiscardPolicy：丢弃该任务 可以看出，当业务情况复杂时，Executors里提供的几种基本的线程池已经不能满足我们的要求，需要我们根据情况自定义线程池，而且可以举个例子，比如对于newCachedThreadPool创建线程池的方法，它传入的maximumPoolSize为Integer的Max值，如果业务资源异常，创建大量线程而不释放，newCachedThreadPool这种创建线程池的方法也能导致OOM异常。 而我们声明最大线程池大小，并声明拒绝策略。如下： 1234567891011121314151617181920ExecutorService myExecutor = new ThreadPoolExecutor(5, 10, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(5), new AbortPolicy()); for (int i = 0; i &lt; 11; i++) &#123; final int index = i; try &#123; myExecutor.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); try &#123; Thread.sleep(10000000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 可以有效防止OOM异常以及及时发现系统运行问题。 自定义线程池也是被推荐的创建线程池的方法。 源码分析下面我们主要对ThreadPoolExecutor这个类进行分析。 我们先看下它的execute方法： 123456789101112131415161718192021222324252627282930public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //获取线程数量 int c = ctl.get(); //如果线程池线程数量小于核心线程数，那么试着向核心线程池添加一个线程 if (workerCountOf(c) &lt; corePoolSize) &#123; //添加成功就返回 if (addWorker(command, true)) return; //添加不成功就再次拿到线程数量 c = ctl.get(); &#125; //如果添加失败了，或者线程池数量达到了核心线程池数量 //那么判断下运行状态，然后试着向工作等待队列里添加此线程 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); //如果不是运行状态，那么试着从工作队列移除此线程 if (! isRunning(recheck) &amp;&amp; remove(command)) //成功就进行拒绝策略处理 reject(command); //如果核心线程池和队列都满了，达到CAPACITY else if (workerCountOf(recheck) == 0) //那么尝试将任务添加至非核心线程池 addWorker(null, false); &#125; //如果添加失败，执行拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 再看下addWorker方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; //sakuratears int c = ctl.get(); //拿下线程运行状态 int rs = runStateOf(c); //如果程序停止或者状态不是暂停并且任务不为空并且任务队列不为空，添加失败 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //数量大于CAPACITY或者数量大于corePoolSize（向核心线程池添加时）或者maximumPoolSize（向非核心线程池添加时），返回添加失败 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //试着计算下当前线程数量 if (compareAndIncrementWorkerCount(c)) //成功跳出循环 break retry; //不成功就重读ctl c = ctl.get(); //如果当前状态与开始线程状态不一致 if (runStateOf(c) != rs) //重试循环 continue retry; &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; //获得锁 final ReentrantLock mainLock = this.mainLock; //创建一个新的Worker w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; //加锁 mainLock.lock(); try &#123; int c = ctl.get(); int rs = runStateOf(c); //如果是运行状态，或者核心线程池暂停，但要将线程添加到非核心线程池中 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) throw new IllegalThreadStateException(); //添加线程 workers.add(w); int s = workers.size(); //记录曾经有过的最大线程数 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; //解锁 mainLock.unlock(); &#125; if (workerAdded) &#123; //启动 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) //线程启动失败，执行失败操作 addWorkerFailed(w); &#125; return workerStarted;&#125; 更多代码不一一赘述。上面代码基本是线程池的核心原理。 通俗点讲，线程池工作分为下面几步： 根据传入参数，设置核心线程池数量，最大线程池数量，拒绝策略，线程工作队列 当添加一个线程时，如果线程池线程数小于核心线程数，直接开启一个新线程执行任务。 如果核心线程池满了，那么把它添加到工作队列中。 如果核心线程池和工作队列都满了，则开启非核心线程执行任务。 如果全部都满了，执行拒绝策略。 以上就是对线程池的全部分析。 关于我的个人博客： https://www.sakuratears.top GitHub地址： https://github.com/javazwt 欢迎关注。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未闻花名]]></title>
    <url>%2Fblog%2F%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D20180609%2F</url>
    <content type="text"><![CDATA[已知花意未见其花已见其花未闻花名再见其花落泪千溟未闻花名但识花香已知花名花已不在未闻花名但识花香再遇花时泪已千行 某天，你无端想起一个人，她曾让你对明天有所期许，但她却完全没有出现在你的明天里。————《再见金华站》]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MarkDown语法]]></title>
    <url>%2Fblog%2FMarkDown%E8%AF%AD%E6%B3%9520180605%2F</url>
    <content type="text"><![CDATA[前言最近自己搭建了一个博客网站，写文章时用到了markdown语法，之前虽未接触过markdown语法，但在两三天的使用过程中感觉markdown语法很棒，写法简洁，而且支持Html语言。特总结一些常用的语法如下： 语法汇总标题markdown有6种标题，写法如下： 级别 写法 1级 # 大标题 2级 ## 前言 3级 ### 语法汇总 4级 #### 标题 5级 ##### 标题5 6级 ###### 标题6 注意：#号和文字之间有空格 段落写完一段话，一般空出一行便可以分为两个段落，如果不主动空行，其便会认为为一个段落。也可以在一段话的结束输入两个空格，markdown也会换行。 注意：这两种换行，第一种间距大。我现在这段话分别用了空格换行和空行换行。 tab键这是一段用了tab缩进的话。 这是用了两个tab缩进的话。 这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长的话。 注意：当一段话过长时，这样生成的句子可能不换行，而是在下面有一个拉动条。 资源引用 引用静态资源时，如img，png，gif等 语法：![图片名称](图片的绝对路径或者相对路径) ![upload successful](/images/pasted-10.png) 引用网址时，直接贴上网址就能识别 我的博客：https://javazwt.github.io/ 表格语法： | Name | Value | |-------------|:-----------: | | sakuratears | 999999 | 效果： Name Value sakuratears 999999 注意：在表头下方的分隔线标记中加入 :，即可标记下方单元格内容的对齐方式，默认左对齐： :— 代表左对齐:–: 代表居中对齐—: 代表右对齐 left（:— ） center（:—-: ） right（ —-:） left center right 加粗 斜体 删除线加粗语法： **要加粗的字** 加粗效果： 要加粗的字 斜体语法：*斜体的字* 斜体效果： 斜体的字 删除线语法： ~~要删除的字~~ 删除线效果： 要删除的字 引用如果需要引用文献资料或者别的引用，可以用大于号解决： 语法：&gt;要引用的文字 效果： 要引用的文字 列表有序列表语法：在指定序号后面加上点（.） 并空格 1. aaa 2. bbb 3. ccc 效果： aaa bbb ccc 无序列表语法：在文字前面加上* 或者 + 或者 - 并空格 * aaa * bbb * ccc 效果： aaa bbb ccc 注意： 如果生成多级列表，可以这样： - 1.a - 1.1.aa - 1.1.1.aaa - 2.b - 3.c 效果图： 1.a 1.1.aa 1.1.1.aaa 2.b 3.c 代码支持语法： 1//代码片段 效果：一段java代码 1System.out.println("Hello World"); 其他由于Markdown支持Html语法，故Markdown以上无法实现的可以借助Html实现，如居中实现： &lt;center&gt;这段话要居中&lt;/center&gt; 效果：这段话要居中]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Html5+JQuery页面标签的禁用问题]]></title>
    <url>%2Fblog%2FHtml5-JQuery%E9%A1%B5%E9%9D%A2%E6%A0%87%E7%AD%BE%E7%9A%84%E7%A6%81%E7%94%A8%E9%97%AE%E9%A2%9820180604%2F</url>
    <content type="text"><![CDATA[前言在做移动应用项目时遇到的问题，完成某些操作后返回到前面页面要保证前面页面的标签内容不可被编辑，修改。有如下方法： 方法 对于input，select框，设置disabled=’disabled’。 如： 12$('#testid1 input').attr('disabled','disabled');$('#testid2 select').attr('disabled','disabled'); 对于一些点击触发事件的button，可以用unbind()移除事件方法。 如： 1$(".testclass3").unbind(); 注：在某些情况下unbind慎用，如页面初始化时调用click事件加载其他事件属性，unbind()可能会导致初始化时无法加载。 插件禁用，有些页面可能使用某些插件，对于插件的禁用，可以设置插件的事件方法return false; 如： 123$("#test #test4").on("touchmove",function()&#123; return false;&#125;); 这个是我们项目一个滑动插件触摸禁用的例子。 对于一些比较特殊的，如li标签，可以使用class样式进行处理。 这是一个两个li标签切换禁用的例子。 例： 1234$("#test_li").addClass('disabledTab');.disabledTab&#123; pointer-events: none;//这样鼠标或触摸就无法选中li标签&#125;]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云主机环境搭建]]></title>
    <url>%2Fblog%2F%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA20180604%2F</url>
    <content type="text"><![CDATA[前言现在很多服务都部署在云服务上，最近项目也要发布到云上，特写此文章。有需要部署云服务的小伙伴可以参考下。 云主机参数在部署云服务时，一般需要知道如下参数： 云ECS主机服务器远程连接： url XXXXXXXX 用户名：XXXXXXX 密码：XXXXXXX 超级管理员：用户名：XXXX 密码 ：XXXX 硬件配置： 实例规格：2 核 4GB 网络：带宽 5Mbps 操作系统：Ubuntu 16.04 64位 系统盘：高效云盘（100 GB） 工具准备远程连接工具 XShell连接工具 Xftp文件上传管理工具 两工具下载地址：https://www.netsarang.com/products/xsh_overview.html 其他说明：由于当前Ubuntu 16.04 64位 Linux系统弃用了一些旧的远程ssh加密方式，故原先的ssh2工具不能连接，当然也可以修改linux配置文件使其支持旧的ssh加密方式，详见http://blog.csdn.net/liucky/article/details/51889112 等文章。本服务器未修改也不建议修改linux配置，进而选择使用XSHell等工具连接。 服务器搭建工具 Linux 64位 jdk压缩包 jdk-7u79-linux-x64.tar.gz Tomcat压缩包 For Linux 64 apache-tomcat-7.0.79.tar.gz Linux 64位 Mysql 安装包 （在线安装可不使用） mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz 编译好的项目 demo 数据库表 demo 连接云主机 连接到Linux服务器 打开XShell，输入前面的云主机配置，连接到Linux远程服务器。如图： 登录成功后如图 使用 pwd 命令可查看当前路径 目前/home 文件夹下已使xxxxxx用户拥有读、写、执行的权限 上传文件 进入/home文件夹 我们在其目录下建三个文件夹jdk,mysql,tomcat用于存放文件 创建文件夹，可以使用 mkdir 文件夹名 这种方式，也可以直接打开xftp 创建，如图： 上传并解压 通过Xftp将 jdk，tomcat，mysql，项目包demo，数据表全部传上去 对于tar.gz文件进行解压 tar -xzvf jdk-7u79-linux-x64.tar.gz 注意：项目demo要放到tomcat webapps目录下，故建议将tomcat 解压后再上传demo项目 配置环境变量 找到jdk路径 同时找到 /etc/profile 使用vi /etc/profile 编辑该文件，设置环境变量 export JAVA_HOME=”/home/jdk/jdk1.7.0_79” export PATH=”$PATH:$JAVA_HOME/bin” export JRE_HOME=”$JAVA_HOME/jre” export CLASSPATH=”.:$JAVA_HOME/lib:$JRE_HOME/lib” 或者通过Xftp把profile文件down下来，修改好后再上传回去 弄好后要使用source /etc/profile 命令进行更新使其变量生效 同时 使用命令 java -version java javac 正常则表示配置成功了 启动Tomcat 由于tomcat自带的startup.sh脚本看不到日志输出，因此写了个脚本startTomcat.sh脚本 进入到tomcat bin目录下，使用命令 sh startTomcat.sh 启动tomcat，看到starup毫秒数表示启动成功。 启动后，在本地机器上访问 http://xxxxxxxxxxx:8080/ 可以看到久违的tomcat页面 bin里的shutdown.sh用于停止tomcat服务，sh shutdown.sh 成功后上传项目 demo到 tomcat webapps文件夹 安装MySQL接下来开始安装Mysql 对于阿里云，我们可以采取在线安装的方式快速安装MySQL 使用Linux root用户登录， 使用命令： sudo apt-get install mysql-server apt-get isntall mysql-client sudo apt-get install libmysqlclient-dev 安装过程中会提示输入root用户密码，设置所需的root用户密码即可。 使用以下命令查看MySQL是否安装成功 sudo netstat -tap | grep mysql 可以看到MySQL处于listen状态，配置成功。 关于离线安装，暂时没弄成功，有时间在补上。 导入数据表将数据表数据导入到MySQL数据库。mysql命令service mysql start 启动service mysql stop 停止 service mysql restart 重启 使用root用户登录MySQ mysql -u root -p 输入密码登录成功。 设置必要数据 进入mysql命令行 查看数据库编码还有表名大小写情况（Linux系统下MySQL表名会区分大小写） 查看字符编码 mysql&gt; show variables like ‘character%’; 查看表名大小写 mysql&gt; show Variables like ‘%table_names’; 如果不是utf-8编码（右图所示为utf-8编码格式），表名默认区分大小写 lower_case_table_names=0 则需要修改默认配置 cd etc/mysql/mysql.conf.d/ vi mysqld.cnf 加入如下配置： character-set-server=utf8 collation-server=utf8_general_ci skip-character-set-client-handshake lower_case_table_names=1 重启mysql服务 开启访问权限 我们可以在开启外网MySQL访问权限在Windows机器上用GUI工具导入MySQL表数据 开启外网权限： mysql&gt; SELECT User, Host FROM mysql.user; mysql&gt; update user set host = ‘%’ where user =’root’; mysql&gt; flush privileges; 同时将etc/mysql/mysql.conf.d/mysqld.cnf 配置文件里的 bind-address = 127.0.0.1注释掉。 这样在MySQL可视化工具如Navicat上远程连接MySQL，设置utf-8编码格式后将数据表导入即可。 注意事项及其他 启动tomcat及MySQL后，通过外网便可以正常访问项目。输入用户名密码后登陆。 注意事项。 使用root用户时，应尽量减少敏感操作，如 rm ， mv ，cp -r 等命令的执行。 tomcat请使用tomcat用户启动，不建议使用root用户启动。 MySQL开启外网访问后，如果无法访问，可能是阿里云禁用了3306端口的外网访问，需要登录阿里云开启，当成功导入数据后，十分建议关闭MySQL远程访问权限。 如果要设置tomcat默认端口为80，也需要登录阿里云设置。 如果操作过程中出现异常破坏了Linux系统，可以登录阿里云进行系统还原。 部署项目时，项目的一些配置文件如数据库连接，请求地址一定要配置正确。 如果要设置tomcat默认端口为80，则需要用root启动，因为linux只有root才能启动1024以内的端口，同时需要修改tomcat server.xml配置文件及demo访问地址。 文档PPT简单阿里云主机搭建文档资源：https://download.csdn.net/download/javazwt/10448764]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>云部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java排序算法]]></title>
    <url>%2Fblog%2FJava%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%9520180602%2F</url>
    <content type="text"><![CDATA[排序算法分类十种常见排序算法可以分为两大类： 非线性时间比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此称为非线性时间比较类排序。 线性时间非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此称为线性时间非比较类排序。 如下图： 排序算法复杂度及稳定性如下图： 相关概念： 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数。 排序算法冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 动图演示 代码实现JavaScript代码 12345678910111213function bubbleSort(arr) &#123; var len = arr.length; for (var i = 0; i &lt; len; i++) &#123; for (var j = 0; j &lt; len - 1 - i; j++) &#123; if (arr[j] &gt; arr[j+1]) &#123; // 相邻元素两两对比 var temp = arr[j+1]; // 元素交换 arr[j+1] = arr[j]; arr[j] = temp; &#125; &#125; &#125; return arr; &#125; Java代码 123456789101112public static int[] bubbleSort(int[] a)&#123; for(int i=0;i&lt;a.length-1;i++)&#123; for(int j=i+1;j&lt;a.length;j++)&#123; if(a[i]&gt;a[j])&#123; int temp=a[i]; a[i]=a[j]; a[j]=temp; &#125; &#125; &#125; return a; &#125; 简单选择排序（Selection Sort）选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 算法描述n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n)。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n]分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； n-1趟结束，数组有序化了。 动图演示 代码实现JavaScript代码 12345678910111213141516function selectionSort(arr) &#123; var len = arr.length; var minIndex, temp; for (var i = 0; i &lt; len - 1; i++) &#123; minIndex = i; for (var j = i + 1; j &lt; len; j++) &#123; if (arr[j] &lt; arr[minIndex]) &#123; // 寻找最小的数 minIndex = j; // 将最小数的索引保存 &#125; &#125; temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; &#125; return arr; &#125; Java代码 12345678910111213141516public static int[] simpleSelectSort(int[] a)&#123; for(int i=0;i&lt;a.length-1;i++)&#123; int min=i; for(int j=i+1;j&lt;a.length;j++)&#123; if(a[min]&gt;a[j])&#123; min=j; &#125; &#125; if(min!=i)&#123; int temp=a[i]; a[i]=a[min]; a[min]=temp; &#125; &#125; return a; &#125; 算法分析表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 插入排序（Insertion Sort）插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 动图演示 代码实现JavaScript代码 1234567891011121314function insertionSort(arr) &#123; var len = arr.length; var preIndex, current; for (var i = 1; i &lt; len; i++) &#123; preIndex = i - 1; current = arr[i]; while (preIndex &gt;= 0 &amp;&amp; arr[preIndex] &gt; current) &#123; arr[preIndex + 1] = arr[preIndex]; preIndex--; &#125; arr[preIndex + 1] = current; &#125; return arr; &#125; Java代码 1234567891011121314public static int[] straightInsertionSort(int[] a)&#123; for(int i=1;i&lt;a.length;i++)&#123; for(int j=i;j&gt;0;j--)&#123; if(a[j]&lt;a[j-1])&#123; int temp=a[j-1]; a[j-1]=a[j]; a[j]=temp; &#125;else&#123; break; &#125; &#125; &#125; return a; &#125; 算法分析插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 希尔排序（Shell Sort）1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。 算法描述先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 动图演示 代码实现JavaScript代码 123456789101112131415161718function shellSort(arr) &#123; var len = arr.length, temp, gap = 1; while (gap &lt; len / 3) &#123; // 动态定义间隔序列 gap = gap * 3 + 1; &#125; for (gap; gap &gt; 0; gap = Math.floor(gap / 3)) &#123; for (var i = gap; i &lt; len; i++) &#123; temp = arr[i]; for (var j = i-gap; j &gt; 0 &amp;&amp; arr[j]&gt; temp; j-=gap) &#123; arr[j + gap] = arr[j]; &#125; arr[j + gap] = temp; &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920public static int[] shellsSort(int a[]) &#123; int d = a.length; while (true) &#123; d = d / 2; for (int x = 0; x &lt; d; x++) &#123; for (int i = x + d; i &lt; a.length; i = i + d) &#123; int temp = a[i]; int j; for (j = i - d; j &gt;= 0 &amp;&amp; a[j] &gt; temp; j = j - d) &#123; a[j + d] = a[j]; &#125; a[j + d] = temp; &#125; &#125; if (d == 1) &#123; break; &#125; &#125; return a; &#125; 算法分析希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的。 归并排序（Merge Sort）归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 算法描述 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 动图演示 代码实现JavaScript代码 123456789101112131415161718192021222324252627282930function mergeSort(arr) &#123; // 采用自上而下的递归方法 var len = arr.length; if (len &lt; 2) &#123; return arr; &#125; var middle = Math.floor(len / 2), left = arr.slice(0, middle), right = arr.slice(middle); return merge(mergeSort(left), mergeSort(right)); &#125; function merge(left, right) &#123; var result = []; while (left.length&gt;0 &amp;&amp; right.length&gt;0) &#123; if (left[0] &lt;= right[0]) &#123; result.push(left.shift()); &#125; else &#123; result.push(right.shift()); &#125; &#125; while (left.length) result.push(left.shift()); while (right.length) result.push(right.shift()); return result; &#125; Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static int[] mergeSort(int a[])&#123; sort(a,0,1); return a; &#125; private static void sort(int[] a, int s, int len) &#123; int size = a.length; int mid = size / (len &lt;&lt; 1); int c = size &amp; ((len &lt;&lt; 1) - 1); // -------归并到只剩一个有序集合的时候结束算法-------// if (mid == 0) return; // ------进行一趟归并排序-------// for (int i = 0; i &lt; mid; ++i) &#123; s = i * 2 * len; merge(a, s, s + len, (len &lt;&lt; 1) + s - 1); &#125; // -------将剩下的数和倒数一个有序集合归并-------// if (c != 0) merge(a, size - c - 2 * len, size - c, size - 1); // -------递归执行下一趟归并排序------// sort(a, 0, 2 * len); &#125; private static void merge(int[] a, int s, int m, int t) &#123; int[] tmp = new int[t - s + 1]; int i = s, j = m, k = 0; while (i &lt; m &amp;&amp; j &lt;= t) &#123; if (a[i] &lt;= a[j]) &#123; tmp[k] = a[i]; k++; i++; &#125; else &#123; tmp[k] = a[j]; j++; k++; &#125; &#125; while (i &lt; m) &#123; tmp[k] = a[i]; i++; k++; &#125; while (j &lt;= t) &#123; tmp[k] = a[j]; j++; k++; &#125; System.arraycopy(tmp, 0, a, s, tmp.length); &#125; 算法分析归并排序是一种稳定的排序方法。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(nlogn）的时间复杂度。代价是需要额外的内存空间。 快速排序（Quick Sort）快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法描述快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 动图演示 代码实现JavaScript代码 1234567891011121314151617181920212223242526272829303132function quickSort(arr, left, right) &#123; var len = arr.length, partitionIndex, left = typeof left != 'number' ? 0 : left, right = typeof right != 'number' ? len - 1 : right; if (left &lt; right) &#123; partitionIndex = partition(arr, left, right); quickSort(arr, left, partitionIndex-1); quickSort(arr, partitionIndex+1, right); &#125; return arr; &#125; function partition(arr, left ,right) &#123; // 分区操作 var pivot = left, // 设定基准值（pivot） index = pivot + 1; for (var i = index; i &lt;= right; i++) &#123; if (arr[i] &lt; arr[pivot]) &#123; swap(arr, i, index); index++; &#125; &#125; swap(arr, pivot, index - 1); return index-1; &#125; function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; Java代码 12345678910111213141516171819202122232425262728public static int[] quickSort(int a[],int low,int high)&#123; if(low&lt;high)&#123; int privotLoc=partition(a,low,high);//将表一分为二 quickSort(a,low,privotLoc-1); quickSort(a,privotLoc+1,high); &#125; return a; &#125; private static int partition(int a[],int low,int high)&#123; int privotKey=a[low];//基准元素 while(low&lt;high)&#123; //从表的两端交替向中间扫描 while(low&lt;high&amp;&amp;a[high]&gt;=privotKey)&#123; //从high 所指位置向前搜索，至多到low+1 位置。将比基准元素小的交换到低端 --high; //从右找比基准元素少的 &#125; a[low]=a[high]; //如果比基准元素小，交换位置 a[high]=privotKey; while(low&lt;high&amp;&amp;a[low]&lt;=privotKey)&#123; ++low; //从右找比基准元素大的 &#125; a[high]=a[low]; //如果比基准元素大，交换位置 a[low]=privotKey; &#125; return low; &#125; 堆排序（Heap Sort）堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 算法描述 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 动图演示 代码实现JavaScript代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344var len; // 因为声明的多个函数都需要数据长度，所以把len设置成为全局变量 function buildMaxHeap(arr) &#123; // 建立大顶堆 len = arr.length; for (var i = Math.floor(len/2); i &gt;= 0; i--) &#123; heapify(arr, i); &#125; &#125; function heapify(arr, i) &#123; // 堆调整 var left = 2 * i + 1, right = 2 * i + 2, largest = i; if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123; largest = left; &#125; if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123; largest = right; &#125; if (largest != i) &#123; swap(arr, i, largest); heapify(arr, largest); &#125; &#125; function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; function heapSort(arr) &#123; buildMaxHeap(arr); for (var i = arr.length - 1; i &gt; 0; i--) &#123; swap(arr, 0, i); len--; heapify(arr, 0); &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public static int[] heapSort(int a[])&#123; buildMaxHeapTree(a); sort(a); return a; &#125; private static void buildMaxHeapTree(int a[])&#123; //没有子节点的才需要创建最大堆，从最后一个的父节点开始 int startIndex=getParentIndex(a.length-1); //从尾端开始创建最大堆，每次都是正确的堆 for(int i=startIndex;i&gt;=0;i--)&#123; maxHeap(a,a.length,i); &#125; &#125; private static void maxHeap(int[] data, int heapSize, int index) &#123; //当前点与左右子节点比较 int left = getChildLeftIndex(index); int right = getChildRightIndex(index); int largest = index; if (left &lt; heapSize &amp;&amp; data[index] &lt; data[left]) &#123; largest = left; &#125; if (right &lt; heapSize &amp;&amp; data[largest] &lt; data[right]) &#123; largest = right; &#125; //得到最大值后可能需要交换，如果交换了，其子节点可能就不是最大堆了，需要重新调整 if (largest != index) &#123; int temp = data[index]; data[index] = data[largest]; data[largest] = temp; maxHeap(data, heapSize, largest); &#125; &#125; private static int getParentIndex(int current)&#123; return (current-1)&gt;&gt;2; &#125; private static int getChildLeftIndex(int current)&#123; return(current&lt;&lt;1)+1; &#125; private static int getChildRightIndex(int current)&#123; return(current&lt;&lt;1)+2; &#125; private static void sort(int[] data)&#123; //末尾与头交换，交换后调整最大堆 for(int i=data.length-1;i&gt;0;i--)&#123; int temp=data[0]; data[0]=data[i]; data[i]=temp; maxHeap(data,i,0); &#125; &#125; 计数排序（Counting Sort）计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 动图演示 代码实现JavaScript代码 12345678910111213141516171819202122function countingSort(arr, maxValue) &#123; var bucket = new Array(maxValue + 1), sortedIndex = 0; arrLen = arr.length, bucketLen = maxValue + 1; for (var i = 0; i &lt; arrLen; i++) &#123; if (!bucket[arr[i]]) &#123; bucket[arr[i]] = 0; &#125; bucket[arr[i]]++; &#125; for (var j = 0; j &lt; bucketLen; j++) &#123; while(bucket[j] &gt; 0) &#123; arr[sortedIndex++] = j; bucket[j]--; &#125; &#125; return arr; &#125; Java代码 123456789101112131415161718192021222324public static int[] countSort(int[]a)&#123; int b[] = new int[a.length]; int max = a[0],min = a[0]; for(int i:a)&#123; if(i&gt;max)&#123; max=i; &#125; if(i&lt;min)&#123; min=i; &#125; &#125;//这里k的大小是要排序的数组中，元素大小的极值差+1 int k=max-min+1; int c[]=new int[k]; for(int i=0;i&lt;a.length;++i)&#123; c[a[i]-min]+=1;//优化过的地方，减小了数组c的大小 &#125; for(int i=1;i&lt;c.length;++i)&#123; c[i]=c[i]+c[i-1]; &#125; for(int i=a.length-1;i&gt;=0;--i)&#123; b[--c[a[i]-min]]=a[i];//按存取的方式取出c的元素 &#125; return b; &#125; 算法分析计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 桶排序（Bucket Sort）桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 算法描述 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来。 图片演示 代码实现JavaScript代码 12345678910111213141516171819202122232425262728293031323334353637383940function bucketSort(arr, bucketSize) &#123; if (arr.length === 0) &#123; return arr; &#125; var i; var minValue = arr[0]; var maxValue = arr[0]; for (i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &lt; minValue) &#123; minValue = arr[i]; // 输入数据的最小值 &#125; else if (arr[i] &gt; maxValue) &#123; maxValue = arr[i]; // 输入数据的最大值 &#125; &#125; // 桶的初始化 var DEFAULT_BUCKET_SIZE = 5; // 设置桶的默认数量为5 bucketSize = bucketSize || DEFAULT_BUCKET_SIZE; var bucketCount = Math.floor((maxValue - minValue) / bucketSize) + 1; var buckets = new Array(bucketCount); for (i = 0; i &lt; buckets.length; i++) &#123; buckets[i] = []; &#125; // 利用映射函数将数据分配到各个桶中 for (i = 0; i &lt; arr.length; i++) &#123; buckets[Math.floor((arr[i] - minValue) / bucketSize)].push(arr[i]); &#125; arr.length = 0; for (i = 0; i &lt; buckets.length; i++) &#123; insertionSort(buckets[i]); // 对每个桶进行排序，这里使用了插入排序 for (var j = 0; j &lt; buckets[i].length; j++) &#123; arr.push(buckets[i][j]); &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526272829public static void basket(int data[])//data为待排序数组 &#123; int n = data.length; int bask[][] = new int[10][n]; int index[] = new int[10]; int max = Integer.MIN_VALUE; for (int i = 0; i &lt; n; i++) &#123; max = max &gt; (Integer.toString(data[i]).length()) ? max : (Integer.toString(data[i]).length()); &#125; String str; for (int i = max - 1; i &gt;= 0; i--) &#123; for (int j = 0; j &lt; n; j++) &#123; str = ""; if (Integer.toString(data[j]).length() &lt; max) &#123; for (int k = 0; k &lt; max - Integer.toString(data[j]).length(); k++) str += "0"; &#125; str += Integer.toString(data[j]); bask[str.charAt(i) - '0'][index[str.charAt(i) - '0']++] = data[j]; &#125; int pos = 0; for (int j = 0; j &lt; 10; j++) &#123; for (int k = 0; k &lt; index[j]; k++) &#123; data[pos++] = bask[j][k]; &#125; &#125; for (int x = 0; x &lt; 10; x++) index[x] = 0; &#125; &#125; 算法分析桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 基数排序（Radix Sort）基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 算法描述 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 动图演示 代码实现JavaScript代码 123456789101112131415161718192021222324var counter = []; function radixSort(arr, maxDigit) &#123; var mod = 10; var dev = 1; for (var i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123; for(var j = 0; j &lt; arr.length; j++) &#123; var bucket = parseInt((arr[j] % mod) / dev); if(counter[bucket]==null) &#123; counter[bucket] = []; &#125; counter[bucket].push(arr[j]); &#125; var pos = 0; for(var j = 0; j &lt; counter.length; j++) &#123; var value = null; if(counter[j]!=null) &#123; while ((value = counter[j].shift()) != null) &#123; arr[pos++] = value; &#125; &#125; &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526public static void sort(int[] number, int d) //d表示最大的数有多少位 &#123; int k = 0; int n = 1; int m = 1; //控制键值排序依据在哪一位 int[][] temp = new int[10][number.length]; //数组的第一维表示可能的余数0-9 int[] order = new int[10]; //数组orderp[i]用来表示该位是i的数的个数 while (m &lt;= d) &#123; for (int i = 0; i &lt; number.length; i++) &#123; int lsd = ((number[i] / n) % 10); temp[lsd][order[lsd]] = number[i]; order[lsd]++; &#125; for (int i = 0; i &lt; 10; i++) &#123; if (order[i] != 0) for (int j = 0; j &lt; order[i]; j++) &#123; number[k] = temp[i][j]; k++; &#125; order[i] = 0; &#125; n *= 10; k = 0; m++; &#125; &#125; 算法分析基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n&gt;&gt;k，因此额外空间需要大概n个左右。 测试新建Test类，测试十种排序算法时间消耗。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119 public static void main(String[] args) &#123; // int arrayLength=10; // int range=100; int arrayLength=100000; int range=100000; //生成一个指定长度数据的int数组 int[] a=new int[arrayLength]; for (int i=0;i&lt;arrayLength;i++)&#123; a[i]=(int)(Math.random()*range); &#125; //创建十个数组用于测试，否则会冲突 int[] a1= a.clone(); int[] a2= a.clone(); int[] a3= a.clone(); int[] a4= a.clone(); int[] a5= a.clone(); int[] a6= a.clone(); int[] a7= a.clone(); int[] a8= a.clone(); int[] a9= a.clone(); int[] a10= a.clone(); System.out.println(a1.toString()); System.out.println(a2.toString()); System.out.println(a3.toString()); System.out.println(a4.toString()); System.out.println(a5.toString()); System.out.println(a6.toString()); System.out.println(a7.toString()); System.out.println(a8.toString()); System.out.println(a9.toString()); System.out.println(a10.toString()); System.gc(); //直接插入排序法 long starta1=System.currentTimeMillis(); a1= StraightInsertionSort.straightInsertionSort(a1); long enda1=System.currentTimeMillis(); System.out.println("插入排序法耗时："+(enda1-starta1)+"ms"); //冒泡排序法 long starta2=System.currentTimeMillis(); a2= BubbleSort.bubbleSort(a2); long enda2=System.currentTimeMillis(); System.out.println("冒泡排序法耗时："+(enda2-starta2)+"ms"); //直接(简单)选择排序法 long starta3=System.currentTimeMillis(); a3= SimpleSelectionSort.simpleSelectSort(a3); long enda3=System.currentTimeMillis(); System.out.println("直接选择排序法耗时："+(enda3-starta3)+"ms"); //堆排序 long starta4=System.currentTimeMillis(); a4= HeapSort.heapSort(a4); long enda4=System.currentTimeMillis(); System.out.println("堆排序法耗时："+(enda4-starta4)+"ms"); //归并排序 long starta5=System.currentTimeMillis(); a5= MergeSort.mergeSort(a5); long enda5=System.currentTimeMillis(); System.out.println("归并排序法耗时："+(enda5-starta5)+"ms"); //快速排序算法 long starta6=System.currentTimeMillis(); a6= QuickSort.quickSort(a6,0,a6.length-1); long enda6=System.currentTimeMillis(); System.out.println("快速排序耗时："+(enda6-starta6)+"ms"); //计数排序算法 long starta7=System.currentTimeMillis(); a7= CountingSort.countSort(a7); long enda7=System.currentTimeMillis(); System.out.println("计数排序耗时："+(enda7-starta7)+"ms"); //希尔排序算法 long starta8=System.currentTimeMillis(); a8= ShellsSort.shellsSort(a8); long enda8=System.currentTimeMillis(); System.out.println("希尔排序耗时："+(enda8-starta8)+"ms"); //桶排序算法 long starta9=System.currentTimeMillis(); a9= BucketSort.basket(a9); long enda9=System.currentTimeMillis(); System.out.println("桶排序耗时："+(enda9-starta9)+"ms"); //基数排序算法 long starta10=System.currentTimeMillis(); a10= RadixSort.sort(a10,6); long enda10=System.currentTimeMillis(); System.out.println("基数排序耗时："+(enda10-starta10)+"ms"); // ArrayHelper.printLine(a1); // System.out.println(); // ArrayHelper.printLine(a2); // System.out.println(); // ArrayHelper.printLine(a3); // System.out.println(); // ArrayHelper.printLine(a4); // System.out.println(); // ArrayHelper.printLine(a5); // System.out.println(); // ArrayHelper.printLine(a6); // System.out.println(); // ArrayHelper.printLine(a7); // System.out.println(); // ArrayHelper.printLine(a8); // System.out.println(); // ArrayHelper.printLine(a9); // System.out.println(); // ArrayHelper.printLine(a10); System.gc(); &#125; 运行结果： 其他 本文为转载，十种算法的Java实现方式及测试为原创。 本文转载自：https://www.cnblogs.com/onepixel/articles/7674659.html 文中涉及到的Java代码地址：https://github.com/JavaZWT/sort]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之代理模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F20180602%2F</url>
    <content type="text"><![CDATA[前言代理模式：即通过代理对象访问目标对象，实现目标对象的方法。这样做的好处是：可以在目标对象实现的基础上，增强额外的功能操作，实现对目标功能的扩展。 这涉及到一个编程思想：不要随意去修改别人已经写好的代码或者方法（有坑）。如果需要修改，可以通过代理模式实现。 写法实现代理模式通常有三种实现写法：静态代理、动态代理、Cglib代理。 我们依次说下这三种代理模式： 我们现有 AnimalsDao接口和其实现AnimalsDaoImpl实现类，有两个方法，run和eat。 12345public interface AnimalsDao &#123; //提供两个方法 void run(); void eat(); &#125; 12345678910public class AnimalsDaoImpl implements AnimalsDao&#123; @Override public void run() &#123; System.out.println("run"); &#125; @Override public void eat() &#123; System.out.println("eat"); &#125; &#125; 现在我们想在run或者eat方法里之前做一些操作。 静态代理12345678910111213141516171819public class StaticProxy implements AnimalsDao&#123; @Override public void run() &#123; System.out.println("StaticProxy---------&gt;"); dao.run(); &#125; @Override public void eat() &#123; System.out.println("StaticProxy---------&gt;"); dao.eat(); &#125; private AnimalsDao dao; public StaticProxy(AnimalsDao dao)&#123; this.dao=dao; &#125; &#125; 说明：静态代理通过实现目标对象接口，然后调用相同方法来实现代理。这种方式的缺点显而易见，当目标对象接口方法变动时，直接影响到代理类，需要对代理类进行修改，十分不方便。而且如果目标对象接口方法较多时，代理类也十分臃肿，不便维护。 动态代理1234567891011121314151617181920public class DynamicProxy &#123; //要代理的对象 private Object obj; public DynamicProxy(Object obj)&#123; this.obj=obj; &#125; public Object getObjProxyIntance()&#123; return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //执行目标对象方法 System.out.println("DynamicProxy---------&gt;"); return method.invoke(obj,args); &#125; &#125;); &#125; &#125; 说明：动态代理模式主要借助JDK代理对象API java.lang.reflect.Proxy来实现的，所以也称作JDK代理。我们看一下JDK这个类，其中重要的一个方法如下： 这个方法的三个参数： ClassLoader loader 目标对象类加载器 Class&lt;?&gt;[] interfaces 目标对象接口类型 InvocationHandler h 事物处理，在这里面可以实现自己想要的逻辑 根据以上，可以看出动态代理实现要求目标对象必须有实现接口。代理类不必实现接口。 Cglib代理要实现Cglib代理，必须引入cglib.jar 包，由于Spring-core包中已经包含了cglib功能，且大部分Java项目均引入了spring 相关jar包，这边使用spring的cglib来讲解。（他俩实现方式都是一样的） 1234567891011121314151617181920212223242526public class CglibProxy implements MethodInterceptor &#123; //目标对象 private Object obj; public CglibProxy(Object obj)&#123; this.obj=obj; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(obj.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("CglibProxy---------&gt;"); return method.invoke(obj,objects); &#125; &#125; 说明：可以看出，Cglib代理模式实现不需要目标对象一定实现接口，故目标对象如果没有实现接口，可以使用cglib代理模式。其实Spring的代理模式也是这么实现的。 应用实例Spring的代理模式：如果容器目标对象有接口，使用JDK动态代理，如果没有接口，使用cglib动态代理。 参考资料 https://www.cnblogs.com/cenyu/p/6289209.html Spring 4.0 源代码 GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之工厂模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F20180602%2F</url>
    <content type="text"><![CDATA[前言工厂模式，顾名思义，即用来实例化一个个对象的模式。把相同类别的对象用工厂模式代替new生成，提高代码质量和系统扩展性。 写法分类 工厂模式的写法通常分为四类，简单工厂、多方法工厂、普通工厂、抽象工厂。 在分别说明他们之前，我们先创建几个对象。 抽象的Animals类，Plants类 1234public abstract class Animals &#123; //动物的描述 public abstract String desc(); &#125; 及Animals的三个基本实现，Tiger类、Bird类、Fish类，Plants的实现Tree类。 1234567public class Tiger extends Animals&#123; @Override public String desc() &#123; System.out.println("这是一只老虎"); return "这是一只老虎"; &#125; &#125; 1234567public class Bird extends Animals &#123; @Override public String desc() &#123; System.out.println("这是一只鸟"); return "这是一只鸟"; &#125; &#125; 1234567public class Fish extends Animals &#123; @Override public String desc() &#123; System.out.println("这是一条鱼"); return "这是一条鱼"; &#125; &#125; 1234public abstract class Plants &#123; //植物的描述 public abstract String desc(); &#125; 123456public class Tree extends Plants &#123; @Override public String desc() &#123; return "这是一棵树"; &#125; &#125; 下面来说说这四种工厂模式。 简单工厂模式1234567891011121314151617 public class AnimalsFactory &#123; public static final String Tiger="Tiger"; public static final String Bird="Bird"; public static final String Fish="Fish"; public static Animals getAnimal(String name)&#123; switch (name)&#123; case Tiger: return new Tiger(); case Bird: return new Bird(); case Fish: return new Fish(); default: return null; &#125; &#125; &#125; 调用：12Animals a=AnimalsFactory.getAnimal("Tiger"); a.desc(); 说明：简单工厂模式易于理解，但扩展性差，如我想在增加一种动物，则还需要修改逻辑内容。而且如果设置额外入参，对参数的管理也较为不便。 多方法工厂模式1234567891011public class AnimalsFactory &#123; public static Animals getTiger()&#123; return new Tiger(); &#125; public static Animals getBird()&#123; return new Bird(); &#125; public static Animals getFish()&#123; return new Fish(); &#125; &#125; 调用：12Animals b= AnimalsFactory.getFish(); b.desc(); 说明：这种模式使用方便，容错率高，且可适应不同需求，如我现在想创建一个羽毛为蓝色的小鸟，明显多方法工厂模式比简单工厂模式实现简单。 普通工厂模式123public abstract class PlantsFactory &#123; public abstract Plants getPlant(); &#125; 123456public class TreeFactory extends PlantsFactory&#123; @Override public Plants getPlant() &#123; return new Tree(); &#125; &#125; 调用：12PlantsFactory p=new TreeFactory(); p.getPlant().desc(); 说明：这种工厂模式划分成两层：抽象工厂层+具体的工厂子类层。 当我们不需要Tree，或者需要其他时，都可以新增或删除一个类实现，这种工厂模式看着很简洁明了易于理解。 抽象工厂模式12345678910public abstract class AbstractFactory&lt;T&gt; &#123; static&#123; // do something &#125; public void doSomeThing()&#123; //do something &#125; private int age; public abstract T getT(); &#125; 123456public class BirdFactory extends AbstractFactory&lt;Animals&gt; &#123; @Override public Animals getT() &#123; return new Bird(); &#125; &#125; 123456public class TreeFactory extends AbstractFactory&lt;Plants&gt; &#123; @Override public Plants getT() &#123; return new Tree(); &#125; &#125; 调用：123AbstractFactory factory= new TreeFactory(); Tree t=(Tree)factory.getT(); t.desc(); 说明：可以看出，这种方法把工厂本身也抽象了，如果对于两个或多个简单工厂模式，相似点较多，比如PlantsFactory和AnimalsFactory，作为生物，它们也有相似之处，dosomething部分的代码。其实抽象工厂模式跟普通工厂模式的区别就在于把工厂也抽象了一层。 工厂模式实例 Spring Bean对象的创建 抽象工厂模式的使用 线程池 Executors 多方法工厂模式的使用 其他：Spring框架中大量使用了工厂模式，单例模式等设计模式，有时间阅读些源代码，对技术能力的提升有很大帮助。 参考资料 https://blog.csdn.net/zxt0601/article/details/52798423 Spring 4.0 源码 JDK8源码 GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之单例模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F20180602%2F</url>
    <content type="text"><![CDATA[前言单例模式，顾名思义，即只产生唯一实例的设计模式。一个类只有一个实例,即一个类只有一个对象实例。 优点1. 减少系统开销及资源消耗。 2. 设置全局访问点，优化共享资源的使用。 注：对于以上优点，我们可以拿Java配置读取举例，对于固定的配置文件，我们完全可以设计一个单例类，读取配置文件，而避免每次都读取文件造成的系统开销，同时也优化了配置文件的使用。 应用场景 Servlet 开源框架Spring中的默认bean配置模式 数据库连接池 项目中对于配置文件的处理等等 写法分类懒汉式单例模式（非线程安全）123456789101112public class Demo_Singleton_01 &#123; private static Demo_Singleton_01 instance=null; private Demo_Singleton_01()&#123; &#125; public static Demo_Singleton_01 getInstance()&#123; if(instance==null)&#123; instance=new Demo_Singleton_01(); &#125; return instance; &#125; &#125; 说明：这种写法不常用，虽然实现了懒加载（lazy-loading），但未考虑线程安全问题，不适用于多线程情况。 懒汉式单例模式（线程安全）123456789101112public class Demo_Singleton_02 &#123; private static Demo_Singleton_02 instance=null; public static synchronized Demo_Singleton_02 getInstance()&#123; if(instance==null)&#123; instance=new Demo_Singleton_02(); &#125; return instance; &#125; private Demo_Singleton_02()&#123; &#125; &#125; 说明：相对于demo1，这种写法实现了懒加载（lazy-loading），也考虑到线程安全问题，可适用于多线程情况。 饿汉式单例模式123456789public class Demo_Singleton_03 &#123; private static final Demo_Singleton_03 instance=new Demo_Singleton_03(); private Demo_Singleton_03()&#123; &#125; public static Demo_Singleton_03 getInstance()&#123; return instance; &#125; &#125; 说明：饿汉式单例模式的明显缺点是无法实现懒加载，在类被创建后就创建了唯一实例。 饿汉式单例模式（变种）123456789101112public class Demo_Singleton_04 &#123; private static Demo_Singleton_04 instance=null; static&#123; instance=new Demo_Singleton_04(); &#125; private Demo_Singleton_04()&#123; &#125; public static Demo_Singleton_04 getInstance()&#123; return instance; &#125; &#125; 说明：可以看到这种方法和demo3的性质是一样的，只不过改为了静态块加载唯一实例。 静态内部类模式1234567891011public class Demo_Singleton_05 &#123; private static class Singleton&#123; private static final Demo_Singleton_05 instance=new Demo_Singleton_05(); &#125; private Demo_Singleton_05()&#123; &#125; public static final Demo_Singleton_05 getInstance()&#123; return Singleton.instance; &#125; &#125; 说明：这种方法既可以实现懒加载，也能保证线程安全。 双重锁单例模式12345678910111213141516 public class Demo_Singleton_06 &#123; private static volatile Demo_Singleton_06 instance=null; private Demo_Singleton_06()&#123; &#125; public static Demo_Singleton_06 getInstance()&#123; if(instance==null)&#123; synchronized(Demo_Singleton_06.class)&#123; if(instance==null)&#123; instance=new Demo_Singleton_06(); &#125; &#125; &#125; return instance; &#125; &#125; 说明：可以看出，双重锁单例模式只有在第一次创建对象的时候才同步，因为创建对象后就没必要了，（实例唯一）相比懒汉式，效率明显提高，相比饿汉式，不用在类加载的时候就创建实例，而是等到什么时候想调用的时候在创建实例。线程安全，效率高。 枚举法123456public enum Demo_Singleton_07 &#123; instance; public void doSomething()&#123; &#125; &#125; 说明：枚举法代码十分直观，线程安全，但是无法实现懒加载。 参考资料 https://blog.csdn.net/goodlixueyong/article/details/51935526 https://www.cnblogs.com/Ycheng/p/7169381.html GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2Fhello-world20180531%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>学习</category>
      </categories>
  </entry>
</search>
