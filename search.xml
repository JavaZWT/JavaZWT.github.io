<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（4）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%EF%BC%884%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言接 搭建自己的Hexo博客（3）这篇文章，最近又对博客进行了一些简单优化，特地分享下。 正文音乐问题原来是直接使用的网易云音乐外链，考虑到音乐版权问题，可能以后外链失效影响到音乐播放。 故我将要使用的音乐文件迁移到了云服务器上，同时我们使用一款Hexo的音乐播放插件hexo-tag-aplayer。 我们直接在package.json文件里添加hexo-tag-aplayer并使用npm install命令进行安装。 在 themes/next/layout/_custom/sidebar.swig文件里，我们原来使用的网易云外链，注释掉。 123456&lt;!--网易云音乐外链，已废弃--&gt;&lt;!--&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=509313150&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=34723470&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=640866&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=35345243&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=70 src="//music.163.com/outchain/player?type=2&amp;id=512733081&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;--&gt; 使用如下Aplayer相关加载代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;!--基于hexo-tag-aplayer的音乐播放器--&gt;&lt;div id="aplayer-rWUYQYrx" class="aplayer aplayer-tag-marker" style="margin: 30px 0px 30px 0px;"&gt;&lt;/div&gt;&lt;script&gt; var options = &#123; "narrow": false,//样式 "autoplay": false,//是否自动播放 "showlrc": 0,//是否显示歌词，需要有歌词文件 "mutex": true,//true的话播放时会暂停其它播放器 "theme": "#e6d0b2",//主题 "preload": "none", //预加载模式 "listmaxheight": "500px",//列表最大高度 "music": [ &#123; "title": "Sakura Tears",//显示的音乐名 "author": "Nigel Silin",//作家 "url": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/Nigel%20Silin%20-%20Sakura%20Tears.mp3",//音乐文件路径 "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music1.jpeg"//音乐图片 &#125;, &#123; "title": "东京不太热", "author": "封茗囧菌", "url": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E5%B0%81%E8%8C%97%E5%9B%A7%E8%8F%8C%20-%20%E4%B8%9C%E4%BA%AC%E4%B8%8D%E5%A4%AA%E7%83%AD.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music2.jpeg" &#125;, &#123; "title": "春风吹", "author": "锦零", "url": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E9%94%A6%E9%9B%B6%20-%20%E6%98%A5%E9%A3%8E%E5%90%B9%EF%BC%88Cover%20%E6%96%B9%E5%A4%A7%E5%90%8C%EF%BC%89.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music3.jpeg" &#125;, &#123; "title": "Secret", "author": "茶太", "url": "https://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E8%8C%B6%E5%A4%AA%20-%20Secret.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music4.jpeg" &#125;, &#123; "title": "secret base ~君がくれたもの~", "author": "茅野愛衣,戸松遥,早見沙織", "url": "https://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/%E8%8C%85%E9%87%8E%E6%84%9B%E8%A1%A3%2C%E6%88%B8%E6%9D%BE%E9%81%A5%2C%E6%97%A9%E8%A6%8B%E6%B2%99%E7%B9%94%20-%20secret%20base%20%7E%E5%90%9B%E3%81%8B%E3%82%99%E3%81%8F%E3%82%8C%E3%81%9F%E3%82%82%E3%81%AE%7E%20%2810%20years%20after%20Ver.%29.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music5.jpeg" &#125;, &#123; "title": "ファンファーレ", "author": "sumika", "url": "https://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/sumika%20-%20%E3%83%95%E3%82%A1%E3%83%B3%E3%83%95%E3%82%A1%E3%83%BC%E3%83%AC.mp3", "pic": "http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/music/music6.jpeg" &#125; ]&#125;; options.element = document.getElementById("aplayer-rWUYQYrx"); var ap = new APlayer(options); window.aplayers || (window.aplayers = []); window.aplayers.push(ap);&lt;/script&gt; 这样我们部署后会看到音乐栏如下图所示，也是比较美观的。 我们使用hexo-tag-aplayer还有一个好处，当我们想向一些文章里添加音乐时，只需在文章markdown文章中添加如下代码即可。 1&#123;% aplayer "Caffeine" "Jeff Williams" "caffeine.mp3" "picture.jpg" "lrc:caffeine.txt" %&#125; hexo-tag-aplayer是基于APlayer而构建的，有兴趣的同学可以看下APlayer，一款非常漂亮的H5音乐播放器。 相册问题上篇文章简单制作了一个相册，但是不是很满意，最近又对相册做了些优化。大致想法如下： 相册页应该支持Tab切换，可以按照不同类别进行区分 原来瀑布流下拉加载有些问题，需要修复下 照片处理时手动操作步骤应该尽量少且简单 根据以上问题，优化后的相册如下动图，可以看到更美观实用一些了。 我简单把自己改动的代码分享下。 photo 文件夹下的index.md文件如下：1&lt;div id = "ImageGrid" class="photo-record"&gt;&lt;div class="tab"&gt;&lt;div class="tab-item active"&gt;动漫&lt;/div&gt;&lt;div class="tab-item"&gt;风景&lt;/div&gt;&lt;div class="tab-item"&gt;TODO&lt;/div&gt;&lt;/div&gt;&lt;div class="photoContent"&gt;&lt;ul class="img-box-ul1 mainCont selected"&gt;&lt;/ul&gt;&lt;ul class="img-box-ul2 mainCont"&gt;&lt;/ul&gt;&lt;ul class="img-box-ul3 mainCont"&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style="height:100px"&gt;&lt;/div&gt; 样式如下，放在custom.styl里。 1234567891011121314151617181920212223242526.photo-record&#123; width: 100%;&#125;.photo-record .tab&#123; height:30px; line-height:30px;&#125;.photo-record .tab .tab-item&#123; display:inline-block; width:60px; text-align:center; font-size:15px;&#125;.photo-record .tab .tab-item.active&#123; color: #ff9a22; border-bottom: 3px solid #ffc472;&#125;.photoContent .mainCont &#123; display: none; width:100%; overflow: auto; text-align: center;&#125;.photoContent .mainCont.selected &#123; display: block;&#125; photos.js的改动较大，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980var photo = function() &#123; this.page = 1, this.offset = 10, this.flag = false, this.init = function(index) &#123; if (this.flag) &#123; return; &#125; this.flag = true; var that = this; $.getJSON("../js/src/photos.json", function(data) &#123; for (var i = 0; i &lt; data.length; i++) &#123; if (i + 1 === index) &#123; that.render(that.page, data[i], index); that.scroll(data[i], index); &#125; &#125; &#125;); &#125;, this.render = function(page, data, index) &#123; var begin = (page - 1) * this.offset; var end = page * this.offset; if (begin &gt;= data.length) return; var imgNameWithPattern, imgName, imageSize, imageX, imageY, li = ""; for (var i = begin; i &lt; end &amp;&amp; i &lt; data.length; i++) &#123; imgNameWithPattern = data[i].split(' ')[1]; imgName = imgNameWithPattern.split('.')[0] imageSize = data[i].split(' ')[0]; imageX = imageSize.split('.')[0]; imageY = imageSize.split('.')[1]; li += '&lt;div class="card" style="width:330px"&gt;' + '&lt;div class="img-box" style="height:' + 330 * imageY / imageX + 'px"&gt;' + '&lt;a data-fancybox="gallery" class="fancybox fancybox.image" href="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + index + "/" + imgNameWithPattern + '?raw=true" data-caption="' + imgName + '"&gt;' + '&lt;img src="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + index + "/" + imgNameWithPattern + '?raw=true"/&gt;' + '&lt;/a&gt;' + '&lt;/div&gt;' + '&lt;/div&gt;' &#125; var box = $(".img-box-ul" + index); box.append(li); box.lazyload(); this.minigrid(index); &#125;, this.minigrid = function(index) &#123; var grid = new Minigrid(&#123; container: ".img-box-ul" + index, item: '.card', gutter: 12 &#125;); grid.mount(); // $(window).resize(function() &#123; // grid.mount(); // &#125;); &#125;, this.scroll = function(data, index) &#123; var that = this; $(window).scroll(function() &#123; var windowPageYOffset = window.pageYOffset; var windowPageYOffsetAddHeight = windowPageYOffset + window.innerHeight; var sensitivity = 0; var grid = $("#ImageGrid"); var offsetTop = grid.offset().top + grid.height(); if (offsetTop &gt;= windowPageYOffset &amp;&amp; offsetTop &lt; windowPageYOffsetAddHeight + sensitivity) &#123; that.render(++that.page, data, index); &#125; &#125;) &#125;&#125;;(function() &#123; $(".photo-record .tab .tab-item").each(function (i,e) &#123; var tempPhoto = new photo(); $(e).click(function()&#123; $(this).addClass("active").siblings().removeClass("active"); $(".photo-record .photoContent .mainCont").eq(i).addClass("selected").siblings().removeClass("selected"); tempPhoto.init(i + 1); &#125;); &#125;); $(".photo-record .tab .tab-item.active").click();&#125;)(); 从上面的photos.js我们可以看到photos.json应该为下面这种格式。 123456789101112131415161718192021222324252627282930313233[ [ "3200.2000 photo-1-10.jpg", "1920.1080 photo-1-1.jpg", "1920.1080 photo-1-12.jpg", "1920.1200 photo-1-11.jpg", "1920.1200 photo-1-13.jpg", "1920.1080 photo-1-14.jpg", "1920.1200 photo-1-16.jpg", "1920.1080 photo-1-15.jpg", "1920.1186 photo-1-18.jpg", "1920.1080 photo-1-19.jpg", "1920.1080 photo-1-17.jpg", "1024.610 photo-1-2.jpg", "3160.2560 photo-1-20.jpg", "2560.1440 photo-1-21.jpg", "1920.1080 photo-1-22.jpg", "3139.2203 photo-1-23.jpg", "1024.1024 photo-1-24.jpeg", "2200.1400 photo-1-3.jpg", "1200.750 photo-1-4.jpg", "3500.1898 photo-1-5.jpg", "1080.1920 photo-1-6.jpg", "1600.1120 photo-1-7.jpg", "1728.1080 photo-1-8.jpg", "1920.1080 photo-1-9.jpg" ], [ "3968.2976 photo-2-1.jpg", "2976.3968 photo-2-2.jpg" ], []] 这就需要我们使用photosTools.js来进行生成了，原来的这个文件是不满足要求的，我们改造如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748"use strict";const fs = require("fs");const sizeOf = require('image-size');const path = "source/images_backup/photos";const output = "themes/next/source/js/src/photos.json";var dimensions;fs.readdir(path, function(err, dirents) &#123; if (err) &#123; return; &#125; let arr = []; for (let i = 0; i &lt; dirents.length; i++) &#123; console.log(dirents); if(dirents[i]===".DS_Store")&#123; continue; &#125; fs.readdir(path + "/" + dirents[i], function(err, files) &#123; console.log(files); let tempArr = []; for (let j = 0; j &lt; files.length; j++) &#123; fs.stat(path + "/" + dirents[i] + "/" + files[j], function(err, stats) &#123; if (!files[j].endsWith(".jpg") || files[j].endsWith(".jpeg")) &#123; &#125; if (err) &#123; return; &#125; if (stats.isFile()) &#123; if (files[j].endsWith(".jpg") || files[j].endsWith(".jpeg") || files[j].endsWith(".mp4") || files[j].endsWith(".png") || files[j].endsWith(".gif")) &#123; dimensions = sizeOf(path + "/" + dirents[i] + "/" + files[j]); console.log(dimensions.width, dimensions.height); tempArr.push(dimensions.width + '.' + dimensions.height + ' ' + files[j]); &#125; &#125; &#125;) &#125; arr.push(tempArr); &#125;); if (i === dirents.length - 1) &#123; setTimeout(function() &#123; fs.writeFile(output, JSON.stringify(arr, null, "\t"), function() &#123;&#125;); &#125;, 10000); return; &#125; &#125;&#125;); 同时原来的photos图片文件夹里，应该使用1，2，3…等文件夹，并将图片放入到这些文件夹中，至于1，2，3…和图片类型的对应关系，就需要自己配置就行了。 以上就是新的图片标签页面的一些设计。 可以看到我们创建了3个Tab页切换（如需更多，可以继续进行配置），每个Tab页对应一个photo对象，该对象会加载并渲染图片信息，每次加载10个，使用向下滚动我们可以加载更多，并使用fancbox美化图片样式。 由于需要知道图片宽高来美化图片样式，因此我们使用了NodeJs的fs和imagesize模块，用来读取文件并生成photos.json文件。 PS：弄完相册后，突然想到了视频相关的一些展示，这个在后面我会考虑美观等特性，考虑是否为博客添加视频功能，及添加的位置等一些要素。 屏蔽相关我们为博客添加一些屏蔽功能，先上代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138/** * 屏蔽f12 审查元素 */document.onkeydown = function()&#123; if(window.event)&#123; var e = window.event; if(e.keyCode == 123) &#123; console.log("禁用F12键"); e.keyCode=0; e.returnValue=false; &#125; // if(e.keyCode == 13) &#123; // console.log("禁用Enter键"); // e.keyCode = 0; // e.returnValue=false; // &#125; // if(e.keyCode == 8) &#123; // console.log("禁用Backspace键"); // e.returnValue=false; // &#125; var ctrlKey = e.ctrlKey || e.metaKey; var shiftKey = e.shiftKey; if(ctrlKey &amp;&amp; shiftKey &amp;&amp; e.keyCode == 73)&#123; console.log("禁用Ctrl+Shift+I键"); e.keyCode=0; e.returnValue=false; &#125; &#125;&#125;/** * 屏蔽右键菜单 * @param event * @returns &#123;boolean&#125; */document.oncontextmenu = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if (!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽粘贴 * @param event * @returns &#123;boolean&#125; */document.onpaste = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if (!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽复制 * @param event * @returns &#123;boolean&#125; */document.oncopy = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if(!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽剪切 * @param event * @returns &#123;boolean&#125; */document.oncut = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if(!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125;catch (e)&#123; return false; &#125;&#125;/** * 屏蔽选中 * @param event * @returns &#123;boolean&#125; */document.onselectstart = function (event)&#123; if(window.event)&#123; event = window.event; &#125;try&#123; var the = event.srcElement; if (!((the.tagName == "INPUT" &amp;&amp; the.type.toLowerCase() == "text") || the.tagName == "TEXTAREA"))&#123; return false; &#125; return true; &#125; catch (e) &#123; return false; &#125;&#125;;/** * 尝试阻止开发者模式 * @type &#123;HTMLElement&#125; */var fuck = document.createElement('div');Object.defineProperty(fuck, 'id', &#123; get:function()&#123; console.log("Hello World!"); location.href="about:blank"; history.replaceState(); &#125;&#125;);console.log(fuck); 我们将上述代码引入到博客代码中，就会欣然发现： 在博客页面上： 鼠标右键审查元素失效 F12失效，Chrome浏览器下Ctrl+Shift+I启动开发者模式快捷键失效 复制、粘贴、剪贴、鼠标选中元素失效 开发者模式和博客页面是无法共存的 当然这并不能阻止大家获取到网站运行的代码，因为js、html或者图片等资源，一旦被加载到客户端，客户端就已经拿到代码了。 然而这仍然是有趣的一件事情。 我们这里主要就是监听一些键盘事件来对某些按键行为进行阻止，对于屏蔽了开发者模式，主要和Object.defineProperty这个属性相关，但这个属性一些低版本浏览器是不支持的。 总结今天到这里就差不多了，我们对博客进行了一些优化操作，及简单介绍了如何进行这些操作，接下来我仍将去探寻博客一些可以优化的地方。 我的博客地址：https://www.sakuratears.top]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于.DS_Store文件的一些问题]]></title>
    <url>%2Fblog%2F%E5%85%B3%E4%BA%8E-DS-Store%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言之前在Mac电脑上拷贝文件到Windows电脑时，文件夹里会额外多出文件，就是我们今天要说的.DS_Store文件，当时并没有在意。 昨天在写一个文件流读取功能时，解析一直出问题，后面发现是文件夹里多了.DS_Store文件，导致文件遍历解析出了些问题，过程我就不再详述。 因此也对这个.DS_Store文件产生了一些兴趣，特地分享记录下。 正文简介.DS_Store，全称 Desktop Services Store，是Mac OS中保存文件夹自定义属性的隐藏文件，目的在于存储文件夹的自定义属性，例如文件图标位置、视图设置，或背景色等，相当于Windows下的 desktop.ini。.DS_Store 默认放在每个文件夹的下面。 解析当我们尝试用文本打开时可以发现它是乱码的，我们接下来尝试解读一下它。 我们使用Notepad++，在其 插件-插件管理 添加Hex-Editor插件，安装好此插件后Notepad++可以以十六进制形式打开指定文件。 我们使用Notepad++借助此插件可以打开.DS_Store，可以看到它的十六进制格式。 当然我们也可以使用VSCode，安装hexdump for vscode插件，来显示十六进制格式。 两种方式插件的安装过程略，有兴趣的可以查看相关文章等。 这儿我们使用VSCode结合插件来打开该文件，如下图，我们尝试解析一下该十六进制数据。 关于该文件二进制的结构说明我们可以参考这篇文章 解析.DS_Store文件格式。 .DS_Store文件转为二进制树后，根据上面这篇文章，它的结构大致如下： 文件头部（Header） 根块 偏移部分（Offsets） 内容表（Toc） 空闲表（FreeList） 文件头部： 通常文件头部用来进行校验，判断这个文件是不是.DS_Store文件（详见代码readHeader方法）。 偏移部分： 偏移部分记录了有关文件中树(叶)块的偏移量信息，这些块存储的都是目录的实际信息，如文件名。获得偏移量需要遍历这个树（详见代码readOffsets方法）。 内容表： 在偏移部分结束后，内容表部分就会呈现出来。他通常存在只有一个名为DSDB的表,并且值为1。这个特殊的表通常引用了我们将要遍历的第一个块的ID（详见代码readTOC方法）。 空闲表： 最后一部分是空闲表，也就是在树中还有哪些地方是没有使用的或者是空闲的模块（详见代码readFreelist方法）。 然后我们使用Java来解析下这个二进制结构数据。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484public class DataBlock &#123; private byte[] data; private int pos; private boolean debug; /** * Returns an byte array of length from data at the given offset or pos. * If offset==0 (no offset is given) , pos will be increased by length. * Throws Exception if offset+length &gt; this.data.length * @Params: [length, offset] * @Return: byte[] */ public byte[] offsetRead(int length,int offset)&#123; int offsetPosition; if(offset==0)&#123; offsetPosition = this.pos; &#125;else&#123; offsetPosition = offset; &#125; if(this.data.length &lt; offsetPosition +length)&#123; throw new RuntimeException("Offset+Length &gt; this.data.length"); &#125; if(offset==0)&#123; this.pos+=length; &#125; byte[] value =new byte[length]; System.arraycopy(this.data,offsetPosition,value,0,length); if(debug)&#123; System.out.println(String.format("Reading: %s-%s =&gt; %s",offsetPosition, offsetPosition+length, value)); &#125; return value; &#125; /** * Increases pos by length without reading data! * @Params: [length] * @Return: void */ public void skip(int length)&#123; this.pos+=length; &#125; /** * Extracts a file name from the current position. * @Params: [] * @Return: java.lang.String */ public String readFileName()&#123; //The length of the file name in bytes. int length = ByteBuffer.wrap(offsetRead(4,0)).getInt(); //The file name in UTF-16, which is two bytes per character. String fileName = new String(offsetRead(2 * length,0), StandardCharsets.UTF_16BE); //A structure ID that I haven't found any use of. int structureId = ByteBuffer.wrap(offsetRead(4,0)).getInt(); //Now read the structure type as a string of four characters and decode it to ascii. String structureType = new String(offsetRead(4,0), StandardCharsets.US_ASCII); if(debug)&#123; System.out.println("Structure type "+ structureType); &#125; //If we don't find a match, skip stays &lt; 0 and we will do some magic to find the right skip due to somehow broken .DS_Store files.. int skip = -1; //Source: http://search.cpan.org/~wiml/Mac-Finder-DSStore/DSStoreFormat.pod while (skip &lt; 0)&#123; if(structureType.equals("bool"))&#123; skip = 1; &#125;else if(structureType.equals("type") || structureType.equals("long") || structureType.equals("shor") || structureType.equals("fwsw") || structureType.equals("fwvh") || structureType.equals("icvt") || structureType.equals("lsvt") || structureType.equals("vSrn") || structureType.equals("vstl"))&#123; skip = 4; &#125;else if(structureType.equals("comp") || structureType.equals("dutc") || structureType.equals("icgo") || structureType.equals("icsp") || structureType.equals("logS") || structureType.equals("lg1S") || structureType.equals("lssp") || structureType.equals("modD") || structureType.equals("moDD") || structureType.equals("phyS") || structureType.equals("ph1S"))&#123; skip = 8; &#125;else if(structureType.equals("blob"))&#123; skip = ByteBuffer.wrap(offsetRead(4,0)).getInt(); &#125;else if(structureType.equals("ustr") || structureType.equals("cmmt") || structureType.equals("extn") || structureType.equals("GRP0"))&#123; skip = 2 * ByteBuffer.wrap(offsetRead(4,0)).getInt(); &#125;else if(structureType.equals("BKGD"))&#123; skip = 12; &#125;else if(structureType.equals("ICVO") || structureType.equals("LSVO") || structureType.equals("dscl"))&#123; skip = 1; &#125;else if(structureType.equals("Iloc") || structureType.equals("fwi0"))&#123; skip = 16; &#125;else if(structureType.equals("dilc"))&#123; skip = 32; &#125;else if(structureType.equals("lsvo"))&#123; skip = 76; &#125;else if(structureType.equals("icvo"))&#123; &#125;else if(structureType.equals("info"))&#123; &#125;else &#123; &#125; if(skip &lt;= 0)&#123; //We somehow didn't find a matching type. Maybe this file name's length value is broken. Try to fix it! //This is a bit voodoo and probably not the nicest way. Beware, there by dragons! if(debug)&#123; System.out.println("Re-reading!"); &#125; // Rewind 8 bytes, so that we can re-read structure_id and structure_type skip(-1 * 2 * 0x4); fileName = new String(offsetRead(0x2,0), StandardCharsets.UTF_16BE); //re-read structure_id and structure_type structureId = ByteBuffer.wrap(offsetRead(4,0)).getInt(); structureType = new String(offsetRead(4,0), StandardCharsets.US_ASCII); //Look-ahead and check if we have structure_type==Iloc followed by blob. //If so, we're interested in blob, not Iloc. Otherwise continue! String futureStructureType = new String(offsetRead(4,this.pos), StandardCharsets.US_ASCII); if(debug)&#123; System.out.println(String.format("Re-read structure_id %s / structure_type %s",structureId, structureType)); &#125; if ((!structureType.equals("blob")) &amp;&amp; (!futureStructureType.equals("blob")))&#123; structureType = ""; if(debug)&#123; System.out.println("Forcing another round!"); &#125; &#125; &#125; &#125; // Skip bytes until the next (file name) block skip(skip); if(debug)&#123; System.out.println(String.format("Filename %s",fileName)); &#125; return fileName; &#125; public DataBlock() &#123; &#125; public DataBlock(byte[] data, int pos, boolean debug) &#123; this.data = data; this.pos = pos; this.debug = debug; &#125; public byte[] getData() &#123; return data; &#125; public void setData(byte[] data) &#123; this.data = data; &#125; public int getPos() &#123; return pos; &#125; public void setPos(int pos) &#123; this.pos = pos; &#125; public boolean isDebug() &#123; return debug; &#125; public void setDebug(boolean debug) &#123; this.debug = debug; &#125;&#125;public class DS_Store extends DataBlock&#123; private byte[] data; private boolean debug; private DataBlock root; private List&lt;Integer&gt; offsets; private Map&lt;String,Integer&gt; toc; private Map&lt;Integer,List&lt;Integer&gt;&gt; freeList; /** * Constructor of DS_Store * @Params: [data, debug] * @Return: */ public DS_Store(byte[] data, boolean debug) &#123; super(data,0,debug); this.data = data; this.debug = debug; this.data = data; this.debug = debug; this.root = readHeader(); this.offsets = readOffsets(); this.toc = readTOC(); this.freeList = readFreelist(); &#125; /** * Checks if this.data is actually a .DS_Store file by checking the magic bytes. * It returns the file's root block. * @Params: [] * @Return: com.zwt.framework.utils.util.dsstore.DataBlock */ private DataBlock readHeader()&#123; // We read at least 32+4 bytes for the header! if (this.data.length &lt; 36)&#123; throw new RuntimeException("Length of data is too short!"); &#125; // Check the magic bytes for .DS_Store int magic1 = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); int magic2 = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); if(magic1 != 0x1 &amp;&amp; magic2 != 0x42756431)&#123; throw new RuntimeException("Magic byte 1 does not match!"); &#125; // After the magic bytes, the offset follows two times with block's size in between. // Both offsets have to match and are the starting point of the root block int offset = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); int size = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); int offset2 = ByteBuffer.wrap(this.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Offset 1: %s",offset)); System.out.println(String.format("Size: %s",size)); System.out.println(String.format("Offset 2: %s",offset2)); &#125; if(offset!=offset2)&#123; throw new RuntimeException("Offsets do not match!"); &#125; //Skip 16 bytes of unknown data... skip(4*4); return new DataBlock(this.offsetRead(size, offset+4),0, this.debug); &#125; /** * Reads the offsets which follow the header * @Params: [] * @Return: java.util.List&lt;java.lang.Integer&gt; */ private List&lt;Integer&gt; readOffsets()&#123; int startPos = this.root.getPos(); // First get the number of offsets in this file. int count = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Offset count: %s",count)); &#125; // Always appears to be zero! this.root.skip(4); // Iterate over the offsets and get the offset addresses. List&lt;Integer&gt; offsets = new ArrayList&lt;&gt;(); for(int i=0;i&lt;count;i++)&#123; // Address of the offset. int address = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Offset %s is %s",i, address)); &#125; if (address == 0)&#123; // We're only interested in non-zero values continue; &#125; offsets.add(address); &#125; // Calculate the end of the address space (filled with zeroes) instead of dumbly reading zero values... int sectionEnd = startPos + (count / 256 + 1) * 256 * 4 - count*4; // Skip to the end of the section this.root.skip(sectionEnd); if(debug)&#123; System.out.println(String.format("Skipped %s to %s",(this.root.getPos() + sectionEnd),this.root.getPos())); System.out.println(String.format("Offsets: %s",offsets)); &#125; return offsets; &#125; /** * Reads the table of contents (TOCs) from the file. * @Params: [] * @Return: java.util.Map&lt;java.lang.String,java.lang.Integer&gt; */ private Map&lt;String,Integer&gt; readTOC()&#123; if(debug)&#123; System.out.println(String.format("POS %s",this.root.getPos())); &#125; // First get the number of ToC entries. int count = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Toc count: %s",count)); &#125; Map&lt;String,Integer&gt; toc = new HashMap&lt;&gt;(); // Iterate over all ToCs for(int i=0;i&lt;count;i++)&#123; // Get the length of a ToC's name int tocLen = this.root.offsetRead(1,0)[0]; // Read the ToC's name String tocName = new String(this.root.offsetRead(tocLen,0), StandardCharsets.UTF_8); // Read the address (block id) in the data section int blockId = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); // Add all values to the dictionary toc.put(tocName,blockId); &#125; if(debug)&#123; System.out.println(String.format("Toc %s",toc)); &#125; return toc; &#125; /** * Read the free list from the header. * The free list has n=0..31 buckets with the index 2^n * @Params: [] * @Return: java.util.Map&lt;java.lang.Integer,java.util.List&lt;java.lang.Integer&gt;&gt; */ private Map&lt;Integer,List&lt;Integer&gt;&gt; readFreelist()&#123; Map&lt;Integer,List&lt;Integer&gt;&gt; freelist = new HashMap&lt;&gt;(); for(int i=0;i&lt;32;i++)&#123; freelist.put(1&lt;&lt;i,new ArrayList&lt;&gt;()); // Read the amount of blocks in the specific free list. int blkcount = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); for(int j=0;j&lt;blkcount;j++)&#123; // Read blkcount block offsets. int freeOffset = ByteBuffer.wrap(this.root.offsetRead(4,0)).getInt(); freelist.get(1&lt;&lt;i).add(freeOffset); &#125; &#125; if(debug)&#123; System.out.println(String.format("Freelist: %s",freelist)); &#125; return freelist; &#125; /** * Create a DataBlock from a given block ID (e.g. from the ToC) * @Params: [blockId] * @Return: com.zwt.framework.utils.util.dsstore.DataBlock */ public DataBlock blockById(int blockId)&#123; // First check if the block_id is within the offsets range if(this.offsets.size() &lt; blockId)&#123; throw new RuntimeException("BlockID out of range!"); &#125; // Get the address of the block int addr = this.offsets.get(blockId); // Do some necessary bit operations to extract the offset and the size of the block. // The address without the last 5 bits is the offset in the file int offset = addr &gt;&gt; 0x5 &lt;&lt; 0x5; //The address' last five bits are the block's size. int size = 1 &lt;&lt; (addr &amp; 0x1f); if(debug)&#123; System.out.println(String.format("New block: addr %s offset %s size %s",addr, offset + 0x4, size)); &#125; // Return the new block return new DataBlock(this.offsetRead(size, offset + 0x4),0, this.debug); &#125; /** * Traverses a block identified by the given block_id and extracts the file names. * @Params: [blockId] * @Return: java.util.List&lt;java.lang.String&gt; */ public List&lt;String&gt; traverse(int blockId)&#123; // Get the responsible block by it's ID DataBlock node = this.blockById(blockId); // Extract the pointer to the next block int nextPointer = ByteBuffer.wrap(node.offsetRead(4,0)).getInt(); // Get the number of next blocks or records int count = ByteBuffer.wrap(node.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Next Ptr %s with %s ",nextPointer,count)); &#125; List&lt;String&gt; filenames =new ArrayList&lt;&gt;(); // If a next_pointer exists (&gt;0), iterate through the next blocks recursively // If not, we extract all file names from the current block if(nextPointer &gt; 0)&#123; for(int i=0;i&lt;count;i++)&#123; // Get the block_id for the next block int nextId = ByteBuffer.wrap(node.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Child: %s",nextId)); &#125; // Traverse it recursively List&lt;String&gt; files = this.traverse(nextId); filenames.addAll(files); // Also get the filename for the current block. String filename = node.readFileName(); if(debug)&#123; System.out.println(String.format("Filename: %s", filename)); &#125; filenames.add(filename); &#125; // Now that we traversed all childs of the next_pointer, traverse the pointer itself. // TODO: Check if that is really necessary as the last child should be the current node... (or so?) List&lt;String&gt; files = this.traverse(nextPointer); filenames.addAll(files); &#125;else&#123; // We're probably in a leaf node, so extract the file names. for(int i=0;i&lt;count;i++)&#123; String f = node.readFileName(); filenames.add(f); &#125; &#125; return filenames; &#125; /** * Traverse from the root block and extract all file names. * @Params: [] * @Return: java.util.List&lt;java.lang.String&gt; */ public List&lt;String&gt; traverseRoot()&#123; // Get the root block from the ToC 'DSDB' DataBlock root = this.blockById(this.toc.get("DSDB")); // Read the following root block's ID, so that we can traverse it. int rootId = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); if(debug)&#123; System.out.println(String.format("Root-ID %s", rootId)); &#125; // Read other values that we might be useful, but we're not interested in... (at least right now) int internalBlockCount = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); int recordCount = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); int blockCount = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); int unknown = ByteBuffer.wrap(root.offsetRead(4,0)).getInt(); // traverse from the extracted root block id. return this.traverse(rootId); &#125; public DS_Store() &#123; &#125; public byte[] getData() &#123; return data; &#125; public void setData(byte[] data) &#123; this.data = data; &#125; public boolean isDebug() &#123; return debug; &#125; public void setDebug(boolean debug) &#123; this.debug = debug; &#125;&#125;public class DS_StoreParser &#123; /** * Return bytes by reading .DS_Store File * Throw Exception if file not exist * Throw Exception if Reading Error * @Params: [fileName] * @Return: byte[] */ public static byte[] readFile(String fileName)&#123; File file = new File(fileName); if((!file.exists())||(!file.isFile()))&#123; throw new RuntimeException(".DS_Store File not exist ！"); &#125; try (FileInputStream fis = new FileInputStream(file); ByteArrayOutputStream bos = new ByteArrayOutputStream())&#123; byte[] b = new byte[1024]; int len; while((len = fis.read(b)) != -1) &#123; bos.write(b, 0, len); &#125; return bos.toByteArray(); &#125;catch (IOException e)&#123; throw new RuntimeException("Reading .DS_Store File Error!"+e); &#125; &#125; public static void main(String[] args) &#123; byte[] data = readFile("/Users/zhangwentong/Desktop/DS_Store/bak.DS_Store"); DS_Store store = new DS_Store(data,true); List&lt;String&gt; files = store.traverseRoot(); System.out.println("Count: "+ files.size()); for(int i=0;i&lt;files.size();i++)&#123; System.out.println(files.get(i)); &#125; &#125;&#125; 上述代码过程较复杂，有兴趣的可以参考最后面的参考资料部分，先对.DS_Store文件结构有些了解，在看代码就比较容易了。 我们运行上述代码后会看到如下输出： 可以看到这里面包含着我们的一些文件目录信息等，这些文件信息时是当前目录下的所有文件信息。这就可能造成一些安全问题。 有什么安全问题呢？我们来看下。 信息安全问题当这一文件上传到了web服务器时，往往会带来一定的危害。 它带来的危害是它包含的文件名。MacOs在几乎所有文件夹都创建了一个.DS_Store文件。 信息泄漏(敏感文件)问题： 我们可以在https://en.internetwache.org/scanning-the-alexa-top-1m-for-ds-store-files-12-03-2018/ 这里查看。这篇文章涉及的Internetwache.org网站项目，对Alexa Top 1000的网站的根目录进行扫描，证明在有的网站中的确存在这一文件，导致信息泄漏。通过解析这一文件，他们发现了数据库备份，配置文件，以及一些缓存文件，甚至是密钥。 产生这种情况的原因就是我们在Git合作中，将.DS_Store进行了上传，而后对项目进行了部署。 PS：需要说明的一个事实是，存储在.DS_Store文件中的文件名仅代表本地MacOS系统上的目录内容。这就意味着解析出来的文件列表中有些文件可能不存在于我们的服务器或者要查找的机器上。 解决方案尽管这样，我们在使用MacOS系统传输文件或者上传文件时，还是要尽量避免该文件的产生。 这个文件除了会记录目录信息，我们在使用程序遍历文件读取时也可能遇到它引起的问题（如我开头所说），我们在版本协作时，也有可能产生由此文件产生的不必要的冲突问题等。 我们处理.DS_Store文件大概有以下一些方法。 通常，在传输文件时，我们可以直接删除文件目录下的.DS_Store文件，如在MacOS上传输文件给Windows系统，这些.DS_Store文件就成了垃圾文件，对我们是毫无用处的。 对于提交给版本控制系统的文件，我们可以将.DS_Store文件加入到.gitignore文件中以达到忽略提交的目的。 如果我们想查询服务器（Linux）或者MacOS上有多少.DS_Store文件，可以使用下面的命令。 1find . -name '*.DS_Store' 如果要删除它们，可以使用下面的指令。 1find . -name '*.DS_Store' -type f -delete 上面的两个命令可以查询和删除当前目录下的.DS_Store文件。 PS：在MacOS上删除后会影响到如文件图标位置等问题，需要注意。 我们也可以让.DS_Store文件不在MacOS外接设备上（U盘等）继续生成，如下指令： 1defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool TRUE 如果要开启外接设备继续生成.DS_Store文件，指令如下： 1defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool FALSE 如果想直接禁止.DS_Store在MacOS上生成，可以使用Asepsis。 Asepsis的工作原理是拦截所有.DS_Store文件的创建或写入，并将它们重定向到 /usr/local/.dscage。 这样 Finder 如常工作，且不会有这种无用文件污染文件系统。 不幸的是，在 OS X 10.11 El Capitan 发布之后，Apple 启用了 System Integrity Protection (SIP)，它会阻止 Asepsis 的安装和正常运行。Asepsis 的作者已经放弃了对它的后续支持，因为他不希望用户为了使用这个工具而禁用系统关键安全服务。 然而网上也有相关可以继续使用Asepsis的方法，如这篇文章禁止.DS_store生成，有兴趣的可以看一看。 总结.DS_Store文件一般情况下是无用文件，也不会造成多大问题，但我们也应该对其存在提高警惕，尤其是在服务器上出现时，我是因为遇到了它造成的一个bug才决定研究下它，同时用程序解析一下锻炼自己，网上关于该文件的解析文章不算多，下面我把一些参考资料分享给大家，希望大家对其有更深的了解。 参考资料 DS_Store Format DS_Store_File_Format fdb.php 解析.DS_Store文件格式 Scanning the Alexa Top 1M for .DS_Store files mac-dumpster-diving-identifying-deleted-file-references-in-the-trash-ds_store-files-part-1 mac-dumpster-diving-identifying-deleted-file-references-in-the-trash-ds_store-files-part-2 源码地址上述代码地址： GitHub .DS_Store Parser]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>.DS_Store</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis功能文档（转）]]></title>
    <url>%2Fblog%2FRedis%E5%8A%9F%E8%83%BD%E6%96%87%E6%A1%A3%EF%BC%88%E8%BD%AC%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言本文转载自 Redis命令参考-功能文档 功能文档Redis 集群规范引言这个文档是正在开发中的 Redis 集群功能的规范（specification）文档， 文档分为两个部分： 第一部分介绍目前已经在 unstable 分支中实现了的那些功能。 第二部分介绍目前仍未实现的那些功能。 文档各个部分的内容可能会随着集群功能的设计修改而发生改变， 其中， 未实现功能发生修改的几率比已实现功能发生修改的几率要高。 这个规范包含了编写客户端库（client library）所需的全部知识， 不过请注意， 这里列出的一部分细节可能会在未来发生变化。 什么是 Redis 集群？Redis 集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis 实现， 集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset）。 Redis 集群中不存在中心（central）节点或者代理（proxy）节点， 集群的其中一个主要设计目标是达到线性可扩展性（linear scalability）。 Redis 集群为了保证一致性（consistency）而牺牲了一部分容错性： 系统会在保证对网络断线（net split）和节点失效（node failure）具有有限（limited）抵抗力的前提下， 尽可能地保持数据的一致性。 Note 集群将节点失效视为网络断线的其中一种特殊情况。 集群的容错功能是通过使用主节点（master）和从节点（slave）两种角色（role）的节点（node）来实现的： 主节点和从节点使用完全相同的服务器实现， 它们的功能（functionally）也完全一样， 但从节点通常仅用于替换失效的主节点。 不过， 如果不需要保证“先写入，后读取”操作的一致性（read-after-write consistency）， 那么可以使用从节点来执行只读查询。 Redis 集群实现的功能子集Redis 集群实现了单机 Redis 中， 所有处理单个数据库键的命令。 针对多个数据库键的复杂计算操作， 比如集合的并集操作、合集操作没有被实现， 那些理论上需要使用多个节点的多个数据库键才能完成的命令也没有被实现。 在将来， 用户也许可以通过 MIGRATE COPY 命令， 在集群的计算节点（computation node）中执行针对多个数据库键的只读操作， 但集群本身不会去实现那些需要将多个数据库键在多个节点中移来移去的复杂多键命令。 Redis 集群不像单机 Redis 那样支持多数据库功能， 集群只使用默认的 0 号数据库， 并且不能使用 SELECT index 命令。 Redis 集群协议中的客户端和服务器Redis 集群中的节点有以下责任： 持有键值对数据。 记录集群的状态，包括键到正确节点的映射（mapping keys to right nodes）。 自动发现其他节点，识别工作不正常的节点，并在有需要时，在从节点中选举出新的主节点。 为了执行以上列出的任务， 集群中的每个节点都与其他节点建立起了“集群连接（cluster bus）”， 该连接是一个 TCP 连接， 使用二进制协议进行通讯。 节点之间使用 Gossip 协议 来进行以下工作： 传播（propagate）关于集群的信息，以此来发现新的节点。 向其他节点发送 PING 数据包，以此来检查目标节点是否正常运作。 在特定事件发生时，发送集群信息。 除此之外， 集群连接还用于在集群中发布或订阅信息。 因为集群节点不能代理（proxy）命令请求， 所以客户端应该在节点返回 -MOVED 或者 -ASK 转向（redirection）错误时， 自行将命令请求转发至其他节点。 因为客户端可以自由地向集群中的任何一个节点发送命令请求， 并可以在有需要时， 根据转向错误所提供的信息， 将命令转发至正确的节点， 所以在理论上来说， 客户端是无须保存集群状态信息的。 不过， 如果客户端可以将键和节点之间的映射信息保存起来， 可以有效地减少可能出现的转向次数， 籍此提升命令执行的效率。 键分布模型Redis 集群的键空间被分割为 16384 个槽（slot）， 集群的最大节点数量也是 16384 个。 Note 推荐的最大节点数量为 1000 个左右。 每个主节点都负责处理 16384 个哈希槽的其中一部分。 当我们说一个集群处于“稳定”（stable）状态时， 指的是集群没有在执行重配置（reconfiguration）操作， 每个哈希槽都只由一个节点进行处理。 Note 重配置指的是将某个/某些槽从一个节点移动到另一个节点。 Note 一个主节点可以有任意多个从节点， 这些从节点用于在主节点发生网络断线或者节点失效时， 对主节点进行替换。 以下是负责将键映射到槽的算法：1HASH_SLOT = CRC16(key) mod 16384 以下是该算法所使用的参数： 算法的名称: XMODEM (又称 ZMODEM 或者 CRC-16/ACORN) 结果的长度: 16 位 多项数（poly）: 1021 (也即是 x16 + x12 + x5 + 1) 初始化值: 0000 反射输入字节（Reflect Input byte）: False 发射输出 CRC （Reflect Output CRC）: False 用于 CRC 输出值的异或常量（Xor constant to output CRC）: 0000 该算法对于输入 “123456789” 的输出: 31C3 附录 A 中给出了集群所使用的 CRC16 算法的实现。 CRC16 算法所产生的 16 位输出中的 14 位会被用到。 在我们的测试中， CRC16 算法可以很好地将各种不同类型的键平稳地分布到 16384 个槽里面。 集群节点属性每个节点在集群中都有一个独一无二的 ID ， 该 ID 是一个十六进制表示的 160 位随机数， 在节点第一次启动时由 /dev/urandom 生成。 节点会将它的 ID 保存到配置文件， 只要这个配置文件不被删除， 节点就会一直沿用这个 ID 。 节点 ID 用于标识集群中的每个节点。 一个节点可以改变它的 IP 和端口号， 而不改变节点 ID 。 集群可以自动识别出 IP/端口号的变化， 并将这一信息通过 Gossip 协议广播给其他节点知道。 以下是每个节点都有的关联信息， 并且节点会将这些信息发送给其他节点： 节点所使用的 IP 地址和 TCP 端口号。 节点的标志（flags）。 节点负责处理的哈希槽。 节点最近一次使用集群连接发送 PING 数据包（packet）的时间。 节点最近一次在回复中接收到 PONG 数据包的时间。 集群将该节点标记为下线的时间。 该节点的从节点数量。 如果该节点是从节点的话，那么它会记录主节点的节点 ID 。 如果这是一个主节点的话，那么主节点 ID 这一栏的值为 0000000 。 以上信息的其中一部分可以通过向集群中的任意节点（主节点或者从节点都可以）发送 CLUSTER NODES 命令来获得。 以下是一个向集群中的主节点发送 CLUSTER NODES 命令的例子， 该集群由三个节点组成：1234$ redis-cli cluster nodesd1861060fe6a534d42d8a19aeb36600e18785e04 :0 myself - 0 1318428930 connected 0-13643886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 connected 1365-2729d289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 connected 2730-4095 在上面列出的三行信息中， 从左到右的各个域分别是： 节点 ID ， IP 地址和端口号， 标志（flag）， 最后发送 PING 的时间， 最后接收 PONG 的时间， 连接状态， 节点负责处理的槽。 节点握手（已实现）节点总是应答（accept）来自集群连接端口的连接请求， 并对接收到的 PING 数据包进行回复， 即使这个 PING 数据包来自不可信的节点。 然而， 除了 PING 之外， 节点会拒绝其他所有并非来自集群节点的数据包。 要让一个节点承认另一个节点同属于一个集群， 只有以下两种方法： 一个节点可以通过向另一个节点发送 MEET 信息， 来强制让接收信息的节点承认发送信息的节点为集群中的一份子。 一个节点仅在管理员显式地向它发送 CLUSTER MEET ip port 命令时， 才会向另一个节点发送 MEET 信息。 另外， 如果一个可信节点向另一个节点传播第三者节点的信息， 那么接收信息的那个节点也会将第三者节点识别为集群中的一份子。 也即是说， 如果 A 认识 B ， B 认识 C ， 并且 B 向 A 传播关于 C 的信息， 那么 A 也会将 C 识别为集群中的一份子， 并尝试连接 C 。 这意味着如果我们将一个/一些新节点添加到一个集群中， 那么这个/这些新节点最终会和集群中已有的其他所有节点连接起来。 这说明只要管理员使用 CLUSTER MEET 命令显式地指定了可信关系， 集群就可以自动发现其他节点。 这种节点识别机制通过防止不同的 Redis 集群因为 IP 地址变更或者其他网络事件的发生而产生意料之外的联合（mix）， 从而使得集群更具健壮性。 当节点的网络连接断开时， 它会主动连接其他已知的节点。 MOVED 转向一个 Redis 客户端可以向集群中的任意节点（包括从节点）发送命令请求。 节点会对命令请求进行分析， 如果该命令是集群可以执行的命令， 那么节点会查找这个命令所要处理的键所在的槽。 如果要查找的哈希槽正好就由接收到命令的节点负责处理， 那么节点就直接执行这个命令。 另一方面， 如果所查找的槽不是由该节点处理的话， 节点将查看自身内部所保存的哈希槽到节点 ID 的映射记录， 并向客户端回复一个 MOVED 错误。 以下是一个 MOVED 错误的例子：123GET x-MOVED 3999 127.0.0.1:6381 错误信息包含键 x 所属的哈希槽 3999 ， 以及负责处理这个槽的节点的 IP 和端口号 127.0.0.1:6381 。 客户端需要根据这个 IP 和端口号， 向所属的节点重新发送一次 GET key 命令请求。 注意， 即使客户端在重新发送 GET key 命令之前， 等待了非常久的时间， 以至于集群又再次更改了配置， 使得节点 127.0.0.1:6381 已经不再处理槽 3999 ， 那么当客户端向节点 127.0.0.1:6381 发送 GET key 命令的时候， 节点将再次向客户端返回 MOVED 错误， 指示现在负责处理槽 3999 的节点。 虽然我们用 ID 来标识集群中的节点， 但是为了让客户端的转向操作尽可能地简单， 节点在 MOVED 错误中直接返回目标节点的 IP 和端口号， 而不是目标节点的 ID 。 虽然不是必须的， 但一个客户端应该记录（memorize）下“槽 3999 由节点 127.0.0.1:6381 负责处理“这一信息， 这样当再次有命令需要对槽 3999 执行时， 客户端就可以加快寻找正确节点的速度。 注意， 当集群处于稳定状态时， 所有客户端最终都会保存有一个哈希槽至节点的映射记录（map of hash slots to nodes）， 使得集群非常高效： 客户端可以直接向正确的节点发送命令请求， 无须转向、代理或者其他任何可能发生单点故障（single point failure）的实体（entiy）。 除了 MOVED 转向错误之外， 一个客户端还应该可以处理稍后介绍的 ASK 转向错误。 集群在线重配置（live reconfiguration）Redis 集群支持在集群运行的过程中添加或者移除节点。 实际上， 节点的添加操作和节点的删除操作可以抽象成同一个操作， 那就是， 将哈希槽从一个节点移动到另一个节点： 添加一个新节点到集群， 等于将其他已存在节点的槽移动到一个空白的新节点里面。 从集群中移除一个节点， 等于将被移除节点的所有槽移动到集群的其他节点上面去。 因此， 实现 Redis 集群在线重配置的核心就是将槽从一个节点移动到另一个节点的能力。 因为一个哈希槽实际上就是一些键的集合， 所以 Redis 集群在重哈希（rehash）时真正要做的， 就是将一些键从一个节点移动到另一个节点。 要理解 Redis 集群如何将槽从一个节点移动到另一个节点， 我们需要对 CLUSTER 命令的各个子命令进行介绍， 这些命理负责管理集群节点的槽转换表（slots translation table）。 以下是 CLUSTER 命令可用的子命令：12345CLUSTER ADDSLOTS slot1 [slot2] ... [slotN]CLUSTER DELSLOTS slot1 [slot2] ... [slotN]CLUSTER SETSLOT slot NODE nodeCLUSTER SETSLOT slot MIGRATING nodeCLUSTER SETSLOT slot IMPORTING node 最开头的两条命令 ADDSLOTS 和 DELSLOTS 分别用于向节点指派（assign）或者移除节点， 当槽被指派或者移除之后， 节点会将这一信息通过 Gossip 协议传播到整个集群。 ADDSLOTS 命令通常在新创建集群时， 作为一种快速地将各个槽指派给各个节点的手段来使用。 CLUSTER SETSLOT slot NODE node 子命令可以将指定的槽 slot 指派给节点 node 。 至于 CLUSTER SETSLOT slot MIGRATING node 命令和 CLUSTER SETSLOT slot IMPORTING node 命令， 前者用于将给定节点 node 中的槽 slot 迁移出节点， 而后者用于将给定槽 slot 导入到节点 node ： 当一个槽被设置为 MIGRATING 状态时， 原来持有这个槽的节点仍然会继续接受关于这个槽的命令请求， 但只有命令所处理的键仍然存在于节点时， 节点才会处理这个命令请求。 如果命令所使用的键不存在与该节点， 那么节点将向客户端返回一个 -ASK 转向（redirection）错误， 告知客户端， 要将命令请求发送到槽的迁移目标节点。 当一个槽被设置为 IMPORTING 状态时， 节点仅在接收到 ASKING 命令之后， 才会接受关于这个槽的命令请求。 如果客户端没有向节点发送 ASKING 命令， 那么节点会使用 -MOVED 转向错误将命令请求转向至真正负责处理这个槽的节点。 上面关于 MIGRATING 和 IMPORTING 的说明有些难懂， 让我们用一个实际的实例来说明一下。 假设现在， 我们有 A 和 B 两个节点， 并且我们想将槽 8 从节点 A 移动到节点 B ， 于是我们： 向节点 B 发送命令 CLUSTER SETSLOT 8 IMPORTING A 向节点 A 发送命令 CLUSTER SETSLOT 8 MIGRATING B 每当客户端向其他节点发送关于哈希槽 8 的命令请求时， 这些节点都会向客户端返回指向节点 A 的转向信息： 如果命令要处理的键已经存在于槽 8 里面， 那么这个命令将由节点 A 处理。 如果命令要处理的键未存在于槽 8 里面（比如说，要向槽添加一个新的键）， 那么这个命令由节点 B 处理。 这种机制将使得节点 A 不再创建关于槽 8 的任何新键。 与此同时， 一个特殊的客户端 redis-trib 以及 Redis 集群配置程序（configuration utility）会将节点 A 中槽 8 里面的键移动到节点 B 。 键的移动操作由以下两个命令执行：1CLUSTER GETKEYSINSLOT slot count 上面的命令会让节点返回 count 个 slot 槽中的键， 对于命令所返回的每个键， redis-trib 都会向节点 A 发送一条 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令， 该命令会将所指定的键原子地（atomic）从节点 A 移动到节点 B （在移动键期间，两个节点都会处于阻塞状态，以免出现竞争条件）。 以下为 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令的运作原理：1MIGRATE target_host target_port key target_database id timeout 执行 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令的节点会连接到 target 节点， 并将序列化后的 key 数据发送给 target ， 一旦 target 返回 OK ， 节点就将自己的 key 从数据库中删除。 从一个外部客户端的视角来看， 在某个时间点上， 键 key 要么存在于节点 A ， 要么存在于节点 B ， 但不会同时存在于节点 A 和节点 B 。 因为 Redis 集群只使用 0 号数据库， 所以当 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令被用于执行集群操作时， target_database 的值总是 0 。 target_database 参数的存在是为了让 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令成为一个通用命令， 从而可以作用于集群以外的其他功能。 我们对 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 命令做了优化， 使得它即使在传输包含多个元素的列表键这样的复杂数据时， 也可以保持高效。 不过， 尽管 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 非常高效， 对一个键非常多、并且键的数据量非常大的集群来说， 集群重配置还是会占用大量的时间， 可能会导致集群没办法适应那些对于响应时间有严格要求的应用程序。 ASK 转向在之前介绍 MOVED 转向的时候， 我们说除了 MOVED 转向之外， 还有另一种 ASK 转向。 当节点需要让一个客户端长期地（permanently）将针对某个槽的命令请求发送至另一个节点时， 节点向客户端返回 MOVED 转向。 另一方面， 当节点需要让客户端仅仅在下一个命令请求中转向至另一个节点时， 节点向客户端返回 ASK 转向。 比如说， 在我们上一节列举的槽 8 的例子中， 因为槽 8 所包含的各个键分散在节点 A 和节点 B 中， 所以当客户端在节点 A 中没找到某个键时， 它应该转向到节点 B 中去寻找， 但是这种转向应该仅仅影响一次命令查询， 而不是让客户端每次都直接去查找节点 B ： 在节点 A 所持有的属于槽 8 的键没有全部被迁移到节点 B 之前， 客户端应该先访问节点 A ， 然后再访问节点 B 。 因为这种转向只针对 16384 个槽中的其中一个槽， 所以转向对集群造成的性能损耗属于可接受的范围。 因为上述原因， 如果我们要在查找节点 A 之后， 继续查找节点 B ， 那么客户端在向节点 B 发送命令请求之前， 应该先发送一个 ASKING 命令， 否则这个针对带有 IMPORTING 状态的槽的命令请求将被节点 B 拒绝执行。 接收到客户端 ASKING 命令的节点将为客户端设置一个一次性的标志（flag）， 使得客户端可以执行一次针对 IMPORTING 状态的槽的命令请求。 从客户端的角度来看， ASK 转向的完整语义（semantics）如下： 如果客户端接收到 ASK 转向， 那么将命令请求的发送对象调整为转向所指定的节点。 先发送一个 ASKING 命令，然后再发送真正的命令请求。 不必更新客户端所记录的槽 8 至节点的映射： 槽 8 应该仍然映射到节点 A ， 而不是节点 B 。 一旦节点 A 针对槽 8 的迁移工作完成， 节点 A 在再次收到针对槽 8 的命令请求时， 就会向客户端返回 MOVED 转向， 将关于槽 8 的命令请求长期地转向到节点 B 。 注意， 即使客户端出现 Bug ， 过早地将槽 8 映射到了节点 B 上面， 但只要这个客户端不发送 ASKING 命令， 客户端发送命令请求的时候就会遇上 MOVED 错误， 并将它转向回节点 A 。 容错节点失效检测以下是节点失效检查的实现方法： 当一个节点向另一个节点发送 PING 命令， 但是目标节点未能在给定的时限内返回 PING 命令的回复时， 那么发送命令的节点会将目标节点标记为 PFAIL （possible failure，可能已失效）。 等待 PING 命令回复的时限称为“节点超时时限（node timeout）”， 是一个节点选项（node-wise setting）。 每次当节点对其他节点发送 PING 命令的时候， 它都会随机地广播三个它所知道的节点的信息， 这些信息里面的其中一项就是说明节点是否已经被标记为 PFAIL 或者 FAIL 。 当节点接收到其他节点发来的信息时， 它会记下那些被其他节点标记为失效的节点。 这称为失效报告（failure report）。 如果节点已经将某个节点标记为 PFAIL ， 并且根据节点所收到的失效报告显式， 集群中的大部分其他主节点也认为那个节点进入了失效状态， 那么节点会将那个失效节点的状态标记为 FAIL 。 一旦某个节点被标记为 FAIL ， 关于这个节点已失效的信息就会被广播到整个集群， 所有接收到这条信息的节点都会将失效节点标记为 FAIL 。 简单来说， 一个节点要将另一个节点标记为失效， 必须先询问其他节点的意见， 并且得到大部分主节点的同意才行。 因为过期的失效报告会被移除， 所以主节点要将某个节点标记为 FAIL 的话， 必须以最近接收到的失效报告作为根据。 在以下两种情况中， 节点的 FAIL 状态会被移除： 如果被标记为 FAIL 的是从节点， 那么当这个节点重新上线时， FAIL 标记就会被移除。 保持（retaning）从节点的 FAIL 状态是没有意义的， 因为它不处理任何槽， 一个从节点是否处于 FAIL 状态， 决定了这个从节点在有需要时能否被提升为主节点。 如果一个主节点被打上 FAIL 标记之后， 经过了节点超时时限的四倍时间， 再加上十秒钟之后， 针对这个主节点的槽的故障转移操作仍未完成， 并且这个主节点已经重新上线的话， 那么移除对这个节点的 FAIL 标记。 在第二种情况中， 如果故障转移未能顺利完成， 并且主节点重新上线， 那么集群就继续使用原来的主节点， 从而免去管理员介入的必要。 集群状态检测（已部分实现）每当集群发生配置变化时（可能是哈希槽更新，也可能是某个节点进入失效状态）， 集群中的每个节点都会对它所知道的节点进行扫描（scan）。 一旦配置处理完毕， 集群会进入以下两种状态的其中一种： FAIL ： 集群不能正常工作。 当集群中有某个节点进入失效状态时， 集群不能处理任何命令请求， 对于每个命令请求， 集群节点都返回错误回复。 OK ： 集群可以正常工作， 负责处理全部 16384 个槽的节点中， 没有一个节点被标记为 FAIL 状态。 这说明即使集群中只有一部分哈希槽不能正常使用， 整个集群也会停止处理任何命令。 不过节点从出现问题到被标记为 FAIL 状态的这段时间里， 集群仍然会正常运作， 所以集群在某些时候， 仍然有可能只能处理针对 16384 个槽的其中一个子集的命令请求。 以下是集群进入 FAIL 状态的两种情况： 至少有一个哈希槽不可用，因为负责处理这个槽的节点进入了 FAIL 状态。 集群中的大部分主节点都进入下线状态。当大部分主节点都进入 PFAIL 状态时，集群也会进入 FAIL 状态。 第二个检查是必须的， 因为要将一个节点从 PFAIL 状态改变为 FAIL 状态， 必须要有大部分主节点进行投票表决， 但是， 当集群中的大部分主节点都进入失效状态时， 单凭一个两个节点是没有办法将一个节点标记为 FAIL 状态的。 因此， 有了第二个检查条件， 只要集群中的大部分主节点进入了下线状态， 那么集群就可以在不请求这些主节点的意见下， 将某个节点判断为 FAIL 状态， 从而让整个集群停止处理命令请求。 从节点选举一旦某个主节点进入 FAIL 状态， 如果这个主节点有一个或多个从节点存在， 那么其中一个从节点会被升级为新的主节点， 而其他从节点则会开始对这个新的主节点进行复制。 新的主节点由已下线主节点属下的所有从节点中自行选举产生， 以下是选举的条件： 这个节点是已下线主节点的从节点。 已下线主节点负责处理的槽数量非空。 从节点的数据被认为是可靠的， 也即是， 主从节点之间的复制连接（replication link）的断线时长不能超过节点超时时限（node timeout）乘以 REDIS_CLUSTER_SLAVE_VALIDITY_MULT 常量得出的积。 如果一个从节点满足了以上的所有条件， 那么这个从节点将向集群中的其他主节点发送授权请求， 询问它们， 是否允许自己（从节点）升级为新的主节点。 如果发送授权请求的从节点满足以下属性， 那么主节点将向从节点返回 FAILOVER_AUTH_GRANTED 授权， 同意从节点的升级要求： 发送授权请求的是一个从节点， 并且它所属的主节点处于 FAIL 状态。 在已下线主节点的所有从节点中， 这个从节点的节点 ID 在排序中是最小的。 这个从节点处于正常的运行状态： 它没有被标记为 FAIL 状态， 也没有被标记为 PFAIL 状态。 一旦某个从节点在给定的时限内得到大部分主节点的授权， 它就会开始执行以下故障转移操作： 通过 PONG 数据包（packet）告知其他节点， 这个节点现在是主节点了。 通过 PONG 数据包告知其他节点， 这个节点是一个已升级的从节点（promoted slave）。 接管（claiming）所有由已下线主节点负责处理的哈希槽。 显式地向所有节点广播一个 PONG 数据包， 加速其他节点识别这个节点的进度， 而不是等待定时的 PING / PONG 数据包。 所有其他节点都会根据新的主节点对配置进行相应的更新，特别地： 所有被新的主节点接管的槽会被更新。 已下线主节点的所有从节点会察觉到 PROMOTED 标志， 并开始对新的主节点进行复制。 如果已下线的主节点重新回到上线状态， 那么它会察觉到 PROMOTED 标志， 并将自身调整为现任主节点的从节点。 在集群的生命周期中， 如果一个带有 PROMOTED 标识的主节点因为某些原因转变成了从节点， 那么该节点将丢失它所带有的 PROMOTED 标识。 发布/订阅（已实现，但仍然需要改善）在一个 Redis 集群中， 客户端可以订阅任意一个节点， 也可以向任意一个节点发送信息， 节点会对客户端所发送的信息进行转发。 在目前的实现中， 节点会将接收到的信息广播至集群中的其他所有节点， 在将来的实现中， 可能会使用 bloom filter 或者其他算法来优化这一操作。 附录 A： CRC16 算法的 ANSI 实现参考12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/* * Copyright 2001-2010 Georges Menie (www.menie.org) * Copyright 2010 Salvatore Sanfilippo (adapted to Redis coding style) * All rights reserved. * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions are met: * * * Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * Neither the name of the University of California, Berkeley nor the * names of its contributors may be used to endorse or promote products * derived from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE * DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. *//* CRC16 implementation acording to CCITT standards. * * Note by @antirez: this is actually the XMODEM CRC 16 algorithm, using the * following parameters: * * Name : "XMODEM", also known as "ZMODEM", "CRC-16/ACORN" * Width : 16 bit * Poly : 1021 (That is actually x^16 + x^12 + x^5 + 1) * Initialization : 0000 * Reflect Input byte : False * Reflect Output CRC : False * Xor constant to output CRC : 0000 * Output for "123456789" : 31C3 */static const uint16_t crc16tab[256]= &#123; 0x0000,0x1021,0x2042,0x3063,0x4084,0x50a5,0x60c6,0x70e7, 0x8108,0x9129,0xa14a,0xb16b,0xc18c,0xd1ad,0xe1ce,0xf1ef, 0x1231,0x0210,0x3273,0x2252,0x52b5,0x4294,0x72f7,0x62d6, 0x9339,0x8318,0xb37b,0xa35a,0xd3bd,0xc39c,0xf3ff,0xe3de, 0x2462,0x3443,0x0420,0x1401,0x64e6,0x74c7,0x44a4,0x5485, 0xa56a,0xb54b,0x8528,0x9509,0xe5ee,0xf5cf,0xc5ac,0xd58d, 0x3653,0x2672,0x1611,0x0630,0x76d7,0x66f6,0x5695,0x46b4, 0xb75b,0xa77a,0x9719,0x8738,0xf7df,0xe7fe,0xd79d,0xc7bc, 0x48c4,0x58e5,0x6886,0x78a7,0x0840,0x1861,0x2802,0x3823, 0xc9cc,0xd9ed,0xe98e,0xf9af,0x8948,0x9969,0xa90a,0xb92b, 0x5af5,0x4ad4,0x7ab7,0x6a96,0x1a71,0x0a50,0x3a33,0x2a12, 0xdbfd,0xcbdc,0xfbbf,0xeb9e,0x9b79,0x8b58,0xbb3b,0xab1a, 0x6ca6,0x7c87,0x4ce4,0x5cc5,0x2c22,0x3c03,0x0c60,0x1c41, 0xedae,0xfd8f,0xcdec,0xddcd,0xad2a,0xbd0b,0x8d68,0x9d49, 0x7e97,0x6eb6,0x5ed5,0x4ef4,0x3e13,0x2e32,0x1e51,0x0e70, 0xff9f,0xefbe,0xdfdd,0xcffc,0xbf1b,0xaf3a,0x9f59,0x8f78, 0x9188,0x81a9,0xb1ca,0xa1eb,0xd10c,0xc12d,0xf14e,0xe16f, 0x1080,0x00a1,0x30c2,0x20e3,0x5004,0x4025,0x7046,0x6067, 0x83b9,0x9398,0xa3fb,0xb3da,0xc33d,0xd31c,0xe37f,0xf35e, 0x02b1,0x1290,0x22f3,0x32d2,0x4235,0x5214,0x6277,0x7256, 0xb5ea,0xa5cb,0x95a8,0x8589,0xf56e,0xe54f,0xd52c,0xc50d, 0x34e2,0x24c3,0x14a0,0x0481,0x7466,0x6447,0x5424,0x4405, 0xa7db,0xb7fa,0x8799,0x97b8,0xe75f,0xf77e,0xc71d,0xd73c, 0x26d3,0x36f2,0x0691,0x16b0,0x6657,0x7676,0x4615,0x5634, 0xd94c,0xc96d,0xf90e,0xe92f,0x99c8,0x89e9,0xb98a,0xa9ab, 0x5844,0x4865,0x7806,0x6827,0x18c0,0x08e1,0x3882,0x28a3, 0xcb7d,0xdb5c,0xeb3f,0xfb1e,0x8bf9,0x9bd8,0xabbb,0xbb9a, 0x4a75,0x5a54,0x6a37,0x7a16,0x0af1,0x1ad0,0x2ab3,0x3a92, 0xfd2e,0xed0f,0xdd6c,0xcd4d,0xbdaa,0xad8b,0x9de8,0x8dc9, 0x7c26,0x6c07,0x5c64,0x4c45,0x3ca2,0x2c83,0x1ce0,0x0cc1, 0xef1f,0xff3e,0xcf5d,0xdf7c,0xaf9b,0xbfba,0x8fd9,0x9ff8, 0x6e17,0x7e36,0x4e55,0x5e74,0x2e93,0x3eb2,0x0ed1,0x1ef0&#125;;uint16_t crc16(const char *buf, int len) &#123; int counter; uint16_t crc = 0; for (counter = 0; counter &lt; len; counter++) crc = (crc&lt;&lt;8) ^ crc16tab[((crc&gt;&gt;8) ^ *buf++)&amp;0x00FF]; return crc;&#125; 持久化（persistence）这篇文章提供了 Redis 持久化的技术性描述， 推荐所有 Redis 用户阅读。 要更广泛地了解 Redis 持久化， 以及这种持久化所保证的耐久性（durability）， 请参考文章 Redis persistence demystified （中文）。 Redis 持久化Redis 提供了多种不同级别的持久化方式： RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 你甚至可以关闭持久化功能，让数据只在服务器运行时存在。 了解 RDB 持久化和 AOF 持久化之间的异同是非常重要的， 以下几个小节将详细地介绍这这两种持久化功能， 并对它们的相同和不同之处进行说明。 RDB 的优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次 RDB 文件，并且在每个月的每一天，也备份一个 RDB 文件。 这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心，或者亚马逊 S3 中。 RDB 可以最大化 Redis 的性能：父进程在保存 RDB 文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB 的缺点 如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 虽然 Redis 允许你设置不同的保存点（save point）来控制保存 RDB 文件的频率， 但是， 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOF 的优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF 的缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH source destination timeout 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 RDB 和 AOF ，我应该用哪一个？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 Note 因为以上提到的种种原因， 未来可能会将 AOF 和 RDB 整合成单个持久化模型。 （这是一个长期计划。） 接下来的几个小节将介绍 RDB 和 AOF 的更多细节。 RDB 快照在默认情况下， Redis 将数据库快照保存在名字为 dump.rdb 的二进制文件中。 你可以对 Redis 进行设置， 让它在“ N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。 你也可以通过调用 SAVE 或者 BGSAVE ， 手动让 Redis 进行数据集保存操作。 比如说， 以下设置会让 Redis 在满足“ 60 秒内有至少有 1000 个键被改动”这一条件时， 自动保存一次数据集：1save 60 1000 这种持久化方式被称为快照（snapshot）。 快照的运作方式当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作： Redis 调用 fork() ，同时拥有父进程和子进程。 子进程将数据集写入到一个临时 RDB 文件中。 当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益。 只进行追加操作的文件（append-only file，AOF）快照功能并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。 尽管对于某些程序来说， 数据的耐久性并不是最重要的考虑因素， 但是对于那些追求完全耐久能力（full durability）的程序来说， 快照功能就不太适用了。 从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化。 你可以通过修改配置文件来打开 AOF 功能：1appendonly yes 从现在开始， 每当 Redis 执行一个改变数据集的命令时（比如 SET key value [EX seconds] [PX milliseconds] [NX|XX]）， 这个命令就会被追加到 AOF 文件的末尾。 这样的话， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。 AOF 重写因为 AOF 的运作方式是不断地将命令追加到文件的末尾， 所以随着写入命令的不断增加， AOF 文件的体积也会变得越来越大。 举个例子， 如果你对一个计数器调用了 100 次 INCR key ， 那么仅仅是为了保存这个计数器的当前值， AOF 文件就需要使用 100 条记录（entry）。 然而在实际上， 只使用一条 SET key value [EX seconds] [PX milliseconds] [NX|XX] 命令已经足以保存计数器的当前值了， 其余 99 条记录实际上都是多余的。 为了处理这种情况， Redis 支持一种有趣的特性： 可以在不打断服务客户端的情况下， 对 AOF 文件进行重建（rebuild）。 执行 BGREWRITEAOF 命令， Redis 将生成一个新的 AOF 文件， 这个文件包含重建当前数据集所需的最少命令。 Redis 2.2 需要自己手动执行 BGREWRITEAOF 命令； Redis 2.4 则可以自动触发 AOF 重写， 具体信息请查看 2.4 的示例配置文件。 AOF 的耐久性如何？你可以配置 Redis 多久才将数据 fsync 到磁盘一次。 有三个选项： 每次有新命令追加到 AOF 文件时就执行一次 fsync ：非常慢，也非常安全。 每秒 fsync 一次：足够快（和使用 RDB 持久化差不多），并且在故障时只会丢失 1 秒钟的数据。 从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。 推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。 总是 fsync 的策略在实际使用中非常慢， 即使在 Redis 2.0 对相关的程序进行了改进之后仍是如此 —— 频繁调用 fsync 注定了这种策略不可能快得起来。 如果 AOF 文件出错了，怎么办？服务器可能在程序正在对 AOF 文件进行写入时停机， 如果停机造成了 AOF 文件出错（corrupt）， 那么 Redis 在重启时会拒绝载入这个 AOF 文件， 从而确保数据的一致性不会被破坏。 当发生这种情况时， 可以用以下方法来修复出错的 AOF 文件： 为现有的 AOF 文件创建一个备份。 使用 Redis 附带的 redis-check-aof 程序，对原来的 AOF 文件进行修复。 1$ redis-check-aof --fix （可选）使用 diff -u 对比修复后的 AOF 文件和原始 AOF 文件的备份，查看两个文件之间的不同之处。 重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复。 AOF 的运作方式AOF 重写和 RDB 创建快照一样，都巧妙地利用了写时复制机制。 以下是 AOF 重写的执行步骤： Redis 执行 fork() ，现在同时拥有父进程和子进程。 子进程开始将新 AOF 文件的内容写入到临时文件。 对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾： 这样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。 搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 怎么从 RDB 持久化切换到 AOF 持久化在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换到 AOF ： 为最新的 dump.rdb 文件创建一个备份。 将备份放到一个安全的地方。 执行以下两条命令： 123redis-cli&gt; CONFIG SET appendonly yesredis-cli&gt; CONFIG SET save "" 确保命令执行之后，数据库的键的数量没有改变。 确保写命令会被正确地追加到 AOF 文件的末尾。 步骤 3 执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。 步骤 3 执行的第二条命令用于关闭 RDB 功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用 RDB 和 AOF 这两种持久化功能。 Note 别忘了在 redis.conf 中打开 AOF 功能！ 否则的话， 服务器重启之后， 之前通过 CONFIG SET 设置的配置就会被遗忘， 程序会按原来的配置来启动服务器。 RDB 和 AOF 之间的相互作用在版本号大于等于 2.4 的 Redis 中， BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE 。 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户， BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。 备份 Redis 数据在阅读这个小节前， 先将下面这句话铭记于心： 一定要备份你的数据库！ 磁盘故障， 节点失效， 诸如此类的问题都可能让你的数据消失不见， 不进行备份是非常危险的。 Redis 对于数据备份是非常友好的， 因为你可以在服务器运行的时候对 RDB 文件进行复制： RDB 文件一旦被创建， 就不会进行任何修改。 当服务器要创建一个新的 RDB 文件时， 它先将文件的内容保存在一个临时文件里面， 当临时文件写入完毕时， 程序才使用 rename(2) 原子地用临时文件替换原来的 RDB 文件。 这也就是说， 无论何时， 复制 RDB 文件都是绝对安全的。 以下是我们的建议： 创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹。 确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照。 至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外。 容灾备份Redis 的容灾备份基本上就是对数据进行备份， 并将这些备份传送到多个不同的外部数据中心。 容灾备份可以在 Redis 运行并产生快照的主数据中心发生严重的问题时， 仍然让数据处于安全状态。 因为很多 Redis 用户都是创业者， 他们没有大把大把的钱可以浪费， 所以下面介绍的都是一些实用又便宜的容灾备份方法： Amazon S3 ，以及其他类似 S3 的服务，是一个构建灾难备份系统的好地方。 最简单的方法就是将你的每小时或者每日 RDB 备份加密并传送到 S3 。 对数据的加密可以通过 gpg -c 命令来完成（对称加密模式）。 记得把你的密码放到几个不同的、安全的地方去（比如你可以把密码复制给你组织里最重要的人物）。 同时使用多个储存服务来保存数据文件，可以提升数据的安全性。 传送快照可以使用 SCP 来完成（SSH 的组件）。 以下是简单并且安全的传送方法： 买一个离你的数据中心非常远的 VPS ， 装上 SSH ， 创建一个无口令的 SSH 客户端 key ， 并将这个 key 添加到 VPS 的 authorized_keys 文件中， 这样就可以向这个 VPS 传送快照备份文件了。 为了达到最好的数据安全性，至少要从两个不同的提供商那里各购买一个 VPS 来进行数据容灾备份。 需要注意的是， 这类容灾系统如果没有小心地进行处理的话， 是很容易失效的。 最低限度下， 你应该在文件传送完毕之后， 检查所传送备份文件的体积和原始快照文件的体积是否相同。 如果你使用的是 VPS ， 那么还可以通过比对文件的 SHA1 校验和来确认文件是否传送完整。 另外， 你还需要一个独立的警报系统， 让它在负责传送备份文件的传送器（transfer）失灵时通知你。 发布与订阅（pub/sub）SUBSCRIBE channel [channel …] 、 UNSUBSCRIBE [channel [channel …]] 和 PUBLISH channel message 三个命令实现了发布与订阅信息泛型（Publish/Subscribe messaging paradigm）， 在这个实现中， 发送者（发送信息的客户端）不是将信息直接发送给特定的接收者（接收信息的客户端）， 而是将信息发送给频道（channel）， 然后由频道将信息转发给所有对这个频道感兴趣的订阅者。 发送者无须知道任何关于订阅者的信息， 而订阅者也无须知道是那个客户端给它发送信息， 它只要关注自己感兴趣的频道即可。 对发布者和订阅者进行解构（decoupling）， 可以极大地提高系统的扩展性（scalability）， 并得到一个更动态的网络拓扑（network topology）。 比如说， 要订阅频道 foo 和 bar ， 客户端可以使用频道名字作为参数来调用 SUBSCRIBE channel [channel …] 命令：1redis&gt; SUBSCRIBE foo bar 当有客户端发送信息到这些频道时， Redis 会将传入的信息推送到所有订阅这些频道的客户端里面。 正在订阅频道的客户端不应该发送除 SUBSCRIBE channel [channel …] 和 UNSUBSCRIBE [channel [channel …]] 之外的其他命令。 其中， SUBSCRIBE channel [channel …] 可以用于订阅更多频道， 而 UNSUBSCRIBE [channel [channel …]] 则可以用于退订已订阅的一个或多个频道。 SUBSCRIBE channel [channel …] 和 UNSUBSCRIBE [channel [channel …]] 的执行结果会以信息的形式返回， 客户端可以通过分析所接收信息的第一个元素， 从而判断所收到的内容是一条真正的信息， 还是 SUBSCRIBE channel [channel …] 或 UNSUBSCRIBE [channel [channel …]] 命令的操作结果。 信息的格式频道转发的每条信息都是一条带有三个元素的多条批量回复（multi-bulk reply）。 信息的第一个元素标识了信息的类型： subscribe ： 表示当前客户端成功地订阅了信息第二个元素所指示的频道。 而信息的第三个元素则记录了目前客户端已订阅频道的总数。 unsubscribe ： 表示当前客户端成功地退订了信息第二个元素所指示的频道。 信息的第三个元素记录了客户端目前仍在订阅的频道数量。 当客户端订阅的频道数量降为 0 时， 客户端不再订阅任何频道， 它可以像往常一样， 执行任何 Redis 命令。 message ： 表示这条信息是由某个客户端执行 PUBLISH channel message 命令所发送的， 真正的信息。 信息的第二个元素是信息来源的频道， 而第三个元素则是信息的内容。 举个例子， 如果客户端执行以下命令：1redis&gt; SUBSCRIBE first second 那么它将收到以下回复：12345671) "subscribe"2) "first"3) (integer) 14) "subscribe"5) "second"6) (integer) 2 如果在这时， 另一个客户端执行以下 PUBLISH channel message 命令：1redis&gt; PUBLISH second Hello 那么之前订阅了 second 频道的客户端将收到以下信息：1231) "message"2) "second"3) "hello" 当订阅者决定退订所有频道时， 它可以执行一个无参数的 UNSUBSCRIBE [channel [channel …]] 命令：1redis&gt; UNSUBSCRIBE 这个命令将接到以下回复：12345671) "unsubscribe"2) "second"3) (integer) 11) "unsubscribe"2) "first"3) (integer) 0 订阅模式Redis 的发布与订阅实现支持模式匹配（pattern matching）： 客户端可以订阅一个带 * 号的模式， 如果某个/某些频道的名字和这个模式匹配， 那么当有信息发送给这个/这些频道的时候， 客户端也会收到这个/这些频道的信息。 比如说，执行命令1redis&gt; PSUBSCRIBE news.* 的客户端将收到来自 news.art.figurative 、 news.music.jazz 等频道的信息。 客户端订阅的模式里面可以包含多个 glob 风格的通配符， 比如 * 、 ? 和 […] ， 等等。 执行命令1redis&gt; PUNSUBSCRIBE news.* 将退订 news.* 模式， 其他已订阅的模式不会被影响。 通过订阅模式接收到的信息， 和通过订阅频道接收到的信息， 这两者的格式不太一样： 通过订阅模式而接收到的信息的类型为 pmessage ： 这代表有某个客户端通过 PUBLISH channel message 向某个频道发送了信息， 而这个频道刚好匹配了当前客户端所订阅的某个模式。 信息的第二个元素记录了被匹配的模式， 第三个元素记录了被匹配的频道的名字， 最后一个元素则记录了信息的实际内容。 客户端处理 PSUBSCRIBE pattern [pattern …] 和 PUNSUBSCRIBE [pattern [pattern …]] 返回值的方式， 和客户端处理 SUBSCRIBE channel [channel …] 和 UNSUBSCRIBE [channel [channel …]] 的方式类似： 通过对信息的第一个元素进行分析， 客户端可以判断接收到的信息是一个真正的信息， 还是 PSUBSCRIBE pattern [pattern …] 或 PUNSUBSCRIBE [pattern [pattern …]] 命令的返回值。 通过频道和模式接收同一条信息如果客户端订阅的多个模式匹配了同一个频道， 或者客户端同时订阅了某个频道、以及匹配这个频道的某个模式， 那么它可能会多次接收到同一条信息。 举个例子， 如果客户端执行了以下命令：12SUBSCRIBE fooPSUBSCRIBE f* 那么当有信息发送到频道 foo 时， 客户端将收到两条信息： 一条来自频道 foo ，信息类型为 message ； 另一条来自模式 f* ，信息类型为 pmessage 。 订阅总数在执行 SUBSCRIBE channel [channel …] 、 UNSUBSCRIBE [channel [channel …]] 、 PSUBSCRIBE pattern [pattern …] 和 PUNSUBSCRIBE [pattern [pattern …]] 命令时， 返回结果的最后一个元素是客户端目前仍在订阅的频道和模式总数。 当客户端退订所有频道和模式， 也即是这个总数值下降为 0 的时候， 客户端将退出订阅与发布状态。 编程示例Pieter Noordhuis 提供了一个使用 EventMachine 和 Redis 编写的 高性能多用户网页聊天软件 ， 这个软件很好地展示了发布与订阅功能的用法。 客户端库实现提示因为所有接收到的信息都会包含一个信息来源： 当信息来自频道时，来源是某个频道； 当信息来自模式时，来源是某个模式。 因此， 客户端可以用一个哈希表， 将特定来源和处理该来源的回调函数关联起来。 当有新信息到达时， 程序就可以根据信息的来源， 在 O(1) 复杂度内， 将信息交给正确的回调函数来处理。 SentinelRedis 的 Sentinel 系统用于管理多个 Redis 服务器（instance）， 该系统执行以下三个任务： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis Sentinel 是一个分布式系统， 你可以在一个架构中运行多个 Sentinel 进程（progress）， 这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移， 以及选择哪个从服务器作为新的主服务器。 虽然 Redis Sentinel 释出为一个单独的可执行文件 redis-sentinel ， 但实际上它只是一个运行在特殊模式下的 Redis 服务器， 你可以在启动一个普通 Redis 服务器时通过给定 –sentinel 选项来启动 Redis Sentinel 。 Warning Redis Sentinel 目前仍在开发中， 这个文档的内容可能随着 Sentinel 实现的修改而变更。 Redis Sentinel 兼容 Redis 2.4.16 或以上版本， 推荐使用 Redis 2.8.0 或以上的版本。 获取 Sentinel目前 Sentinel 系统是 Redis 的 unstable 分支的一部分， 你必须到 Redis 项目的 Github 页面 克隆一份 unstable 分值， 然后通过编译来获得 Sentinel 系统。 Sentinel 程序可以在编译后的 src 文档中发现， 它是一个命名为 redis-sentinel 的程序。 你也可以通过下一节介绍的方法， 让 redis-server 程序运行在 Sentinel 模式之下。 另外， 一个新版本的 Sentinel 已经包含在了 Redis 2.8.0 版本的释出文件中。 启动 Sentinel对于 redis-sentinel 程序， 你可以用以下命令来启动 Sentinel 系统：1redis-sentinel /path/to/sentinel.conf 对于 redis-server 程序， 你可以用以下命令来启动一个运行在 Sentinel 模式下的 Redis 服务器：1redis-server /path/to/sentinel.conf --sentinel 两种方法都可以启动一个 Sentinel 实例。 启动 Sentinel 实例必须指定相应的配置文件， 系统会使用配置文件来保存 Sentinel 的当前状态， 并在 Sentinel 重启时通过载入配置文件来进行状态还原。 如果启动 Sentinel 时没有指定相应的配置文件， 或者指定的配置文件不可写（not writable）， 那么 Sentinel 会拒绝启动。 配置 SentinelRedis 源码中包含了一个名为 sentinel.conf 的文件， 这个文件是一个带有详细注释的 Sentinel 配置文件示例。 运行一个 Sentinel 所需的最少配置如下所示：123456789sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1sentinel monitor resque 192.168.1.3 6380 4sentinel down-after-milliseconds resque 10000sentinel failover-timeout resque 180000sentinel parallel-syncs resque 5 第一行配置指示 Sentinel 去监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 127.0.0.1 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个 Sentinel 同意 （只要同意 Sentinel 的数量不达标，自动故障迁移就不会执行）。 不过要注意， 无论你设置要多少个 Sentinel 同意才能判断一个服务器失效， 一个 Sentinel 都需要获得系统中多数（majority） Sentinel 的支持， 才能发起一次自动故障迁移， 并预留一个给定的配置纪元 （configuration Epoch ，一个配置纪元就是一个新主服务器配置的版本号）。 换句话说， 在只有少数（minority） Sentinel 进程正常运作的情况下， Sentinel 是不能执行自动故障迁移的。 其他选项的基本格式如下： sentinel &lt;选项的名字&gt; &lt;主服务器的名字&gt; &lt;选项的值&gt; 各个选项的功能如下： down-after-milliseconds 选项指定了 Sentinel 认为服务器已经断线所需的毫秒数。 如果服务器在给定的毫秒数之内， 没有返回 Sentinel 发送的 PING 命令的回复， 或者返回一个错误， 那么 Sentinel 将这个服务器标记为主观下线（subjectively down，简称 SDOWN ）。 不过只有一个 Sentinel 将服务器标记为主观下线并不一定会引起服务器的自动故障迁移： 只有在足够数量的 Sentinel 都将一个服务器标记为主观下线之后， 服务器才会被标记为客观下线（objectively down， 简称 ODOWN ）， 这时自动故障迁移才会执行。 将服务器标记为客观下线所需的 Sentinel 数量由对主服务器的配置决定。 parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。 如果从服务器被设置为允许使用过期数据集（参见对 redis.conf 文件中对 slave-serve-stale-data 选项的说明）， 那么你可能不希望所有从服务器都在同一时间向新的主服务器发送同步请求， 因为尽管复制过程的绝大部分步骤都不会阻塞从服务器， 但从服务器在载入主服务器发来的 RDB 文件时， 仍然会造成从服务器在一段时间内不能处理命令请求： 如果全部从服务器一起对新的主服务器进行同步， 那么就可能会造成所有从服务器在短时间内全部不可用的情况出现。 你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。 本文档剩余的内容将对 Sentinel 系统的其他选项进行介绍， 示例配置文件 sentinel.conf 也对相关的选项进行了完整的注释。 主观下线和客观下线前面说过， Redis 的 Sentinel 中关于下线（down）有两个不同的概念： 主观下线（Subjectively Down， 简称 SDOWN）指的是单个 Sentinel 实例对服务器做出的下线判断。 客观下线（Objectively Down， 简称 ODOWN）指的是多个 Sentinel 实例在对同一个服务器做出 SDOWN 判断， 并且通过 SENTINEL is-master-down-by-addr 命令互相交流之后， 得出的服务器下线判断。 （一个 Sentinel 可以通过向另一个 Sentinel 发送 SENTINEL is-master-down-by-addr 命令来询问对方是否认为给定的服务器已下线。） 如果一个服务器没有在 master-down-after-milliseconds 选项所指定的时间内， 对向它发送 PING 命令的 Sentinel 返回一个有效回复（valid reply）， 那么 Sentinel 就会将这个服务器标记为主观下线。 服务器对 PING 命令的有效回复可以是以下三种回复的其中一种： 返回 +PONG 。 返回 -LOADING 错误。 返回 -MASTERDOWN 错误。 如果服务器返回除以上三种回复之外的其他回复， 又或者在指定时间内没有回复 PING 命令， 那么 Sentinel 认为服务器返回的回复无效（non-valid）。 注意， 一个服务器必须在 master-down-after-milliseconds 毫秒内， 一直返回无效回复才会被 Sentinel 标记为主观下线。 举个例子， 如果 master-down-after-milliseconds 选项的值为 30000 毫秒（30 秒）， 那么只要服务器能在每 29 秒之内返回至少一次有效回复， 这个服务器就仍然会被认为是处于正常状态的。 从主观下线状态切换到客观下线状态并没有使用严格的法定人数算法（strong quorum algorithm）， 而是使用了流言协议： 如果 Sentinel 在给定的时间范围内， 从其他 Sentinel 那里接收到了足够数量的主服务器下线报告， 那么 Sentinel 就会将主服务器的状态从主观下线改变为客观下线。 如果之后其他 Sentinel 不再报告主服务器已下线， 那么客观下线状态就会被移除。 客观下线条件只适用于主服务器： 对于任何其他类型的 Redis 实例， Sentinel 在将它们判断为下线前不需要进行协商， 所以从服务器或者其他 Sentinel 永远不会达到客观下线条件。 只要一个 Sentinel 发现某个主服务器进入了客观下线状态， 这个 Sentinel 就可能会被其他 Sentinel 推选出， 并对失效的主服务器执行自动故障迁移操作。 每个 Sentinel 都需要定期执行的任务 每个 Sentinel 以每秒钟一次的频率向它所知的主服务器、从服务器以及其他 Sentinel 实例发送一个 PING 命令。 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 那么这个实例会被 Sentinel 标记为主观下线。 一个有效回复可以是： +PONG 、 -LOADING 或者 -MASTERDOWN 。 如果一个主服务器被标记为主观下线， 那么正在监视这个主服务器的所有 Sentinel 要以每秒一次的频率确认主服务器的确进入了主观下线状态。 如果一个主服务器被标记为主观下线， 并且有足够数量的 Sentinel （至少要达到配置文件指定的数量）在指定的时间范围内同意这一判断， 那么这个主服务器被标记为客观下线。 在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有主服务器和从服务器发送 INFO [section] 命令。 当一个主服务器被 Sentinel 标记为客观下线时， Sentinel 向下线主服务器的所有从服务器发送 INFO [section] 命令的频率会从 10 秒一次改为每秒一次。 当没有足够数量的 Sentinel 同意主服务器已经下线， 主服务器的客观下线状态就会被移除。 当主服务器重新向 Sentinel 的 PING 命令返回有效回复时， 主服务器的主管下线状态就会被移除。 自动发现 Sentinel 和从服务器一个 Sentinel 可以与其他多个 Sentinel 进行连接， 各个 Sentinel 之间可以互相检查对方的可用性， 并进行信息交换。 你无须为运行的每个 Sentinel 分别设置其他 Sentinel 的地址， 因为 Sentinel 可以通过发布与订阅功能来自动发现正在监视相同主服务器的其他 Sentinel ， 这一功能是通过向频道 sentinel:hello 发送信息来实现的。 与此类似， 你也不必手动列出主服务器属下的所有从服务器， 因为 Sentinel 可以通过询问主服务器来获得所有从服务器的信息。 每个 Sentinel 会以每两秒一次的频率， 通过发布与订阅功能， 向被它监视的所有主服务器和从服务器的 sentinel:hello 频道发送一条信息， 信息中包含了 Sentinel 的 IP 地址、端口号和运行 ID （runid）。 每个 Sentinel 都订阅了被它监视的所有主服务器和从服务器的 sentinel:hello 频道， 查找之前未出现过的 sentinel （looking for unknown sentinels）。 当一个 Sentinel 发现一个新的 Sentinel 时， 它会将新的 Sentinel 添加到一个列表中， 这个列表保存了 Sentinel 已知的， 监视同一个主服务器的所有其他 Sentinel 。 Sentinel 发送的信息中还包括完整的主服务器当前配置（configuration）。 如果一个 Sentinel 包含的主服务器配置比另一个 Sentinel 发送的配置要旧， 那么这个 Sentinel 会立即升级到新配置上。 在将一个新 Sentinel 添加到监视主服务器的列表上面之前， Sentinel 会先检查列表中是否已经包含了和要添加的 Sentinel 拥有相同运行 ID 或者相同地址（包括 IP 地址和端口号）的 Sentinel ， 如果是的话， Sentinel 会先移除列表中已有的那些拥有相同运行 ID 或者相同地址的 Sentinel ， 然后再添加新 Sentinel 。 Sentinel API在默认情况下， Sentinel 使用 TCP 端口 26379 （普通 Redis 服务器使用的是 6379 ）。 Sentinel 接受 Redis 协议格式的命令请求， 所以你可以使用 redis-cli 或者任何其他 Redis 客户端来与 Sentinel 进行通讯。 有两种方式可以和 Sentinel 进行通讯： 第一种方法是通过直接发送命令来查询被监视 Redis 服务器的当前状态， 以及 Sentinel 所知道的关于其他 Sentinel 的信息， 诸如此类。 另一种方法是使用发布与订阅功能， 通过接收 Sentinel 发送的通知： 当执行故障转移操作， 或者某个被监视的服务器被判断为主观下线或者客观下线时， Sentinel 就会发送相应的信息。 Sentinel 命令以下列出的是 Sentinel 接受的命令： PING ：返回 PONG 。 SENTINEL masters ：列出所有被监视的主服务器，以及这些主服务器的当前状态。 SENTINEL slaves \ ：列出给定主服务器的所有从服务器，以及这些从服务器的当前状态。 SENTINEL get-master-addr-by-name \ ： 返回给定名字的主服务器的 IP 地址和端口号。 如果这个主服务器正在执行故障转移操作， 或者针对这个主服务器的故障转移操作已经完成， 那么这个命令返回新的主服务器的 IP 地址和端口号。 SENTINEL reset \&lt;pattern> ： 重置所有名字和给定模式 pattern 相匹配的主服务器。 pattern 参数是一个 Glob 风格的模式。 重置操作清除主服务器目前的所有状态， 包括正在执行中的故障转移， 并移除目前已经发现和关联的， 主服务器的所有从服务器和 Sentinel 。 SENTINEL failover \ ： 当主服务器失效时， 在不询问其他 Sentinel 意见的情况下， 强制开始一次自动故障迁移 （不过发起故障转移的 Sentinel 会向其他 Sentinel 发送一个新的配置，其他 Sentinel 会根据这个配置进行相应的更新）。 发布与订阅信息客户端可以将 Sentinel 看作是一个只提供了订阅功能的 Redis 服务器： 你不可以使用 PUBLISH channel message 命令向这个服务器发送信息， 但你可以用 SUBSCRIBE channel [channel …] 命令或者 PSUBSCRIBE pattern [pattern …] 命令， 通过订阅给定的频道来获取相应的事件提醒。 一个频道能够接收和这个频道的名字相同的事件。 比如说， 名为 +sdown 的频道就可以接收所有实例进入主观下线（SDOWN）状态的事件。 通过执行 PSUBSCRIBE * 命令可以接收所有事件信息。 以下列出的是客户端可以通过订阅来获得的频道和信息的格式： 第一个英文单词是频道/事件的名字， 其余的是数据的格式。 注意， 当格式中包含 instance details 字样时， 表示频道所返回的信息中包含了以下用于识别目标实例的内容：1&lt;instance-type&gt; &lt;name&gt; &lt;ip&gt; &lt;port&gt; @ &lt;master-name&gt; &lt;master-ip&gt; &lt;master-port&gt; @ 字符之后的内容用于指定主服务器， 这些内容是可选的， 它们仅在 @ 字符之前的内容指定的实例不是主服务器时使用。 +reset-master \ ：主服务器已被重置。 +slave \ ：一个新的从服务器已经被 Sentinel 识别并关联。 +failover-state-reconf-slaves \ ：故障转移状态切换到了 reconf-slaves 状态。 +failover-detected \ ：另一个 Sentinel 开始了一次故障转移操作，或者一个从服务器转换成了主服务器。 +slave-reconf-sent \ ：领头（leader）的 Sentinel 向实例发送了 SLAVEOF host port 命令，为实例设置新的主服务器。 +slave-reconf-inprog \ ：实例正在将自己设置为指定主服务器的从服务器，但相应的同步过程仍未完成。 +slave-reconf-done \ ：从服务器已经成功完成对新主服务器的同步。 -dup-sentinel \ ：对给定主服务器进行监视的一个或多个 Sentinel 已经因为重复出现而被移除 —— 当 Sentinel 实例重启的时候，就会出现这种情况。 +sentinel \ ：一个监视给定主服务器的新 Sentinel 已经被识别并添加。 +sdown \ ：给定的实例现在处于主观下线状态。 -sdown \ ：给定的实例已经不再处于主观下线状态。 +odown \ ：给定的实例现在处于客观下线状态。 -odown \ ：给定的实例已经不再处于客观下线状态。 +new-epoch \ ：当前的纪元（epoch）已经被更新。 +try-failover \ ：一个新的故障迁移操作正在执行中，等待被大多数 Sentinel 选中（waiting to be elected by the majority）。 +elected-leader \ ：赢得指定纪元的选举，可以进行故障迁移操作了。 +failover-state-select-slave \ ：故障转移操作现在处于 select-slave 状态 —— Sentinel 正在寻找可以升级为主服务器的从服务器。 no-good-slave \ ：Sentinel 操作未能找到适合进行升级的从服务器。Sentinel 会在一段时间之后再次尝试寻找合适的从服务器来进行升级，又或者直接放弃执行故障转移操作。 selected-slave \ ：Sentinel 顺利找到适合进行升级的从服务器。 failover-state-send-slaveof-noone \ ：Sentinel 正在将指定的从服务器升级为主服务器，等待升级功能完成。 failover-end-for-timeout \ ：故障转移因为超时而中止，不过最终所有从服务器都会开始复制新的主服务器（slaves will eventually be configured to replicate with the new master anyway）。 failover-end \ ：故障转移操作顺利完成。所有从服务器都开始复制新的主服务器了。 +switch-master \ \&lt;oldip> \&lt;oldport> \&lt;newip> \&lt;newport> ：配置变更，主服务器的 IP 和地址已经改变。 这是绝大多数外部用户都关心的信息。 +tilt ：进入 tilt 模式。 -tilt ：退出 tilt 模式。 故障转移一次故障转移操作由以下步骤组成： 发现主服务器已经进入客观下线状态。 对我们的当前纪元进行自增（详情请参考 Raft leader election ）， 并尝试在这个纪元中当选。 如果当选失败， 那么在设定的故障迁移超时时间的两倍之后， 重新尝试当选。 如果当选成功， 那么执行以下步骤。 选出一个从服务器，并将它升级为主服务器。 向被选中的从服务器发送 SLAVEOF NO ONE 命令，让它转变为主服务器。 通过发布与订阅功能， 将更新后的配置传播给所有其他 Sentinel ， 其他 Sentinel 对它们自己的配置进行更新。 向已下线主服务器的从服务器发送 SLAVEOF host port 命令， 让它们去复制新的主服务器。 当所有从服务器都已经开始复制新的主服务器时， 领头 Sentinel 终止这次故障迁移操作。 Note 每当一个 Redis 实例被重新配置（reconfigured） —— 无论是被设置成主服务器、从服务器、又或者被设置成其他主服务器的从服务器 —— Sentinel 都会向被重新配置的实例发送一个 CONFIG REWRITE 命令， 从而确保这些配置会持久化在硬盘里。 Sentinel 使用以下规则来选择新的主服务器： 在失效主服务器属下的从服务器当中， 那些被标记为主观下线、已断线、或者最后一次回复 PING 命令的时间大于五秒钟的从服务器都会被淘汰。 在失效主服务器属下的从服务器当中， 那些与失效主服务器连接断开的时长超过 down-after 选项指定的时长十倍的从服务器都会被淘汰。 在经历了以上两轮淘汰之后剩下来的从服务器中， 我们选出复制偏移量（replication offset）最大的那个从服务器作为新的主服务器； 如果复制偏移量不可用， 或者从服务器的复制偏移量相同， 那么带有最小运行 ID 的那个从服务器成为新的主服务器。 Sentinel 自动故障迁移的一致性特质Sentinel 自动故障迁移使用 Raft 算法来选举领头（leader） Sentinel ， 从而确保在一个给定的纪元（epoch）里， 只有一个领头产生。 这表示在同一个纪元中， 不会有两个 Sentinel 同时被选中为领头， 并且各个 Sentinel 在同一个纪元中只会对一个领头进行投票。 更高的配置纪元总是优于较低的纪元， 因此每个 Sentinel 都会主动使用更新的纪元来代替自己的配置。 简单来说， 我们可以将 Sentinel 配置看作是一个带有版本号的状态。 一个状态会以最后写入者胜出（last-write-wins）的方式（也即是，最新的配置总是胜出）传播至所有其他 Sentinel 。 举个例子， 当出现网络分割（network partitions）时， 一个 Sentinel 可能会包含了较旧的配置， 而当这个 Sentinel 接到其他 Sentinel 发来的版本更新的配置时， Sentinel 就会对自己的配置进行更新。 如果要在网络分割出现的情况下仍然保持一致性， 那么应该使用 min-slaves-to-write 选项， 让主服务器在连接的从实例少于给定数量时停止执行写操作， 与此同时， 应该在每个运行 Redis 主服务器或从服务器的机器上运行 Redis Sentinel 进程。 Sentinel 状态的持久化Sentinel 的状态会被持久化在 Sentinel 配置文件里面。 每当 Sentinel 接收到一个新的配置， 或者当领头 Sentinel 为主服务器创建一个新的配置时， 这个配置会与配置纪元一起被保存到磁盘里面。 这意味着停止和重启 Sentinel 进程都是安全的。 Sentinel 在非故障迁移的情况下对实例进行重新配置即使没有自动故障迁移操作在进行， Sentinel 总会尝试将当前的配置设置到被监视的实例上面。 特别是： 根据当前的配置， 如果一个从服务器被宣告为主服务器， 那么它会代替原有的主服务器， 成为新的主服务器， 并且成为原有主服务器的所有从服务器的复制对象。 那些连接了错误主服务器的从服务器会被重新配置， 使得这些从服务器会去复制正确的主服务器。不过， 在以上这些条件满足之后， Sentinel 在对实例进行重新配置之前仍然会等待一段足够长的时间， 确保可以接收到其他 Sentinel 发来的配置更新， 从而避免自身因为保存了过期的配置而对实例进行了不必要的重新配置。 TILT 模式Redis Sentinel 严重依赖计算机的时间功能： 比如说， 为了判断一个实例是否可用， Sentinel 会记录这个实例最后一次相应 PING 命令的时间， 并将这个时间和当前时间进行对比， 从而知道这个实例有多长时间没有和 Sentinel 进行任何成功通讯。 不过， 一旦计算机的时间功能出现故障， 或者计算机非常忙碌， 又或者进程因为某些原因而被阻塞时， Sentinel 可能也会跟着出现故障。 TILT 模式是一种特殊的保护模式： 当 Sentinel 发现系统有些不对劲时， Sentinel 就会进入 TILT 模式。 因为 Sentinel 的时间中断器默认每秒执行 10 次， 所以我们预期时间中断器的两次执行之间的间隔为 100 毫秒左右。 Sentinel 的做法是， 记录上一次时间中断器执行时的时间， 并将它和这一次时间中断器执行的时间进行对比： 如果两次调用时间之间的差距为负值， 或者非常大（超过 2 秒钟）， 那么 Sentinel 进入 TILT 模式。 如果 Sentinel 已经进入 TILT 模式， 那么 Sentinel 延迟退出 TILT 模式的时间。 当 Sentinel 进入 TILT 模式时， 它仍然会继续监视所有目标， 但是： 它不再执行任何操作，比如故障转移。 当有实例向这个 Sentinel 发送 SENTINEL is-master-down-by-addr 命令时， Sentinel 返回负值： 因为这个 Sentinel 所进行的下线判断已经不再准确。 如果 TILT 可以正常维持 30 秒钟， 那么 Sentinel 退出 TILT 模式。 处理 -BUSY 状态Warning 该功能尚未实现 当 Lua 脚本的运行时间超过指定时限时， Redis 就会返回 -BUSY 错误。 当出现这种情况时， Sentinel 在尝试执行故障转移操作之前， 会先向服务器发送一个 SCRIPT KILL 命令， 如果服务器正在执行的是一个只读脚本的话， 那么这个脚本就会被杀死， 服务器就会回到正常状态。 Sentinel 的客户端实现关于 Sentinel 客户端的实现信息可以参考 Sentinel 客户端指引手册 。 集群教程本文档是 Redis 集群的入门教程， 从用户的角度介绍了设置、测试和操作集群的方法。 本教程不包含晦涩难懂的分布式概念， 也没有像 Redis 集群规范 那样包含 Redis 集群的实现细节， 如果你打算深入地学习 Redis 集群的部署方法， 那么推荐你在阅读完这个教程之后， 再去看一看集群规范。 Redis 集群目前仍处于 Alpha 测试版本， 如果在使用过程中发现任何问题， 请到 Redis 的邮件列表 发贴， 或者到 Redis 的 Github 页面 报告错误。 集群简介Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施（installation）。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处： 将数据自动切分（split）到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。 Redis 集群数据共享。Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说： 如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。 因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。 Redis 集群中的主从复制为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000 号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis 集群的一致性保证（guarantee）Redis 集群不保证数据的强一致性（strong consistency）： 在特定条件下， Redis 集群可能会丢失已经被执行过的写命令。 使用异步复制（asynchronous replication）是 Redis 集群可能会丢失写命令的其中一个原因。 考虑以下这个写命令的例子： 客户端向主节点 B 发送一条写命令。 主节点 B 执行写命令，并向客户端返回命令回复。 主节点 B 将刚刚执行的写命令复制给它的从节点 B1 、 B2 和 B3 。 如你所见， 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 Note 如果真的有必要的话， Redis 集群可能会在将来提供同步地（synchronou）执行写命令的方法。 Redis 集群另外一种可能会丢失命令的情况是， 集群出现网络分裂（network partition）， 并且一个客户端与至少包括一个主节点在内的少数（minority）实例被孤立。 举个例子， 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， 而 A1 、B1 、C1 分别为三个主节点的从节点， 另外还有一个客户端 Z1 。 假设集群中发生网络分裂， 那么集群可能会分裂为两方， 大多数（majority）的一方包含节点 A 、C 、A1 、B1 和 C1 ， 而少数（minority）的一方则包含节点 B 和客户端 Z1 。 在网络分裂期间， 主节点 B 仍然会接受 Z1 发送的写命令： 如果网络分裂出现的时间很短， 那么集群会继续正常运行； 但是， 如果网络分裂出现的时间足够长， 使得大多数一方将从节点 B1 设置为新的主节点， 并使用 B1 来代替原来的主节点 B ， 那么 Z1 发送给主节点 B 的写命令将丢失。 注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项： 对于大多数一方来说， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么集群会将这个主节点视为下线， 并使用从节点来代替这个主节点继续工作。 对于少数一方， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么它将停止处理写命令， 并向客户端报告错误。 创建并使用 Redis 集群Redis 集群由多个运行在集群模式（cluster mode）下的 Redis 实例组成， 实例的集群模式需要通过配置来开启， 开启集群模式的实例将可以使用集群特有的功能和命令。 以下是一个包含了最少选项的集群配置文件示例： 12345port 7000cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes 文件中的 cluster-enabled 选项用于开实例的集群模式， 而 cluster-conf-file 选项则设定了保存节点配置文件的路径， 默认值为 nodes.conf 。 节点配置文件无须人为修改， 它由 Redis 集群在启动时创建， 并在有需要时自动进行更新。 要让集群正常运作至少需要三个主节点， 不过在刚开始试用集群功能时， 强烈建议使用六个节点： 其中三个为主节点， 而其余三个则是各个主节点的从节点。 首先， 让我们进入一个新目录， 并创建六个以端口号为名字的子目录， 稍后我们在将每个目录中运行一个 Redis 实例： 123mkdir cluster-testcd cluster-testmkdir 7000 7001 7002 7003 7004 7005 在文件夹 7000 至 7005 中， 各创建一个 redis.conf 文件， 文件的内容可以使用上面的示例配置文件， 但记得将配置中的端口号从 7000 改为与文件夹名字相同的号码。 现在， 从 Redis Github 页面 的 unstable 分支中取出最新的 Redis 源码， 编译出可执行文件 redis-server ， 并将文件复制到 cluster-test 文件夹， 然后使用类似以下命令， 在每个标签页中打开一个实例： 12cd 7000../redis-server ./redis.conf 实例打印的日志显示， 因为 nodes.conf 文件不存在， 所以每个节点都为它自身指定了一个新的 ID ： 1[82462] 26 Nov 11:56:55.329 * No cluster configuration found, I'm 97a3a64667477371c4479320d683e4c8db5858b1 实例会一直使用同一个 ID ， 从而在集群中保持一个独一无二（unique）的名字。 每个节点都使用 ID 而不是 IP 或者端口号来记录其他节点， 因为 IP 地址和端口号都可能会改变， 而这个独一无二的标识符（identifier）则会在节点的整个生命周期中一直保持不变。 我们将这个标识符称为节点 ID。 创建集群现在我们已经有了六个正在运行中的 Redis 实例， 接下来我们需要使用这些实例来创建集群， 并为每个节点编写配置文件。 通过使用 Redis 集群命令行工具 redis-trib ， 编写节点配置文件的工作可以非常容易地完成： redis-trib 位于 Redis 源码的 src 文件夹中， 它是一个 Ruby 程序， 这个程序通过向实例发送特殊命令来完成创建新集群， 检查集群， 或者对集群进行重新分片（reshared）等工作。 我们需要执行以下命令来创建集群： 12./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 命令的意义如下： 给定 redis-trib.rb 程序的命令是 create ， 这表示我们希望创建一个新的集群。 选项 –replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。 之后跟着的其他参数则是实例的地址列表， 我们希望程序使用这些地址所指示的实例来创建新集群。 简单来说， 以上命令的意思就是让 redis-trib 程序创建一个包含三个主节点和三个从节点的集群。 接着， redis-trib 会打印出一份预想中的配置给你看， 如果你觉得没问题的话， 就可以输入 yes ， redis-trib 就会将这份配置应用到集群当中： 12345678910111213141516171819202122232425&gt;&gt;&gt; Creating clusterConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7005: OK&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002127.0.0.1:7000 replica #1 is 127.0.0.1:7003127.0.0.1:7001 replica #1 is 127.0.0.1:7004127.0.0.1:7002 replica #1 is 127.0.0.1:7005M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5460 (5461 slots) masterM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5461-10921 (5461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:10922-16383 (5462 slots) masterS: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003S: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004S: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005Can I set the above configuration? (type 'yes' to accept): yes 输入 yes 并按下回车确认之后， 集群就会将配置应用到各个节点， 并连接起（join）各个节点 —— 也即是， 让各个节点开始互相通讯： 1234567891011121314151617&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5460 (5461 slots) masterM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5461-10921 (5461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:10922-16383 (5462 slots) masterM: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003slots: (0 slots) masterM: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004slots: (0 slots) masterM: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005slots: (0 slots) master[OK] All nodes agree about slots configuration. 如果一切正常的话， redis-trib 将输出以下信息： 123&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 这表示集群中的 16384 个槽都有至少一个主节点在处理， 集群运作正常。 集群的客户端Redis 集群现阶段的一个问题是客户端实现很少。 以下是一些我知道的实现： redis-rb-cluster 是我（@antirez）编写的 Ruby 实现， 用于作为其他实现的参考。 该实现是对 redis-rb 的一个简单包装， 高效地实现了与集群进行通讯所需的最少语义（semantic）。 redis-py-cluster 看上去是 redis-rb-cluster 的一个 Python 版本， 这个项目有一段时间没有更新了（最后一次提交是在六个月之前）， 不过可以将这个项目用作学习集群的起点。 流行的 Predis 曾经对早期的 Redis 集群有过一定的支持， 但我不确定它对集群的支持是否完整， 也不清楚它是否和最新版本的 Redis 集群兼容 （因为新版的 Redis 集群将槽的数量从 4k 改为 16k 了）。 Redis unstable 分支中的 redis-cli 程序实现了非常基本的集群支持， 可以使用命令 redis-cli -c 来启动。 测试 Redis 集群比较简单的办法就是使用 redis-rb-cluster 或者 redis-cli ， 接下来我们将使用 redis-cli 为例来进行演示： 12345678910111213141516$ redis-cli -c -p 7000redis 127.0.0.1:7000&gt; set foo bar-&gt; Redirected to slot [12182] located at 127.0.0.1:7002OKredis 127.0.0.1:7002&gt; set hello world-&gt; Redirected to slot [866] located at 127.0.0.1:7000OKredis 127.0.0.1:7000&gt; get foo-&gt; Redirected to slot [12182] located at 127.0.0.1:7002"bar"redis 127.0.0.1:7000&gt; get hello-&gt; Redirected to slot [866] located at 127.0.0.1:7000"world" redis-cli 对集群的支持是非常基本的， 所以它总是依靠 Redis 集群节点来将它转向（redirect）至正确的节点。 一个真正的（serious）集群客户端应该做得比这更好： 它应该用缓存记录起哈希槽与节点地址之间的映射（map）， 从而直接将命令发送到正确的节点上面。 这种映射只会在集群的配置出现某些修改时变化， 比如说， 在一次故障转移（failover）之后， 或者系统管理员通过添加节点或移除节点来修改了集群的布局（layout）之后， 诸如此类。 使用 redis-rb-cluster 编写一个示例应用在展示如何使用集群进行故障转移、重新分片等操作之前， 我们需要创建一个示例应用， 了解一些与 Redis 集群客户端进行交互的基本方法。 在运行示例应用的过程中， 我们会尝试让节点进入失效状态， 又或者开始一次重新分片， 以此来观察 Redis 集群在真实世界运行时的表现， 并且为了让这个示例尽可能地有用， 我们会让这个应用向集群进行写操作。 本节将通过两个示例应用来展示 redis-rb-cluster 的基本用法， 以下是本节的第一个示例应用， 它是一个名为 example.rb 的文件， 包含在redis-rb-cluster 项目里面： 123456789101112131415161718192021222324252627282930require './cluster'startup_nodes = [ &#123;:host =&gt; "127.0.0.1", :port =&gt; 7000&#125;, &#123;:host =&gt; "127.0.0.1", :port =&gt; 7001&#125;]rc = RedisCluster.new(startup_nodes,32,:timeout =&gt; 0.1)last = falsewhile not last begin last = rc.get("__last__") last = 0 if !last rescue =&gt; e puts "error #&#123;e.to_s&#125;" sleep 1 endend((last.to_i+1)..1000000000).each&#123;|x| begin rc.set("foo#&#123;x&#125;",x) puts rc.get("foo#&#123;x&#125;") rc.set("__last__",x) rescue =&gt; e puts "error #&#123;e.to_s&#125;" end sleep 0.1&#125; 这个应用所做的工作非常简单： 它不断地以 foo 为键， number 为值， 使用 SET key value [EX seconds] [PX milliseconds] [NX|XX] 命令向数据库设置键值对。 如果我们执行这个应用的话， 应用将按顺序执行以下命令： SET foo0 0 SET foo1 1 SET foo2 2 诸如此类。。。 代码中的每个集群操作都使用一个 begin 和 rescue 代码块（block）包裹着， 因为我们希望在代码出错时， 将错误打印到终端上面， 而不希望应用因为异常（exception）而退出。 代码的第七行是代码中第一个有趣的地方， 它创建了一个 Redis 集群对象， 其中创建对象所使用的参数及其意义如下： 第一个参数是记录了启动节点的 startup_nodes 列表， 列表中包含了两个集群节点的地址。 第二个参数指定了对于集群中的各个不同的节点， Redis 集群对象可以获得（take）的最大连接数 （maximum number of connections this object is allowed to take）。 第三个参数 timeout 指定了一个命令在执行多久之后， 才会被看作是执行失败。 记住， 启动列表中并不需要包含所有集群节点的地址， 但这些地址中至少要有一个是有效的（reachable）： 一旦 redis-rb-cluster 成功连接上集群中的某个节点时， 集群节点列表就会被自动更新， 任何真正的（serious）的集群客户端都应该这样做。 现在， 程序创建的 Redis 集群对象实例被保存到 rc 变量里面， 我们可以将这个对象当作普通 Redis 对象实例来使用。 在十一至十九行， 我们先尝试阅读计数器中的值， 如果计数器不存在的话， 我们才将计数器初始化为 0 ： 通过将计数值保存到 Redis 的计数器里面， 我们可以在示例重启之后， 仍然继续之前的执行过程， 而不必每次重启之后都从 foo0 开始重新设置键值对。 为了让程序在集群下线的情况下， 仍然不断地尝试读取计数器的值， 我们将读取操作包含在了一个 while 循环里面， 一般的应用程序并不需要如此小心。 二十一至三十行是程序的主循环， 这个循环负责设置键值对， 并在设置出错时打印错误信息。 程序在主循环的末尾添加了一个 sleep 调用， 让写操作的执行速度变慢， 帮助执行示例的人更容易看清程序的输出。 执行 example.rb 程序将产生以下输出： 1234567891011ruby ./example.rb123456789... 这个程序并不是十分有趣， 稍后我们就会看到一个更有趣的集群应用示例， 不过在此之前， 让我们先使用这个示例来演示集群的重新分片操作。 对集群进行重新分片现在， 让我们来试试对集群进行重新分片操作。 在执行重新分片的过程中， 请让你的 example.rb 程序处于运行状态， 这样你就会看到， 重新分片并不会对正在运行的集群程序产生任何影响， 你也可以考虑将 example.rb 中的 sleep 调用删掉， 从而让重新分片操作在近乎真实的写负载下执行。 重新分片操作基本上就是将某些节点上的哈希槽移动到另外一些节点上面， 和创建集群一样， 重新分片也可以使用 redis-trib 程序来执行。 执行以下命令可以开始一次重新分片操作： 1$ ./redis-trib.rb reshard 127.0.0.1:7000 你只需要指定集群中其中一个节点的地址， redis-trib 就会自动找到集群中的其他节点。 目前 redis-trib 只能在管理员的协助下完成重新分片的工作， 要让 redis-trib 自动将哈希槽从一个节点移动到另一个节点， 目前来说还做不到 （不过实现这个功能并不难）。 执行 redis-trib 的第一步就是设定你打算移动的哈希槽的数量： 12345678910111213141516171819202122232425$ ./redis-trib.rb reshard 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7004: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5460 (5461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:10922-16383 (5462 slots) masterS: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005slots: (0 slots) slaveM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5461-10921 (5461 slots) masterS: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003slots: (0 slots) slaveS: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004slots: (0 slots) slave[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 1000 我们将打算移动的槽数量设置为 1000 个， 如果 example.rb 程序一直运行着的话， 现在 1000 个槽里面应该有不少键了。 除了移动的哈希槽数量之外， redis-trib 还需要知道重新分片的目标（target node）， 也即是， 负责接收这 1000 个哈希槽的节点。 指定目标需要使用节点的 ID ， 而不是 IP 地址和端口。 比如说， 我们打算使用集群的第一个主节点来作为目标， 它的 IP 地址和端口是 127.0.0.1:7000 ， 而节点 ID 则是 9991306f0e50640a5684f1958fd754b38fa034c9 ， 那么我们应该向 redis-trib 提供节点的 ID ： 123$ ./redis-trib.rb reshard 127.0.0.1:7000...What is the receiving node ID? 9991306f0e50640a5684f1958fd754b38fa034c9 Note redis-trib 会打印出集群中所有节点的 ID ， 并且我们也可以通过执行以下命令来获得节点的运行 ID ： 12$ ./redis-cli -p 7000 cluster nodes | grep myself9991306f0e50640a5684f1958fd754b38fa034c9 :0 myself,master - 0 0 0 connected 0-5460 接着， redis-trib 会向你询问重新分片的源节点（source node）， 也即是， 要从哪个节点中取出 1000 个哈希槽， 并将这些槽移动到目标节点上面。 如果我们不打算从特定的节点上取出指定数量的哈希槽， 那么可以向 redis-trib 输入 all ， 这样的话， 集群中的所有主节点都会成为源节点， redis-trib 将从各个源节点中各取出一部分哈希槽， 凑够 1000 个， 然后移动到目标节点上面： 123456$ ./redis-trib.rb reshard 127.0.0.1:7000...Please enter all the source node IDs.Type 'all' to use all the nodes as source nodes for the hash slots.Type 'done' once you entered all the source nodes IDs.Source node #1:all 输入 all 并按下回车之后， redis-trib 将打印出哈希槽的移动计划， 如果你觉得没问题的话， 就可以输入 yes 并再次按下回车： 123456789$ ./redis-trib.rb reshard 127.0.0.1:7000...Moving slot 11421 from 393c6df5eb4b4cec323f0e4ca961c8b256e3460aMoving slot 11422 from 393c6df5eb4b4cec323f0e4ca961c8b256e3460aMoving slot 5461 from e68e52cee0550f558b03b342f2f0354d2b8a083bMoving slot 5469 from e68e52cee0550f558b03b342f2f0354d2b8a083b...Moving slot 5959 from e68e52cee0550f558b03b342f2f0354d2b8a083bDo you want to proceed with the proposed reshard plan (yes/no)? yes 输入 yes 并使用按下回车之后， redis-trib 就会正式开始执行重新分片操作， 将指定的哈希槽从源节点一个个地移动到目标节点上面： 12345678$ ./redis-trib.rb reshard 127.0.0.1:7000...Moving slot 5934 from 127.0.0.1:7001 to 127.0.0.1:7000:Moving slot 5935 from 127.0.0.1:7001 to 127.0.0.1:7000:Moving slot 5936 from 127.0.0.1:7001 to 127.0.0.1:7000:Moving slot 5937 from 127.0.0.1:7001 to 127.0.0.1:7000:...Moving slot 5959 from 127.0.0.1:7001 to 127.0.0.1:7000: 在重新分片的过程中， example.rb 应该可以继续正常运行， 不会出现任何问题。 在重新分片操作执行完毕之后， 可以使用以下命令来检查集群是否正常： 123456789101112131415161718192021222324$ ./redis-trib.rb check 127.0.0.1:7000Connecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7005: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7004: OK&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: 9991306f0e50640a5684f1958fd754b38fa034c9 127.0.0.1:7000slots:0-5959,10922-11422 (6461 slots) masterM: 393c6df5eb4b4cec323f0e4ca961c8b256e3460a 127.0.0.1:7002slots:11423-16383 (4961 slots) masterS: 3375be2ccc321932e8853234ffa87ee9fde973ff 127.0.0.1:7005slots: (0 slots) slaveM: e68e52cee0550f558b03b342f2f0354d2b8a083b 127.0.0.1:7001slots:5960-10921 (4962 slots) masterS: 48b728dbcedff6bf056231eb44990b7d1c35c3e0 127.0.0.1:7003slots: (0 slots) slaveS: 345ede084ac784a5c030a0387f8aaa9edfc59af3 127.0.0.1:7004slots: (0 slots) slave[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 根据检查结果显示， 集群运作正常。 需要注意的就是， 在三个主节点中， 节点 127.0.0.1:7000 包含了 6461 个哈希槽， 而节点 127.0.0.1:7001 和节点 127.0.0.1:7002 都只包含了 4961 个哈希槽， 因为后两者都将自己的 500 个哈希槽移动到了节点 127.0.0.1:7000 。 一个更有趣的示例应用我们在前面使用的示例程序 example.rb 并不是十分有趣， 因为它只是不断地对集群进行写入， 但并不检查写入结果是否正确。 比如说， 集群可能会错误地将 example.rb 发送的所有 SET key value [EX seconds] [PX milliseconds] [NX|XX] 命令都改成了 SET foo 42 ， 但因为 example.rb 并不检查写入后的值， 所以它不会意识到集群实际上写入的值是错误的。 因为这个原因， redis-rb-cluster 项目包含了一个名为 consistency-test.rb 的示例应用， 这个应用比起 example.rb 有趣得多： 它创建了多个计数器（默认为 1000 个）， 并通过发送 INCR key 命令来增加这些计数器的值。 在增加计数器值的同时， consistency-test.rb 还执行以下操作： 每次使用 INCR key 命令更新一个计数器时， 应用会记录下计数器执行 INCR key 命令之后应该有的值。 举个例子， 如果计数器的起始值为 0 ， 而这次是程序第 50 次向它发送 INCR key 命令， 那么计数器的值应该是 50 。 在每次发送 INCR key 命令之前， 程序会随机从集群中读取一个计数器的值， 并将它与自己记录的值进行对比， 看两个值是否相同。 换句话说， 这个程序是一个一致性检查器（consistency checker）： 如果集群在执行 INCR key 命令的过程中， 丢失了某条 INCR key 命令， 又或者多执行了某条客户端没有确认到的 INCR key 命令， 那么检查器将察觉到这一点 —— 在前一种情况中， consistency-test.rb 记录的计数器值将比集群记录的计数器值要大； 而在后一种情况中， consistency-test.rb 记录的计数器值将比集群记录的计数器值要小。 运行 consistency-test 程序将产生类似以下的输出： 12345678$ ruby consistency-test.rb925 R (0 err) | 925 W (0 err) |5030 R (0 err) | 5030 W (0 err) |9261 R (0 err) | 9261 W (0 err) |13517 R (0 err) | 13517 W (0 err) |17780 R (0 err) | 17780 W (0 err) |22025 R (0 err) | 22025 W (0 err) |25818 R (0 err) | 25818 W (0 err) | 每行输出都打印了程序执行的读取次数和写入次数， 以及执行操作的过程中因为集群不可用而产生的错误数。 如果程序察觉了不一致的情况出现， 它将在输出行的末尾显式不一致的详细情况。 比如说， 如果我们在 consistency-test.rb 运行的过程中， 手动修改某个计数器的值： 12$ redis 127.0.0.1:7000&gt; set key_217 0OK 那么 consistency-test.rb 将向我们报告不一致情况： 123456(in the other tab I see...)94774 R (0 err) | 94774 W (0 err) |98821 R (0 err) | 98821 W (0 err) |102886 R (0 err) | 102886 W (0 err) | 114 lost |107046 R (0 err) | 107046 W (0 err) | 114 lost | 在我们修改计数器值的时候， 计数器的正确值是 114 （执行了 114 次 INCR key 命令）， 因为我们将计数器的值设成了 0 ， 所以 consistency-test.rb 会向我们报告说丢失了 114 个 INCR key 命令。 因为这个示例程序具有一致性检查功能， 所以我们用它来测试 Redis 集群的故障转移操作。 故障转移测试Note 在执行本节操作的过程中， 请一直运行 consistency-test 程序。 要触发一次故障转移， 最简单的办法就是令集群中的某个主节点进入下线状态。 首先用以下命令列出集群中的所有主节点： 1234$ redis-cli -p 7000 cluster nodes | grep master3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385482984082 0 connected 5960-109212938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 master - 0 1385482983582 0 connected 11423-1638397a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422 通过命令输出， 我们知道端口号为 7000 、 7001 和 7002 的节点都是主节点， 然后我们可以通过向端口号为 7002 的主节点发送 DEBUG SEGFAULT 命令， 让这个主节点崩溃： 12$ redis-cli -p 7002 debug segfaultError: Server closed the connection 现在， 切换到运行着 consistency-test 的标签页， 可以看到， consistency-test 在 7002 下线之后的一段时间里将产生大量的错误警告信息： 1234567891018849 R (0 err) | 18849 W (0 err) |23151 R (0 err) | 23151 W (0 err) |27302 R (0 err) | 27302 W (0 err) |... many error warnings here ...29659 R (578 err) | 29660 W (577 err) |33749 R (578 err) | 33750 W (577 err) |37918 R (578 err) | 37919 W (577 err) |42077 R (578 err) | 42078 W (577 err) | 从 consistency-test 的这段输出可以看到， 集群在执行故障转移期间， 总共丢失了 578 个读命令和 577 个写命令， 但是并没有产生任何数据不一致。 这听上去可能有点奇怪， 因为在教程的开头我们提到过， Redis 使用的是异步复制， 在执行故障转移期间， 集群可能会丢失写命令。 但是在实际上， 丢失命令的情况并不常见， 因为 Redis 几乎是同时执行将命令回复发送给客户端， 以及将命令复制给从节点这两个操作， 所以实际上造成命令丢失的时间窗口是非常小的。 不过， 尽管出现的几率不高， 但丢失命令的情况还是有可能会出现的， 所以我们对 Redis 集群不能提供强一致性的这一描述仍然是正确的。 现在， 让我们使用 cluster nodes 命令， 查看集群在执行故障转移操作之后， 主从节点的布局情况： 1234567$ redis-cli -p 7000 cluster nodes3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385503418521 0 connecteda211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385503419023 0 connected97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-114223c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385503419023 3 connected 11423-163833e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385503417005 0 connected 5960-109212938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385503418016 3 connected 我重启了之前下线的 127.0.0.1:7002 节点， 该节点已经从原来的主节点变成了从节点， 而现在集群中的三个主节点分别是 127.0.0.1:7000 、 127.0.0.1:7001 和 127.0.0.1:7005 ， 其中 127.0.0.1:7005 就是因为 127.0.0.1:7002 下线而变成主节点的。 cluster nodes 命令的输出有点儿复杂， 它的每一行都是由以下信息组成的： 节点 ID ：例如 3fc783611028b1707fd65345e763befb36454d73 。 ip:port ：节点的 IP 地址和端口号， 例如 127.0.0.1:7000 ， 其中 :0 表示的是客户端当前连接的 IP 地址和端口号。 flags ：节点的角色（例如 master 、 slave 、 myself ）以及状态（例如 fail ，等等）。 如果节点是一个从节点的话， 那么跟在 flags 之后的将是主节点的节点 ID ： 例如 127.0.0.1:7002 的主节点的节点 ID 就是 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 。 集群最近一次向节点发送 PING 命令之后， 过去了多长时间还没接到回复。 节点最近一次返回 PONG 回复的时间。 节点的配置纪元（configuration epoch）：详细信息请参考 Redis 集群规范 。 本节点的网络连接情况：例如 connected 。 节点目前包含的槽：例如 127.0.0.1:7001 目前包含号码为 5960 至 10921 的哈希槽。 添加新节点到集群根据新添加节点的种类， 我们需要用两种方法来将新节点添加到集群里面： 如果要添加的新节点是一个主节点， 那么我们需要创建一个空节点（empty node）， 然后将某些哈希桶移动到这个空节点里面。 另一方面， 如果要添加的新节点是一个从节点， 那么我们需要将这个新节点设置为集群中某个节点的复制品（replica）。 本节将对以上两种情况进行介绍， 首先介绍主节点的添加方法， 然后再介绍从节点的添加方法。 无论添加的是那种节点， 第一步要做的总是添加一个空节点。 我们可以继续使用之前启动 127.0.0.1:7000 、 127.0.0.1:7001 等节点的方法， 创建一个端口号为 7006 的新节点， 使用的配置文件也和之前一样， 只是记得要将配置中的端口号改为 7000 。 以下是启动端口号为 7006 的新节点的详细步骤： 在终端里创建一个新的标签页。 进入 cluster-test 文件夹。 创建并进入 7006 文件夹。 将 redis.conf 文件复制到 7006 文件夹里面，然后将配置中的端口号选项改为 7006 。 使用命令 ../../redis-server redis.conf 启动节点。 如果一切正常， 那么节点应该会正确地启动。 接下来， 执行以下命令， 将这个新节点添加到集群里面： 1./redis-trib.rb add-node 127.0.0.1:7006 127.0.0.1:7000 命令中的 add-node 表示我们要让 redis-trib 将一个节点添加到集群里面， add-node 之后跟着的是新节点的 IP 地址和端口号， 再之后跟着的是集群中任意一个已存在节点的 IP 地址和端口号， 这里我们使用的是 127.0.0.1:7000 。 通过 cluster nodes 命令， 我们可以确认新节点 127.0.0.1:7006 已经被添加到集群里面了： 12345678redis 127.0.0.1:7006&gt; cluster nodes3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385543178575 0 connected 5960-109213fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385543179583 0 connectedf093c80dde814da99c5cf72a7dd01590792b783b :0 myself,master - 0 0 0 connected2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543178072 3 connecteda211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385543178575 0 connected97a3a64667477371c4479320d683e4c8db5858b1 127.0.0.1:7000 master - 0 1385543179080 0 connected 0-5959 10922-114223c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385543177568 3 connected 11423-16383 新节点现在已经连接上了集群， 成为集群的一份子， 并且可以对客户端的命令请求进行转向了， 但是和其他主节点相比， 新节点还有两点区别： 新节点没有包含任何数据， 因为它没有包含任何哈希桶。 尽管新节点没有包含任何哈希桶， 但它仍然是一个主节点， 所以在集群需要将某个从节点升级为新的主节点时， 这个新节点不会被选中。 接下来， 只要使用 redis-trib 程序， 将集群中的某些哈希桶移动到新节点里面， 新节点就会成为真正的主节点了。 因为使用 redis-trib 移动哈希桶的方法在前面已经介绍过， 所以这里就不再重复介绍了。 现在， 让我们来看看， 将一个新节点转变为某个主节点的复制品（也即是从节点）的方法。 举个例子， 如果我们打算让新节点成为 127.0.0.1:7005 的从节点， 那么我们只要用客户端连接上新节点， 然后执行以下命令就可以了： 1redis 127.0.0.1:7006&gt; cluster replicate 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 其中命令提供的 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 就是主节点 127.0.0.1:7005 的节点 ID 。 执行 cluster replicate 命令之后， 我们可以使用以下命令来确认 127.0.0.1:7006 已经成为了 ID 为 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 的节点的从节点： 123$ redis-cli -p 7000 cluster nodes | grep slave | grep 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912ef093c80dde814da99c5cf72a7dd01590792b783b 127.0.0.1:7006 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617702 3 connected2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617198 3 connected 3c3a0c… 现在有两个从节点， 一个从节点的端口号为 7002 ， 而另一个从节点的端口号为 7006 。 移除一个节点未完待续。 键空间通知（keyspace notification）功能概览键空间通知使得客户端可以通过订阅频道或模式， 来接收那些以某种方式改动了 Redis 数据集的事件。 以下是一些键空间通知发送的事件的例子： 所有修改键的命令。 所有接收到 LPUSH key value [value …] 命令的键。 0 号数据库中所有已过期的键。 事件通过 Redis 的订阅与发布功能（pub/sub）来进行分发， 因此所有支持订阅与发布功能的客户端都可以在无须做任何修改的情况下， 直接使用键空间通知功能。 因为 Redis 目前的订阅与发布功能采取的是发送即忘（fire and forget）策略， 所以如果你的程序需要可靠事件通知（reliable notification of events）， 那么目前的键空间通知可能并不适合你： 当订阅事件的客户端断线时， 它会丢失所有在断线期间分发给它的事件。 未来将会支持更可靠的事件分发， 这种支持可能会通过让订阅与发布功能本身变得更可靠来实现， 也可能会在 Lua 脚本中对消息（message）的订阅与发布进行监听， 从而实现类似将事件推入到列表这样的操作。 事件的类型对于每个修改数据库的操作，键空间通知都会发送两种不同类型的事件。 比如说，对 0 号数据库的键 mykey 执行 DEL key [key …] 命令时， 系统将分发两条消息， 相当于执行以下两个 PUBLISH channel message 命令：12PUBLISH __keyspace@0__:mykey delPUBLISH __keyevent@0__:del mykey 订阅第一个频道 __keyspace@0__:mykey 可以接收 0 号数据库中所有修改键 mykey 的事件， 而订阅第二个频道 __keyevent@0__:del 则可以接收 0 号数据库中所有执行 del 命令的键。 以 keyspace 为前缀的频道被称为键空间通知（key-space notification）， 而以 keyevent 为前缀的频道则被称为键事件通知（key-event notification）。 当 del mykey 命令执行时： 键空间频道的订阅者将接收到被执行的事件的名字，在这个例子中，就是 del 。 键事件频道的订阅者将接收到被执行事件的键的名字，在这个例子中，就是 mykey 。 配置因为开启键空间通知功能需要消耗一些 CPU ， 所以在默认配置下， 该功能处于关闭状态。 可以通过修改 redis.conf 文件， 或者直接使用 CONFIG SET 命令来开启或关闭键空间通知功能： 当 notify-keyspace-events 选项的参数为空字符串时，功能关闭。 另一方面，当参数不是空字符串时，功能开启。 notify-keyspace-events 的参数可以是以下字符的任意组合， 它指定了服务器该发送哪些类型的通知： 字符 发送的通知 K 键空间通知，所有通知以 __keyspace@\&lt;db>__ 为前缀 E 键事件通知，所有通知以 __keyevent@\&lt;db>__ 为前缀 g DEL 、 EXPIRE 、 RENAME 等类型无关的通用命令的通知 $ 字符串命令的通知 l 列表命令的通知 s 集合命令的通知 h 哈希命令的通知 z 有序集合命令的通知 x 过期事件：每当有过期键被删除时发送 e 驱逐(evict)事件：每当有键因为 maxmemory 政策而被删除时发送 A 参数 g$lshzxe 的别名 输入的参数中至少要有一个 K 或者 E ， 否则的话， 不管其余的参数是什么， 都不会有任何通知被分发。 举个例子， 如果只想订阅键空间中和列表相关的通知， 那么参数就应该设为 Kl ， 诸如此类。 将参数设为字符串 “AKE” 表示发送所有类型的通知。 命令产生的通知以下列表记录了不同命令所产生的不同通知： DEL key [key …] 命令为每个被删除的键产生一个 del 通知。 RENAME key newkey 产生两个通知：为来源键（source key）产生一个 rename_from 通知，并为目标键（destination key）产生一个 rename_to 通知。 EXPIRE key seconds 和 EXPIREAT key timestamp 在键被正确设置过期时间时产生一个 expire 通知。当 EXPIREAT key timestamp 设置的时间已经过期，或者 EXPIRE key seconds 传入的时间为负数值时，键被删除，并产生一个 del 通知。 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] 在命令带有 STORE 参数时产生一个 sortstore 事件。如果 STORE 指示的用于保存排序结果的键已经存在，那么程序还会发送一个 del 事件。 SET key value [EX seconds] [PX milliseconds] [NX|XX] 以及它的所有变种（SETEX key seconds value 、 SETNX key value 和 GETSET key value）都产生 set 通知。其中 SETEX key seconds value 还会产生 expire 通知。 MSET key value [key value …] 为每个键产生一个 set 通知。 SETRANGE key offset value 产生一个 setrange 通知。 INCR key 、 DECR key 、 INCRBY key increment 和 DECRBY key decrement 都产生 incrby 通知。 INCRBYFLOAT key increment 产生 incrbyfloat 通知。 APPEND key value 产生 append 通知。 LPUSH key value [value …] 和 LPUSHX key value 都产生单个 lpush 通知，即使有多个输入元素时，也是如此。 RPUSH key value [value …] 和 RPUSHX key value 都产生单个 rpush 通知，即使有多个输入元素时，也是如此。 RPOP key 产生 rpop 通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个 del 通知。 LPOP key 产生 lpop 通知。如果被弹出的元素是列表的最后一个元素，那么还会产生一个 del 通知。 LINSERT key BEFORE|AFTER pivot value 产生一个 linsert 通知。 LSET key index value 产生一个 lset 通知。 LTRIM key start stop 产生一个 ltrim 通知。如果 LTRIM key start stop 执行之后，列表键被清空，那么还会产生一个 del 通知。 RPOPLPUSH source destination 和 BRPOPLPUSH source destination timeout 产生一个 rpop 通知，以及一个 lpush 通知。两个命令都会保证 rpop 的通知在 lpush 的通知之前分发。如果从键弹出元素之后，被弹出的列表键被清空，那么还会产生一个 del 通知。 HSET hash field value 、 HSETNX hash field value 和 HMSET 都只产生一个 hset 通知。 HINCRBY 产生一个 hincrby 通知。 HINCRBYFLOAT 产生一个 hincrbyfloat 通知。 HDEL 产生一个 hdel 通知。如果执行 HDEL 之后，哈希键被清空，那么还会产生一个 del 通知。 SADD key member [member …] 产生一个 sadd 通知，即使有多个输入元素时，也是如此。 SREM key member [member …] 产生一个 srem 通知，如果执行 SREM key member [member …] 之后，集合键被清空，那么还会产生一个 del 通知。 SMOVE source destination member 为来源键（source key）产生一个 srem 通知，并为目标键（destination key）产生一个 sadd 事件。 SPOP key 产生一个 spop 事件。如果执行 SPOP key 之后，集合键被清空，那么还会产生一个 del 通知。 SINTERSTORE destination key [key …] 、 SUNIONSTORE destination key [key …] 和 SDIFFSTORE destination key [key …] 分别产生 sinterstore 、 sunionostore 和 sdiffstore 三种通知。如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 ZINCRBY key increment member 产生一个 zincr 通知。（译注：非对称，请注意。） ZADD key score member [[score member] [score member] …] 产生一个 zadd 通知，即使有多个输入元素时，也是如此。 ZREM key member [member …] 产生一个 zrem 通知，即使有多个输入元素时，也是如此。如果执行 ZREM key member [member …] 之后，有序集合键被清空，那么还会产生一个 del 通知。 ZREMRANGEBYSCORE key min max 产生一个 zrembyscore 通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 ZREMRANGEBYRANK key start stop 产生一个 zrembyrank 通知。（译注：非对称，请注意。）如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 ZINTERSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 和 ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 分别产生 zinterstore 和 zunionstore 两种通知。如果用于保存结果的键已经存在，那么还会产生一个 del 通知。 每当一个键因为过期而被删除时，产生一个 expired 通知。 每当一个键因为 maxmemory 政策而被删除以回收内存时，产生一个 evicted 通知。 Note 所有命令都只在键真的被改动了之后，才会产生通知。 比如说，当 SREM key member [member …] 试图删除不存在于集合的元素时，删除操作会执行失败，因为没有真正的改动键，所以这一操作不会发送通知。 如果对命令所产生的通知有疑问， 最好还是使用以下命令， 自己来验证一下：1234$ redis-cli config set notify-keyspace-events KEA$ redis-cli --csv psubscribe '__key*__:*'Reading messages... (press Ctrl-C to quit)"psubscribe","__key*__:*",1 然后， 只要在其他终端里用 Redis 客户端发送命令， 就可以看到产生的通知了：123"pmessage","__key*__:*","__keyspace@0__:foo","set""pmessage","__key*__:*","__keyevent@0__:set","foo"... 过期通知的发送时间Redis 使用以下两种方式删除过期的键： 当一个键被访问时，程序会对这个键进行检查，如果键已经过期，那么该键将被删除。 底层系统会在后台渐进地查找并删除那些过期的键，从而处理那些已经过期、但是不会被访问到的键。 当过期键被以上两个程序的任意一个发现、 并且将键从数据库中删除时， Redis 会产生一个 expired 通知。 Redis 并不保证生存时间（TTL）变为 0 的键会立即被删除： 如果程序没有访问这个过期键， 或者带有生存时间的键非常多的话， 那么在键的生存时间变为 0 ， 直到键真正被删除这中间， 可能会有一段比较显著的时间间隔。 因此， Redis 产生 expired 通知的时间为过期键被删除的时候， 而不是键的生存时间变为 0 的时候。 通信协议（protocol）Redis 协议在以下三个目标之间进行折中： 易于实现 可以高效地被计算机分析（parse） 可以很容易地被人类读懂 网络层客户端和服务器通过 TCP 连接来进行数据交互， 服务器默认的端口号为 6379 。 客户端和服务器发送的命令或数据一律以 \r\n （CRLF）结尾。 请求Redis 服务器接受命令以及命令的参数。 服务器会在接到命令之后，对命令进行处理，并将命令的回复传送回客户端。 新版统一请求协议新版统一请求协议在 Redis 1.2 版本中引入， 并最终在 Redis 2.0 版本成为 Redis 服务器通信的标准方式。 你的 Redis 客户端应该按照这个新版协议来进行实现。 在这个协议中， 所有发送至 Redis 服务器的参数都是二进制安全（binary safe）的。 以下是这个协议的一般形式：123456*&lt;参数数量&gt; CR LF$&lt;参数 1 的字节数量&gt; CR LF&lt;参数 1 的数据&gt; CR LF...$&lt;参数 N 的字节数量&gt; CR LF&lt;参数 N 的数据&gt; CR LF Note 译注：命令本身也作为协议的其中一个参数来发送。 举个例子， 以下是一个命令协议的打印版本：1234567*3$3SET$5mykey$7myvalue 这个命令的实际协议值如下：1"*3\r\n$3\r\nSET\r\n$5\r\nmykey\r\n$7\r\nmyvalue\r\n" 稍后我们会看到， 这种格式除了用作命令请求协议之外， 也用在命令的回复协议中： 这种只有一个参数的回复格式被称为批量回复（Bulk Reply）。 统一协议请求原本是用在回复协议中， 用于将列表的多个项返回给客户端的， 这种回复格式被称为多条批量回复（Multi Bulk Reply）。 一个多条批量回复以 *\&lt;argc>\r\n 为前缀， 后跟多条不同的批量回复， 其中 argc 为这些批量回复的数量。 回复Redis 命令会返回多种不同类型的回复。 通过检查服务器发回数据的第一个字节， 可以确定这个回复是什么类型： 状态回复（status reply）的第一个字节是 “+” 错误回复（error reply）的第一个字节是 “-“ 整数回复（integer reply）的第一个字节是 “:” 批量回复（bulk reply）的第一个字节是 “$” 多条批量回复（multi bulk reply）的第一个字节是 “*” 状态回复一个状态回复（或者单行回复，single line reply）是一段以 “+” 开始、 “\r\n” 结尾的单行字符串。 以下是一个状态回复的例子：1+OK 客户端库应该返回 “+” 号之后的所有内容。 比如在在上面的这个例子中， 客户端就应该返回字符串 “OK” 。 状态回复通常由那些不需要返回数据的命令返回，这种回复不是二进制安全的，它也不能包含新行。 状态回复的额外开销非常少，只需要三个字节（开头的 “+” 和结尾的 CRLF）。 错误回复错误回复和状态回复非常相似， 它们之间的唯一区别是， 错误回复的第一个字节是 “-“ ， 而状态回复的第一个字节是 “+” 。 错误回复只在某些地方出现问题时发送： 比如说， 当用户对不正确的数据类型执行命令， 或者执行一个不存在的命令， 等等。 一个客户端库应该在收到错误回复时产生一个异常。 以下是两个错误回复的例子：12-ERR unknown command 'foobar'-WRONGTYPE Operation against a key holding the wrong kind of value 在 “-“ 之后，直到遇到第一个空格或新行为止，这中间的内容表示所返回错误的类型。 ERR 是一个通用错误，而 WRONGTYPE 则是一个更特定的错误。 一个客户端实现可以为不同类型的错误产生不同类型的异常， 或者提供一种通用的方式， 让调用者可以通过提供字符串形式的错误名来捕捉（trap）不同的错误。 不过这些特性用得并不多， 所以并不是特别重要， 一个受限的（limited）客户端可以通过简单地返回一个逻辑假（false）来表示一个通用的错误条件。 整数回复整数回复就是一个以 “:” 开头， CRLF 结尾的字符串表示的整数。 比如说， “:0\r\n” 和 “:1000\r\n” 都是整数回复。 返回整数回复的其中两个命令是 INCR key 和 LASTSAVE 。 被返回的整数没有什么特殊的含义， INCR key 返回键的一个自增后的整数值， 而 LASTSAVE 则返回一个 UNIX 时间戳， 返回值的唯一限制是这些数必须能够用 64 位有符号整数表示。 整数回复也被广泛地用于表示逻辑真和逻辑假： 比如 EXISTS key 和 SISMEMBER key member 都用返回值 1 表示真， 0 表示假。 其他一些命令， 比如 SADD key member [member …] 、 SREM key member [member …] 和 SETNX key value ， 只在操作真正被执行了的时候， 才返回 1 ， 否则返回 0 。 以下命令都返回整数回复： SETNX key value 、 DEL key [key …] 、 EXISTS key 、 INCR key 、 INCRBY key increment 、 DECR key 、 DECRBY key decrement 、 DBSIZE 、 LASTSAVE 、 RENAMENX key newkey 、 MOVE key db 、 LLEN key 、 SADD key member [member …] 、 SREM key member [member …] 、 SISMEMBER key member 、 SCARD key 。 批量回复服务器使用批量回复来返回二进制安全的字符串，字符串的最大长度为 512 MB 。12客户端：GET mykey服务器：foobar 服务器发送的内容中： 第一字节为 “$” 符号 接下来跟着的是表示实际回复长度的数字值 之后跟着一个 CRLF 再后面跟着的是实际回复数据 最末尾是另一个 CRLF 对于前面的 GET key 命令，服务器实际发送的内容为：1"$6\r\nfoobar\r\n" 如果被请求的值不存在， 那么批量回复会将特殊值 -1 用作回复的长度值， 就像这样：12客户端：GET non-existing-key服务器：$-1 这种回复称为空批量回复（NULL Bulk Reply）。 当请求对象不存在时，客户端应该返回空对象，而不是空字符串： 比如 Ruby 库应该返回 nil ， 而 C 库应该返回 NULL （或者在回复对象中设置一个特殊标志）， 诸如此类。 多条批量回复像 LRANGE key start stop 这样的命令需要返回多个值， 这一目标可以通过多条批量回复来完成。 多条批量回复是由多个回复组成的数组， 数组中的每个元素都可以是任意类型的回复， 包括多条批量回复本身。 多条批量回复的第一个字节为 “*” ， 后跟一个字符串表示的整数值， 这个值记录了多条批量回复所包含的回复数量， 再后面是一个 CRLF 。12345678910客户端： LRANGE mylist 0 3服务器： *4服务器： $3服务器： foo服务器： $3服务器： bar服务器： $5服务器： Hello服务器： $5服务器： World 在上面的示例中，服务器发送的所有字符串都由 CRLF 结尾。 正如你所见到的那样， 多条批量回复所使用的格式， 和客户端发送命令时使用的统一请求协议的格式一模一样。 它们之间的唯一区别是： 统一请求协议只发送批量回复。 而服务器应答命令时所发送的多条批量回复，则可以包含任意类型的回复。 以下例子展示了一个多条批量回复， 回复中包含四个整数值， 以及一个二进制安全字符串：1234567*5\r\n:1\r\n:2\r\n:3\r\n:4\r\n$6\r\nfoobar\r\n 在回复的第一行， 服务器发送 *5\r\n ， 表示这个多条批量回复包含 5 条回复， 再后面跟着的则是 5 条回复的正文。 多条批量回复也可以是空白的（empty）， 就像这样：12客户端： LRANGE nokey 0 1服务器： *0\r\n 无内容的多条批量回复（null multi bulk reply）也是存在的， 比如当 BLPOP key [key …] timeout 命令的阻塞时间超过最大时限时， 它就返回一个无内容的多条批量回复， 这个回复的计数值为 -1 ：12客户端： BLPOP key 1服务器： *-1\r\n 客户端库应该区别对待空白多条回复和无内容多条回复： 当 Redis 返回一个无内容多条回复时， 客户端库应该返回一个 null 对象， 而不是一个空数组。 多条批量回复中的空元素多条批量回复中的元素可以将自身的长度设置为 -1 ， 从而表示该元素不存在， 并且也不是一个空白字符串（empty string）。 当 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] 命令使用 GET pattern 选项对一个不存在的键进行操作时， 就会发生多条批量回复中带有空白元素的情况。 以下例子展示了一个包含空元素的多重批量回复：123456服务器： *3服务器： $3服务器： foo服务器： $-1服务器： $3服务器： bar 其中， 回复中的第二个元素为空。 对于这个回复， 客户端库应该返回类似于这样的回复：1["foo", nil, "bar"] 多命令和流水线客户端可以通过流水线， 在一次写入操作中发送多个命令： 在发送新命令之前， 无须阅读前一个命令的回复。 多个命令的回复会在最后一并返回。 内联命令当你需要和 Redis 服务器进行沟通， 但又找不到 redis-cli ， 而手上只有 telnet 的时候， 你可以通过 Redis 特别为这种情形而设的内联命令格式来发送命令。 以下是一个客户端和服务器使用内联命令来进行交互的例子：12客户端： PING服务器： +PONG 以下另一个返回整数值的内联命令的例子：12客户端： EXISTS somekey服务器： :0 因为没有了统一请求协议中的 “*” 项来声明参数的数量， 所以在 telnet 会话输入命令的时候， 必须使用空格来分割各个参数， 服务器在接收到数据之后， 会按空格对用户的输入进行分析（parse）， 并获取其中的命令参数。 高性能 Redis 协议分析器尽管 Redis 的协议非常利于人类阅读， 定义也很简单， 但这个协议的实现性能仍然可以和二进制协议一样快。 因为 Redis 协议将数据的长度放在数据正文之前， 所以程序无须像 JSON 那样， 为了寻找某个特殊字符而扫描整个 payload ， 也无须对发送至服务器的 payload 进行转义（quote）。 程序可以在对协议文本中的各个字符进行处理的同时， 查找 CR 字符， 并计算出批量回复或多条批量回复的长度， 就像这样：12345678910111213141516#include &lt;stdio.h&gt;int main(void) &#123; unsigned char *p = "$123\r\n"; int len = 0; p++; while(*p != '\r') &#123; len = (len*10)+(*p - '0'); p++; &#125; /* Now p points at '\r', and the len is in bulk_len. */ printf("%d\n", len); return 0;&#125; 得到了批量回复或多条批量回复的长度之后， 程序只需调用一次 read 函数， 就可以将回复的正文数据全部读入到内存中， 而无须对这些数据做任何的处理。 在回复最末尾的 CR 和 LF 不作处理，丢弃它们。 Redis 协议的实现性能可以和二进制协议的实现性能相媲美， 并且由于 Redis 协议的简单性， 大部分高级语言都可以轻易地实现这个协议， 这使得客户端软件的 bug 数量大大减少。 复制（Replication）Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(master server)的精确复制品。 以下是关于 Redis 复制功能的几个重要方面： Redis 使用异步复制。 从 Redis 2.8 开始， 从服务器会以每秒一次的频率向主服务器报告复制流（replication stream）的处理进度。 一个主服务器可以有多个从服务器。 不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个图状结构。 复制功能不会阻塞主服务器： 即使有一个或多个从服务器正在进行初次同步， 主服务器也可以继续处理命令请求。 复制功能也不会阻塞从服务器： 只要在 redis.conf 文件中进行了相应的设置， 即使从服务器正在进行初次同步， 服务器也可以使用旧版本的数据集来处理命令查询。 不过， 在从服务器删除旧版本数据集并载入新版本数据集的那段时间内， 连接请求会被阻塞。 你还可以配置从服务器， 让它在与主服务器之间的连接断开时， 向客户端发送一个错误。 复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] 命令可以交给附属节点去运行。 可以通过复制功能来让主服务器免于执行持久化操作： 只要关闭主服务器的持久化功能， 然后由从服务器去执行持久化操作即可。 关闭主服务器持久化时，复制功能的数据安全当配置Redis复制功能时，强烈建议打开主服务器的持久化功能。 否则的话，由于延迟等问题，部署的服务应该要避免自动拉起。 为了帮助理解主服务器关闭持久化时自动拉起的危险性，参考一下以下会导致主从服务器数据全部丢失的例子： 假设节点A为主服务器，并且关闭了持久化。 并且节点B和节点C从节点A复制数据 节点A崩溃，然后由自动拉起服务重启了节点A. 由于节点A的持久化被关闭了，所以重启之后没有任何数据 节点B和节点C将从节点A复制数据，但是A的数据是空的， 于是就把自身保存的数据副本删除。 在关闭主服务器上的持久化，并同时开启自动拉起进程的情况下，即便使用Sentinel来实现Redis的高可用性，也是非常危险的。 因为主服务器可能拉起得非常快，以至于Sentinel在配置的心跳时间间隔内没有检测到主服务器已被重启，然后还是会执行上面的数据丢失的流程。 无论何时，数据安全都是极其重要的，所以应该禁止主服务器关闭持久化的同时自动拉起。 复制功能的运作原理无论是初次连接还是重新连接， 当建立一个从服务器时， 从服务器都将向主服务器发送一个 SYNC 命令。 接到 SYNC 命令的主服务器将开始执行 BGSAVE ， 并在保存操作执行期间， 将所有新执行的写入命令都保存到一个缓冲区里面。 当 BGSAVE 执行完毕后， 主服务器将执行保存操作所得的 .rdb 文件发送给从服务器， 从服务器接收这个 .rdb 文件， 并将文件中的数据载入到内存中。 之后主服务器会以 Redis 命令协议的格式， 将写命令缓冲区中积累的所有内容都发送给从服务器。 你可以通过 telnet 命令来亲自验证这个同步过程： 首先连上一个正在处理命令请求的 Redis 服务器， 然后向它发送 SYNC 命令， 过一阵子， 你将看到 telnet 会话（session）接收到服务器发来的大段数据（.rdb 文件）， 之后还会看到， 所有在服务器执行过的写命令， 都会重新发送到 telnet 会话来。 即使有多个从服务器同时向主服务器发送 SYNC ， 主服务器也只需执行一次 BGSAVE 命令， 就可以处理所有这些从服务器的同步请求。 从服务器可以在主从服务器之间的连接断开时进行自动重连， 在 Redis 2.8 版本之前， 断线之后重连的从服务器总要执行一次完整重同步（full resynchronization）操作， 但是从 Redis 2.8 版本开始， 从服务器可以根据主服务器的情况来选择执行完整重同步还是部分重同步（partial resynchronization）。 部分重同步从 Redis 2.8 开始， 在网络连接短暂性失效之后， 主从服务器可以尝试继续执行原有的复制进程（process）， 而不一定要执行完整重同步操作。 这个特性需要主服务器为被发送的复制流创建一个内存缓冲区（in-memory backlog）， 并且主服务器和所有从服务器之间都记录一个复制偏移量（replication offset）和一个主服务器 ID （master run id）， 当出现网络连接断开时， 从服务器会重新连接， 并且向主服务器请求继续执行原来的复制进程： 如果从服务器记录的主服务器 ID 和当前要连接的主服务器的 ID 相同， 并且从服务器记录的偏移量所指定的数据仍然保存在主服务器的复制流缓冲区里面， 那么主服务器会向从服务器发送断线时缺失的那部分数据， 然后复制工作可以继续执行。 否则的话， 从服务器就要执行完整重同步操作。 Redis 2.8 的这个部分重同步特性会用到一个新增的 PSYNC master_run_id offset 内部命令， 而 Redis 2.8 以前的旧版本只有 SYNC 命令， 不过， 只要从服务器是 Redis 2.8 或以上的版本， 它就会根据主服务器的版本来决定到底是使用 PSYNC master_run_id offset 还是 SYNC ： 如果主服务器是 Redis 2.8 或以上版本，那么从服务器使用 PSYNC master_run_id offset 命令来进行同步。 如果主服务器是 Redis 2.8 之前的版本，那么从服务器使用 SYNC 命令来进行同步。 配置配置一个从服务器非常简单， 只要在配置文件中增加以下的这一行就可以了：1slaveof 192.168.1.1 6379 当然， 你需要将代码中的 192.168.1.1 和 6379 替换成你的主服务器的 IP 和端口号。 另外一种方法是调用 SLAVEOF host port 命令， 输入主服务器的 IP 和端口， 然后同步就会开始：12127.0.0.1:6379&gt; SLAVEOF 192.168.1.1 10086OK 只读从服务器从 Redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。 只读模式由 redis.conf 文件中的 slave-read-only 选项控制， 也可以通过 CONFIG SET parameter value 命令来开启或关闭这个模式。 只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。 即使从服务器是只读的， DEBUG 和 CONFIG 等管理式命令仍然是可以使用的， 所以我们还是不应该将服务器暴露给互联网或者任何不可信网络。 不过， 使用 redis.conf 中的命令改名选项， 我们可以通过禁止执行某些命令来提升只读从服务器的安全性。 你可能会感到好奇， 既然从服务器上的写数据会被重同步数据覆盖， 也可能在从服务器重启时丢失， 那么为什么要让一个从服务器变得可写呢？ 原因是， 一些不重要的临时数据， 仍然是可以保存在从服务器上面的。 比如说， 客户端可以在从服务器上保存主服务器的可达性（reachability）信息， 从而实现故障转移（failover）策略。 从服务器相关配置如果主服务器通过 requirepass 选项设置了密码， 那么为了让从服务器的同步操作可以顺利进行， 我们也必须为从服务器进行相应的身份验证设置。 对于一个正在运行的服务器， 可以使用客户端输入以下命令：1config set masterauth &lt;password&gt; 要永久地设置这个密码， 那么可以将它加入到配置文件中：1masterauth &lt;password&gt; 另外还有几个选项， 它们和主服务器执行部分重同步时所使用的复制流缓冲区有关， 详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。 主服务器只在有至少 N 个从服务器的情况下，才执行写操作从 Redis 2.8 开始， 为了保证数据的安全性， 可以通过配置， 让主服务器只在有至少 N 个当前已连接从服务器的情况下， 才执行写命令。 不过， 因为 Redis 使用异步复制， 所以主服务器发送的写数据并不一定会被从服务器接收到， 因此， 数据丢失的可能性仍然是存在的。 以下是这个特性的运作原理： 从服务器以每秒一次的频率 PING 主服务器一次， 并报告复制流的处理情况。 主服务器会记录各个从服务器最后一次向它发送 PING 的时间。 用户可以通过配置， 指定网络延迟的最大值 min-slaves-max-lag ， 以及执行写操作所需的至少从服务器数量 min-slaves-to-write 。 如果至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag 秒， 那么主服务器就会执行客户端请求的写操作。 你可以将这个特性看作 CAP 理论中的 C 的条件放宽版本： 尽管不能保证写操作的持久性， 但起码丢失数据的窗口会被严格限制在指定的秒数中。 另一方面， 如果条件达不到 min-slaves-to-write 和 min-slaves-max-lag 所指定的条件， 那么写操作就不会被执行， 主服务器会向请求执行写操作的客户端返回一个错误。 以下是这个特性的两个选项和它们所需的参数： min-slaves-to-write \ min-slaves-max-lag \ 详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。 事务（transaction）MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC 命令负责触发并执行事务中的所有命令： 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。 然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。 如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。 使用 redis-check-aof 程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。 从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作，具体信息请参考文档的后半部分。 用法MULTI 命令用于开启一个事务，它总是返回 OK 。 MULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC 命令被调用时， 所有队列中的命令才会被执行。 另一方面， 通过调用 DISCARD ， 客户端可以清空事务队列， 并放弃执行事务。 以下是一个事务例子， 它原子地增加了 foo 和 bar 两个键的值：123456789101112&gt; MULTIOK&gt; INCR fooQUEUED&gt; INCR barQUEUED&gt; EXEC1) (integer) 12) (integer) 1 EXEC 命令的回复是一个数组， 数组中的每个元素都是执行事务中的命令所产生的回复。 其中， 回复元素的先后顺序和命令发送的先后顺序一致。 当客户端处于事务状态时， 所有传入的命令都会返回一个内容为 QUEUED 的状态回复（status reply）， 这些被入队的命令将在 EXEC 命令被调用时执行。 事务中的错误使用事务时可能会遇上以下两种错误： 事务在执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。 命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。 对于发生在 EXEC 执行之前的错误，客户端以前的做法是检查命令入队所得的返回值：如果命令入队时返回 QUEUED ，那么入队成功；否则，就是入队失败。如果有命令在入队时失败，那么大部分客户端都会停止并取消这个事务。 不过，从 Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。 在 Redis 2.6.5 以前， Redis 只执行事务中那些入队成功的命令，而忽略那些入队失败的命令。 而新的处理方式则使得在流水线（pipeline）中包含事务变得简单，因为发送事务和读取事务的回复都只需要和服务器进行一次通讯。 至于那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。 从协议的角度来看这个问题，会更容易理解一些。 以下例子中， LPOP key 命令的执行将出错， 尽管调用它的语法是正确的：12345678910111213141516171819Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.MULTI+OKSET a 3abc+QUEUEDLPOP a+QUEUEDEXEC*2+OK-ERR Operation against a key holding the wrong kind of value EXEC 返回两条批量回复（bulk reply）： 第一条是 OK ，而第二条是 -ERR 。 至于怎样用合适的方法来表示事务中的错误， 则是由客户端自己决定的。 最重要的是记住这样一条， 即使事务中有某条/某些命令执行失败了， 事务队列中的其他命令仍然会继续执行 —— Redis 不会停止执行事务中的命令。 以下例子展示的是另一种情况， 当命令在入队时产生错误， 错误会立即被返回给客户端：12345MULTI+OKINCR a b c-ERR wrong number of arguments for 'incr' command 因为调用 INCR key 命令的参数格式不正确， 所以这个 INCR key 命令入队失败。 为什么 Redis 不支持回滚（roll back）如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。 以下是这种做法的优点： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR key 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR key ， 回滚是没有办法处理这些情况的。 鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。 放弃事务当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出：1234567891011121314redis&gt; SET foo 1OKredis&gt; MULTIOKredis&gt; INCR fooQUEUEDredis&gt; DISCARDOKredis&gt; GET foo"1" 使用 check-and-set 操作实现乐观锁WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已经失败。 举个例子， 假设我们需要原子性地为某个值进行增 1 操作（假设 INCR key 不存在）。 首先我们可能会这样做：123val = GET mykeyval = val + 1SET mykey val 上面的这个实现在只有一个客户端的时候可以执行得很好。 但是， 当多个客户端同时对同一个键进行这样的操作时， 就会产生竞争条件。 举个例子， 如果客户端 A 和 B 都读取了键原来的值， 比如 10 ， 那么两个客户端都会将键的值设为 11 ， 但正确的结果应该是 12 才对。 有了 WATCH ， 我们就可以轻松地解决这类问题了：12345678WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey valEXEC 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。 这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。 了解 WATCHWATCH 使得 EXEC 命令需要有条件地执行： 事务只能在所有被监视键都没有被修改的前提下执行， 如果这个前提不能满足的话，事务就不会被执行。 Note 如果你使用 WATCH 监视了一个带过期时间的键， 那么即使这个键过期了， 事务仍然可以正常执行， 关于这方面的详细情况，请看这个帖子： http://code.google.com/p/redis/issues/detail?id=270 WATCH 命令可以被调用多次。 对键的监视从 WATCH 执行之后开始生效， 直到调用 EXEC 为止。 用户还可以在单个 WATCH 命令中监视任意多个键， 就像这样：12redis&gt; WATCH key1 key2 key3OK 当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。 另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。 使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。 使用 WATCH 实现 ZPOPWATCH 可以用于创建 Redis 没有内置的原子操作。 举个例子， 以下代码实现了原创的 ZPOP 命令， 它可以原子地弹出有序集合中分值（score）最小的元素：12345WATCH zsetelement = ZRANGE zset 0 0MULTI ZREM zset elementEXEC 程序只要重复执行这段代码， 直到 EXEC 的返回值不是空多条回复（null multi-bulk reply）即可。 Redis 脚本和事务从定义上来说， Redis 中的脚本本身就是一种事务， 所以任何在事务里可以完成的事， 在脚本里面也能完成。 并且一般来说， 使用脚本要来得更简单，并且速度更快。 因为脚本功能是 Redis 2.6 才引入的， 而事务功能则更早之前就存在了， 所以 Redis 才会同时存在两种处理事务的方法。 不过我们并不打算在短时间内就移除事务功能， 因为事务提供了一种即使不使用脚本， 也可以避免竞争条件的方法， 而且事务本身的实现并不复杂。 不过在不远的将来， 可能所有用户都会只使用脚本来实现事务也说不定。 如果真的发生这种情况的话， 那么我们将废弃并最终移除事务功能。]]></content>
  </entry>
  <entry>
    <title><![CDATA[跨域问题及解决方案]]></title>
    <url>%2Fblog%2F%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html</url>
    <content type="text"><![CDATA[前言要了解跨域问题，我们先来了解下浏览器的同源策略。 浏览器的同源策略限制了从同一个源加载的文档或脚本与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。 同源的定义： 如果两个URL的协议、端口、IP（域名）都相同，则这两个URL具有相同的源。 根据这个定义，我们给出了下面的表格，可以简单看一下。 URL A URL B 是否同源 原因 http://abc.xyz.kk:8080/index.html http://abc.xyz.kk:8080/demo/demo.html 是 协议、端口、域名均相同 http://abc.xyz.kk:8080/api http://abc.xyz.kk:8888/index.html 否 端口不相同 http://abc.xyz.kk:8080/api http://xyz.abc.kk:8080/index.html 否 域名（IP）不相同 http://abc.xyz.kk:8080/index.html https://abc.xyz.kk:8888/index.html 否 协议不相同 http://abc.xyz.kk:8080/hello https://mmm.sss.ll:8888/hello 否 协议、端口、域名均不相同 跨域问题看到前言所说，同源策略提高了数据安全性，为什么我们还要研究跨域问题呢？ 如果单纯的Web网站，页面资源（html、js或jsp等）在服务端，我们是不用考虑跨域问题的，因为它们就在一个域下。 但是现在很多项目是前后端分离的，无论原生APP还是WebApp，由于IP、端口或者协议等的不同，它们的请求在访问后端系统时，如果不做些处理，就会受到浏览器同源策略的约束，进而出现403错误。 跨域方法我们目前有以下几种方法解决跨域问题，我们分别来看下吧。 Cross-Origin Resource Sharing (CORS)简介CORS是一个跨域资源共享方案，为了解决跨域问题，通过增加一系列HTTP请求头和响应头，规范安全地进行跨站数据传输。 请求头主要包括以下参数： 参数名 说明 Origin 用于在跨域请求或预先请求中，标明发起跨域请求的源域名 Access-Control-Request-Method 用于表明跨域请求使用的实际HTTP方法 Access-Control-Request-Headers 用于在预先请求时，告知服务器要发起的跨域请求中会携带的请求头信息 withCredentials 跨域请求是否携带凭据信息，如果设置为true，响应头的Access-Control-Allow-Origin必须指定具体域名，且Access-Control-Allow-Credentials参数为true 响应头主要包括以下参数： 参数名 说明 Access-Control-Allow-Origin 该参数中携带了服务器端验证后的允许的跨域请求域名，可以是一个具体的域名或是一个*（表示任意域名） Access-Control-Expose-Headers 该参数用于允许返回给跨域请求的响应头列表，在列表中的响应头的内容，才可以被浏览器访问 Access-Control-Max-Age 该参数用于告知浏览器可以将预先检查请求返回结果缓存的时间，在缓存有效期内，浏览器会使用缓存的预先检查结果判断是否发送跨域请求 Access-Control-Allow-Methods 该参数用于告知浏览器可以在实际发送跨域请求时，可以支持的请求方法，可以是一个具体的方法列表或是一个*（表示任意方法） Access-Control-Allow-Credentials 是否允许携带凭据信息。默认凭据信息 不包括在 CORS 请求之中 当我们给客户端添加符合的上述请求头参数，给服务端添加符合的响应头参数后，客户端对服务端的请求便可以实现跨越访问。 对CORS有更多兴趣的同学可以参考MDN的这篇文章。 Cross-Origin Resource Sharing (CORS) 例子我们使用前端JS+后台SpringBoot的例子来看一下。 前端部分代码： 123456789101112$("#test").click(function() &#123; $.ajax(&#123; type: "POST", url: "http://abc.xyz.kk/hello", data:&#123; name:"hello" &#125;, success: function(result) &#123; alert(result); &#125; &#125;);&#125;); 后端部分代码： 123456789101112131415161718192021222324@Configuration@Slf4jpublic class CorsConfig &#123; @Bean public CorsFilter corsFilter() &#123; UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); //允许所有类型请求头 corsConfiguration.addAllowedHeader("*"); //允许HEAD、POST和OPTIONS方法 corsConfiguration.setAllowedMethods(Arrays.asList("HEAD","POST", "OPTIONS")); //允许携带Cookie corsConfiguration.setAllowCredentials(true); //允许所有的源 corsConfiguration.setAllowedOrigins(Arrays.asList("*")); //时间设置为3600s corsConfiguration.setMaxAge(3600L); //跨域设置 //所有请求都允许跨域 source.registerCorsConfiguration("/**", corsConfiguration); return new CorsFilter(source); &#125;&#125; 一般出现跨域问题，如果使用CORS，只需要后台配置CORS过滤器（如上）即可实现跨域访问。 JSONP简介JSONP(JSON with Padding)是JSON的一种“使用模式”，可用于解决主流浏览器的跨域数据访问的问题。 JSONP的原理就是借助HTML中的&lt;script&gt;标签可以跨域引入资源。所以动态创建一个&lt;srcipt&gt;标签，src为目的接口 + get数据包 + 处理数据的函数名。后台收到GET请求后解析并返回函数名(数据)给前端，前端&lt;script&gt;标签动态执行处理函数。 &lt;script&gt;标签的src属性是没有跨域的限制的。这样说来，这种跨域方式其实与Ajax XmlHttpRequest协议无关了。 例子我们看一下JSONP方式实现跨域的前后端大致代码： 前端部分代码： 1234567&lt;script&gt; //jsonp回调方法，一定要写在jsonp请求之前 function jsonptest(result)&#123; alert(result); &#125;&lt;/script&gt;&lt;script src ="/hello/test?callback=jsonptest" type="text/javascript" &gt;&lt;/script&gt; 后端部分代码： 12345678910@Controller@RequestMapping("/hello")public class HelloController&#123; @RequestMapping(value="test",method=RequestMethod.GET) @ResponseBody public String jsonpTest(String callback)&#123; //do something return callback +"('Hello World!');"; &#125;&#125; 可以看到如果客户期望返回Hello World！ 实际收到的请求为 jsonptest(‘Hello World!’)，然后调用jsonptest函数获得实际想要的结果。 因为JSONP使用js的&lt;script&gt;标签进行传参，故该种方式只支持GET请求，这也是JSONP的一个缺点。 Nginx反向代理简介出现跨域限制的根本原因是浏览器同源问题的限制。 我们如果把前端项目和前端要请求的后台API接口地址放在同源下不就可以实现跨域请求了么？ 这样我们前后端都不需要做任何跨域配置处理。 例子比如我们有一个H5项目，部署在 http://abc.xyz.kk:8088 Nginx服务器上，后台地址为 http://abc.123.ss:8888 ，后台项目API接口地址为 http://abc.123.ss:8888/api/ 。 则Nginx的配置文件 nginx.conf 的 server部分配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556server &#123; listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; proxy_pass http://localhost:8000/; # 前端本机地址，实现自动更新 autoindex on; autoindex_exact_size on; autoindex_localtime on; &#125; location /api/ &#123; proxy_pass http://abc.123.ss:8888; # 后台API接口地址 proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125;&#125; 当我们访问了 http://abc.xyz.kk:8088 地址（代理前端地址），访问后台API时，通过反向代理，相当于访问 http://abc.xyz.kk:8088/api/ 这个地址，就不会出现跨域问题了。 其他跨域解决方案简介如果我们要通过A网页访问另一个域的B网页时，根据同源策略，也会出现跨域问题。 这种情况我们可以通过window.postMessage、window.name共享、window.location.hash共享等方法来解决，关于这块，我们简单的用window.postMessage来看下，其他的大家可以查询相关资料进行了解。 使用WebSocket也可以实现资源跨域访问，WebSocket是长连接，资源消耗较大，除在一些即时通讯等特殊场景，专门用来解决跨域问题还是少之又少的，这一块我们也不在详述。 例子我们用window.postMessage来进行举例。 下面是两个HTML，http://aaa.aaa.aa/A.html 和 http://bbb.bbb.bb/B.html 用 postMessage进行交互的例子。 A.html (发送端) 123456789101112131415161718192021222324252627&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;A.html&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt; &lt;input id="test" type="text" value="B.html" /&gt; &lt;button id="send" &gt;A发送消息给B&lt;/button&gt;&lt;/div&gt;&lt;iframe id="receiver" src="http://bbb.bbb.bb/B.html" width="500" height="60"&gt; &lt;p&gt;你的浏览器不支持IFrame。&lt;/p&gt;&lt;/iframe&gt;&lt;script&gt; window.onload = function() &#123; var receiver = document.getElementById('receiver').contentWindow; var btn = document.getElementById('send'); btn.addEventListener('click', function (e) &#123; e.preventDefault(); var val = document.getElementById('test').value; receiver.postMessage("Hello "+val+"！", "http://aaa.aaa.aa/A.html"); &#125;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; B.html (接收端) 123456789101112131415161718192021222324&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;B.html&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="message"&gt; Hello World!&lt;/div&gt;&lt;script&gt; window.onload = function() &#123; var messageEle = document.getElementById('message'); window.addEventListener('message', function (e) &#123; alert(e.origin); if (e.origin !== "http://aaa.aaa.aa/A.html") &#123; return; &#125; messageEle.innerHTML = "从"+ e.origin +"收到消息： " + e.data; &#125;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 总结以上跨域解决方案，最常用的还是CORS和反向代理，其次是JSONP，其他很少会被使用。 在允许Ajax XmlHttpRequest的浏览器（高版本浏览器）并与后端交互的场景，CORS和反向代理应用最广。 如果浏览器不支持XmlHttpRequest（IE6、IE7….），可以考虑使用JSONP。 如果涉及到不同源网页交互，支持H5的浏览器可以采用window.postMessage，不支持的可以使用window.name共享、window.location.hash共享等。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>跨域</tag>
        <tag>CORS</tag>
        <tag>JSONP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK里那些有趣的代码（2）]]></title>
    <url>%2Fblog%2FJDK%E9%87%8C%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%882%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言接 JDK里那些有趣的代码这篇文章。 今天我们来看下另一个比较有意思的代码部分。 在说这个之前，我们先来研究一道比较有意思的题目。 使用Java程序 获取下一个最小的比入参n大的2的高次幂 这个题的意思就是：比如入参为10，则最小的比入参大的2的高次幂为 ${2}^{4} = {16}$；入参为100，则最小的比入参大的2的高次幂为${2}^{7}={128}$。 分析对于这种题目，我们如何处理呢？ 最简单的是想到循环，2，4，8…..逐渐增大值，并与入参进行对比，相关代码如下： 123456789public static int getNum1(int n)&#123; for(int i=0;i&lt;= 31;i++)&#123; int b = 1&lt;&lt;i; if(b &gt; n)&#123; return b; &#125; &#125; return -1;&#125; 如上方法 1&lt;&lt;i 是把1左移i位，每次左移一位就是乘以2，所以 1&lt;&lt;i 的结果是1乘以2的i次方。 当然我们也可以使用Java自带的Math.pow或者乘法算法方法，不过显然这种方法效率要低。 12345678910111213141516171819public static int getNum2(int n)&#123; for(int i=0;i&lt;= 31;i++)&#123; double b = Math.pow(2,i); if(b &gt; n)&#123; return (int)b; &#125; &#125; return -1;&#125;public static int getNum3(int n)&#123; int k = 1; for(int i=0;i&lt;= 31;i++)&#123; if(k &gt; n)&#123; return k; &#125; k *= 2; &#125; return -1;&#125; 或者我们可以想到将入参每次除以2，直到小于1，记录次数i，然后2的i次方即是我们所要求的值。 相关代码如下： 12345678public static int getNum4(int n)&#123; int i = 0; while (n &gt; 0) &#123; n = n &gt;&gt; 1; i++; &#125; return 1&lt;&lt;i;&#125; 可以看到我们仍使用了移位运算， n = n &gt;&gt; 1 每次将n向右移一位即除以2，当n &lt;= 0 时记录次数 i,并使用 1&lt;&lt;i 算出要求的值。 当然我们也可以使用普通的除法算法，但这种效率要低些，代码如下： 12345678public static int getNum5(int n)&#123; int i=1; while (n&gt;1)&#123; n /=2; i++; &#125; return 1&lt;&lt;i;&#125; 其实上面几种情况原理都是类似的。 还有什么别的方法么？ 正文很巧，在JDK相关源码中也有类似的问题，即获取下一个最小的比入参n大的2的高次幂。 在哪儿会用到呢？ 当然是HashMap了，HashMap在扩容时，扩容指定的大小就是下一个最小的比入参n大的2的高次幂。 下面是具体tableSizeFor方法源码： 123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 我们可以在CourrentHashMap、ForkJoinPool中发现类似的处理逻辑，这种处理的优点体现在哪儿呢？ 我们把上面的代码在整理下，如下，对于入参n，该方法可以计算出最小的比入参n大的2的高次幂。 12345678public static int getNum6(int n)&#123; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return n &lt; 0 ? 1 : n + 1;&#125; PS：我们忽略源码中的int n = cap - 1; 这一步的作用是对于入参比如8，tableSizeFor方法会返回8，而getNum6会返回16，其实主要看题目怎么出，这儿我们找的是比入参n大的数，不包括n。 我们先来手动计算一下，以32和2000为例。 1234567// 32 = 100000 = 0100000n|=n&gt;&gt;&gt;1;// n=n|(n&gt;&gt;&gt;1) = 0100000|0010000 = 0110000 = 48n|=n&gt;&gt;&gt;2;// n=n|(n&gt;&gt;&gt;2) = 0110000|0001100 = 0111100 = 60n|=n&gt;&gt;&gt;4;// n=n|(n&gt;&gt;&gt;4) = 0111100|0000011 = 0111111 = 63n|=n&gt;&gt;&gt;8;// n=n|(n&gt;&gt;&gt;8) = 0111111|0000000 = 0111111 = 63n|=n&gt;&gt;&gt;16;// n=n|(n&gt;&gt;&gt;16) = 0111111|0000000 = 0111111 = 63// n + 1 =64 1234567// 2000 = 11111010000 = 11111010000n|=n&gt;&gt;&gt;1;// n=n|(n&gt;&gt;&gt;1) = 11111010000|01111101000 = 11111111000 = 2040n|=n&gt;&gt;&gt;2;// n=n|(n&gt;&gt;&gt;2) = 11111111000|00111111110 = 11111111110 = 2046n|=n&gt;&gt;&gt;4;// n=n|(n&gt;&gt;&gt;4) = 11111111110|00001111111 = 11111111111 = 2047n|=n&gt;&gt;&gt;8;// n=n|(n&gt;&gt;&gt;8) = 11111111111|00000000111 = 11111111111 = 2047n|=n&gt;&gt;&gt;16;// n=n|(n&gt;&gt;&gt;16) = 11111111111|00000000000 = 11111111111 = 2047// n + 1 =2048 计算过程比较简单，只要明白以下两点： n&gt;&gt;&gt;i 是指二进制的n的值向右移i位； i|k指的是i和k进行位或运算，| 是把某两个二进制数中, 只要其中一个的某一位为1，则结果的该位就为1，与&amp;运算相反。 我们来分析一下： 首先，如果是2的整数次方数，其除最高位（指第一个不为0的数）外，其他位必然是0。比如 ${2}^{11}={2048}$，其二进制为 $100000000000$。 则2的整数次方数-1必定最高位为0，其他位必然为1。大致如下： 1234567892 -1 = 000000010 -1 = 0000000014 -1 = 000000100 -1 = 0000000118 -1 = 000001000 -1 = 00000011116 -1 = 000010000 -1 = 00000111132 -1 = 000100000 -1 = 00001111164 -1 = 001000000 -1 = 000111111128 -1 = 010000000 -1 = 001111111256 -1 = 100000000 -1 = 011111111...... 我们对于任意数，如21，二进制为 000010101，当使用移位+位或运算时，最终该值会逐渐增大到 000011111，而这个值加1就是我们要找的值。其实质是右传播最左侧的一位，来找到最大值。 问：为什么右移位要按照1、2、4、8、16这样移动呢？而不是其他数字呢？ 答：这很好理解，我们拿 $128 （010000000）$来举例，比它大的最小的2的高次幂是256，则需要得到255。 12345010000000011000000 右移1位+位或011110000 右移2位+位或011111111 右移4位+位或...... 可以看到我们用了一个最小值128，得到255，只需要最左侧的1右移（1、2、4）并进行位或操作。int最大32位，故右移最大16位即可保证最高位的1对右边的0进行全覆盖（位或操作）。 测试到底高不高效还是取决于测试结果，我们写一个简单的测试方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static void main(String[] args) &#123; //生成若干数量的随机数，找到它们的最小的2的高次幂 int [] a = new int[100000000]; Random random = new Random(); for (int i=0;i&lt;a.length;i++)&#123; a[i] = random.nextInt(1073741824); &#125; //方法1 long start1 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum1(a[i]); &#125; System.out.println("方法1耗时:"+(System.currentTimeMillis()-start1)+"ms"); //方法2 long start2 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum2(a[i]); &#125; System.out.println("方法2耗时:"+(System.currentTimeMillis()-start2)+"ms"); //方法3 long start3 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum3(a[i]); &#125; System.out.println("方法3耗时:"+(System.currentTimeMillis()-start3)+"ms"); //方法4 long start4 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum4(a[i]); &#125; System.out.println("方法4耗时:"+(System.currentTimeMillis()-start4)+"ms"); //方法5 long start5 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum5(a[i]); &#125; System.out.println("方法5耗时:"+(System.currentTimeMillis()-start5)+"ms"); //方法6 long start6 = System.currentTimeMillis(); for (int i=0;i&lt;a.length;i++)&#123; getNum6(a[i]); &#125; System.out.println("方法6耗时:"+(System.currentTimeMillis()-start6)+"ms");&#125; 某次结果如下： 123456方法1耗时:1064ms//由于方法2耗时实在无法接受，便不再展示。调用Math.pow方法，同学们可实际测试下。方法3耗时:1097ms方法4耗时:2232ms方法5耗时:3885ms方法6耗时:155ms 经过多次测试其结果相差不大。可以看到方法6（也就是JDK里的tableSizeFor方法）确实高效。 结语该方法在 Hacker’s Delight （高效程序的奥秘）一书 3.2节中有一些介绍，有兴趣的同学也可以去看看。 通过上面的讲解，我们可以看到JDK源码中使用高效算法的艺术，多读源码，对我们也受益匪浅。 今天就先到这里，有时间我们在分析JDK源码中比较有趣的一些代码。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.7与JDK1.8中ConcurrentHashMap的一些变化]]></title>
    <url>%2Fblog%2FJDK1-7%E4%B8%8EJDK1-8%E4%B8%ADConcurrentHashMap%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8F%98%E5%8C%96.html</url>
    <content type="text"><![CDATA[前言今天我们来了解下ConcurrentHashMap的设计，并看下它在JDK1.7和JDK1.8中的一些改变。 说到ConcurrentHashMap，或许大家并不陌生，都知道它可以在并发访问的情况下使用，可以保证线程数据安全，相对于Hashtable和线程同步的HashMap-&gt;Collections.synchronizedMap(new HashMap&lt;&gt;()),它的效率更高。 在学习ConcurrentHashMap时，大家最好先对HashMap有一些认识，可以看一下我之前的一篇文章。HashMap实现原理 JDK1.7和JDK1.8 ConcurrentHashMap的设计实现是不同的，我们分别来看下，以进行对比。 正文JDK1.7的ConcurrentHashMap1.7的ConcurrentHashMap设计思路1.7的 ConcurrentHashMap的设计是通过分段锁的方式实现的，提高了并发度。分段是一开始就确定的了，后期不能再进行扩容。 所谓分段锁，主要是根据Segment段来实现的。 其中的段Segment继承了重入锁ReentrantLock，有了锁的功能，同时含有类似HashMap中的数组加链表结构（这里没有使用红黑树）。 虽然Segment的个数是不能扩容的，但是单个Segment里面的数组是可以扩容的。 整体概览ConcurrentHashMap有3个参数： initialCapacity：初始总容量，默认16 loadFactor：加载因子，默认0.75 concurrencyLevel：并发级别，默认16 然后我们需要知道的是： Segment的个数即ssize：取大于等于并发级别的最小的2的幂次。如concurrencyLevel=16，那么sszie=16,如concurrencyLevel=10，那么ssize=16。 单个Segment的初始容量cap：c=initialCapacity/ssize,并且可能需要+1。如15/7=2，那么c要取3，如16/8=2，那么c取2，c可能是一个任意值，那么同上述一样，cap取的值就是大于等于c的最下2的幂次。最小值要求是MIN_SEGMENT_TABLE_CAPACITY = 2。 单个Segment的阈值threshold：threshold=cap*loadFactor。 所以默认情况下，Segment的个数sszie=16,每个Segment的初始容量cap=2，单个Segment的阈值threshold=1。 如下图： 通过上图我们可以算出上述数据。 put过程 首先根据key计算出一个hash值，找到对应的Segment； 调用Segment的lock方法，为后面的put操作加锁； 根据key计算出hash值，找到Segment中数组中对应index的链表，并将该数据放置到该链表中； 判断当前Segment包含元素的数量大于阈值，则Segment进行扩容。 代码逻辑如下图源码： 其中Segment的put过程源码如下图： 我们看一下加锁方法： 可以看到如果不成功会尝试进行重试直到成功，同时如果找不到key，会返回一个新的node节点，如果key存在，会返回null。 扩容过程（rehash）这个扩容是在Segment的锁的保护下进行扩容的，不需要关注并发问题。 我们来看下相关源码： 我们看红框部分的内容，扩容的重点在于： 首先找到一个lastRun，lastRun之后的元素和lastRun是在同一个桶中，所以后面的不需要进行变动。 然后对开始到lastRun部分的元素，重新计算下设置到newTable中，每次都是将当前元素作为newTable的首元素，之前老的链表作为该首元素的next部分。 get过程 根据key计算出对应的Segment； 再根据key计算出对应Segment中数组的index； 最终遍历上述index位置的链表，查找出对应的key的value； 源码如下： remove过程 根据key值计算hash找到对应的Segment； 如果Segment不为空就调用Segment的remove方法； 对Segment段进行加锁，根据hash计算出index，找到链表（如果存在的话）； 对于找到的链表，循环找到key对应的值，并进行删除。 相关代码如下： size方法我们先来看下源码： 其大致原理如下： 使用一个循环，循环的退出条件是sum = last， 这次总数 = 上次总数，即Segment没有变化了； 每次循环，都记录 sum += modCount 和 size，如果超了int长度就返回最大int值； 循环一定次数（RETRIES_BEFORE_LOCK = 2）后如果Segment大小还在改变，就尝试对所有Segment加锁，来获取size； 最后要判断下所试次数（retries）是否大于RETRIES_BEFORE_LOCK，如果大于说明加过锁，还要对它们进行解锁。 其他方法大家可以参考下源码，不再详述。 JDK1.8的ConcurrentHashMap1.8的ConcurrentHashMap设计思路1.8的ConcurrentHashMap摒弃了1.7的Segment设计，而是在1.8HashMap的基础上实现了线程安全的版本，即也是采用数组+链表+红黑树的形式。 数组可以扩容，链表可以转化为红黑树。 整体概览有一个重要的参数sizeCtl，代表数组的大小； 用户可以设置一个初始容量initialCapacity给ConcurrentHashMap； sizeCtl = 大于（1.5倍initialCapacity+1）的最小的2的幂次， 即initialCapacity=20，则sizeCtl=32,如initialCapacity=24，则sizeCtl=64。 初始化的时候，会按照sizeCtl的大小创建出对应大小的数组。 相关代码如下： put过程 如果数组还未初始化，那么进行初始化，这里会通过一个CAS操作将sizeCtl设置为-1，设置成功的，可以进行初始化操作； 根据key的hash值找到对应的桶，如果桶还不存在，那么通过一个CAS操作来设置桶的第一个元素，失败的继续执行下面的逻辑即向桶中插入或更新； 如果找到的桶存在，但是桶中第一个元素的hash值是-1，说明此时该桶正在进行迁移操作，这一块会在下面的扩容中详细介绍； 如果找到的桶存在，那么要么是链表结构要么是红黑树结构，此时需要获取该桶的锁，在锁定的情况下执行链表或者红黑树的插入或更新； 如果桶中第一个元素的hash值大于0，说明是链表结构，则对链表插入或者更新； 如果桶中的第一个元素类型是TreeBin，说明是红黑树结构，则按照红黑树的方式进行插入或者更新； 在锁的保护下插入或者更新完毕后，如果是链表结构，需要判断链表中元素的数量是否超过8（默认），一旦超过就要考虑进行数组扩容或者是链表转红黑树。 如下图源码： initTable方法代码如下： 我们再来看下扩容过程。 扩容过程一旦链表中的元素个数超过了8个，那么可以执行数组扩容或者链表转为红黑树，这里依据的策略跟HashMap依据的策略是一致的。 当数组长度还未达到64个时，优先数组的扩容，否则选择链表转为红黑树。 源码如下所示： 重点来看看这个扩容过程，即看下上述tryPresize方法，也可以看到上述是2倍扩容的方式。 第一个执行的线程会首先设置sizeCtl属性为一个负值，然后执行transfer(tab, null)，其他晚进来的线程会检查当前扩容是否已经完成，没完成则帮助进行扩容，完成了则直接退出。 该ConcurrentHashMap的扩容操作可以允许多个线程并发执行，那么就要处理好任务的分配工作。每个线程获取一部分桶的迁移任务，如果当前线程的任务完成，查看是否还有未迁移的桶，若有则继续领取任务执行，若没有则退出。在退出时需要检查是否还有其他线程在参与迁移工作，如果有则自己什么也不做直接退出，如果没有了则执行最终的收尾工作。 Q1：当前线程如何感知其他线程也在参与迁移工作？ A1: 靠sizeCtl的值，它初始值是一个负值=(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)，每当一个线程参与进来执行迁移工作，则该值进行CAS自增，该线程的任务执行完毕要退出时对该值进行CAS自减操作，所以当sizeCtl的值等于上述初值则说明了此时未有其他线程还在执行迁移工作，可以去执行收尾工作了。见如下代码: Q2: 任务按照什么规则进行分片？ A2： 下图stride即是每个分片的大小，目前有最低要求16，即每个分片至少需要16个桶。stride的计算依赖于CPU的核数，如果只有1个核，那么此时就不用分片，即stride=n。其他情况就是 (n &gt;&gt;&gt; 3) / NCPU。 Q3：如何记录目前已经分出去的任务？ A3: ConcurrentHashMap含有一个属性transferIndex（初值为最后一个桶），表示从transferIndex开始到后面所有的桶的迁移任务已经被分配出去了。所以每次线程领取扩容任务，则需要对该属性进行CAS的减操作，即一般是transferIndex-stride。 Q4：每个线程如何处理分到的部分桶的迁移工作？ A4：第一个获取到分片的线程会创建一个新的数组，容量是之前的2倍。 遍历自己所分到的桶： 桶中元素不存在，则通过CAS操作设置桶中第一个元素为ForwardingNode，其Hash值为MOVED（-1）,同时该元素含有新的数组引用 此时若其他线程进行put操作，发现第一个元素的hash值为-1则代表正在进行扩容操作（并且表明该桶已经完成扩容操作了，可以直接在新的数组中重新进行hash和插入操作），该线程就可以去参与进去，或者没有任务则不用参与，此时可以去直接操作新的数组了 桶中元素存在且hash值为-1，则说明该桶已经被处理了（本不会出现多个线程任务重叠的情况，这里主要是该线程在执行完所有的任务后会再次进行检查，再次核对） 桶中为链表或者红黑树结构，则需要获取桶锁，防止其他线程对该桶进行put操作，然后处理方式同HashMap的处理方式一样，对桶中元素分为2类，分别代表当前桶中和要迁移到新桶中的元素。设置完毕后代表桶迁移工作已经完成，旧数组中该桶可以设置成ForwardingNode了 下面来看下详细的代码： get过程 根据k计算出hash值，找到对应的数组index; 如果该index位置无元素则直接返回null; 如果该index位置有元素: 如果第一个元素的hash值小于0，则该节点可能为ForwardingNode或者红黑树节点TreeBin; 如果是ForwardingNode（表示当前正在进行扩容），使用新的数组来进行查找; 如果是红黑树节点TreeBin，使用红黑树的查找方式来进行查找; 如果第一个元素的hash大于等于0，则为链表结构，依次遍历即可找到对应的元素。 详细代码如下： 其他方法过程ConcurrentHashMap的一些其它方法，如remove，size等也是十分复杂的。我们后面在详聊JDK1.8 ConcurrentHashMap的一些其它方法。 问题分析ConcurrentHashMap读为什么不需要锁？我们通常使用读写锁来保护对一堆数据的读写操作。读时加读锁，写时加写锁。在什么样的情况下可以不需要读锁呢？ 如果对数据的读写是一个原子操作，那么此时是可以不需要读锁的。如ConcurrentHashMap对数据的读写，写操作是不需要分2次写的（没有中间状态），读操作也是不需要2次读取的。假如一个写操作需要分多次写，必然会有中间状态，如果读不加锁，那么可能就会读到中间状态，那就不对了。 假如ConcurrentHashMap提供put(key1,value1,key2,value2)，写入的时候必然会存在中间状态即key1写完成，但是key2还未写，此时如果读不加锁，那么就可能读到key1是新数据而key2是老数据的中间状态。 虽然ConcurrentHashMap的读不需要锁，但是需要保证能读到最新数据，所以必须加volatile。即数组的引用需要加volatile，同时一个Node节点中的val和next属性也必须要加volatile。 ConcurrentHashMap是否可以在无锁的情况下进行迁移？目前1.8的ConcurrentHashMap迁移是在锁定旧桶的前提下进行迁移的，然而并没有去锁定新桶。那么就可能提出如下问题： Q1：在某个桶的迁移过程中，别的线程想要对该桶进行put操作怎么办？ A1: 一旦某个桶在迁移过程中了，必然要获取该桶的锁，所以其他线程的put操作要被阻塞，一旦迁移完毕，该桶中第一个元素就会被设置成ForwardingNode节点，所以其他线程put时需要重新判断下桶中第一个元素是否被更改了，如果被改了重新获取重新执行逻辑，如下代码： Q2: 某个桶已经迁移完成（其他桶还未完成），别的线程想要对该桶进行put操作怎么办？ A2: 该线程会首先检查是否还有未分配的迁移任务，如果有则先去执行迁移任务，如果没有即全部任务已经分发出去了，那么此时该线程可以直接对新的桶进行插入操作（映射到的新桶必然已经完成了迁移，所以可以放心执行操作）。 Q3: 从上面看到我们在迁移的时候还是需要对旧桶锁定的，能否在无锁的情况下实现迁移？ A3: 一旦扩容就涉及到迁移桶中元素的操作，将一个桶中的元素迁移到另一个桶中的操作不是一个原子操作，所以需要在锁的保护下进行迁移。如果扩容操作是移动桶的指向，那么就可以通过一个CAS操作来完成扩容操作。可以参考参考这篇论文Split-Ordered Lists: Lock-Free Extensible Hash Tables ConcurrentHashMap曾经的弱一致性曾经老版本的ConcurrentHashMap是弱一致的，大家可以参考相关文档或者较早的ConcurrentHashMap源码。 曾经引发弱一致性的原因： 对数组的引用是volatile来修饰的，但是数组中的元素并不是。即读取数组的引用总是能读取到最新的值，但是读取数组中某一个元素的时候并不一定能读到最新的值。所以说是弱一致性的。 要实现强一致性，可以这样： 对于新加的key，通过写入到链表的末尾即可。因为一个元素的next属性是volatile的，可以保证写入后立马看的到，如下1.8的方式； 或者对数组中元素的更新采用volatile写的方式，如下1.7的形式。 但是现在1.7版本的ConcurrentHashMap对于数组中元素的写也是加了volatile的，如下代码： 1.8的方式就是直接将新加入的元素写入next属性（含有volatile修饰）中而不是修改桶中的第一个元素，如下代码： 所以在1.7和1.8版本的ConcurrentHashMap中不再是弱一致性，写入的数据是可以立即被读到的。 结语本文介绍了JDK1.7和JDK1.8版本下的ConcurrentHashMap的一些差异，也了解了1.7和1.8下ConcurrentHashMap的一些原理及方法，让我们对ConcurrentHashMap有了更深刻的一些认识。 参考资料 jdk1.8的HashMap和ConcurrentHashMap (有改动) JDK1.8 ConcurrentHashMap源码 JDK1.7 ConcurrentHashMap源码]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）分布式系统下的CAP理论]]></title>
    <url>%2Fblog%2F%EF%BC%88%E8%BD%AC%EF%BC%89%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%8B%E7%9A%84CAP%E7%90%86%E8%AE%BA.html</url>
    <content type="text"><![CDATA[原文：分布式系统的CAP理论 2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。 无论你是一个系统架构师，还是一个普通开发，当你开发或者设计一个分布式系统的时候，CAP理论是无论如何也绕不过去的。本文就来介绍一下到底什么是CAP理论，如何证明CAP理论，以及CAP的权衡问题。 CAP理论概述CAP理论：一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 读者需要注意的的是，CAP理论中的CA和数据库事务中ACID的CA并完全是同一回事儿。两者之中的A都是C都是一致性(Consistency)。CAP中的A指的是可用性（Availability），而ACID中的A指的是原子性（Atomicity)，切勿混为一谈。 CAP的定义Consistency 一致性一致性指 “all nodes see the same data at the same time” ，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。 对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。 一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。 从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。 三种一致性策略 对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。 如果能容忍后续的部分或者全部访问不到，则是弱一致性。 如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。 CAP中说，不可能同时满足的这个一致性指的是强一致性。 Availability 可用性可用性指 “Reads and writes always succeed” ，即服务一直可用，而且是正常响应时间。 对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，一般我们在衡量一个系统的可用性的时候，都是通过停机时间来计算的。 可用性分类 可用水平（%） 年可容忍停机时间 容错可用性 99.9999 &lt;1 min 极高可用性 99.999 &lt;5 min 具有故障自动恢复能力的可用性 99.99 &lt;53 min 高可用性 99.9 &lt;8.8h 商品可用性 99 &lt;43.8 min 通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 (1-0.99999)36524*60 = 5.256 min，这是一个极高的要求。 好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布式系统，上下游设计很多系统如负载均衡、WEB服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以影响可用性。 Partition Tolerance分区容错性分区容错性指 “the system continues to operate despite arbitrary message loss or failure of part of the system” ，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。 简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。 CAP的证明 如上图，是我们证明CAP的基本场景，网络中有两个节点N1和N2，可以简单的理解N1和N2分别是两台计算机，他们之间网络可以连通，N1中有一个应用程序A，和一个数据库V，N2也有一个应用程序B2和一个数据库V。现在，A和B是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。 在满足一致性的时候，N1和N2中的数据是一样的，V0=V0。在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的正常运作。 如上图，是分布式系统正常运转的流程，用户向N1机器请求数据更新，程序A更新数据库Vo为V1，分布式系统将数据进行同步操作M，将V1同步的N2中V0，使得N2中的数据V0也更新为V1，N2中的数据再响应N2的请求。 这里，可以定义N1和N2的数据库V之间的数据是否一样为一致性；外部对N1和N2的请求响应为可用行；N1和N2之间的网络环境为分区容错性。这是正常运作的场景，也是理想的场景，然而现实是残酷的，当错误发生的时候，一致性和可用性还有分区容错性，是否能同时满足，还是说要进行取舍呢？ 作为一个分布式系统，它和单机系统的最大区别，就在于网络，现在假设一种极端情况，N1和N2之间的网络断开了，我们要支持这种网络异常，相当于要满足分区容错性，能不能同时满足一致性和响应性呢？还是说要对他们进行取舍。 假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1，由于网络是断开的，所以分布式系统同步操作M，所以N2中的数据依旧是V0；这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据V1，怎么办呢？ 有二种选择，第一，牺牲数据一致性，保证可用性。响应旧的数据V0给用户； 第二，牺牲可用性，保证数据一致性。阻塞等待，直到网络连接恢复，数据更新操作M完成之后，再给用户响应最新的数据V1。 这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。 CAP权衡通过CAP理论及前面的证明，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ 我们分三种情况来阐述一下。 CA without P这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。这也是为什么在前面的CAP证明中，我们以系统满足P为前提论述了无法同时满足C和A。 比如我们熟知的关系型数据库，如My Sql和Oracle就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把P也考虑进来。 其实，在CAP理论中。C，A，P三者并不是平等的，CAP之父在《Spanner，真时，CAP理论》一文中写到： 如果说Spanner真有什么特别之处，那就是谷歌的广域网。Google通过建立私有网络以及强大的网络工程能力来保证P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。 从Google的经验中可以得到的结论是，无法通过降低CA来提升P。要想提升系统的分区容错性，需要通过提升基础设施的稳定性来保障。 所以，对于一个分布式系统来说。P是一个基本要求，CAP三者中，只能在CA两者之间做权衡，并且要想尽办法提升P。 CP without A如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在CAP三者中保障CP而舍弃A。 一个保证了CP而一个舍弃了A的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。 设计成CP的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成CP的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如Redis、HBase等，还有分布式系统中常用的Zookeeper也是在CAP三者之中选择优先保证CP的。 无论是像Redis、HBase这种分布式存储系统，还是像Zookeeper这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？ ZooKeeper是个CP（一致性+分区容错性）的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。 AP wihtout C要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。 这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到N个9，所以，对于很多业务系统来说，比如淘宝的购物，12306的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。 你在12306买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。 但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到N个9，即保证P和A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。 适合的才是最好的上面介绍了如何CAP中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。 对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CP，舍弃A。比如前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。 对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的安全。 总结无论你是一个架构师，还是一个普通开发，在设计或开发分布式系统的时候，不可避免的要在CAP中做权衡。需要根据自己的系统的实际情况，选择最适合自己的方案。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>CAP</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM堆内存及垃圾回收简介]]></title>
    <url>%2Fblog%2FJVM%E5%A0%86%E5%86%85%E5%AD%98%E5%8F%8A%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言我们知道在JVM内存模型中，堆是十分重要的一块，堆是内存占用最大，管理最复杂的一个区域，其用途就是存放生成的对象实例，所有的对象都会在堆上进行分配使用。 JDK1.8后，字符串常量池从永久代剥离了出来，也存放在了堆上。 正文堆内存结构我们来看一下堆内存结构，JDK1.8后JVM堆内存结构如下图： 可以看到堆内存分为年轻代（Young Generation）、年老代（Old Generation）及元空间（MetaData Space）。 PS:JDK8 完全移除永久代(Permanent Generation), 取而代之的是元空间MetaData Space（JVM使用本地内存，存放类的元数据）。 年轻代（Young Generation）又分为 Eden Space 和 Survivor Space，其中Survivor区有两部分构成 Survivor 1 和 Survivor 2 。 JVM虚拟机默认Eden区和两块Survivor区的内存比例为8:1:1。 GC流程年轻代内存的大致使用过程为： 年轻代将内存分为Eden和2块Survivor区（分别叫from和to）。 一般情况下，新创建的对象都会被分配到Eden区(一些大对象特殊处理),这些对象经过第一次Minor GC后，如果仍然存活，将会被移到Survivor区。 对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度时，就会被移动到年老代中。 在GC开始的时候，对象只会存在于Eden区和名为“From”的Survivor区，Survivor区“To”是空的。 紧接着进行GC，Eden区中所有存活的对象都会被复制到“To”，而在“From”区中，仍存活的对象会根据他们的年龄值来决定去向。年龄达到一定值的对象会被移动到年老代中，没有达到阈值的对象会被复制到“To”区域。 经过这次GC后，Eden区和From区已经被清空。这个时候，“From”和“To”会交换他们的角色，也就是新的“To”就是上次GC前的“From”，新的“From”就是上次GC前的“To”。不管怎样，都会保证名为To的Survivor区域是空的。 经过Minor GC之后，如果Survivor存放不下存活的对象，对象就会通过分配担保机制进入老年代，而如果老年代空间还不够，就会进行Full GC。 Minor GC会一直重复这样的过程，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。 因为年轻代中的对象基本都是朝生夕死的(80%以上)，所以在年轻代的垃圾回收算法使用的是复制算法，复制算法的基本思想就是将内存分为两块，每次只用其中一块，当这一块内存用完，就将还活着的对象复制到另外一块上面。复制算法不会产生内存碎片。 因此对象进入年老代有以下4种情况： 经过Minor GC后，Survivor区存放不下存活的对象进入年老代。 对象长期存活，当年龄达到一定阈值后进入年老代，默认15。年龄阈值，可以通过-XX:MaxTenuringThreshold来设置。 大对象直接进入年老代，通过 -XX:PretenureSizeThreshold 参数可以进行设置多大的对象直接在年老代进行分配，从而避免大对象在年轻代（Eden和Survivor区）发生大量内存赋值操作。 如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。（动态对象年龄绑定） GC的大致回收流程如下图： GC的回收分为垃圾的收集和回收两部分，收集和回收都涉及到一些算法逻辑，我们来整理下。 垃圾收集算法JVM中常用的垃圾收集算法大致有两种，引用计数法和根搜索法。 引用计数法 引用计数法本质是给对象添加引用计数器，当引用对象时计数器+1，引用失效时，计数器-1，当计数器等于0时，对象失效，内存可以被回收。 但会有一个问题，如果A对象引用B对象，同时B对象又引用A对象，但它们都不会再被系统使用，则它们可认为为垃圾，但是它们的引用计数是永不为0的，因此该方法永远也不会将其标位垃圾。 优点：实现简单高效。 缺点：对象之间的互相循环引用问题不好解决。 根搜索法 通过GC roots可达的对象路径称为引用链（reference chain），当一个对象没有引用链时（即从GC roots不可达）则视为不可用对象，内存可以被回收。 JVM主要使用根搜索法进行垃圾收集。 那在JVM中，哪些对象可以视为GC roots呢？ 虚拟机栈中（即栈帧中的本地变量）的引用对象； 本地方法栈中的引用对象； 方法区中的静态变量引用的对象和常量池中引用的对象。 垃圾回收算法 标记-清除算法 分两步进行，第一步标记出可以回收的对象，第二步统一清理可以回收的对象内存。 缺点：如果在被标记后直接对对象进行清除，会带来另一个新的问题——内存碎片化。如果下次有比较大的对象实例需要在堆上分配较大的内存空间时，可能会出现无法找到足够的连续内存而不得不再次触发垃圾回收。 复制算法 此GC算法实际上解决了标记-清除算法带来的“内存碎片化”问题。首先还是先标记处待回收内存和不用回收的内存，下一步将不用回收的内存复制到新的内存区域，这样旧的内存区域就可以全部回收，而新的内存区域则是连续的。 缺点：就是会损失掉部分系统内存，因为你总要腾出一部分内存用于复制。 标记-整理算法 标记-压缩算法首先还是“标记”，标记过后，将不用回收的内存对象压缩到内存一端，此时即可直接清除边界处的内存，这样就能避免复制算法带来的效率问题，同时也能避免内存碎片化的问题。 分代收集算法 对于JVM堆内存的垃圾回收，可以认为是分代收集算法。 对于年轻代，大部分对象都不会存活，所以在新生代中使用复制算法较为高效。 而对于年老代来讲，大部分对象可能会继续存活下去，如果此时还是利用复制算法，效率则会降低，此时使用标记-整理算法，不仅提高效率，更节约内存。 当然，具体使用哪种垃圾回收算法，也和垃圾收集器的实现有具体关系。 垃圾收集器再来看一下JVM的几种垃圾收集器。 目前JVM有7种作用于不同分代的垃圾收集器。如下图： 上图两个垃圾收集器之间的连线表示它们可以搭配使用。 Serial收集器 Serial收集器是最基本、发展历史最悠久的收集器。是单线程的收集器。它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集完成。 Serial收集器依然是虚拟机运行在Client模式下默认新生代（年轻代）收集器，对于运行在Client模式下的虚拟机来说是一个很好的选择。 ParNew收集器 ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial 收集器完全一样。 ParNew收集器是许多运行在Server模式下的虚拟机中首选新生代收集器，其中有一个与性能无关但很重要的原因是，除Serial收集器之外，目前只有ParNew它能与CMS收集器配合工作。 Parallel Scavenge（并行回收）收集器 Parallel Scavenge收集器是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器。 该收集器的目标是达到一个可控制的吞吐量（Throughput）。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即 吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间）。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可用高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 Parallel Scavenge收集器提供两个参数用于精确控制吞吐量，分别是控制最大垃圾收起停顿时间的 -XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数 Parallel Scavenge收集器还有一个参数：-XX:+UseAdaptiveSizePolicy。这是一个开关参数，当这个参数打开后，就不需要手工指定新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数，只需要把基本的内存数据设置好（如-Xmx设置最大堆），然后使用MaxGVPauseMillis参数或GCTimeRation参数给虚拟机设立一个优化目标。 自适应调节策略也是Parallel Scavenge收集器与ParNew收集器的一个重要区别。 Serial Old 收集器 Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记整理算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。 如果在Server模式下，主要两大用途： （1）在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用。 （2）作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。 Parallel Old 收集器 Parallel Old 是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。这个收集器在1.6中才开始提供。 CMS收集器 CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务器的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 CMS收集器是基于“标记-清除”算法实现的。它的运作过程相对前面几种收集器来说更复杂一些，整个过程分为4个步骤： （1）初始标记 （2）并发标记 （3）重新标记 （4）并发清除 其中，初始标记、重新标记这两个步骤仍然需要“Stop The World”. CMS收集器主要优点：并发收集，低停顿。 CMS三个明显的缺点： （1）CMS收集器对CPU资源非常敏感。CPU个数少于4个时，CMS对于用户程序的影响就可能变得很大，为了应付这种情况，虚拟机提供了一种称为“增量式并发收集器”的CMS收集器变种。所做的事情和单CPU年代PC机操作系统使用抢占式来模拟多任务机制的思想。 （2）CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。在JDK1.5的默认设置下，CMS收集器当老年代使用了68%的空间后就会被激活，这是一个偏保守的设置，如果在应用中老年代增长不是太快，可以适当调高参数-XX:CMSInitiatingOccupancyFraction的值来提高触发百分比，以便降低内存回收次数从而获取更好的性能，在JDK1.6中，CMS收集器的启动阀值已经提升至92%。 （3）CMS是基于“标记-清除”算法实现的收集器，收集结束时会有大量空间碎片产生。空间碎片过多，可能会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发FullGC。为了解决这个问题，CMS收集器提供了一个-XX:+UseCMSCompactAtFullCollection开关参数（默认就是开启的），用于在CMS收集器顶不住要进行FullGC时开启内存碎片合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间变长了。虚拟机设计者还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction,这个参数是用于设置执行多少次不压缩的Full GC后，跟着来一次带压缩的Full GC（默认值为0，表示每次进入Full GC时都进行碎片整理）。 G1收集器 G1收集器的优势： （1）并行与并发 （2）分代收集 （3）空间整理 （标记——整理算法，复制算法） （4）可预测的停顿（G1除处理追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经实现Java（RTSJ）的垃圾收集器的特征） 备注： The Real-time Specification for Java (RTSJ) is an open specification that augments the Java language to open the door more widely to using the language to build real-time systems (see Related topics). Implementing the RTSJ requires support in the operating system, the JRE, and the Java Class Library (JCL). 详见：RTSJ 中的Garbage collection的规范。 使用G1收集器时，Java堆的内存布局是整个规划为多个大小相等的独立区域（Region）,虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region的集合。 G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获取的空间大小以及回收所需要的时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的又来）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽量可能高的收集效率。 G1 内存“化整为零”的思路： 在GC根节点的枚举范围中加入Remembered Set即可保证不对全堆扫描也不会遗漏。 如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为一下步骤： （1）初始标记 （2）并发标记 （3）最终标记 （4）筛选回收 参考：https://www.cnblogs.com/chengxuyuanzhilu/p/7088316.html JVM的一些参数我们再来看下JVM的一些常用参数设置。 JVM的基础参数 -Xmx2048m：设置JVM最大堆内存为2048M。 -Xms2048m：设置JVM初始堆内存为2048M。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 -Xss128k：设置每个线程的栈大小。JDK5.0以后每个线程栈大小为1M，之前每个线程栈大小为256K。应当根据应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。需要注意的是：当这个值被设置的较大（例如&gt;2MB）时将会在很大程度上降低系统的性能。 -Xmn1g：设置年轻代大小为1G。在整个堆内存大小确定的情况下，增大年轻代将会减小年老代，反之亦然。此值关系到JVM垃圾回收，对系统性能影响较大，官方推荐配置为整个堆大小的3/8。 -XX:NewSize=1024m：设置年轻代初始值为1024M。 -XX:MaxNewSize=1024m：设置年轻代最大值为1024M。 -XX:PermSize=256m：设置持久代初始值为256M。（1.7以下JDK版本有效） -XX:MaxPermSize=256m：设置持久代最大值为256M。（1.7以下JDK版本有效） -XX:MetaspaceSize=8m：初始元数据空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。（1.7以上JDK版本有效） -XX:MaxMetaspaceSize=50m：元数据最大空间大小，默认是没有限制的。（1.7以上JDK版本有效） -XX:NewRatio=4：设置年轻代（包括1个Eden和2个Survivor区）与年老代的比值。表示年轻代比年老代为1:4。 -XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的比值。表示2个Survivor区与1个Eden区的比值为2:4，即1个Survivor区占整个年轻代大小的1/6。 -XX:MaxTenuringThreshold=7：表示一个对象如果在Survivor区（救助空间）移动了7次还没有被垃圾回收就进入年老代。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代，对于需要大量常驻内存的应用，这样做可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象在年轻代存活时间，增加对象在年轻代被垃圾回收的概率，减少Full GC的频率，这样做可以在某种程度上提高服务稳定性。 -XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集。（1.7以上JDK版本有效） -XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集。（1.7以上JDK版本有效） PS:可以看到-Xmn，-XX:NewSize/-XX:MaxNewSize，-XX:NewRatio 3组参数都可以影响年轻代的大小，它们混合使用生效的优先级为： 高优先级：-XX:NewSize/-XX:MaxNewSize 中优先级：-Xmn（默认等效 -Xmn=-XX:NewSize=-XX:MaxNewSize=?） 低优先级：-XX:NewRatio 推荐使用-Xmn参数。 JVM垃圾回收参数 -XX:+UseSerialGC：设置串行收集器。 -XX:+UseParallelGC：设置为并行收集器。此配置仅对年轻代有效。即年轻代使用并行收集，而年老代仍使用串行收集。 -XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时有多少个线程一起进行垃圾回收。此值建议配置与CPU数目相等。 -XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0开始支持对年老代并行收集。 -XX:MaxGCPauseMillis=100：设置每次年轻代垃圾回收的最长时间（单位毫秒）。如果无法满足此时间，JVM会自动调整年轻代大小，以满足此时间。 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动调整年轻代Eden区大小和Survivor区大小的比例，以达成目标系统规定的最低响应时间或者收集频率等指标。此参数建议在使用并行收集器时，一直打开。 -XX:+UseConcMarkSweepGC：即CMS收集，设置年老代为并发收集。CMS收集是JDK1.4后期版本开始引入的新GC算法。它的主要适合场景是对响应时间的重要性需求大于对吞吐量的需求，能够承受垃圾回收线程和应用线程共享CPU资源，并且应用中存在比较多的长生命周期对象。CMS收集的目标是尽量减少应用的暂停时间，减少Full GC发生的几率，利用和应用程序线程并发的垃圾回收线程来标记清除年老代内存。 -XX:+UseParNewGC：设置年轻代为并发收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此参数。 -XX:CMSFullGCsBeforeCompaction=0：由于并发收集器不对内存空间进行压缩和整理，所以运行一段时间并行收集以后会产生内存碎片，内存使用效率降低。此参数设置运行0次Full GC后对内存空间进行压缩和整理，即每次Full GC后立刻开始压缩和整理内存。 -XX:+UseCMSCompactAtFullCollection：打开内存空间的压缩和整理，在Full GC后执行。可能会影响性能，但可以消除内存碎片。 -XX:+CMSIncrementalMode：设置为增量收集模式。一般适用于单CPU情况。 -XX:CMSInitiatingOccupancyFraction=70：表示年老代内存空间使用到70%时就开始执行CMS收集，以确保年老代有足够的空间接纳来自年轻代的对象，避免Full GC的发生。 -XX:+ScavengeBeforeFullGC：年轻代GC优于Full GC执行。 -XX:+DisableExplicitGC：不响应 System.gc() 代码。 -XX:+UseThreadPriorities：启用本地线程优先级API。即使 java.lang.Thread.setPriority() 生效，不启用则无效。 -XX:SoftRefLRUPolicyMSPerMB=0：软引用对象在最后一次被访问后能存活0毫秒（JVM默认为1000毫秒）。 -XX:TargetSurvivorRatio=90：允许90%的Survivor区被占用（JVM默认为50%）。提高对于Survivor区的使用率。 -XX:+UseG1GC: 设置使用G1垃圾回收器（1.7以上JDK版本有效） -XX:G1HeapRegionSize=n:设置g1 region大小，不设置的话自己会根据堆大小算，目标是根据最小堆内存划分2048个区域（1.7以上JDK版本有效） JVM其它参数 -XX:+CITime：打印消耗在JIT编译的时间。 -XX:ErrorFile=./hs_err_pid.log：保存错误日志或数据到指定文件中。 -XX:HeapDumpPath=./java_pid.hprof：指定Dump堆内存时的路径。 -XX:+HeapDumpOnOutOfMemoryError：当首次遭遇内存溢出时Dump出此时的堆内存。 -XX:OnError=”;”：出现致命ERROR后运行自定义命令。 -XX:OnOutOfMemoryError=”;”：当首次遭遇内存溢出时执行自定义命令。 -XX:+PrintClassHistogram：按下 Ctrl+Break 后打印堆内存中类实例的柱状信息，同JDK的 jmap -histo 命令。 -XX:+PrintConcurrentLocks：按下 Ctrl+Break 后打印线程栈中并发锁的相关信息，同JDK的 jstack -l 命令。 -XX:+PrintCompilation：当一个方法被编译时打印相关信息。 -XX:+PrintGC：每次GC时打印相关信息。 -XX:+PrintGCDetails：每次GC时打印详细信息。 -XX:+PrintGCTimeStamps：打印每次GC的时间戳。 -XX:+TraceClassLoading：跟踪类的加载信息。 -XX:+TraceClassLoadingPreorder：跟踪被引用到的所有类的加载信息。 -XX:+TraceClassResolution：跟踪常量池。 -XX:+TraceClassUnloading：跟踪类的卸载信息。 -client：设置JVM使用Client模式，特点是启动速度比较快，但运行时性能和内存管理效率不高，通常用于客户端应用程序或开发调试；在32位环境下直接运行Java程序默认启用该模式。 -server：设置JVM使Server模式，特点是启动速度比较慢，但运行时性能和内存管理效率很高，适用于生产环境。在具有64位能力的JDK环境下默认启用该模式。 PS：关于参数名称定义如下。 标准参数（-），所有JVM都必须支持这些参数的功能，而且向后兼容； 非标准参数（-X），默认JVM实现这些参数的功能，但是并不保证所有JVM实现都满足，且不保证向后兼容； 非稳定参数（-XX），此类参数各个JVM实现会有所不同，将来可能会不被支持，需要慎重使用； 参考：https://blog.csdn.net/kthq/article/details/8618052 各个区域的OOM我们来看下JVM各个区域的OOM。 堆的OOM 我们创建如下类，进行测试。 123456789101112131415public class JvmTest &#123; public void oomTest()&#123; List&lt;JvmTest&gt; list = new ArrayList&lt;&gt;(); try&#123; while (true)&#123; list.add(new JvmTest()); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; new JvmTest().oomTest(); &#125; &#125; 运行后可以看到抛出如下异常： 这也是非常常见的一种OOM异常。出现的原因可能是创建了大量大对象、一些流未及时关闭等，导致堆内存溢出。 出现这种情况，必须考虑程序的优化解决方法。而不是单纯的通过-Xmn参数增大内存来解决。 栈的OOM 当栈深度超过虚拟机分配给线程的栈大小时，就会出现栈的溢出异常。 我们创建测试类，来看一下。 123456789101112public class JvmTest &#123; public int stackOverTest(int n)&#123; if(n==1)&#123; return 8; &#125;else&#123; return stackOverTest(n-1)+2; &#125; &#125; public static void main(String[] args) &#123; new JvmTest().stackOverTest(200000); &#125; &#125; 运行后可以看到如下异常： 这种异常一般是调用递归或者死循环等产生的，导致栈深度超过虚拟机分配给线程的栈大小。 当然可以通过-Xss参数控制每个线程的栈大小来解决，但通常情况下，应检查程序，减少递归的使用。 关于Metaspace与PermGen（永久代） JDK1.8移除了PermGen（永久代），取而代之的是Metaspace（元空间），元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 我们在JDK1.8环境下，设置Metaspace的大小，进行测试。(-XX:MetaspaceSize=5M -XX:MaxMetaspaceSize=5M) 123456789101112131415161718192021public class JvmTest &#123; public void MetaSpaceOOMTest() &#123; while (true) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(JvmTest.class); enhancer.setUseCache(false); enhancer.setCallback( new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; return methodProxy.invokeSuper(o,objects); &#125; &#125; ); enhancer.create(); &#125; &#125; public static void main(String[] args) &#123; new JvmTest().MetaSpaceOOMTest(); &#125; &#125; 运行后可以看到如下异常： 这种问题出现较少，如果出现一般为动态代理生成大量class类引起的问题。 我们在JDK1.6环境下，设置PermGen（永久代）大小，进行测试。（-XX:PermSize=10m -XX:MaxPermSize=10m） 测试方法同上。 可以看到日志输出如下： 由于项目大多数JDK版本都在8及以上，故这种OOM异常已经很少在见到了。 JVM 日志我们再来看下JVM的垃圾回收日志，并简单解读下。我们这儿主要来看新的JVM（1.8及其后）的GC日志。 我们在运行时添加如下参数： -XX:-PrintGCDetails 我们用上面的 JvmTest类里的oomTest方法来进行测试。 可以看到如下一些GC运行日志和OOM的dump日志。 我们先来看下GC的运行日志部分： 12345678[GC (Allocation Failure) [PSYoungGen: 207268K-&gt;200192K(339968K)] 879015K-&gt;879170K(1489408K), 0.8137540 secs] [Times: user=2.95 sys=0.06, real=0.81 secs] [GC (Allocation Failure) [PSYoungGen: 339968K-&gt;243200K(339456K)] 1292739K-&gt;1292955K(1488896K), 0.8972164 secs] [Times: user=3.18 sys=0.20, real=0.90 secs] [Full GC (Ergonomics) [PSYoungGen: 243200K-&gt;0K(339456K)] [ParOldGen: 1049755K-&gt;1110054K(1722880K)] 1292955K-&gt;1110054K(2062336K), [Metaspace: 3502K-&gt;3502K(1056768K)], 7.8097561 secs] [Times: user=16.27 sys=0.19, real=7.81 secs] [GC (Allocation Failure) [PSYoungGen: 96256K-&gt;96384K(424448K)] 1206310K-&gt;1206438K(2147328K), 0.3838048 secs] [Times: user=1.39 sys=0.05, real=0.38 secs] [GC (Allocation Failure) [PSYoungGen: 192640K-&gt;192672K(425472K)] 1302694K-&gt;1302726K(2148352K), 0.6567791 secs] [Times: user=2.53 sys=0.00, real=0.66 secs] [GC (Allocation Failure) [PSYoungGen: 303776K-&gt;303840K(430080K)] 1824520K-&gt;1824584K(2152960K), 1.1635894 secs] [Times: user=4.29 sys=0.00, real=1.16 secs] [GC (Allocation Failure) [PSYoungGen: 414944K-&gt;347136K(496640K)] 1935688K-&gt;1935808K(2219520K), 1.6472200 secs] [Times: user=5.76 sys=0.19, real=1.65 secs] [Full GC (Ergonomics) [PSYoungGen: 347136K-&gt;0K(496640K)] [ParOldGen: 1588672K-&gt;1661667K(2083840K)] 1935808K-&gt;1661667K(2580480K), [Metaspace: 3502K-&gt;3502K(1056768K)], 10.5294207 secs] [Times: user=23.63 sys=0.09, real=10.53 secs] (Allocation Failure)：Allocation Failure表示向young generation(eden)给新对象申请空间，但是young generation(eden)剩余的合适空间不够所需的大小导致的GC。 [PSYoungGen: 207268K-&gt;200192K(339968K)] 879015K-&gt;879170K(1489408K), 0.8137540 secs] 这段分别表示 [年轻代: GC前内存容量 -&gt; GC后内存容量 (年轻代总容量)] GC前堆内存大小 -&gt; GC后堆内存大小(堆内存总大小),该内存区域GC耗时（与Times的real相等），单位是秒。 [Times: user=2.95 sys=0.06, real=0.81 secs] 这段分别表示用户态耗时，内核态耗时和总耗时。 Full GC (Ergonomics) 表明该次发生了Full GC，Ergonomics就是Full GC的原因，可以认为如果晋升到老生代的平均大小大于老生代的剩余大小，则认为需要一次full gc。某些垃圾回收器会负责自动的调解gc暂停时间和吞吐量之间的平衡，然后JVM虚拟机性能更好，因而会出现这种Full GC原因。 ParOldGen部分表示年老代的相关GC信息。 Metaspace部分表示元空间的相关GC信息。 我们在GC相关源码(openjdk源码中gcCause.cpp文件)中还可以看到多种GC原因，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include "precompiled.hpp"#include "gc/shared/gcCause.hpp"const char* GCCause::to_string(GCCause::Cause cause) &#123; switch (cause) &#123; case _java_lang_system_gc: return "System.gc()"; case _full_gc_alot: return "FullGCAlot"; case _scavenge_alot: return "ScavengeAlot"; case _allocation_profiler: return "Allocation Profiler"; case _jvmti_force_gc: return "JvmtiEnv ForceGarbageCollection"; case _gc_locker: return "GCLocker Initiated GC"; case _heap_inspection: return "Heap Inspection Initiated GC"; case _heap_dump: return "Heap Dump Initiated GC"; case _wb_young_gc: return "WhiteBox Initiated Young GC"; case _wb_conc_mark: return "WhiteBox Initiated Concurrent Mark"; case _wb_full_gc: return "WhiteBox Initiated Full GC"; case _update_allocation_context_stats_inc: case _update_allocation_context_stats_full: return "Update Allocation Context Stats"; case _no_gc: return "No GC"; case _allocation_failure: return "Allocation Failure"; case _tenured_generation_full: return "Tenured Generation Full"; case _metadata_GC_threshold: return "Metadata GC Threshold"; case _metadata_GC_clear_soft_refs: return "Metadata GC Clear Soft References"; case _cms_generation_full: return "CMS Generation Full"; case _cms_initial_mark: return "CMS Initial Mark"; case _cms_final_remark: return "CMS Final Remark"; case _cms_concurrent_mark: return "CMS Concurrent Mark"; case _old_generation_expanded_on_last_scavenge: return "Old Generation Expanded On Last Scavenge"; case _old_generation_too_full_to_scavenge: return "Old Generation Too Full To Scavenge"; case _adaptive_size_policy: return "Ergonomics"; case _g1_inc_collection_pause: return "G1 Evacuation Pause"; case _g1_humongous_allocation: return "G1 Humongous Allocation"; case _dcmd_gc_run: return "Diagnostic Command"; case _last_gc_cause: return "ILLEGAL VALUE - last gc cause - ILLEGAL VALUE"; default: return "unknown GCCause"; &#125; ShouldNotReachHere();&#125; 这儿就不在对上面的所有GC情况做详细介绍了，有兴趣的同学可以查阅相关资料了解。 gcCause相关资料： openjdk-gcCause.cpp jvm-gcCause-info 我们再来看下出现OOM后GC的dump日志部分。 123456789Heap PSYoungGen total 584192K, used 10301K [0x0000000780700000, 0x00000007ba880000, 0x00000007c0000000) eden space 257536K, 3% used [0x0000000780700000,0x000000078110f510,0x0000000790280000) from space 326656K, 0% used [0x00000007a6980000,0x00000007a6980000,0x00000007ba880000) to space 347136K, 0% used [0x0000000790280000,0x0000000790280000,0x00000007a5580000) ParOldGen total 2083840K, used 2054113K [0x0000000701400000, 0x0000000780700000, 0x0000000780700000) object space 2083840K, 98% used [0x0000000701400000,0x000000077e9f8790,0x0000000780700000) Metaspace used 3535K, capacity 4506K, committed 4864K, reserved 1056768K class space used 392K, capacity 394K, committed 512K, reserved 1048576K 它们打印的JVM终止时Heap（堆内存）的信息，从该日志中我们能分析出JVM终止的一些原因。 可以看到PSYoungGen（年轻代） eden区使用了3%，（两个Survivor）from和to区使用了0%，ParOldGen（年老代） object space（对象区）使用了98%，Metaspace（元空间） class space（类加载区）的使用情况。 因此明显由于创建了大量对象，一直存在，无法被垃圾回收，导致内存空间用尽，出现OOM异常。 JVM监控Demo现在有许多JVM监控工具，如JConsole、Java VisualVM等，我们这里不过多介绍。 我们自写一个监控Demo来看下JVM在内存使用过程中的一些变化特点。 来看下java.lang.management包下的一些类。 MemoryMXBean ： 它里面有两个方法 getHeapMemoryUsage （获取堆内存使用情况）和getNonHeapMemoryUsage（获取非堆内存使用情况），返回MemoryUsage对象。 MemoryUsage：包含init（初始化了多少内存）、used（使用了多少内存）、committed（申请了多少内存）、max（最大内存）信息。 MemoryPoolMXBean：这里包含young（eden和survivor）、old等内存区的使用情况，我们可以通过 ManagementFactory.getMemoryPoolMXBeans() 获取到一个 MemoryPoolMXBean 列表，MemoryPoolMXBean里还有一个getName方法可以获得当前区域的名称。 GarbageCollectorMXBean：这个是垃圾收集相关的Bean，可以通过ManagementFactory.getGarbageCollectorMXBeans()获取其列表。其getName方法可以获得垃圾收集器的名称，getCollectionCount可以获得当前已经进行了多少次垃圾收集，getCollectionTime返回垃圾收集时间。 我们写一个测试类来看一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class JVMMonitorMemoryTest &#123; //定时任务线程 private static final ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(1); private ScheduledFuture future = null; private JvmTest jvmTest = new JvmTest(); public void doMonitor()&#123; future = executorService.scheduleAtFixedRate(()-&gt;&#123; JSONObject jsonObject = new JSONObject(); MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean(); jsonObject.put("totalMaxMemery", memoryMXBean.getHeapMemoryUsage().getMax()&gt;&gt;10&gt;&gt;10); jsonObject.put("totalUsedMemery", memoryMXBean.getHeapMemoryUsage().getUsed()&gt;&gt;10&gt;&gt;10); jsonObject.put("totalInitMemery", memoryMXBean.getHeapMemoryUsage().getInit()&gt;&gt;10&gt;&gt;10); //这里会返回老年代，新生代等内存区的使用情况 List&lt;MemoryPoolMXBean&gt; memoryPoolMXBeans = ManagementFactory.getMemoryPoolMXBeans(); memoryPoolMXBeans.forEach((pool) -&gt; &#123; String poolName = pool.getName().trim(); long max = pool.getUsage().getMax()&gt;&gt;10&gt;&gt;10; long used = pool.getUsage().getUsed()&gt;&gt;10&gt;&gt;10; long init = pool.getUsage().getInit()&gt;&gt;10&gt;&gt;10; long maxPeak = pool.getPeakUsage().getMax()&gt;&gt;10&gt;&gt;10; long usedPeak = pool.getPeakUsage().getUsed()&gt;&gt;10&gt;&gt;10; long initPeak = pool.getPeakUsage().getInit()&gt;&gt;10&gt;&gt;10; JSONObject poolJSON = new JSONObject(); poolJSON.put("max", max); poolJSON.put("used", used); poolJSON.put("init", init); poolJSON.put("maxPeak", maxPeak); poolJSON.put("usedPeak", usedPeak); poolJSON.put("initPeak", initPeak); if("PS Eden Space".equalsIgnoreCase(poolName))&#123; jsonObject.put("eden", poolJSON); &#125;else if("PS Survivor Space".equalsIgnoreCase(poolName))&#123; jsonObject.put("survivor", poolJSON); &#125;else if("PS Old Gen".equalsIgnoreCase(poolName))&#123; jsonObject.put("old", poolJSON); &#125;else if("Metaspace".equalsIgnoreCase(poolName))&#123; jsonObject.put("metaspace",poolJSON); &#125; &#125;); //垃圾收集相关 List&lt;GarbageCollectorMXBean&gt; garbageCollectorMXBeans = ManagementFactory.getGarbageCollectorMXBeans(); garbageCollectorMXBeans.forEach(collector -&gt; &#123; String gcName = collector.getName(); long gcCount = collector.getCollectionCount(); long gcTime = collector.getCollectionTime(); JSONObject gcJSON = new JSONObject(); gcJSON.put("gcCount", gcCount); gcJSON.put("gcTime", gcTime); if(gcName.toLowerCase().contains("scavenge"))&#123; jsonObject.put("edenGc", gcJSON); &#125;else if(gcName.toLowerCase().contains("marksweep"))&#123; jsonObject.put("oldGc", gcJSON); &#125; &#125;); System.out.println(JSON.toJSONString(jsonObject)); &#125;, 1, 1, TimeUnit.SECONDS); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //调用生成对象的方法 jvmTest.test(); &#125; public static void main(String[] args) &#123; JVMMonitorMemoryTest jvmMonitorMemoryTest = new JVMMonitorMemoryTest(); jvmMonitorMemoryTest.doMonitor(); &#125;&#125;//创建一个方法不停生产对象public class JvmTest &#123; public void test() &#123; List&lt;JvmTest&gt; list = new ArrayList&lt;&gt;(); try&#123; while (true)&#123; list.add(new JvmTest()); if(list.size()&gt;10000)&#123; list = new ArrayList&lt;&gt;(); TimeUnit.MILLISECONDS.sleep(100); &#125; &#125; &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 我们运行后可以看到相关输出信息，即JVM堆内存变化情况及垃圾收集情况。 数字数据不是很直观，我们结合Echart图表，动态展示JVM相关信息，因此我们把项目改造下，结合WebSocket来实现。 项目大致结构如下： 说一下里面的关键部分，MonitorJVMMemory.java（监控JVM内存变化类）和jvm-echart.js（Echart前端动态展示）。 MonitorJVMMemory相关代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135@ServerEndpoint("/websocket/jvm/monitor")public class MonitorJVMMemory implements ServletContextListener &#123; /** * 日志 */ private static final Logger logger = LoggerFactory.getLogger(MonitorJVMMemory.class); //定时任务线程 private static final ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(1); private ScheduledFuture future = null; private JvmTest jvmTest = new JvmTest(); /** * websocket会话 */ private Session session; @OnOpen public void init(Session session) &#123; this.session = session; future = executorService.scheduleAtFixedRate(() -&gt; &#123; JSONObject jsonObject = new JSONObject(); MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean(); jsonObject.put("totalMaxMemery", memoryMXBean.getHeapMemoryUsage().getMax() &gt;&gt; 10 &gt;&gt; 10); jsonObject.put("totalUsedMemery", memoryMXBean.getHeapMemoryUsage().getUsed() &gt;&gt; 10 &gt;&gt; 10); jsonObject.put("totalInitMemery", memoryMXBean.getHeapMemoryUsage().getInit() &gt;&gt; 10 &gt;&gt; 10); //这里会返回老年代，新生代等内存区的使用情况，按需自取就好 List&lt;MemoryPoolMXBean&gt; memoryPoolMXBeans = ManagementFactory.getMemoryPoolMXBeans(); memoryPoolMXBeans.forEach((pool) -&gt; &#123; String poolName = pool.getName().trim(); long max = pool.getUsage().getMax() &gt;&gt; 10 &gt;&gt; 10; long used = pool.getUsage().getUsed() &gt;&gt; 10 &gt;&gt; 10; long init = pool.getUsage().getInit() &gt;&gt; 10 &gt;&gt; 10; long maxPeak = pool.getPeakUsage().getMax() &gt;&gt; 10 &gt;&gt; 10; long usedPeak = pool.getPeakUsage().getUsed() &gt;&gt; 10 &gt;&gt; 10; long initPeak = pool.getPeakUsage().getInit() &gt;&gt; 10 &gt;&gt; 10; JSONObject poolJSON = new JSONObject(); poolJSON.put("max", max); poolJSON.put("used", used); poolJSON.put("init", init); poolJSON.put("maxPeak", maxPeak); poolJSON.put("usedPeak", usedPeak); poolJSON.put("initPeak", initPeak); if ("PS Eden Space".equalsIgnoreCase(poolName)) &#123; jsonObject.put("eden", poolJSON); &#125; else if ("PS Survivor Space".equalsIgnoreCase(poolName)) &#123; jsonObject.put("survivor", poolJSON); &#125; else if ("PS Old Gen".equalsIgnoreCase(poolName)) &#123; jsonObject.put("old", poolJSON); &#125; &#125;); //垃圾收集 List&lt;GarbageCollectorMXBean&gt; garbageCollectorMXBeans = ManagementFactory.getGarbageCollectorMXBeans(); garbageCollectorMXBeans.forEach(collector -&gt; &#123; String gcName = collector.getName(); long gcCount = collector.getCollectionCount(); long gcTime = collector.getCollectionTime(); JSONObject gcJSON = new JSONObject(); gcJSON.put("gcCount", gcCount); gcJSON.put("gcTime", gcTime); if (gcName.toLowerCase().contains("scavenge")) &#123; jsonObject.put("edenGc", gcJSON); &#125; else if (gcName.toLowerCase().contains("marksweep")) &#123; jsonObject.put("oldGc", gcJSON); &#125; &#125;); try &#123; session.getBasicRemote().sendText(jsonObject.toJSONString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;, 1, 1, TimeUnit.SECONDS); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; jvmTest.test(); &#125; /** * 接收信息 */ @OnMessage public void acceptMessage(String message) &#123; logger.info("Accept&gt;&gt;&gt;" + message); &#125; /** * 关闭会话 */ @OnClose public void closeSession(CloseReason closeReason) &#123; this.destory(); logger.info(closeReason.getReasonPhrase()); &#125; /** * 异常处理 */ @OnError public void errorHandler(Throwable e) &#123; this.destory(); logger.info("MonitorJVMMemory websocket error ：" + e.getMessage()); &#125; /** * 关闭资源 */ private void destory() &#123; try &#123; if (future != null &amp;&amp; !future.isCancelled()) &#123; future.cancel(true); &#125; if (session != null) &#123; session.close(); &#125; &#125; catch (Exception e) &#123; logger.error("destory", e); &#125; &#125; @Override public void contextInitialized(ServletContextEvent servletContextEvent) &#123; &#125; @Override public void contextDestroyed(ServletContextEvent servletContextEvent) &#123; jvmTest.stop(); executorService.shutdownNow(); &#125;&#125; 可以看到我们使用了Websocket，当连接Open后，使用定长线程池，里面维护一个每隔1s调用一次的方法，来查看当前内存情况，并使用jvmTest.test()来生成测试对象。 线程池里运行的线程执行的就是我们上面JVMMonitorMemoryTest类的doMonitor方法。 再看下jvm-echart.js： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134//echartvar memoryEchart = echarts.init(document.getElementById('memory_main'));var memoryData = new Array(3);//定义图表样式var memoryOption = &#123; tooltip : &#123; trigger: 'axis', axisPointer : &#123; // 坐标轴指示器，坐标轴触发有效 type : 'shadow' // 默认为直线，可选为：'line' | 'shadow' &#125;, formatter: function (params) &#123; var dataIndex = params[0].dataIndex; var res = params[0].axisValue; if(dataIndex==0 || dataIndex==1)&#123; res += '&lt;br/&gt;累计回收次数：' + params[0].data; res += '&lt;br/&gt;累计回收时间：' + params[1].data + "ms"; res += '&lt;br/&gt;平均回收时间：' + parseInt(params[1].data/params[0].data) + "ms"; &#125;else&#123; res += '&lt;br/&gt;已用内存量：' + params[0].data + "MB"; if(params[0].axisValue!='峰值内存消耗')&#123; var maxData = memoryData[2]; res += '&lt;br/&gt;可用内存量：' + params[1].data + "MB"; res += '&lt;br/&gt;最大内存量：' + maxData[dataIndex] + "MB"; &#125; &#125; return res; &#125; &#125;, color: ['#ff0000','#91C7AE'], legend: &#123; data: ['已用内存(MB)', '可用内存(MB)'] &#125;, grid: &#123; left: '3%', right: '4%', bottom: '3%', containLabel: true &#125;, xAxis: &#123; type: 'value' &#125;, yAxis: &#123; type: 'category', data: ['OldGenGC','EdenGC','Old Gen','Survivor Space','Eden Space','峰值内存消耗','JVM总内存'] &#125;, series: [ &#123; name: '已用内存(MB)', type: 'bar', stack: '总量', barWidth: 60, label: &#123; normal: &#123; show: true, position: 'insideRight' &#125; &#125;, data: memoryData[0] &#125;, &#123; name: '可用内存(MB)', type: 'bar', stack: '总量', barWidth: 60, label: &#123; normal: &#123; show: true, position: 'insideRight' &#125; &#125;, data: memoryData[1] &#125; ]&#125;;memoryEchart.setOption(memoryOption);//刷新图表数据function refreshMemoryData() &#123; memoryEchart.setOption(&#123; series: [&#123; data: memoryData[0] &#125;,&#123; data: memoryData[1] &#125;] &#125;);&#125;//与websocket建立连接var memorySocket;function initMemorySocket() &#123; if(memorySocket!=undefined || memorySocket!=null)&#123; memorySocket.close("3000", "断开连接"); &#125; var wsUrl = 'ws://'+$('#hid_host').val()+'/websocket/jvm/monitor'; console.log(wsUrl); memorySocket = new WebSocket(wsUrl); memorySocket.onopen = function (evt) &#123; console.log("Connection the jvm monitor server success!!!"); &#125;; memorySocket.onmessage = function (evt) &#123; var memory = $.parseJSON(evt.data); var peakUsed = memory.old.usedPeak + memory.eden.usedPeak; var usedData = [memory.oldGc.gcCount, memory.edenGc.gcCount, memory.old.used, memory.survivor.used, memory.eden.used, peakUsed, memory.totalUsedMemery]; var peakMax = parseInt((memory.old.maxPeak + memory.eden.maxPeak)*0.8); var usable = [memory.oldGc.gcTime, memory.edenGc.gcTime, memory.old.max-memory.old.used, memory.survivor.max-memory.survivor.used, memory.eden.max-memory.eden.used, 0, memory.totalMaxMemery-memory.totalUsedMemery]; var maxData = [memory.oldGc.gcTime, memory.edenGc.gcTime, memory.old.max, memory.survivor.max, memory.eden.max, 0, memory.totalMaxMemery]; memoryData[0] = usedData; memoryData[1] = usable; memoryData[2] = maxData; refreshMemoryData(); &#125;; memorySocket.onerror = function (evt) &#123; memorySocket.close(); &#125;;&#125;//断开监控连接function closeMemoryMonitor() &#123; if(memorySocket!=undefined || memorySocket!=null)&#123; memorySocket.close("3000", "断开连接"); &#125; memoryData[0] = []; memoryData[1] = []; memoryData[2] = []; refreshMemoryData(); $.messager.show(&#123; title: '系统提示', msg: '已断开监控连接！'&#125;); console.log("Disconnect the jvm monitor server success!!!");&#125;$(function () &#123; initMemorySocket();&#125;); 这个就是在解析后台数据构造Echart图表，这儿就不详细介绍了。 详细源码可以在 jvm-monitor-memory 看到。 我们可以简单看下运行效果图，可以看到JVM进行垃圾回收后内存的变化情况。 总结通过这篇文章，我们了解了JVM垃圾回收的一些运行原理，对JVM堆内存有了更深入的认识。了解虚拟机内存及垃圾回收的一些特性，有助于我们在工作过程中排查定位问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Lookup注解]]></title>
    <url>%2Fblog%2FSpring-Lookup%E6%B3%A8%E8%A7%A3.html</url>
    <content type="text"><![CDATA[前言今天我们看下Spring的Lookup注解，这个注解可以使Spring替换一个bean原有的，获取其它对象具体的方法，并自动返回在容器中的查找结果。 正文在了解它之前，我们先来看下一个例子。 我们有一个Bean，TestClassB，它是多例的。大致如下： 1234567@Component@Scope(value = SCOPE_PROTOTYPE)public class TestClassB &#123; public void printClass() &#123; System.out.println("This is TestClass B: " + this); &#125;&#125; 现在一个单例Bean，TestClassA，使用到了TestClassB，代码大致如下： 12345678910@Componentpublic class TestClassA &#123; @Autowired private TestClassB testClassB; public void printClass() &#123; System.out.println("This is TestClass A: " + this); testClassB.printClass(); &#125;&#125; 我们进行下测试，可以发现一些问题。 123456789101112@RunWith(SpringRunner.class)@SpringBootTestpublic class LookUpTests &#123; @Autowired private TestClassA testClassA; @Test public void test()&#123; for (int i=0;i&lt;5;i++)&#123; testClassA.printClass(); &#125; &#125;&#125; 可以看到输出结果： 12345678910This is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2aThis is TestClass A: com.zwt.demo.util.TestClassA@2c768adaThis is TestClass B: com.zwt.demo.util.TestClassB@c1fca2a 对于TestClassA，它因为是单例，所以一直是一个实例，我们是可以理解的，但是对于TestClassB，我们明明设置了多例，但是我们发现它仍是一个实例，相当于单例。 对于这种情况的产生，很好理解，因为TestClassA为单例，因此TestClassB只有一次注入的机会，即在生成单例TestClassA的时候，因此导致了TestClassB的多例不体现，仍相当于个单例模式。 如果我们要求TestClassB必须为多例的，那么上面这种情况是会出现问题的。 如何解决这种问题呢？ 其实我们最常用的一种方法是拿到SpringContext，然后手动获取Bean。代码大致如下： 1234567891011121314@Componentpublic class SpringUtils implements ApplicationContextAware &#123; private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; SpringUtils.applicationContext=applicationContext; &#125; public static ApplicationContext getApplicationContext()&#123; return applicationContext; &#125; public static &lt;T&gt; T getBean(Class&lt;T&gt; clazz)&#123; return applicationContext.getBean(clazz); &#125;&#125; 同时TestClassA里的方法如下： 1234567@Componentpublic class TestClassA &#123; public void printClass()&#123; System.out.println("This is TestClass A: " + this); SpringUtils.getBean(TestClassB.class).printClass(); &#125;&#125; 可以看到输出结果,TestClassB已经是多例的了。 12345678910This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@1640c151This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@5d5b5fa7This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@2a32fb6This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@6107165This is TestClass A: com.zwt.demo.util.TestClassA@7c447c76This is TestClass B: com.zwt.demo.util.TestClassB@164a62bf 当然也可以让TestClassA继承ApplicationContextAware直接拿到ApplicationContext，然后获取TestClassB。 这种方式在项目使用中还是比较多的，也是很方便的。 还有其他方法吗？ 下面我们来说另一种方式，当然就是我们今天的主角，Lookup注解。那具体如何使用呢，我们来看下。 1234567891011@Componentpublic class TestClassA &#123; @Lookup public TestClassB getTestClassB() &#123; return null; &#125; public void printClass() &#123; System.out.println("This is TestClass A: " + this); getTestClassB().printClass(); &#125;&#125; 我们运行测试类，可以看到输出结果： 12345678910This is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@1aac188dThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@7026b7eeThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@2d23faefThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@7cb8437dThis is TestClass A: com.zwt.demo.util.TestClassA$$EnhancerBySpringCGLIB$$a967ae38@5a00eb1eThis is TestClass B: com.zwt.demo.util.TestClassB@62a4417 发现TestClassB是多例的，可是我们根据上面的代码，感觉TestClassB返回应该不是null么。 当然，这就要来了解下Lookup注解了，我们分析一下它的源码： 123456@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Lookup &#123; String value() default "";&#125; 可以看到该注解作用于方法上，有一个参数value，这个值可以指定要look up的Bean的名字。如果不指定，就会默认方法返回的类型寻找Bean并进行Look up。 我们在Spring源码中寻找下该注解 @Lookup，会发现只有一个地方使用到了该注解。 在AutowiredAnnotationBeanPostProcessor类的determineCandidateConstructors方法里。该方法部分内容如下： 12345678910111213141516171819202122232425262728293031323334public Constructor&lt;?&gt;[] determineCandidateConstructors(Class&lt;?&gt; beanClass, final String beanName) throws BeanCreationException &#123; // Let's check for lookup methods here.. if (!this.lookupMethodsChecked.contains(beanName)) &#123; try &#123; ReflectionUtils.doWithMethods(beanClass, new ReflectionUtils.MethodCallback() &#123; @Override public void doWith(Method method) throws IllegalArgumentException, IllegalAccessException &#123; Lookup lookup = method.getAnnotation(Lookup.class); if (lookup != null) &#123; LookupOverride override = new LookupOverride(method, lookup.value()); try &#123; RootBeanDefinition mbd = (RootBeanDefinition) beanFactory.getMergedBeanDefinition(beanName); mbd.getMethodOverrides().addOverride(override); &#125; catch (NoSuchBeanDefinitionException ex) &#123; throw new BeanCreationException(beanName, "Cannot apply @Lookup to beans without corresponding bean definition"); &#125; &#125; &#125; &#125;); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, "Lookup method resolution failed", ex); &#125; catch (NoClassDefFoundError err) &#123; throw new BeanCreationException(beanName, "Failed to introspect bean class [" + beanClass.getName() + "] for lookup method metadata: could not find class that it depends on", err); &#125; this.lookupMethodsChecked.add(beanName); &#125; //部分代码略...... 可以看到Spring会首先判断该Bean是否有Lookup注解的方法，现在缓存里看，缓存没有的话会尝试获取方法上的Lookup注解，如果存在，拿到需要重写（覆盖）的方法信息放入LookupOverride，最后为RootBeanDefinition添加LookupOverride的属性。这个方法最终会被AbstractAutowireCapableBeanFactory类中的createBeanInstance方法调用，去生成新的Bean并重写，实现改变Bean的效果。 因此原理大致为：方法执行返回的对象，使用 Spring 内原有的这类对象替换，通过改变方法返回值来动态改变方法。内部实现为使用 cglib 方法，重新生成子类，重写配置的方法和返回对象，达到动态改变的效果。因此Bean的多列特性也被体现了。 总结通过解决一个单例Bean（无状态Bean）调用多例Bean（有状态Bean）的问题，我们了解了Lookup注解的一些简单用法，对Spring也有了一些深入的认识。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>Lookup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringRetry框架简介]]></title>
    <url>%2Fblog%2FSpringRetry%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言今天我们来聊一聊Spring Retry框架。 Spring Retry提供了一个关于重试失败操作的抽象，强调对流程和基于策略的行为的声明性控制，易于扩展和定制。例如，对于一个操作，如果它失败了，我们可以根据异常的类型，使用一个固定的或指数级的回退来重试它。 并不是所有的异常失败都适合重试，比如参数校验错误，显然不适合重试，而Spring Retry可以指定要重试的异常类型，对于指定类型的异常进行重试。 考虑到网络原因，可能一些方法失败后不立即进行下一次重试，而等待若干时间后再进行，Spring Retry里也支持此种类型的重试。 可能所有的重试都不成功，此时需要返回一个程序默认值或者直接抛出异常等，Spring Retry的兜底函数可以解决此类问题。 另外Spring Retry还支持简单的熔断策略。 正文说了这么多，我们来看下Spring Retry吧。 要使用Spring Retry，首先要引入相关jar包，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt; &lt;version&gt;1.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 先简单的写一个例子。 123456789101112131415161718192021222324252627282930313233343536public class Test &#123; public static void main(String[] args) &#123; RetryTemplate template = new RetryTemplate(); //重试策略：次数重试策略 SimpleRetryPolicy retryPolicy = new SimpleRetryPolicy(); retryPolicy.setMaxAttempts(3); template.setRetryPolicy(retryPolicy); //退避策略：固定退避策略 FixedBackOffPolicy backOffPolicy = new FixedBackOffPolicy(); backOffPolicy.setBackOffPeriod(4*1000L); template.setBackOffPolicy(backOffPolicy); String str = ""; //当重试失败后，执行RecoveryCallback String result2 = template.execute((retryCallback)-&gt;&#123; System.out.println("retry count:" + retryCallback.getRetryCount()); return getStr(str); &#125;, (recoveryCallback)-&gt; &#123; System.out.println("所有重试均失败!!"); return ""; &#125;); System.out.println("返回值为："+result2); &#125; /** * 测试方法 * @param str * @return */ public static String getStr(String str)&#123; if(StringUtils.isBlank(str))&#123; throw new RuntimeException("数据为空！"); &#125; return str; &#125;&#125; 上面的代码，当我们传入空或者空字符串时，可以看到程序会重试3次（每隔4s），均不成功，最后返回recoveryCallback的数据。 12345678910111218:05:20.879 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=0retry count:018:05:24.889 [main] DEBUG org.springframework.retry.support.RetryTemplate - Checking for rethrow: count=118:05:24.889 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=1retry count:118:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Checking for rethrow: count=218:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry: count=2retry count:218:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Checking for rethrow: count=318:05:28.890 [main] DEBUG org.springframework.retry.support.RetryTemplate - Retry failed last attempt: count=3所有重试均失败!!返回值为：失败 我们来看下例子中涉及到的一些东西。 可以看到，要使用重试功能，首先要创建一个RetryTemplate，并设置它的两个重要参数：重试策略（RetryPolicy）和退避策略（BackOffPolicy）。 重试策略这两个策略还是比较好理解的，对于重试策略，指的就是请求不成功后下次请求的策略。很明显我们可以看到它是一个接口RetryPolicy。 这个接口里比较重要的一个方法为canRetry，它的返回值决定下一次是否重试。 1boolean canRetry(RetryContext context); 对于这个接口，可以看到它目前有8种重试策略。 NeverRetryPolicy 只调用被执行方法一次，不会进行重试操作。 我们可以看到它的canRetry方法。可以看到这个方法会一直返回false。 123456789101112131415161718 public boolean canRetry(RetryContext context) &#123; return !((NeverRetryContext) context).isFinished();&#125; private static class NeverRetryContext extends RetryContextSupport &#123; private boolean finished = false; public NeverRetryContext(RetryContext parent) &#123; super(parent); &#125; public boolean isFinished() &#123; return finished; &#125; public void setFinished() &#123; this.finished = true; &#125;&#125; AlwaysRetryPolicy 如果被执行方法调用不成功会一直重试，这种方法如果操作不当会出现死循环的情况，应当注意。 我们可以看到它里面的canRetry方法一直返回true，即如果调用失败，会一直重试直到成功。 123 public boolean canRetry(RetryContext context) &#123; return true;&#125; SimpleRetryPolicy 固定次数重试策略，默认最多重试3次，我们可以通过指定其maxAttempts参数的值来规定最多重试多少次。 它的canRetry方法可以看到和当前已重试次数做了比较来确定下一次是否重试。 1234 public boolean canRetry(RetryContext context) &#123; Throwable t = context.getLastThrowable(); return (t == null || retryForException(t)) &amp;&amp; context.getRetryCount() &lt; maxAttempts;&#125; TimeoutRetryPolicy 超时重试策略，只有在超时时间内才可以重试，超过后就不会再进行重试，超时时间可以认为是在第一次请求开始时计数。默认超时时间1000ms，我们可以通过设置timeout的值来指定超时时间。 它的canRetry方法，可以看到时间的对比来确定是否进行重试。 123456789101112131415161718 public boolean canRetry(RetryContext context) &#123; return ((TimeoutRetryContext) context).isAlive();&#125; private static class TimeoutRetryContext extends RetryContextSupport &#123; private long timeout; private long start; public TimeoutRetryContext(RetryContext parent, long timeout) &#123; super(parent); this.start = System.currentTimeMillis(); this.timeout = timeout; &#125; public boolean isAlive() &#123; return (System.currentTimeMillis() - start) &lt;= timeout; &#125;&#125; CompositeRetryPolicy 组合重试策略，有乐观重试和悲观重试两种情况。可以看到它有两个参数，optimistic和policies。 optimistic表示是否乐观，默认false。 policies表示所有传入的重试策略。 我们根据它的canRetry方法，可以清楚的知道，如果乐观情况下，有一个策略（policies[i]）canRetry为true就可以进行重试，悲观情况下只有所有的传入的重试策略canRetry为true才可以进行重试。 123456789101112131415161718192021222324 public boolean canRetry(RetryContext context) &#123; RetryContext[] contexts = ((CompositeRetryContext) context).contexts; RetryPolicy[] policies = ((CompositeRetryContext) context).policies; boolean retryable = true; if(this.optimistic) &#123; retryable = false; for (int i = 0; i &lt; contexts.length; i++) &#123; if (policies[i].canRetry(contexts[i])) &#123; retryable = true; &#125; &#125; &#125; else &#123; for (int i = 0; i &lt; contexts.length; i++) &#123; if (!policies[i].canRetry(contexts[i])) &#123; retryable = false; &#125; &#125; &#125; return retryable;&#125; ExpressionRetryPolicy 异常重试策略，会对抛出指定异常的情况下进行重试，继承SimpleRetryPolicy。可以指定要重试的异常参数expression，也可以指定异常的全名字符串，会被转化为指定异常。 我们看一下它的canRetry方法。 12345678910 public boolean canRetry(RetryContext context) &#123; Throwable lastThrowable = context.getLastThrowable(); if (lastThrowable == null) &#123; return super.canRetry(context); &#125; else &#123; return super.canRetry(context) &amp;&amp; this.expression.getValue(this.evaluationContext, lastThrowable, Boolean.class); &#125;&#125; 可以看到除了使用了SimpleRetryPolicy的canRetry判断还有对是不是当前异常的判断，来确定是否重试。 当然这个策略也是可以指定最大重试次数maxAttempts的。 ExceptionClassifierRetryPolicy 根据最新的异常动态的适应注入的策略，需要设置参数exceptionClassifier。 比如第一次重试时，抛出异常A，对应传入策略A，当第二次重试时，抛出异常B，则对应传入的策略B。 1234 public boolean canRetry(RetryContext context) &#123; RetryPolicy policy = (RetryPolicy) context; return policy.canRetry(context);&#125; 可以看到它的canRetry返回值取决于当前使用的策略的canRetry方法的返回值，而策略的动态切换由ExceptionClassifierRetryContext这个类来处理，这儿不再过多介绍。 CircuitBreakerRetryPolicy 带有熔断的重试策略，该策略提供过载保护功能，它的canRetry代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 public boolean canRetry(RetryContext context) &#123; CircuitBreakerRetryContext circuit = (CircuitBreakerRetryContext) context; //如果熔断器处于打开状态，就直接短路，返回失败 if (circuit.isOpen()) &#123; circuit.incrementShortCircuitCount(); return false; &#125; else &#123; //重置熔断器 circuit.reset(); &#125; return this.delegate.canRetry(circuit.context);&#125; //------- isOpen方法如下 public boolean isOpen() &#123; long time = System.currentTimeMillis() - this.start; boolean retryable = this.policy.canRetry(this.context); //当前不允许重试 if (!retryable) &#123; //如果已经超过重置时间，重新闭合，关闭熔断器 if (time &gt; this.timeout) &#123; logger.trace("Closing"); this.context = createDelegateContext(policy, getParent()); this.start = System.currentTimeMillis(); retryable = this.policy.canRetry(this.context); &#125; // 如果小于熔断器打开时间，读取关闭状态，如果熔断器是关闭的，就打开熔断器，重置熔断计时器 else if (time &lt; this.openWindow) &#123; if ((Boolean) getAttribute(CIRCUIT_OPEN) == false) &#123; logger.trace("Opening circuit"); setAttribute(CIRCUIT_OPEN, true); &#125; this.start = System.currentTimeMillis(); return true; &#125; &#125; //允许重试 else &#123; //判断是否在openWindow熔断器电路打开的超时时间之外，超过打开时间，就重置上下文，并且返回false if (time &gt; this.openWindow) &#123; logger.trace("Resetting context"); this.start = System.currentTimeMillis(); this.context = createDelegateContext(policy, getParent()); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace("Open: " + !retryable); &#125; setAttribute(CIRCUIT_OPEN, !retryable); return !retryable;&#125; 它接受三个参数，delegate、resetTimeout和openTimeout。 delegate指使用的重试策略，默认使用SimpleRetryPolicy。 resetTimeout表示重置线路超时时间(以毫秒为单位)。当线路打开后，它会在此时间过后重新关闭，上下文将重新启动。 openTimeout表示断开线路的超时时间。如果委托策略无法重试，则自上下文启动以来经过的时间小于此时间，则打开线路。 退避策略我们再来看一下退避策略（BackOffPolicy）。 退避策略接口（BackOffPolicy）目前有5种已实现策略。如下图： 我们来分别看一下它们。 要实现退避策略，重要的是实现接口的backoff方法。 1234public interface BackOffPolicy &#123; BackOffContext start(RetryContext context); void backOff(BackOffContext backOffContext) throws BackOffInterruptedException;&#125; 这个方法的实现有两个主要类，抽象类StatelessBackOffPolicy和实现类ExponentialBackOffPolicy。 如图： StatelessBackOffPolicy 这是用于在调用之间不维护任何状态的退避策略实现的简单基类，它的backoff方法调用了子类的doBackOff方法。 123public final void backOff(BackOffContext backOffContext) throws BackOffInterruptedException &#123;doBackOff();&#125; 可以看到它的三个实现并简单分析，如下图。 NoBackOffPolicy 无任何退避策略，可以看到doBackOff方法什么也没做。 12 protected void doBackOff() throws BackOffInterruptedException &#123;&#125; 这种情况下，如果一次重试不成功，下一次会直接再进行重试。 FixedBackOffPolicy 固定退避策略，这种情况下，一次重试不成功，下一次会间隔一段时间后在进行重试。 可以看到它可以通过设置backOffPeriod（退避间隔）来指定与下一次重试的间隔时间。这个值默认为1000ms。 这个类里面另一个比较重要的参数为Sleeper（休眠器），它可以指定程序的休眠方式，默认使用ThreadWaitSleeper休眠器。 可以看到它的doBackOff方法直接调用了休眠器的sleep方法休眠一段时间。 12345678 protected void doBackOff() throws BackOffInterruptedException &#123; try &#123; sleeper.sleep(backOffPeriod); &#125; catch (InterruptedException e) &#123; throw new BackOffInterruptedException("Thread interrupted while sleeping", e); &#125;&#125; UniformRandomBackOffPolicy 随机休眠退避策略，当一次重试失败后，下一次重试之前，这个策略会随机退避一段时间。 看到这个我们明显就知道它会有minBackOffPeriod（最小退避时间）和maxBackOffPeriod（最大退避时间）两个值了。最小退避值默认500ms，最大退避值默认1500ms。 除了上面两个参数，它里面比较重要的两个参数一个是取值器和休眠器。 12 private Random random = new Random(System.currentTimeMillis());private Sleeper sleeper = new ThreadWaitSleeper(); 上面代码可以看到它们的值（random取值器不可人为修改）。 再来看下doBackOff方法。 123456789 protected void doBackOff() throws BackOffInterruptedException &#123; try &#123; long delta = maxBackOffPeriod==minBackOffPeriod ? 0 : random.nextInt((int) (maxBackOffPeriod - minBackOffPeriod)); sleeper.sleep(minBackOffPeriod + delta ); &#125; catch (InterruptedException e) &#123; throw new BackOffInterruptedException("Thread interrupted while sleeping", e); &#125;&#125; 也是比较好理解的，可以看到当最大时间和最小时间相等时，delta=0，即每次重试之前都休眠minBackOffPeriod时间。 ExponentialBackOffPolicy 指数型退避策略，顾名思义，它的退避时间是指数增长的。 我们来看下它的三个参数，initialInterval 初始时间间隔，maxInterval 最大时间间隔，multiplier指数因子。 来看一下它的backOff方法： 1234567891011121314 public void backOff(BackOffContext backOffContext) throws BackOffInterruptedException &#123; ExponentialBackOffContext context = (ExponentialBackOffContext) backOffContext; try &#123; long sleepTime = context.getSleepAndIncrement(); if (logger.isDebugEnabled()) &#123; logger.debug("Sleeping for " + sleepTime); &#125; sleeper.sleep(sleepTime); &#125; catch (InterruptedException e) &#123; throw new BackOffInterruptedException("Thread interrupted while sleeping", e); &#125;&#125; 以及它涉及到的下面的方法： 1234567891011121314 public synchronized long getSleepAndIncrement() &#123; long sleep = this.interval; if (sleep &gt; maxInterval) &#123; sleep = maxInterval; &#125; else &#123; this.interval = getNextInterval(); &#125; return sleep;&#125;protected long getNextInterval() &#123; return (long) (this.interval * this.multiplier);&#125; 可以看到逻辑很好理解，默认退避时间为interval，如果interval超过maxInterval，退避时间就为maxInterval，否则就获取下一次的interval时间，这个时间就是interval*multiplier，所以退避时间会以指数增长。 它的另一个参数Sleeper（休眠器）默认也是ThreadWaitSleeper。 initialInterval初始时间默认值为100ms，maxInterval最大时间默认为30000ms，multiplier指数因子默认为2. ExponentialRandomBackOffPolicy 随机指数退避策略，对于上面的指数策略，这儿不一样的就是指数因子会随机变化。 我们大致看一下这个策略的源码。 12345678910111213141516171819202122 public class ExponentialRandomBackOffPolicy extends ExponentialBackOffPolicy &#123; //部分代码略 @Overridestatic class ExponentialRandomBackOffContext extends ExponentialBackOffPolicy.ExponentialBackOffContext &#123; private final Random r = new Random(); public ExponentialRandomBackOffContext(long expSeed, double multiplier, long maxInterval) &#123; super(expSeed, multiplier, maxInterval); &#125; @Override public synchronized long getSleepAndIncrement() &#123; long next = super.getSleepAndIncrement(); next = (long) (next * (1 + r.nextFloat() * (getMultiplier() - 1))); return next; &#125;&#125; //部分代码略 &#125; 可以看到它继承了ExponentialBackOffPolicy，并重写了ExponentialBackOffContext里的getSleepAndIncrement方法，原来的指数因子改为随机的了。 其它与ExponentialBackOffPolicy一致，这儿不再介绍。 RetryTemplate再来看下重试模板RetryTemplate，除了上面说到RetryPolicy和BackOffPolicy，它还有几个比较重要的参数。 RetryListener ：可以传入一个listener数组，主要功能是用于监控重试行为。 RetryCallback ：重试回调，用户包装业务流，第一次执行和产生重试执行都会调用这个callback代码。 RecoveryCallback ：当所有重试都失败后，回调该接口，提供给业务重试回复机制。 RetryState ：重试状态，对于一些有事务的方法，如果出现某些异常，可能需要回滚而不是进行重试，这个参数可以完成这一功能。 RetryContext ： 重试上下文，每次重试都会将其作为参数传入RetryCallback中使用。 然后我们大致来看下RetryTemplate的部分关键代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192protected &lt;T, E extends Throwable&gt; T doExecute(RetryCallback&lt;T, E&gt; retryCallback, RecoveryCallback&lt;T&gt; recoveryCallback, RetryState state) throws E, ExhaustedRetryException &#123; //拿到重试策略和退避策略 RetryPolicy retryPolicy = this.retryPolicy; BackOffPolicy backOffPolicy = this.backOffPolicy; //初始化重试上下文 RetryContext context = open(retryPolicy, state); RetrySynchronizationManager.register(context); Throwable lastException = null; boolean exhausted = false; try &#123; //执行切面方法，在执行业务之前可以通过listener进行监控 boolean running = doOpenInterceptors(retryCallback, context); if (!running) &#123; throw new TerminatedRetryException( "Retry terminated abnormally by interceptor before first attempt"); &#125; //确定退避上下文环境 BackOffContext backOffContext = null; Object resource = context.getAttribute("backOffContext"); if (resource instanceof BackOffContext) &#123; backOffContext = (BackOffContext) resource; &#125; if (backOffContext == null) &#123; backOffContext = backOffPolicy.start(context); if (backOffContext != null) &#123; context.setAttribute("backOffContext", backOffContext); &#125; &#125; //开始重试循环 //如果重试策略认为可以重试 while (canRetry(retryPolicy, context) &amp;&amp; !context.isExhaustedOnly()) &#123; try &#123; //清空上次的异常 lastException = null; //执行业务方法 return retryCallback.doWithRetry(context); &#125; catch (Throwable e) &#123; //出现异常，最新异常就是此次异常 lastException = e; try &#123; //将异常信息通知到retryPolicy、state和context registerThrowable(retryPolicy, state, context, e); &#125; catch (Exception ex) &#123; throw new TerminatedRetryException("Could not register throwable", ex); &#125; finally &#123; //对于重试出现的异常，我们使用切面listener进行监听 doOnErrorInterceptors(retryCallback, context, e); &#125; //如果重试策略认为还可以重试 if (canRetry(retryPolicy, context) &amp;&amp; !context.isExhaustedOnly()) &#123; try &#123; //执行退避策略 backOffPolicy.backOff(backOffContext); &#125; catch (BackOffInterruptedException ex) &#123; lastException = e; throw ex; &#125; &#125; //确认下是否需要重新抛出（对于有事务的逻辑，重新抛出指定异常方便事务回滚） if (shouldRethrow(retryPolicy, context, state)) &#123; throw RetryTemplate.&lt;E&gt;wrapIfNecessary(e); &#125; &#125; if (state != null &amp;&amp; context.hasAttribute(GLOBAL_STATE)) &#123; break; &#125; &#125; //重试完成后，执行recoveryCallback操作 exhausted = true; return handleRetryExhausted(recoveryCallback, context, state); &#125; catch (Throwable e) &#123; throw RetryTemplate.&lt;E&gt;wrapIfNecessary(e); &#125; finally &#123; //关闭 close(retryPolicy, context, state, lastException == null || exhausted); //使用切面listener进行监控关闭等流程 doCloseInterceptors(retryCallback, context, lastException); RetrySynchronizationManager.clear(); &#125; &#125; 上述代码中用到的一些方法如下： 1234567891011121314151617181920212223242526272829303132 //该异常是否抛出protected boolean shouldRethrow(RetryPolicy retryPolicy, RetryContext context, RetryState state) &#123; return state != null &amp;&amp; state.rollbackFor(context.getLastThrowable());&#125; //监听open操作private &lt;T, E extends Throwable&gt; boolean doOpenInterceptors( RetryCallback&lt;T, E&gt; callback, RetryContext context) &#123; boolean result = true; for (RetryListener listener : this.listeners) &#123; result = result &amp;&amp; listener.open(context, callback); &#125; return result;&#125; //监听close操作private &lt;T, E extends Throwable&gt; void doCloseInterceptors( RetryCallback&lt;T, E&gt; callback, RetryContext context, Throwable lastException) &#123; for (int i = this.listeners.length; i-- &gt; 0;) &#123; this.listeners[i].close(context, callback, lastException); &#125;&#125; //监听error操作private &lt;T, E extends Throwable&gt; void doOnErrorInterceptors( RetryCallback&lt;T, E&gt; callback, RetryContext context, Throwable throwable) &#123; for (int i = this.listeners.length; i-- &gt; 0;) &#123; this.listeners[i].onError(context, callback, throwable); &#125;&#125; 根据上面的描述，RetryTemplate的执行流程大致如下： 其它我们把开始提到的例子复杂化下。引入Listener和RetryState参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Test &#123; public static void main(String[] args) &#123; RetryTemplate template = new RetryTemplate(); //重试策略：次数重试策略 SimpleRetryPolicy retryPolicy = new SimpleRetryPolicy(); retryPolicy.setMaxAttempts(3); template.setRetryPolicy(retryPolicy); //退避策略：固定退避策略 FixedBackOffPolicy backOffPolicy = new FixedBackOffPolicy(); backOffPolicy.setBackOffPeriod(1000L); template.setBackOffPolicy(backOffPolicy); //设置有状态重试 BinaryExceptionClassifier classifier = new BinaryExceptionClassifier( Collections.singleton(NullPointerException.class) ); RetryState state = new DefaultRetryState("rollbackKey", false, classifier); //设置监听 DefaultStatisticsRepository defaultStatisticsRepository =new DefaultStatisticsRepository(); template.setListeners(new RetryListener[]&#123;new StatisticsListener(defaultStatisticsRepository)&#125;); String str = "1"; //当重试失败后，执行RecoveryCallback String result2 = template.execute((retryCallback)-&gt;&#123; System.out.println("retry count:" + retryCallback.getRetryCount()); retryCallback.setAttribute(RetryContext.NAME,"method.key"); return getStr(str); &#125;, (recoveryCallback)-&gt; &#123; System.out.println("所有重试均失败!!"); return "失败"; &#125;,state); RetryStatistics statistics = defaultStatisticsRepository.findOne("method.key"); System.out.println(statistics); &#125; public static String getStr(String str)&#123; if(StringUtils.isBlank(str))&#123; throw new NullPointerException("数据为空！"); &#125; if("1".equals(str))&#123; throw new RuntimeException("参数为1！！！"); &#125; return str; &#125;&#125; 我们对str赋值1和””，可以清楚的看到输出的日志。当赋值””时，执行一次后直接抛出空指针异常，不会再进行重试。如果调用的方法有事务，可以进行回滚等操作，这就是有状态的重试。 当str=”1”时，可以看到监听分析的结果： 1DefaultRetryStatistics [name=method.key, startedCount=0, completeCount=0, recoveryCount=1, errorCount=3, abortCount=0] 重试注解Spring Retry也支持使用注解的形式标注。如下： EnableRetry12345678@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@EnableAspectJAutoProxy(proxyTargetClass = false)@Import(RetryConfiguration.class)@Documentedpublic @interface EnableRetry &#123; boolean proxyTargetClass() default false;&#125; proxyTargetClass指是否使用CGLIB增强代理，默认false。 这个注解作用在类上，如果想要某个方法可以进行重试，则这个方法所在的类需要有EnableRetry注解。 Retryable内容如下： 123456789101112131415@Target(&#123; ElementType.METHOD, ElementType.TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Retryable &#123; String interceptor() default ""; Class&lt;? extends Throwable&gt;[] value() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] include() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] exclude() default &#123;&#125;; String label() default ""; boolean stateful() default false; int maxAttempts() default 3; String maxAttemptsExpression() default ""; Backoff backoff() default @Backoff(); String exceptionExpression() default "";&#125; 该注解作用在方法上，指定的方法会进行重试操作。 参数说明： interceptor：拦截器 value：可以重试的异常类型，如果为空并且exclude为空，则会重试所有异常，与include同义。 include：与value同义。 exclude：不需要重试的异常。 label：分析报告的名称，listener相关使用。 stateful：是否有状态重试，有的话指定的异常要抛出而不是重试。 maxAttempts：最大重试次数。 maxAttemptsExpression：最大重试次数表达式。 backoff：退避策略，详见BackOff注解。 exceptionExpression：异常表达式，要抛出的异常（有状态情况下）的表达式。 Backoff1234567891011121314@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Import(RetryConfiguration.class)@Documentedpublic @interface Backoff &#123; long value() default 1000; long delay() default 0; long maxDelay() default 0; double multiplier() default 0; String delayExpression() default ""; String maxDelayExpression() default ""; String multiplierExpression() default ""; boolean random() default false;&#125; 退避策略注解，使用方式见上面Retryable的backoff值。 主要参数说明： value：退避间隔，和delay同义。 delay：与value同义。在随机退避策略里表示最小值，在指数退避策略和随机指数退避策略里表示起始值。 maxDelay：在随机退避策略里表示最大值，在指数退避策略和随机指数退避策略里表示最大值。 multiplier：指数因子。 delayExpression：退避间隔表达式。 maxDelayExpression：最大值表达式。 multiplierExpression：指数因子表达式。 random：是否随机。 可以看到，如果什么也不设置，将使用NoBackOffPolicy。如果只设置value或者delay值，将使用FixedBackOffPolicy。如果还设置了maxDelay和random，将使用UniformRandomBackOffPolicy…… CircuitBreaker熔断注解。 12345678910111213@Target(&#123; ElementType.METHOD, ElementType.TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Retryable(stateful = true)public @interface CircuitBreaker &#123; Class&lt;? extends Throwable&gt;[] value() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] include() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] exclude() default &#123;&#125;; int maxAttempts() default 3; String label() default ""; long resetTimeout() default 20000; long openTimeout() default 5000;&#125; 主要参数与上面说的Retryable基本说明一样，它的其它两个参数resetTimeout和openTimeout上面已经讲过。 Recover123456@Target(&#123; ElementType.METHOD, ElementType.TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Import(RetryConfiguration.class)@Documentedpublic @interface Recover &#123;&#125; 这个注解也作用于方法上，表示所有重试失败后兜底的返回信息，这个作用的方法，应该有以下特性： 第一个参数是重试的程序抛出的异常（需要重试的异常）。 后面的参数应该与Retryable注释的入参一致，返回值也应一致。 第一个参数可选，但是如果不写，需要保证Retryable在没有其他的Recover匹配的情况下才会被调用。 我们使用注解来简单写个例子，如下： 123456789101112131415161718192021@Service@Slf4j@EnableRetrypublic class RetryTest &#123; @Transactional @Retryable(value = Exception.class,maxAttempts = 5,backoff = @Backoff(2000L)) public String retryTest(String str1,Integer integer1)&#123; if(StringUtils.isBlank(str1))&#123; throw new NullPointerException("str1参数为空！！"); &#125; if(integer1==null)&#123; throw new RuntimeException("integer1参数不正确！！！"); &#125; return str1+integer1; &#125; @Recover public String recover(Exception e,String str1,Integer integer1)&#123; log.info("所有重试均失败，返回兜底值",e); return ""; &#125;&#125; 可以看到我们创建了一个重试方法，这个方法最多重试5次，每重试一次之前都会退避2s后再进行，重试所有异常，当所有重试均不成功后会返回兜底值””。 总结通过对Spring Retry框架的理解，我们对重试框架有了一个更全面的认识，了解了它的一些简单实现原理，明白了它的一些关键参数。如果有方法有重试需求，可以适当进行Spring Retry框架的考虑。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringRetry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java将文件上传到云服务器]]></title>
    <url>%2Fblog%2FJava%E5%B0%86%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%88%B0%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8.html</url>
    <content type="text"><![CDATA[前言今天总结了一个云上传的小demo特地来跟大家分享下。 这个demo可以将一些文件上传到云服务器。我们来看下吧。 正文我这里使用了阿里云、亚马逊S3和微软Azure这三种云上传做的demo。 要使用云上传，我们需要引入相关jar包，如下： 123456789101112131415161718192021222324&lt;!--阿里云OSS--&gt;&lt;dependency&gt; &lt;groupId&gt;com.aliyun.oss&lt;/groupId&gt; &lt;artifactId&gt;aliyun-sdk-oss&lt;/artifactId&gt; &lt;version&gt;2.2.3&lt;/version&gt;&lt;/dependency&gt;&lt;!--微软Azure--&gt;&lt;dependency&gt; &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt; &lt;artifactId&gt;azure-storage&lt;/artifactId&gt; &lt;version&gt;7.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--亚马逊S3--&gt;&lt;dependency&gt; &lt;groupId&gt;com.amazonaws&lt;/groupId&gt; &lt;artifactId&gt;aws-java-sdk&lt;/artifactId&gt; &lt;version&gt;1.8.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 腾讯云cos --&gt;&lt;dependency&gt; &lt;groupId&gt;com.qcloud&lt;/groupId&gt; &lt;artifactId&gt;cos_api&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt; PS:如果只使用一种云上传方式，引入对应的jar包即可，不必全部引入。 我们可以提供一个通用的上传接口upload，而具体的上传逻辑让各个实现类去实现。 同时我们暴露公共方法出来供上传使用。 上传的文件有可能是本地文件，也有可能是前端传过来的Base64图片字符串，也有可能是MultipartFile等。 我们提供一个抽象的上传方法，大致如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144public abstract class UploadAbstractUtil &#123; public static final Logger logger = LoggerFactory.getLogger(UploadAbstractUtil.class); /** * 文件缓存路径 */ private String basedir; public UploadAbstractUtil(String basedir) &#123; this.basedir = basedir; &#125; /** * 上传文件到云 * @param tempFile * @param realName * @return */ protected abstract String upload(File tempFile, String realName); /** * 上传文件到云 * @param file * @return */ abstract String upload(MultipartFile file); /** * 上传文件到云 * @param bytes * @param contentType * @return */ protected abstract String upload(byte[] bytes,String contentType); /** * 尝试初始化客户端 */ protected abstract void initClient(); /** * 生成一个唯一的上传文件名 * @param file * @return */ protected String generateUploadFileName(MultipartFile file)&#123; String name = file.getOriginalFilename(); String ext = name.substring(name.lastIndexOf(".")); String uuid = UUID.randomUUID().toString(); // 生成唯一的key return uuid + ext; &#125; /** * base64转为文件后在进行上传 * @param base64Str * @return */ public String base64UploadUseTempFile(String base64Str)&#123; String realName = UUID.randomUUID().toString() + ".jpg"; File file = new File(basedir + "/" + realName); if (!file.exists()) &#123; try &#123; file.createNewFile(); &#125;catch (IOException e)&#123; logger.error("文件上传，尝试创建文件时失败！！！",e); throw new RuntimeException("文件上传失败！！！"); &#125; &#125; boolean flag = Base64Utils.Base64ToImage(base64Str, file.getPath()); if(!flag)&#123; throw new RuntimeException("base64转换为文件时发生错误！！！"); &#125; String url; try &#123; long startTime = System.currentTimeMillis(); url = upload(file, realName); logger.info("tempFile---上传服务耗时------time:[&#123;&#125;ms]", System.currentTimeMillis()-startTime); &#125; catch (Exception e) &#123; logger.error("文件上传，上传文件时发生异常！！！",e); throw new RuntimeException("文件上传失败!"); &#125; if (file.exists()) &#123; file.delete(); &#125; return url; &#125; /** * 通用文件上传 * @param filePath 文件路径 * @return */ public String fileUpload(String filePath)&#123; if(StringUtils.isBlank(filePath))&#123; throw new RuntimeException("请输入正确的文件路径！"); &#125; File file = new File(filePath); if(!file.exists())&#123; throw new RuntimeException("请输入正确的文件路径！"); &#125; int position = filePath.lastIndexOf("."); String fileSuffix = ""; if(position &gt; 0)&#123; fileSuffix = filePath.substring(position); &#125; //上传到云上的文件名 String realName = UUID.randomUUID().toString() + fileSuffix; String url; try &#123; long startTime = System.currentTimeMillis(); url = upload(file, realName); logger.info("filePath---上传服务耗时------time:[&#123;&#125;ms]", System.currentTimeMillis()-startTime); &#125; catch (Exception e) &#123; logger.error("文件上传，上传文件时发生异常！！！",e); throw new RuntimeException("文件上传失败!"); &#125; return url; &#125; /** * 使用流来进行文件上传 * @param base64Str * @return */ public String base64UploadUseInputStream(String base64Str)&#123; byte[] bytes = Base64Utils.Base64ToByte(base64Str); String url; try &#123; long startTime = System.currentTimeMillis(); url = upload(bytes,"image/jpeg"); logger.info("Stream---上传服务耗时------time:[&#123;&#125;ms]", System.currentTimeMillis()-startTime); &#125; catch (Exception e) &#123; logger.error("文件上传，上传文件时发生异常！！！",e); throw new RuntimeException("文件上传失败!"); &#125; return url; &#125;&#125; PS： 这个类看着比较多……其实都是对文件进行处理，生成上传文件名，然后交给上传方法，开始写的时候代码较少，在学习优化的过程中不断添加新功能，导致了该结果。 上面的上传抽象类大致逻辑如下： 针对图片文件： 如果在服务器上的，可以直接获取到文件后进行上传。（fileUpload方法） 如果APP端传过来的Base64编码的图片文件，可以把它生成临时文件，然后进行上传，也可以直接把Base64转换为流后进行上传。（base64UploadUseTempFile方法和base64UploadUseInputStream方法） 如果APP端传过来MultipartFile文件，直接将其进行转换并上传。（upload(MultipartFile file)方法） 对于每种云上传，各个实现类具体如下： 阿里云OSS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133public class AliOssUploadUtil extends UploadAbstractUtil&#123; public static final Logger logger = LoggerFactory.getLogger(AliOssUploadUtil.class); /** * 阿里云accessKey */ private String aliyunaccessKey; /** * 阿里云secretKey */ private String aliyunsecretKey; /** * 阿里云endpoint */ private String aliyunendpoint; /** * 阿里云endpointexternal */ private String aliyunendpointexternal; /** * 阿里云bucket */ private String aliyunbucket; /** * ossClient */ private OSSClient ossClient; /** * 构造器 * @param basedir * @param aliyunaccessKey * @param aliyunsecretKey * @param aliyunendpoint * @param aliyunendpointexternal * @param aliyunbucket */ public AliOssUploadUtil(String basedir, String aliyunaccessKey, String aliyunsecretKey, String aliyunendpoint, String aliyunendpointexternal, String aliyunbucket) &#123; super(basedir); this.aliyunaccessKey = aliyunaccessKey; this.aliyunsecretKey = aliyunsecretKey; this.aliyunendpoint = aliyunendpoint; this.aliyunendpointexternal = aliyunendpointexternal; this.aliyunbucket = aliyunbucket; &#125; /** * 阿里云文件上传 * @param tempFile * @param realName * @return */ @Override protected String upload(File tempFile,String realName)&#123; initClient(); ossClient.putObject(aliyunbucket, realName, tempFile); URL url = ossClient.generatePresignedUrl(aliyunbucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("阿里云OSS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(aliyunendpoint, aliyunendpointexternal); logger.info("阿里云OSS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125; /** * 阿里云文件上传 * @param file * @return */ @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try (InputStream is = new ByteArrayInputStream(file.getBytes())) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(file.getSize()); metadata.setContentType(file.getContentType()); // 上传 ossClient.putObject(aliyunbucket,key, is, metadata); URL url = ossClient.generatePresignedUrl(aliyunbucket, key, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("阿里云OSS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(aliyunendpoint, aliyunendpointexternal); logger.info("阿里云OSS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用阿里云OSS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 尝试初始化client */ @Override protected void initClient() &#123; if(ossClient == null)&#123; ossClient = new OSSClient(aliyunendpoint, aliyunaccessKey, aliyunsecretKey); &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try (InputStream is = new ByteArrayInputStream(bytes)) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(is.available()); metadata.setContentType(contentType); // 上传 ossClient.putObject(aliyunbucket,realName, is, metadata); URL url = ossClient.generatePresignedUrl(aliyunbucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("阿里云OSS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(aliyunendpoint, aliyunendpointexternal); logger.info("阿里云OSS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用阿里云OSS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * shutdown Client */ public void shutdown()&#123; ossClient.shutdown(); &#125;&#125; 亚马逊S3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class AmazonS3UploadUtil extends UploadAbstractUtil&#123; /** * S3 accessKey */ private String s3accessKey; /** * S3 secretKey */ private String s3secretKey; /** * S3 endpoint */ private String s3endpoint; /** * S3 bucket */ private String s3bucket; /** * 协议 */ private static Protocol protocol = Protocol.HTTP; /** * 亚马逊s3 client */ private AmazonS3 client; /** * 构造器 * @param basedir * @param s3accessKey * @param s3secretKey * @param s3endpoint * @param s3bucket */ public AmazonS3UploadUtil(String basedir, String s3accessKey, String s3secretKey, String s3endpoint, String s3bucket) &#123; super(basedir); this.s3accessKey = s3accessKey; this.s3secretKey = s3secretKey; this.s3endpoint = s3endpoint; this.s3bucket = s3bucket; &#125; /** * 上传文件到 Amazon S3 * @param tempFile * @param realName * @return */ @Override protected String upload(File tempFile, String realName)&#123; try &#123; initClient(); client.setEndpoint(s3endpoint); client.setS3ClientOptions(new S3ClientOptions().withPathStyleAccess(true)); client.putObject(new PutObjectRequest(s3bucket, realName, tempFile) .withCannedAcl(CannedAccessControlList.AuthenticatedRead)); String imageUrl = "http://" + s3endpoint + "/" + s3bucket + "/" + realName; logger.info("亚马逊S3上传服务------图片url:[&#123;&#125;]", imageUrl); return imageUrl; &#125; catch (AmazonClientException e) &#123; logger.error("使用亚马逊S3上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 上传文件到S3 * @param file * @return */ @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try (InputStream is = new ByteArrayInputStream(file.getBytes()))&#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(file.getSize()); metadata.setContentType(file.getContentType()); PutObjectRequest mall = new PutObjectRequest(s3bucket, key, is, metadata) .withCannedAcl(CannedAccessControlList.AuthenticatedRead); // 上传 client.putObject(mall); return "http://" + s3endpoint + "/" + s3bucket + "/" + key; &#125;catch (IOException e)&#123; logger.error("使用亚马逊S3上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 尝试初始化S3Client */ @Override protected void initClient() &#123; if(client == null)&#123; AWSCredentials credential = new BasicAWSCredentials(s3accessKey, s3secretKey); ClientConfiguration clientConfig = new ClientConfiguration(); clientConfig.setProtocol(protocol); client = new AmazonS3Client(credential, clientConfig); &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try (InputStream is = new ByteArrayInputStream(bytes))&#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(is.available()); metadata.setContentType(contentType); PutObjectRequest mall = new PutObjectRequest(s3bucket, realName, is, metadata) .withCannedAcl(CannedAccessControlList.AuthenticatedRead); // 上传 client.putObject(mall); return "http://" + s3endpoint + "/" + s3bucket + "/" + realName; &#125;catch (IOException e)&#123; logger.error("使用亚马逊S3上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125;&#125; 微软Azure123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125public class AzureUploadUtil extends UploadAbstractUtil&#123; /** * 用户名 */ private String accountName; /** * 密码 */ private String accountKey; /** * endPoint */ private String endPoint; /** * containerName */ private String containerName; /** * 连接串 */ private String storageConnectionString; /** * Azure client */ private CloudBlobClient blobClient; /** * 构造器 * @param accountName * @param accountKey * @param endPoint * @param containerName */ public AzureUploadUtil(String basedir,String accountName, String accountKey, String endPoint, String containerName) &#123; super(basedir); this.accountName = accountName; this.accountKey = accountKey; this.endPoint = endPoint; this.containerName = containerName; this.storageConnectionString = "DefaultEndpointsProtocol=https;AccountName="+ accountName +";AccountKey="+ accountKey +";EndpointSuffix=" + endPoint; &#125; /** * 上传文件到Azure * @param * @return */ @Override protected String upload(File tempFile,String realName) &#123; try &#123; initClient(); CloudBlobContainer container = blobClient.getContainerReference(containerName); // Create the container if it does not exist with public access. container.createIfNotExists(BlobContainerPublicAccessType.CONTAINER, new BlobRequestOptions(), new OperationContext()); //Getting a blob reference CloudBlockBlob blob = container.getBlockBlobReference(tempFile.getName()); //Creating blob and uploading file to it blob.uploadFromFile(tempFile.getAbsolutePath()); String imageUrl = "https://"+ accountName +".blob."+ endPoint +"/"+ containerName +"/" + tempFile.getName(); logger.info("微软Azure上传服务------图片url:[&#123;&#125;]", imageUrl); return imageUrl; &#125; catch (IOException| StorageException | URISyntaxException e) &#123; logger.error("使用微软Azure上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 上传文件到Azure * @param file * @return */ @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try(InputStream is = new ByteArrayInputStream(file.getBytes()))&#123; CloudBlobContainer container = blobClient.getContainerReference(containerName); // Create the container if it does not exist with public access. container.createIfNotExists(BlobContainerPublicAccessType.CONTAINER, new BlobRequestOptions(), new OperationContext()); CloudBlockBlob blob = container.getBlockBlobReference(key); //Creating blob and uploading file to it blob.upload(is, is.available()); return "https://"+ accountName +".blob."+ endPoint +"/"+ containerName +"/" + key; &#125;catch (IOException| StorageException | URISyntaxException e)&#123; logger.error("使用微软Azure上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * 尝试初始化client */ @Override protected void initClient()&#123; if(blobClient == null)&#123; try&#123; CloudStorageAccount storageAccount = CloudStorageAccount.parse(storageConnectionString); blobClient = storageAccount.createCloudBlobClient(); &#125;catch (URISyntaxException|InvalidKeyException e)&#123; logger.error("使用微软Azure初始化client失败！",e); throw new RuntimeException(e); &#125; &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try(InputStream is = new ByteArrayInputStream(bytes))&#123; CloudBlobContainer container = blobClient.getContainerReference(containerName); // Create the container if it does not exist with public access. container.createIfNotExists(BlobContainerPublicAccessType.CONTAINER, new BlobRequestOptions(), new OperationContext()); CloudBlockBlob blob = container.getBlockBlobReference(realName); //Creating blob and uploading file to it blob.upload(is, is.available()); return "https://"+ accountName +".blob."+ endPoint +"/"+ containerName +"/" + realName; &#125;catch (IOException| StorageException | URISyntaxException e)&#123; logger.error("使用微软Azure上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125;&#125; 腾讯云COS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class TencentCOSUploadUtil extends UploadAbstractUtil &#123; public static final Logger logger = LoggerFactory.getLogger(TencentCOSUploadUtil.class); /** * 腾讯云COS AccessKey */ private String qAccessKey; /** * 腾讯云COS SecretKey */ private String qSecretKey; /** * 腾讯云COS bucket */ private String qBucket; /** * 腾讯云COS region */ private String qRegion; /** * 腾讯云COS qEndpoint */ private String qEndpoint; /** * 腾讯云COS qEndpointExternal */ private String qEndpointExternal; /** * COSClient */ private COSClient cosClient; public TencentCOSUploadUtil(String basedir, String qAccessKey, String qSecretKey, String qBucket, String qRegion, String qEndpoint, String qEndpointExternal) &#123; super(basedir); this.qAccessKey = qAccessKey; this.qSecretKey = qSecretKey; this.qBucket = qBucket; this.qRegion = qRegion; this.qEndpoint = qEndpoint; this.qEndpointExternal = qEndpointExternal; &#125; @Override protected String upload(File tempFile, String realName) &#123; initClient(); cosClient.putObject(qBucket,realName,tempFile); URL url =cosClient.generatePresignedUrl(qBucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("腾讯云COS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(qEndpoint, qEndpointExternal); logger.info("腾讯云COS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125; @Override String upload(MultipartFile file) &#123; initClient(); String key = generateUploadFileName(file); try (InputStream is = new ByteArrayInputStream(file.getBytes())) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(file.getSize()); metadata.setContentType(file.getContentType()); // 上传 cosClient.putObject(qBucket,key, is, metadata); URL url = cosClient.generatePresignedUrl(qBucket, key, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("腾讯云COS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(qEndpoint, qEndpointExternal); logger.info("腾讯云COS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用腾讯云COS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; @Override protected String upload(byte[] bytes, String contentType) &#123; initClient(); String realName = UUID.randomUUID().toString() + ".jpg"; try (InputStream is = new ByteArrayInputStream(bytes)) &#123; ObjectMetadata metadata = new ObjectMetadata(); metadata.setContentEncoding(StandardCharsets.UTF_8.name()); metadata.setContentLength(is.available()); metadata.setContentType(contentType); // 上传 cosClient.putObject(qBucket,realName, is, metadata); URL url = cosClient.generatePresignedUrl(qBucket, realName, new Date(System.currentTimeMillis() + 3600L * 1000 * 24 * 365 * 100)); String urlString = String.valueOf(url).split("\\?")[0]; logger.info("腾讯云COS上传服务------图片内网url:[&#123;&#125;]", urlString); urlString = urlString.replaceAll(qEndpoint, qEndpointExternal); logger.info("腾讯云COS上传服务------图片外网url:[&#123;&#125;]", urlString); return urlString; &#125;catch (IOException e)&#123; logger.error("使用腾讯云COS上传文件出现异常",e); throw new RuntimeException(e); &#125; &#125; /** * @Description: 初始化COSCilent * @Date: 2019/6/2 9:42 AM * @Params: [] * @Return: void */ @Override protected void initClient() &#123; if(cosClient==null)&#123; COSCredentials cosCredentials = new BasicCOSCredentials(qAccessKey,qSecretKey); ClientConfig clientConfig = new ClientConfig(new Region(qRegion)); cosClient = new COSClient(cosCredentials,clientConfig); &#125; &#125; /** * shutdown Client */ public void shutdown()&#123; cosClient.shutdown(); &#125;&#125; 用到的相关工具类如下： 1234567891011121314151617181920212223242526272829303132333435/** * base64字符串转换成图片 * 对字节数组字符串进行Base64解码并生成图片 * @param imgStr base64字符串 * @param imgFilePath 图片存放路径 * @return **/public static boolean Base64ToImage(String imgStr,String imgFilePath) &#123; // 图像数据为空 if (StringUtils.isEmpty(imgStr)) &#123; return false; &#125; //如果包含 data:image/jpeg;base64, 前缀需要去掉 if(imgStr.contains(","))&#123; imgStr = imgStr.split(",")[1]; &#125; BASE64Decoder decoder = new BASE64Decoder(); try &#123; // Base64解码 byte[] b = decoder.decodeBuffer(imgStr); for (int i = 0; i &lt; b.length; ++i) &#123; // 调整异常数据 if (b[i] &lt; 0) &#123; b[i] += 256; &#125; &#125; OutputStream out = new FileOutputStream(imgFilePath); out.write(b); out.flush(); out.close(); return true; &#125; catch (Exception e) &#123; return false; &#125;&#125; 结合SpringBoot，引入自动配置，生成相关上传Bean，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495@Configurationpublic class OssUtilsConfiguration &#123; @Value("$&#123;file.cache.dir&#125;") private String basedir; @Value("$&#123;file.upload.server.type&#125;") private String uploadServerType; @Autowired private Environment environment; /** * 亚马逊S3配置 */ private String s3accessKey; private String s3secretKey; private String s3endpoint; private String s3bucket; /** * 阿里云OSS配置 */ private String aliyunAccessKey; private String aliyunSecretKey; private String aliyunBucket; private String aliyunEndpoint; private String aliyunEndpointexternal; /** * 微软Azure配置 */ private String azureAccountName; private String azureAccountKey; private String azureEndpointSuffix; private String azureContainerName; /** * 腾讯云COS配置 */ private String qAccessKey; private String qSecretKey; private String qBucket; private String qRegion; private String qEndpoint; private String qEndpointExternal; /** * 根据配置的 file.upload.server.type 选择一个上传服务器 * @return */ @Bean public UploadAbstractUtil uploadAbstractUtil()&#123; //可以根据枚举进行配置 使用阿里云或者亚马逊S3或者Azure UploadServerEnum uploadServerEnum = UploadServerEnum.getEnum(uploadServerType); UploadAbstractUtil uploadAbstractUtil; switch (uploadServerEnum)&#123; //亚马逊s3 case AMAZON: s3accessKey = environment.getRequiredProperty("s3.accessKey"); s3secretKey = environment.getRequiredProperty("s3.secretKey"); s3endpoint = environment.getRequiredProperty("s3.endpoint"); s3bucket = environment.getRequiredProperty("s3.bucket"); uploadAbstractUtil = new AmazonS3UploadUtil(basedir,s3accessKey,s3secretKey,s3endpoint,s3bucket); return uploadAbstractUtil; //阿里云OSS case ALIOSS: aliyunAccessKey = environment.getRequiredProperty("aliyun.accessKey"); aliyunSecretKey = environment.getRequiredProperty("aliyun.secretKey"); aliyunBucket = environment.getRequiredProperty("aliyun.bucket"); aliyunEndpoint = environment.getRequiredProperty("aliyun.endpoint"); aliyunEndpointexternal = environment.getRequiredProperty("aliyun.endpointexternal"); uploadAbstractUtil = new AliOssUploadUtil(basedir,aliyunAccessKey,aliyunSecretKey,aliyunEndpoint,aliyunEndpointexternal,aliyunBucket); return uploadAbstractUtil; //微软Azure case AZURE: azureAccountName = environment.getRequiredProperty("azure.accountName"); azureAccountKey = environment.getRequiredProperty("azure.accountKey"); azureEndpointSuffix = environment.getRequiredProperty("azure.endpointSuffix"); azureContainerName = environment.getRequiredProperty("azure.containerName"); uploadAbstractUtil = new AzureUploadUtil(basedir,azureAccountName,azureAccountKey,azureEndpointSuffix,azureContainerName); return uploadAbstractUtil; case TENCENTCOS: qAccessKey = environment.getRequiredProperty("tencent.accessKey"); qSecretKey = environment.getRequiredProperty("tencent.secretKey"); qBucket = environment.getRequiredProperty("tencent.bucket"); qEndpoint = environment.getRequiredProperty("tencent.endpoint"); qRegion = environment.getRequiredProperty("tencent.region"); qEndpointExternal = environment.getRequiredProperty("tencent.endpointexternal"); uploadAbstractUtil = new TencentCOSUploadUtil(basedir,qAccessKey,qSecretKey,qBucket,qRegion,qEndpoint,qEndpointExternal); return uploadAbstractUtil; default: throw new RuntimeException("暂不支持其他类型的云上传！！！"); &#125; &#125;&#125; UploadServerEnum 枚举如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public enum UploadServerEnum &#123; /** * 阿里云OSS */ ALIOSS("aliyun_oss","阿里云OSS"), /** * 亚马逊s3 */ AMAZON("amazon_s3","亚马逊S3"), /** * 微软azure */ AZURE("azure","微软Azure"), /** * 腾讯云cos */ TENCENTCOS("tencent_cos","腾讯云cos"); private String value; private String desc; UploadServerEnum(String value, String desc) &#123; this.value = value; this.desc = desc; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public String getDesc() &#123; return desc; &#125; public void setDesc(String desc) &#123; this.desc = desc; &#125; /** * 获取一个枚举 * @param value * @return */ public static UploadServerEnum getEnum(String value)&#123; return Arrays.stream(UploadServerEnum.values()).filter(e-&gt;e.value.equals(value)).findFirst().get(); &#125;&#125; 我们看到代码比较多…… 其实几种云上传的核心只要理解，便非常清楚了。它们的大致步骤如下： 根据配置信息创建上传client 上传文件（有多种方式，直接上传文件或根据文件流来上传等） 上传结果，获取上传文件路径等等。 如需关闭client，需要关闭client。 总结通过学习如何进行文件云上传，我们掌握了云上传的方法，也可以体验到一些封装、继承、多态的好处，总的来说是蛮不错的一次体验。 项目地址： ossutils-spring-boot]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>云上传</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java序列化和反序列化的几种方式]]></title>
    <url>%2Fblog%2FJava%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言在Java中，我们知道可以随意创建对象，只要对象未被GC回收，我们都可以继续在程序里使用，但这些对象只是存在于JVM内存中的，我们JVM一旦停止，这些对象就消失不见了。 经常有些时候，我们需要把这些对象持久化下来，再次需要时，再重新把对象读取出来，Java中有一种机制，对象序列化机制（object serialization）便可以帮我们完成相关功能。 对象序列化，可以方便的把对象状态保存为字节数组，可以通过字节流进行远程网络传输等，接收到字节流，通过反序列化机制，可以将字节数组转换为相关对象。 常说的RPC远程调用，相关传输对象的生成类就必须实现序列化以便在网络间传输。 正文Serializable接口在Java中，我们最常用的实现序列化和反序列化的方法就是相关类实现 java.io.Serializable 接口了，这也是Java给我们提供的一个方便的API。 我们创建一个Apple类，实现序列化接口，通过测试，可以看到相关对象生成的字节码和反序列化后的对象。 123456789101112131415161718192021222324252627public class Apple implements Serializable &#123; private String color; private int weight; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; public int getWeight() &#123; return weight; &#125; public void setWeight(int weight) &#123; this.weight = weight; &#125; public Apple(String color, int weight) &#123; this.color = color; this.weight = weight; &#125; @Override public String toString() &#123; return "Apple&#123;" + "color='" + color + '\'' + ", weight=" + weight + '&#125;'; &#125;&#125; 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) throws IOException,ClassNotFoundException&#123; byte [] bytes = null; Apple apple = new Apple("red",150); //序列化 try(ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(baos) )&#123; oos.writeObject(apple); bytes = baos.toByteArray(); for (byte b : baos.toByteArray()) &#123; System.out.print(Byte.toString(b) + " "); &#125; &#125; System.out.println(); //反序列化 try ( ByteArrayInputStream bais = new ByteArrayInputStream(bytes); ObjectInputStream ois = new ObjectInputStream(bais) ) &#123; System.out.println(ois.readObject().toString()); &#125; &#125;&#125; 测试输出结果： 当我们去掉Apple类的Serializable接口后，执行测试会抛出异常，说明对象无法被序列化。 序列化ID（serialVersionUID）JVM虚拟机是否可以对某个对象进行反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的点是两个类的序列化ID是否一致（就是 private static final long serialVersionUID)。 序列化ID有两种生成策略，一个是固定的 1L，一个是随机生成一个不重复的 long 类型数据（实际上是使用 JDK 工具生成），在这里有一个建议，如果没有特殊需求，就是用默认的 1L 就可以，这样可以确保代码一致时反序列化成功。那么随机生成的序列化ID有什么作用呢，有些时候，通过改变序列化 ID 可以用来限制某些用户的使用。 序列化的实现方式在Java中，我们还可以利用其它方式对对象进行序列化，我总结了几种序列化方式如下。 让我们一起来看一下： 我们提供一个序列化与反序列化通用接口 123456789101112131415161718192021public interface Serializer &#123; /** * 序列化名称 * @return */ String name(); /** * 序列化 * @param obj * @return * @throws IOException */ byte[] serialize(Object obj) throws IOException ; /** * 反序列化 * @param bytes * @return * @throws IOException */ Object deserialize(byte[] bytes) throws IOException ;&#125; 标准的Java序列化123456789101112131415161718192021222324252627282930public class JavaSerializer implements Serializer &#123; @Override public String name() &#123; return "java"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; try( ByteArrayOutputStream baos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(baos); )&#123; oos.writeObject(obj); return baos.toByteArray(); &#125; &#125; @Override public Object deserialize(byte[] bits) throws IOException&#123; if(bits == null || bits.length == 0) &#123; return null; &#125; try ( ByteArrayInputStream bais = new ByteArrayInputStream(bits); ObjectInputStream ois = new ObjectInputStream(bais); )&#123; return ois.readObject(); &#125;catch (ClassNotFoundException e)&#123; throw new RuntimeException(e); &#125; &#125;&#125; 可以看到序列化就是我们将对象通过ObjectOutputStream转化为ByteArrayOutputStream字节流，反序列化就是将字节流转换为对象流并读取。 FST实现序列化需要引入相关jar包 12345&lt;dependency&gt; &lt;groupId&gt;de.ruedigermoeller&lt;/groupId&gt; &lt;artifactId&gt;fst&lt;/artifactId&gt; &lt;version&gt;2.57&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930public class FSTSerializer implements Serializer &#123; @Override public String name() &#123; return "fst"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; try ( ByteArrayOutputStream out = new ByteArrayOutputStream(); FSTObjectOutput fout = new FSTObjectOutput(out); )&#123; fout.writeObject(obj); fout.flush(); return out.toByteArray(); &#125; &#125; @Override public Object deserialize(byte[] bytes) throws IOException &#123; if(bytes == null || bytes.length == 0) &#123; return null; &#125; try ( FSTObjectInput in = new FSTObjectInput(new ByteArrayInputStream(bytes)); )&#123; return in.readObject(); &#125; catch (ClassNotFoundException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; Kryo实现序列化需要引入相关jar包 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;5.0.0-RC4&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223242526272829public class KryoSerializer implements Serializer &#123; private final static Kryo kryo = new Kryo(); @Override public String name() &#123; return "kryo"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; try (ByteArrayOutputStream baos = new ByteArrayOutputStream(); Output output = new Output(baos)) &#123; kryo.register(obj.getClass()); kryo.writeClassAndObject(output, obj); output.flush(); return baos.toByteArray(); &#125; &#125; @Override public Object deserialize(byte[] bits) throws IOException &#123; if (bits == null || bits.length == 0) &#123; return null; &#125; try (ByteArrayInputStream bais = new ByteArrayInputStream(bits); Input ois = new Input(bais) ) &#123; return kryo.readClassAndObject(ois); &#125; &#125;&#125; KryoPool实现序列化由于kryo创建的代价相对较高，我们可以使用一个KryoPool池来管理Kryo，使用空间换取时间，提高运行效率。 我们使用一个双端队列来对Kryo进行管理，相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155public class KryoPoolSerializer implements Serializer&#123; /** * Kryo 的包装 */ private static class KryoHolder &#123; private Kryo kryo; static final int BUFFER_SIZE = 1024; /** * reuse */ private Output output = new Output(BUFFER_SIZE, -1); private Input input = new Input(); KryoHolder(Kryo kryo) &#123; this.kryo = kryo; &#125; &#125; interface KryoPool &#123; /** * get o kryo object * @return KryoHolder instance */ KryoHolder get(); /** * return object * @param kryo holder */ void offer(KryoHolder kryo); &#125; /** * 由于kryo创建的代价相对较高 ，这里使用空间换时间 * 对KryoHolder对象进行重用 */ public static class KryoPoolImpl implements KryoPool &#123; /** * default is 1500 * online server limit 3K */ /** * thread safe list */ private final Deque&lt;KryoHolder&gt; kryoHolderDeque=new ConcurrentLinkedDeque&lt;KryoHolder&gt;(); private KryoPoolImpl() &#123; &#125; /** * @return KryoPool instance */ public static KryoPool getInstance() &#123; return Singleton.pool; &#125; /** * get o KryoHolder object * * @return KryoHolder instance */ @Override public KryoHolder get() &#123; // Retrieves and removes the head of the queue represented by this table KryoHolder kryoHolder = kryoHolderDeque.pollFirst(); return kryoHolder == null ? creatInstnce() : kryoHolder; &#125; /** * create a new kryo object to application use * @return KryoHolder instance */ public KryoHolder creatInstnce() &#123; Kryo kryo = new Kryo(); kryo.setReferences(false); return new KryoHolder(kryo); &#125; /** * return object * Inserts the specified element at the tail of this queue. * * @param kryoHolder ... */ @Override public void offer(KryoHolder kryoHolder) &#123; kryoHolderDeque.addLast(kryoHolder); &#125; /** * creat a Singleton */ private static class Singleton &#123; private static final KryoPool pool = new KryoPoolImpl(); &#125; &#125; @Override public String name() &#123; return "Kryo_Pool"; &#125; /** * Serialize object * @param obj what to serialize * @return return serialize data */ @Override public byte[] serialize(Object obj) throws IOException &#123; KryoHolder kryoHolder = null; if (obj == null)&#123; throw new RuntimeException("obj can not be null"); &#125; try &#123; kryoHolder = KryoPoolImpl.getInstance().get(); kryoHolder.kryo.register(obj.getClass()); //reset Output --&gt;每次调用的时候 重置 kryoHolder.output.reset(); kryoHolder.kryo.writeClassAndObject(kryoHolder.output, obj); // 无法避免拷贝 ~~~ return kryoHolder.output.toBytes(); &#125; catch (RuntimeException e) &#123; e.printStackTrace(); throw new RuntimeException("Serialize obj exception"); &#125; finally &#123; KryoPoolImpl.getInstance().offer(kryoHolder); //GC obj = null; &#125; &#125; /** * Deserialize data * @param bytes what to deserialize * @return object */ @Override public Object deserialize(byte[] bytes) throws IOException &#123; KryoHolder kryoHolder = null; if (bytes == null)&#123; throw new RuntimeException("bytes can not be null"); &#125; try &#123; kryoHolder = KryoPoolImpl.getInstance().get(); //call it ,and then use input object ,discard any array kryoHolder.input.setBuffer(bytes, 0, bytes.length); return kryoHolder.kryo.readClassAndObject(kryoHolder.input); &#125; catch (RuntimeException e) &#123; throw new RuntimeException("Deserialize bytes exception"); &#125; finally &#123; KryoPoolImpl.getInstance().offer(kryoHolder); // for gc bytes = null; &#125; &#125;&#125; Jackson 实现序列化和反序列化Jackson也可以实现相关序列化和反序列化功能，需要引入jackson 的jar包。 使用writeValueAsBytes和readValue方法即可完成相关功能。 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt;&lt;/dependency&gt; 代码如下： 123456789101112131415161718public class JacksonSerializer implements Serializer&#123; private static final ObjectMapper mapper = new ObjectMapper(); @Override public String name() &#123; return "Jackson"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; return mapper.writeValueAsBytes(obj); &#125; @Override public Object deserialize(byte[] bytes) throws IOException &#123; return mapper.readValue(bytes,Object.class); &#125;&#125; FastJson实现序列化和反序列化FastJson实现序列化与反序列化，需要引入相关jar包，如下： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.58&lt;/version&gt;&lt;/dependency&gt; 相关代码如下： 12345678910111213141516public class FastJsonSerializer implements Serializer&#123; @Override public String name() &#123; return "FastJson"; &#125; @Override public byte[] serialize(Object obj) throws IOException &#123; return JSON.toJSONString(obj, SerializerFeature.WriteClassName).getBytes(); &#125; @Override public Object deserialize(byte[] bytes) throws IOException&#123; return JSON.parse(new String(bytes), Feature.SupportAutoType); &#125;&#125; 以上的序列化与反序列化的对象都需要实现Serializable接口。 我们对上述代码进行相关测试： 12345678910111213141516171819202122232425public class SerializationUtils &#123; public static void main(String[] args) throws IOException &#123; Apple apple =new Apple(); apple.setColor("red"); apple.setWeight(100); printData(new JavaSerializer(),apple); printData(new FSTSerializer(),apple); printData(new KryoSerializer(),apple); printData(new KryoPoolSerializer(),apple); printData(new JacksonSerializer(),apple); printData(new FastJsonSerializer(),apple); &#125; public static void printData(Serializer serializer,Apple apple) throws IOException&#123; long start = System.currentTimeMillis(); byte[] bits = serializer.serialize(apple); System.out.println(serializer.name()+"序列化所需时间："+(System.currentTimeMillis()-start)+"ms"); System.out.println(serializer.name()+"序列化后字节码长度："+bits.length); long start1 = System.currentTimeMillis(); Object obj = serializer.deserialize(bits); System.out.println(serializer.name()+"反序列化所需时间："+(System.currentTimeMillis()-start1)+"ms"); System.out.println(serializer.name()+"反序列化后对象："+obj.toString()); &#125;&#125; 可以看到输出后的结果： 根据结果判断正确性后，也大致能看出各种序列化方式的一些优点和缺点。 JavaSerializer 明显的优点是不用引用包，也是Java程序默认的序列化方式，但是其序列化后占用空间是几种序列化方式里最大的，如果遇到大对象序列化，处理起来可能就比较力不从心了。 FSTSerializer、JacksonSerializer、FastJsonSerializer 它们是一种比较适中的序列化方式，序列化后的字节比Java方法少，时间也差不多。 KryoSerializer 是一种比较优异的序列化方式，可以看到它的序列化后的字节很短，占用空间少，且序列化和反序列化时间短。 KryoPoolSerializer 这种相当于KryoSerializer的改进版，利用了一部分内存空间，进一步降低了序列化和反序列化的时间。 正因为Kryo如此高效的序列化和反序列化性能，因此在大数据领域应用广泛。如Apache的spark、hive等。 如果需要更准确的结果比较各种序列化方式的性能，可以创建大量对象并对它们进行序列化记录时间等参数比较，这儿就不过多讨论了。 结语通过对序列化和反序列化的简单介绍，并比较了一些常用的序列化方式，我们对对象的序列化与反序列化有了更进一步的认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>序列化与反序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产者和消费者模式]]></title>
    <url>%2Fblog%2F%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言今天我们来学习下生产者与消费者模式。 生产者和消费者模式可以解决绝大多数并发问题，一般由生产者、数据缓冲区、消费者构成。 如下图，其原理是将原来的直接调用（消费者-&gt;生产者）变为了生产者生产数据放入缓存区，消费者从缓存区获取数据并消费这种模式。 可以知道MQ就是生产者与消费者模式的典型代表。 我们可以举例比如一个定时任务，每天要批处理数据，比如上传文件，每天如果要上传1000个文件或者更多，这时候我们使用平常的循环上传方法，明显大部分时间均浪费在了上传的时间上。 如果按照每个文件处理需要3s，1000个文件则至少需要3000s时间。 如果我们引入生产者和消费者模式，生产者部分负责查询组装数据并把它们放入数据缓存区，消费者部分负责处理数据并上传，可以大大提高并发性能。 使用生产者与消费者模式的典型优点如下： 并发支持 可以看到，如果消费者处理比较耗时，我们可以使用多个生产者生产数据或者消费者去处理队列数据，从而提高系统并发性能。即消费者和生产者可以为两个独立的并发主体。 解耦 我们将生产者和消费者分开后，即使生产者部分处理数据的逻辑有变化，也不会影响到消费者部分，而相比之前在一起的逻辑，我们可能需要改动整个业务部分以完成数据处理。即生产者和消费者没有过分的依赖关系，只要保证传输数据格式的正确性即可。 解决忙闲不均问题 可以看到生产者和消费者模式可以完美解决忙闲不均的问题，当生产者数据过多时，进入数据缓存区等待消费者慢慢处理，生产者数据少时，由于缓存区的数据，也不至于消费者无事可做。即无论生产者或者消费者谁快谁慢，我们总可以通过对他们的数量控制来均衡资源的分配。 正文我们通过上面的例子来实践下消费者和生产者模式。 我们正常逻辑可能如下： 123456789101112131415161718public static void main(String[] args) throws Exception&#123; //1. 组装数据 //数据库查询、组装数据过程略，由for循环插入数据代替 List&lt;String&gt; list = new ArrayList&lt;&gt;(); for(int i = 0;i&lt;1000;i++)&#123; //假设处理每条数据花费平均10ms时间 Thread.sleep(10); list.add(i+""); &#125; //2. 上传数据/文件 //上传过程略 for (int i =0;i&lt;list.size();i++)&#123; //假设每个文件平均耗时1s Thread.sleep(1000); System.out.println(i); &#125;&#125; 可以看到这个过程是非常耗时的，我们使用生产者和消费者模式来设计下这个业务场景。 我们数据缓存区使用队列来暂存数据，生产者组生产数据时会将数据放入队列，消费者消费数据时会从队列中获取数据。 我们用阻塞队列LinkedBlockingQueue来作为数据缓存区，写一个生产者放入数据和消费者取出数据的方法。 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class Context&lt;E&gt; &#123; private static final Logger log = LoggerFactory.getLogger(Context.class); //阻塞队列用来暂存数据 private final LinkedBlockingQueue&lt;E&gt; consumptionQueue = new LinkedBlockingQueue&lt;E&gt;(2500); // 生产线程的状态 private volatile ThreadState producersThreadState; // 消费线程的状态 private volatile ThreadState consumersThreadState; /** * 获取队列大小 * @return * @throws Exception */ int getConsumptionQueueSize() &#123; return consumptionQueue.size(); &#125; /** * 将指定元素插入到此队列的尾部，如有必要（队列空间已满且消费线程未停止运行），则等待空间变得可用。 * @param e * @return boolean true:插入成功;false:插入失败（消费线程已停止运行） * @throws Exception */ public boolean offerDataToConsumptionQueue(E e) throws Exception &#123; //设置生产者线程为运行 setProducersThreadState(ThreadState.RUNNING); // 如果消费线程停止了，不再生产数据 if (ThreadState.DEAD == this.getConsumersThreadState())&#123; return false; &#125; //一直尝试将数据放入队列 while (true) &#123; //将数据放入队列，如果成功返回成功 if (consumptionQueue.offer(e, 2, TimeUnit.SECONDS))&#123; return true; &#125; // 添加元素失败，很有可能是队列已满，再次检查消费线程是否工作中 // 如果消费线程停止了，不再生产数据 if (ThreadState.DEAD == this.getConsumersThreadState()) &#123; return false; &#125; &#125; &#125; /** * 获取并移除此队列的头，如果此队列为空且生产线程已停止，则返回 null * @return E 队列的头元素，如果队列为空且生产线程已停止则返回null * @throws Exception */ public E pollDataFromConsumptionQueue() throws Exception &#123; //设置消费者线程为运行 setConsumersThreadState(ThreadState.RUNNING); //一直尝试从队列里获取数据 while (true) &#123; //尝试从队列里获取数据 E e = consumptionQueue.poll(20, TimeUnit.MILLISECONDS); if (e != null)&#123; return e; &#125; // 没有从队列里获取到元素，并且生产线程已停止，则返回null if (ThreadState.DEAD == this.getProducersThreadState())&#123; return null; &#125; log.debug("demand exceeds supply(供不应求，需生产数据)..."); Thread.sleep(50); &#125; &#125; /** * 获取 producersThreadState * @return producersThreadState */ ThreadState getProducersThreadState() &#123; return producersThreadState; &#125; /** * 设置 producersThreadState * @param producersThreadState */ void setProducersThreadState(ThreadState producersThreadState) &#123; this.producersThreadState = producersThreadState; &#125; /** * 获取 consumersThreadState * @return consumersThreadState */ ThreadState getConsumersThreadState() &#123; return consumersThreadState; &#125; /** * 设置 consumersThreadState * @param consumersThreadState */ void setConsumersThreadState(ThreadState consumersThreadState) &#123; this.consumersThreadState = consumersThreadState; &#125;&#125; 线程状态枚举：新线程(NEW)、可运行的(RUNNABLE)、运行中(RUNNING)、死亡(DEAD)、阻塞(BLOCKED)。 123enum ThreadState &#123; NEW, RUNNABLE, RUNNING, DEAD, BLOCKED;&#125; 然后我们构造两个模板接口，一个生产者模板接口一个消费者模板接口，分别提供生产者产生数据的方法和消费者消费数据的方法。具体实现有各自的业务实现类实现即可。 123456789101112131415161718192021222324/** * 生产者模板 * @param &lt;C_E&gt; */public interface ProducerTemplate&lt;C_E&gt; &#123; /** * 生产数据 * @param context * @throws Exception */ void production(Context&lt;C_E&gt; context) throws Exception;&#125;/** * 消费者模板 * @param &lt;C_E&gt; */public interface ConsumerTemplate&lt;C_E&gt; &#123; /** * 消费数据 * @param context * @throws Exception */ void consumption(Context&lt;C_E&gt; context) throws Exception;&#125; 创建一个生产者与消费者的协调者类，用来启动生产者或者消费者。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * 生产与消费协调者 */public class Coordinator &#123; private static final Logger log = LoggerFactory.getLogger(Coordinator.class); private final Lock lock = new ReentrantLock(); private final Condition enabledConsumers = lock.newCondition(); private volatile boolean isEnabledForConsumers; private final Context&lt;?&gt; context; // 是否等待生产及消费完成 private boolean isWaitingToFinish; // 最大消费线程数 private int consumersMaxTotal; public Coordinator(Context&lt;?&gt; context, int consumersMaxTotal) &#123; this(context, consumersMaxTotal, true); &#125; public Coordinator(Context&lt;?&gt; context, int consumersMaxTotal, boolean isWaitingToFinish) &#123; this.context = context; this.consumersMaxTotal = consumersMaxTotal; this.isWaitingToFinish = isWaitingToFinish; &#125; /** *启动生产、消费 * @param producerTemplate 生产者模板 * @param consumerTemplate 消费者模板 */ public void start(ProducerTemplate&lt;?&gt; producerTemplate,ConsumerTemplate&lt;?&gt; consumerTemplate) throws Exception &#123; if (context.getConsumersThreadState() != null || context.getProducersThreadState() != null)&#123; return; &#125; ProducersThreadUnit producersThreadUnit = new ProducersThreadUnit(producerTemplate, "production", context); ConsumersThreadUnit consumersThreadUnit = new ConsumersThreadUnit(consumerTemplate, "consumption", context); this.start(producersThreadUnit, consumersThreadUnit); &#125; /** * 启动生产、消费（适用于生产函数、消费函数不在一个类里实现，或者一个类里有多对生产、消费组合，或者方法入参列表复杂） * @param producersThreadUnit * @param consumersThreadUnit */ public void start(ProducersThreadUnit producersThreadUnit, ConsumersThreadUnit consumersThreadUnit) throws Exception &#123; if (context.getConsumersThreadState() != ThreadState.NEW || context.getProducersThreadState() != ThreadState.NEW)&#123; return; &#125; long time = System.currentTimeMillis(); try &#123; //启动生产者 Thread startProducersThread = this.startProducers(producersThreadUnit); //启动消费者 Thread startConsumersThread = this.startConsumers(consumersThreadUnit); if (!this.isWaitingToFinish)&#123; return; &#125; startProducersThread.join(); if (startConsumersThread != null)&#123; startConsumersThread.join(); &#125; &#125; catch (Exception e) &#123; log.error("start worker error...", e); throw e; &#125; log.info(String.format("processing is completed... man-hour(millisecond)=[%s]", System.currentTimeMillis() - time)); &#125; /** * 启动生产 * @param producersThreadUnit * @return */ private Thread startProducers(ProducersThreadUnit producersThreadUnit) throws Exception &#123; Thread thread = new Thread(producersThreadUnit); thread.start(); return thread; &#125; /** * 启动消费 * @param consumersThreadUnit * @return */ private Thread startConsumers(ConsumersThreadUnit consumersThreadUnit) throws Exception &#123; lock.lock(); try &#123; log.info("wating for producers..."); while (!isEnabledForConsumers)&#123; // 等待生产（造成当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态）,假定可能发生虚假唤醒（这并非是因为等待超时），因此总是在一个循环中等待 // 间隔检查，防止意外情况下线程没能被成功唤醒（机率小之又小,导致线程无限挂起） enabledConsumers.await(5, TimeUnit.SECONDS); &#125; if (context.getConsumptionQueueSize() == 0)&#123; return null; &#125; log.info("start consumers before..."); Thread thread = new Thread(consumersThreadUnit); thread.start(); return thread; &#125; catch (Exception e) &#123; log.error("start consumers error...", e); throw e; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 生产者和消费者的线程单元如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156 /** * 生产线程 */ public class ProducersThreadUnit implements Runnable &#123; private Object targetObject; private String targetMethodName; private Object[] targetMethodParameters; private ExecutorService executorService = Executors.newFixedThreadPool(1); public ProducersThreadUnit(Object targetObject, String targetMethodName, Object... targetMethodParameters) &#123; this.targetObject = targetObject; this.targetMethodName = targetMethodName; this.targetMethodParameters = targetMethodParameters; context.setProducersThreadState(ThreadState.NEW); &#125; @Override public void run() &#123; try &#123; executorService.execute(new RunnableThreadUnit(targetObject, targetMethodName, targetMethodParameters)); context.setProducersThreadState(ThreadState.RUNNABLE); executorService.shutdown(); // 阻塞线程，直到生产中（消费队列不为空）或者停止生产 while (!executorService.isTerminated() &amp;&amp; context.getConsumptionQueueSize() == 0)&#123; Thread.sleep(20); &#125; log.info("production the end or products have been delivered,ready to inform consumers..."); this.wakeConsumers(); log.info("wait until the production is complete..."); while (!executorService.isTerminated())&#123; // 等待生产完毕 Thread.sleep(200); &#125; &#125; catch (Exception e) &#123; log.error(String.format("production error... targetObject=[%s],targetMethodName=[%s],targetMethodParameters=[%s]", targetObject, targetMethodName, targetMethodParameters), e); if (!executorService.isShutdown())&#123; executorService.shutdown(); &#125; &#125; finally &#123; log.info("production the end..."); context.setProducersThreadState(ThreadState.DEAD); // 无论在何种情况下，必须确保能够结束挂起中的消费者线程 isEnabledForConsumers = true; &#125; &#125; /** * 向消费者发送信号 */ private void wakeConsumers() &#123; // 即使唤醒消费者线程失败，也可以使用该句柄结束挂起中的消费者线程 isEnabledForConsumers = true; lock.lock(); try &#123; enabledConsumers.signal(); &#125; catch (Exception e) &#123; log.error("inform to consumers error...", e); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; /** * 消费线程 */ public class ConsumersThreadUnit implements Runnable &#123; private Object targetObject; private String targetMethodName; private Object[] targetMethodParameters; public ConsumersThreadUnit(Object targetObject, String targetMethodName, Object... targetMethodParameters) &#123; this.targetObject = targetObject; this.targetMethodName = targetMethodName; this.targetMethodParameters = targetMethodParameters; context.setConsumersThreadState(ThreadState.NEW); &#125; @Override public void run() &#123; ThreadPoolExecutor threadPoolExecutor = null; int concurrencyMaxTotal = Coordinator.this.consumersMaxTotal; try &#123; threadPoolExecutor = new ThreadPoolExecutor(0, concurrencyMaxTotal, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); while (concurrencyMaxTotal &gt; 0) &#123; if (threadPoolExecutor.getPoolSize() &gt; context.getConsumptionQueueSize()) &#123; if (ThreadState.DEAD == context.getProducersThreadState()) &#123; // 无须再提交新任务 break; &#125;else &#123; Thread.sleep(50); // 再次检查是否有必要提交新任务 continue; &#125; &#125; RunnableThreadUnit consumers = new RunnableThreadUnit(targetObject, targetMethodName, targetMethodParameters); threadPoolExecutor.execute(consumers); context.setConsumersThreadState(ThreadState.RUNNABLE); log.info("submit consumption task..."); concurrencyMaxTotal--; &#125; threadPoolExecutor.shutdown(); while (!threadPoolExecutor.isTerminated()) &#123; // 等待消费完毕 Thread.sleep(100); &#125; &#125; catch (Exception e) &#123; log.error(String.format("consumption error... targetObject=[%s],targetMethodName=[%s],targetMethodParameters=[%s]", targetObject, targetMethodName, targetMethodParameters), e); if (threadPoolExecutor != null &amp;&amp; !threadPoolExecutor.isShutdown()) &#123; threadPoolExecutor.shutdown(); &#125; &#125; finally &#123; log.info("consumption the end..."); context.setConsumersThreadState(ThreadState.DEAD); &#125; &#125; &#125;/** *线程单元（无返回值） */public class RunnableThreadUnit implements Runnable &#123; private final static Logger logger = LoggerFactory.getLogger(RunnableThreadUnit.class); private Object object; private String methodName; private Object[] methodParameters; public RunnableThreadUnit(Object object, String methodName, Object... methodParameters) &#123; if (object == null || StringUtils.isBlank(methodName) || methodParameters == null) &#123; throw new RuntimeException("init runnable thread unit error..."); &#125; this.object = object; this.methodName = methodName; this.methodParameters = methodParameters; &#125; @Override public void run() &#123; try &#123; Class&lt;?&gt;[] classes = new Class[methodParameters.length]; for (int i = 0; i &lt; methodParameters.length; i++) &#123; classes[i] = methodParameters[i].getClass(); &#125; Method method = object.getClass().getMethod(methodName, classes); method.invoke(object, methodParameters); &#125; catch (Exception e) &#123; logger.error(String.format("execute runnable thread unit error... service=[%s],invokeMethodName=[%s]", object, methodName), e); &#125; &#125;&#125; 可以看到我们使用反射获取了production和consumption方法，并执行它们。启动了两个线程，生产者线程和消费者线程去处理业务，其中消费者线程利用了线程池，可以放置concurrencyMaxTotal个子线程去消费任务。 我们创建一个测试类进行测试，如下： 123456789101112131415161718192021222324252627282930313233public class CourrentTest implements ProducerTemplate&lt;String&gt;, ConsumerTemplate&lt;String&gt;&#123; @Override public void production(Context&lt;String&gt; context) throws Exception &#123; //1. 组装数据 //数据库查询、组装数据过程略，由for循环插入数据代替 for(int i = 0;i&lt;1000;i++)&#123; Thread.sleep(10); //插入不成功，说明可能是消费者线程死亡或者队列已满 if(!context.offerDataToConsumptionQueue(i+""))&#123; return; &#125; &#125; &#125; @Override public void consumption(Context&lt;String&gt; context) throws Exception &#123; //2. 上传数据/文件 //消费者消费数据 while (true) &#123; String str = context.pollDataFromConsumptionQueue(); if (str == null) &#123; break; &#125; //假设每个文件上传消耗1s时间 Thread.sleep(1000); System.out.println(str); &#125; &#125; //测试 public static void main(String[] args) throws Exception&#123; CourrentTest courrentTest = new CourrentTest(); new Coordinator(new Context&lt;String&gt;(),10).start(courrentTest,courrentTest); &#125;&#125; 运行后可以看到输出的结果。 这儿我们可以看到对比较耗时的上传方法（消费者端）进行了并发处理以提高效率，生产端如果保证了数据的安全性，我们可以使用并行流等放入数据以提高放入数据的效率。 其实我们看到这儿，可以理解线程池也是一个类似于生产者消费者模式的东西。线程池里面有任务就会去执行，相当于消费者，线程池里的队列相当于缓存区，而生产者就是我们一个个放入线程的Runable方法。 上述代码的运行原理图大致如下： PS: 上述代码可以在我的GitHub项目里找到。 https://github.com/JavaZWT/framework-base 另外提供了一个简易模板SimpleTemplate可以适用生产者方法和消费者方法在一个类里的情况，只继承这一个方法即可。不用分别继承ConsumerTemplate和ProducerTemplate接口了。 总结通过对上面一个列子使用生产者和消费者模式，我们了解了这种模式的一些适用情形和优点。 当然也了解了它的一些缺点，对于解决并发问题的方案，最要重视的应该就是数据安全问题了。 我们在平时工作中也可以考虑什么样的场景下可以使用这种模式，其实这种模式的适用场景还是蛮多的，对于一些处理较耗时的操作，文件上传、图片生成转换等都可以考虑这种模式。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
        <tag>生产者与消费者模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis在项目中的一些应用]]></title>
    <url>%2Fblog%2FRedis%E5%9C%A8%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言今天我们来总结下Redis在项目中的一些应用。 Redis在实际项目中除了可以作为缓存或者持久化数据库外，还能解决项目中遇到的一些棘手的问题。 正文限流/防高频问题这基本上属于项目中一个比较经典的问题了，我们以防止用户高频访问来举例，大多数的项目中都是通过Redis来解决高频访问问题的。 我们知道，对于高频访问问题，要有以下3要素：单位时间、单位时间限制访问次数、超频后的限制访问时间长。 我们定义一个Bean，控制这几个参数，当然也可以直接使用配置文件的方式进行配置等。 123456789101112131415161718public class LimitRule &#123; /** * 单位访问时间 */ private int seconds; /** * 单位时间内限制的访问次数 */ private int limitCount; /** * 单位时间超过访问次数后的锁定时间 */ private int lockTime; //Get Set略 public boolean enableLimitLock() &#123; return getLockTime() &gt; 0 &amp;&amp; getLimitCount() &gt; 0; &#125;&#125; Redis有一种数据结构，名字为Zset，可以通过方法zadd添加元素，通过zcount统计记录数，我们可以用Zset的有序集的value来存放访问时间，判断超频时，只需要用zcount判断单位时间seconds内Zset里的元素数据是否超过limitCount即可，超过后即为该用户添加一个锁定lockTime的Redis key。 因此，我们代码大致如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class HighFreqLimit &#123; //记录用户行为并判断高频访问的zset private static final String REDIS_VISIT_KEY_FORMAT = "redis.visit:user:zset:%s"; //高频访问用户key private static final String REDIS_LIMIT_KEY_FORMAT = "redis.limit:user:%s"; //Redis 工具 private RedisUtil redisUtil; public HighFreqLimit(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; /** * 方案一：使用Zset来判断高频访问情况 */ /** * 判断用户是否高频访问 * 高频访问抛出异常 * @param userNo 用户唯一的编号 * @param limitRule 访问规则 这个规则可以用配置文件的方式处理 * @return */ public void checkLimit1(String userNo,LimitRule limitRule)&#123; //判断是否启用了高频访问 if(!limitRule.enableLimitLock())&#123; return; &#125; String redisKeyUserVisitZset = String.format(REDIS_VISIT_KEY_FORMAT,userNo); String redisUserFreqLimitKey = String.format(REDIS_LIMIT_KEY_FORMAT,userNo); //如果存在高频访问key说明已经高频访问了 if (redisUtil.exists(redisUserFreqLimitKey)) &#123; throw new RuntimeException("您操作的太快了，请稍后访问"); &#125; long currentTimeMillis=System.currentTimeMillis(); //访问信息，可以根据具体业务定制 String visitInfo = userNo +":"+ System.currentTimeMillis(); //将信息添加到zset里 redisUtil.zadd(redisKeyUserVisitZset,System.currentTimeMillis(), visitInfo); //设置过期时间为单位时间 redisUtil.expire(redisKeyUserVisitZset, limitRule.getSeconds()); long startTimeMillis = currentTimeMillis - limitRule.getSeconds() * 1000; //统计单位时间内Zset的元素个数 long visitCount = redisUtil.zcount(redisKeyUserVisitZset, startTimeMillis, currentTimeMillis); //超过阈值则成为高频用户 if (visitCount &gt; limitRule.getLimitCount()) &#123; redisUtil.setString(redisUserFreqLimitKey, visitInfo); redisUtil.expire(redisUserFreqLimitKey, limitRule.getLockTime()); throw new RuntimeException("您操作的太快了，请稍后访问"); &#125; &#125;&#125; Redis在2.6版本后支持Lua表达式，因此我们也可以构建使用Lua表达式来解决上述问题。 构建Lua表达式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344private static final String REDIS_VISIT_KEY_FORMAT2 = "redis.visit:user:lua:%s"; /** * 方案二：使用Lua表达式来判断高频访问情况 */ /** * 判断用户是否高频访问 * 高频访问抛出异常 * @param userNo * @param limitRule */ public void checkLimit2(String userNo, LimitRule limitRule) &#123; String redisKeyUserVisit = String.format(REDIS_VISIT_KEY_FORMAT2,userNo); long count; List&lt;String&gt; keys = new ArrayList&lt;String&gt;(); keys.add(redisKeyUserVisit); List&lt;String&gt; args = new ArrayList&lt;String&gt;(); args.add(limitRule.getLimitCount() + ""); args.add(limitRule.getSeconds() + ""); args.add(limitRule.getLockTime() + ""); count = Long.parseLong(redisUtil.getJedisFactory().getJedisCluster().eval(buildLuaScript(limitRule), keys, args) + ""); if(count &gt; limitRule.getLimitCount())&#123; throw new RuntimeException("您操作的太快了，请稍后访问"); &#125; &#125; private String buildLuaScript(LimitRule limitRule) &#123; StringBuilder lua = new StringBuilder(); lua.append("\nlocal c"); lua.append("\nc = redis.call('get',KEYS[1])"); lua.append("\nif c and tonumber(c) &gt; tonumber(ARGV[1]) then"); lua.append("\nreturn c;"); lua.append("\nend"); lua.append("\nc = redis.call('incr',KEYS[1])"); lua.append("\nif tonumber(c) == 1 then"); lua.append("\nredis.call('expire',KEYS[1],ARGV[2])"); lua.append("\nend"); if (limitRule.enableLimitLock()) &#123; lua.append("\nif tonumber(c) &gt; tonumber(ARGV[1]) then"); lua.append("\nredis.call('expire',KEYS[1],ARGV[3])"); lua.append("\nend"); &#125; lua.append("\nreturn c;"); return lua.toString(); &#125; 对于上述表达式，KEYS[1]即为redisKeyUserVisit，可以看到先进行取值，如果有值并且值比limitCount大就返回了，根据后面count &gt; limitRule.getLimitCount()的判断说明已经超频了，如果不大于该值，则进行自增，如果该值是1，说明单位时间第一次访问，就设置它的单位时间过期，然后如果该值超频后会这是这个Key的过期时间为lockTime。 序列号生成问题项目中另一种常见的情况就是流水号的生成了，很多业务流水号有如下格式 XXXX2019040100001 等，我最近的一个项目就有类似的复杂需求，这种情况下我们可以使用Redis来生成某一天的自增流水号，大致如下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class SequenceUtils &#123; private RedisUtil redisUtil; public SequenceUtils(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; //业务规定序号为00001 ，00002 这种5位格式 private static final int DEFAULT_LENGTH = 5; //缓存时长 private static final int ONE_DAY_TIME = 24*60*60; private static final String REDIS_CACHE_KEY = "redis.serialnumber:%s:%s"; /** * 获取自增数字的字符串形式,包含0前缀 * @param seq * @return */ private String getSequenceWithZeroPrefix(long seq) &#123; String str = String.valueOf(seq); int len = str.length(); if (len &gt;= DEFAULT_LENGTH) &#123; throw new RuntimeException("Sequence generate failed!"); &#125; int rest = DEFAULT_LENGTH - len; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; rest; i++) &#123; sb.append('0'); &#125; sb.append(str); return sb.toString(); &#125; /** * 获取自增数字的字符串形式,不包含0前缀 * @param seq * @return */ private String getSequenceNoZeroPrefix(long seq)&#123; return String.valueOf(seq); &#125; /** * 序列号生成器 * @param bizCode 业务码 * @param needZero 是否需要0前缀 * @return */ public String generate(String bizCode,boolean needZero)&#123; String date = DateFormatUtils.format(new Date(),"yyyyMMdd"); //redis key String key = String.format(REDIS_CACHE_KEY,bizCode,date); //自增并设置过期时间 long sequence = redisUtil.incr(key); redisUtil.expire(key,ONE_DAY_TIME); String seq; if(needZero)&#123; seq = getSequenceWithZeroPrefix(sequence); &#125;else&#123; seq = getSequenceNoZeroPrefix(sequence); &#125; StringBuffer sb = new StringBuffer(); sb.append(bizCode).append(date).append(seq); return sb.toString(); &#125;&#125; 为保证绝对可靠，还可以进行改善，当Redis拿不到值时可以去数据库初始化今天的起始流水号等，这儿不再过多介绍，可以看到主要就是利用了Redis的自增incr和指定时间过期expire这两个关键方法。 分布式锁还可以使用Redis做分布式锁，相比较之前说的Zookeeper实现分布式锁，使用Redis实现分布式锁，最明显的优点就是指令为内存操作，速度较快，性能较高；但缺点也比较明显，使用Redis实现分布式锁较为复杂，需要考虑超时、原子性、误删等情形，较为复杂，且由于没有等待锁的队列，等待锁只能依靠客户端自旋，效率较为低下。反观ZK实现的分布式锁，有等待锁的队列，但是添加删除节点性能较低。 我们使用Redis来简单实现一个分布式锁。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class RedisLock &#123; //锁的key前缀 private final static String LOCK_PREFIX="redis.lock:%s"; private RedisUtil redisUtil; public RedisLock(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; /** * 尝试获得锁 * @param key String key * @param lockTimeOut long 超时时间(毫秒) * @return 大于 0 获得到锁并,等于0获取锁失败 */ public long tryLock(String key,long lockTimeOut)&#123; key= String.format(LOCK_PREFIX,key); long expireTime = 0; expireTime = System.currentTimeMillis() + lockTimeOut +1; if(redisUtil.setStringIfNotExists(key, String.valueOf(expireTime))==1)&#123; return expireTime; &#125;else &#123; String curLockTimeStr = redisUtil.getString(key); //判断是否过期 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr)) &#123; expireTime = System.currentTimeMillis() + lockTimeOut +1; curLockTimeStr = redisUtil.getSet(key, String.valueOf(expireTime)); //仍然过期,则得到锁 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr))&#123; return expireTime; &#125;else &#123; return 0; &#125; &#125;else &#123; return 0; &#125; &#125; &#125; /** * 一直等待获得锁 * @param key String key * @param lockTimeOut long 超时时间(毫秒) * @param perSleep long 获得锁循环等待休眠时间 * @return 大于 0 获得到锁并,等于0获取锁失败 * @throws InterruptedException */ public long lock(String key,long lockTimeOut,long perSleep) throws InterruptedException&#123; key= String.format(LOCK_PREFIX,key); long starttime = System.currentTimeMillis(); long sleep = (perSleep==0 ? lockTimeOut/ 10 : perSleep); //得到锁后设置的过期时间，未得到锁返回0 long expireTime = 0; for (;;) &#123; expireTime = System.currentTimeMillis() + lockTimeOut +1; if (redisUtil.setStringIfNotExists(key, String.valueOf(expireTime)) == 1) &#123; //得到了锁返回 return expireTime; &#125;else &#123; String curLockTimeStr = redisUtil.getString(key); //判断是否过期 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr)) &#123; expireTime = System.currentTimeMillis() + lockTimeOut +1; curLockTimeStr = redisUtil.getSet(key, String.valueOf(expireTime)); //仍然过期,则得到锁 if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr))&#123; return expireTime; &#125;else &#123; Thread.sleep(sleep); &#125; &#125;else &#123; Thread.sleep(sleep); &#125; &#125; if (lockTimeOut &gt; 0 &amp;&amp; ((System.currentTimeMillis() - starttime) &gt;= lockTimeOut)) &#123; expireTime = 0; return expireTime; &#125; &#125; &#125; /** * 释放锁 * @param key String key * @param expireTime long 超时时间(毫秒) */ public void unlock(String key,long expireTime)&#123; key= String.format(LOCK_PREFIX,key); if (System.currentTimeMillis()-expireTime&gt;0) &#123; return ; &#125; String curLockTimeStr = redisUtil.getString(key); if (StringUtils.isNotBlank(curLockTimeStr) &amp;&amp; Long.valueOf(curLockTimeStr)&gt;System.currentTimeMillis()) &#123; redisUtil.delKey(key); &#125; &#125;&#125; 可以看到在等待锁的阶段，如果设置超时时间，则客户端只能自旋等待锁，如果在指定时间内未获得锁，就会超时。 可以看到主要逻辑是，首先使用setStringIfNotExists （对应Redis的set str NX 命令）方法尝试设置key，如果成功说明获得锁，返回超时时间；如果不成功说明已经有程序在使用该锁，需要判断剩余过期时间，如果没有剩余过期时间，再尝试获得锁，否则线程sleep剩余过期时间。 可以看到，我们需要指定锁的使用时间，如果不指定时间，有可能会造成死锁等问题。 配置中心Redis也可以用来实现配置中心的相关功能。 Redis 做配置中心，需要结合数据库来实现以确保稳定性。 数据库设计一张配置表用于存储配置数据，在Redis可以将数据存入哈希表来进行处理。 当然我们新增修改或者删除数据时需要同时对Redis和数据库进行操作。 并可以添加数据从Redis刷新到数据库和从数据库刷新到Redis等功能。 部分代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105public class RedisConfigCenter &#123; /** * 配置中心key */ public static final String CONFIG_CENTER_KEY = "redis.configcenter:hash:key"; //Redis 工具 private RedisUtil redisUtil; public RedisConfigCenter(RedisUtil redisUtil)&#123; this.redisUtil = redisUtil; &#125; /** * 配置中心添加数据 * 可以添加一个或多个 * @return */ public boolean insertData(Map&lt;String,String&gt; insertData)&#123; //放入Redis hash表 boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(exists)&#123; //拿到key对应的Redis hash表数据 Map&lt;String,String&gt; redisMap = redisUtil.hashGetAll(CONFIG_CENTER_KEY); insertData.putAll(redisMap); &#125; //写入Redis hash表 redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,insertData); //数据库处理部分代码略 return true; &#125; /** * 配置中心更新数据 * @param updateData * @return */ public boolean updateData(Map&lt;String,String&gt; updateData)&#123; boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(!exists)&#123; throw new RuntimeException("请先新增数据!"); &#125; //拿到key对应的Redis hash表数据 Map&lt;String,String&gt; redisMap = redisUtil.hashGetAll(CONFIG_CENTER_KEY); redisMap.putAll(updateData); //写入Redis hash表 redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,updateData); //数据库处理部分代码略 return true; &#125; /** * 配置中心删除数据 * @param deleteKeys * @return */ public boolean deleteData(List&lt;String&gt; deleteKeys)&#123; boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(!exists)&#123; throw new RuntimeException("请先新增数据!"); &#125; //拿到key对应的Redis hash表数据 Map&lt;String,String&gt; redisMap = redisUtil.hashGetAll(CONFIG_CENTER_KEY); deleteKeys.forEach(key-&gt;&#123; redisMap.remove(key); &#125;); //写入Redis hash表 redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,redisMap); //数据库处理部分代码略 return true; &#125; /** * 查询数据列表 * @return */ public Map&lt;String,String&gt; selectData()&#123; Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); boolean exists = redisUtil.exists(CONFIG_CENTER_KEY); if(!exists)&#123; return map; &#125; map = redisUtil.hashGetAll(CONFIG_CENTER_KEY); return map; &#125; /** * 将数据库数据刷新到Redis * @return */ public boolean refreshToRedis()&#123; //数据库获取到数据集合 略 Map&lt;String,String&gt; map = new HashMap&lt;&gt;();//TODO redisUtil.hashMultipleSet(CONFIG_CENTER_KEY,map); return true; &#125; /** * Redis刷新到数据库 * @return */ public boolean refreshToDataBase()&#123; Map&lt;String,String&gt; map = redisUtil.hashGetAll(CONFIG_CENTER_KEY); //数据库操作，略 return true; &#125;&#125; 当然，为保证数据可靠性，可以启动两个线程，指定时间从Redis刷新到数据库或者从数据库刷新到Redis的功能。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738/** * 可以改为配置的形式 */ private static boolean refreshRedisThreadStop = false; private static boolean refreshDataBaseThreadStop = false; /** * 刷新数据的线程 */ ExecutorService executorService = Executors.newFixedThreadPool(2); /** * 初始化 */ public void init() &#123; // refresh thread executorService.submit(() -&gt;&#123; while (!refreshRedisThreadStop) &#123; try &#123; //30 min 刷新一次 TimeUnit.MINUTES.sleep(30); refreshToRedis(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); executorService.submit(() -&gt;&#123; while (!refreshDataBaseThreadStop) &#123; try &#123; //30 min 刷新一次 TimeUnit.MINUTES.sleep(30); refreshToDataBase(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 结语通过以上对Redis在应用中的各个功能的应用，让我们对Redis的应用有了更深入的了解，以及对Redis的应用场景有了更加深刻的认识。 Redis 在项目中基本上已经是很平常的存在了，如何使用好它，解决棘手问题乃是我们的重中之重。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发中遇到的一些问题与解决思路（1）]]></title>
    <url>%2Fblog%2F%E5%BC%80%E5%8F%91%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF-1.html</url>
    <content type="text"><![CDATA[前言整理了一下开发中遇到的一些问题及解决思路，特地分享下。 正文Mysql临时变量（局部变量）的使用Mysql临时变量又称为局部变量，其带有@符号，使用时不需要声明，只在局部起作用。 用法 set @num=1; 或set @num:=1; 这里要使用变量来保存数据，直接使用@num变量即可 select @num:=1; 或 select @num:=字段名 from 表名 where …… 注意上面两种赋值符号，使用set时可以用“=”或“：=”，但是使用select时必须用“：=赋值” 巧妙的使用临时变量，可以解决我们遇到的一些比较棘手的问题。 例子假设有一张客户表，customer ，其有字段 id（表示唯一id），字段 locale （表示国家，0001 中国，0002 美国，0003 日本 等），字段 create_time 表示这个用户的创建日期（yyyy-MM-dd hh:MM:ss）形式，等其他字段。 现在这张表已经上线一段时间，有了大量数据，由于某些原因（需求），需要新增字段 客户号 customer_no ，客户号的生成遵循一定规则，比如 2018-11-12 日当天第一个中国用户，则客户号为 CN201811120001 这种格式，第20个中国用户客户号为 CN201811120020，即用户号的生成规则是 国家简称 + 日期yyyyMMdd + 0001 递增，当天第一个美国用户客户号为 AM201811120001 这种形式，如果日期变为2018-11-13 则这一天的第一个中国用户为 CN201811130001 这种形式。 这儿就不讨论客户号之后的生成逻辑，现在这张表已经有一定量的数据，新增了客户号字段，需要维护旧数据的此字段，手动修改显然是不现实的…… 这时候我们可以借助临时变量来实现对旧数据的更新维护，locale为中国（CN）的代码更新可以如下（其它国家类似）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152-- 临时自增变量SET @rownum = 0;-- 初始值，需要手动设置，选择一个日期SET @startdate = '20181227';-- 更新语句UPDATE customer pSET p.customer_no = ( SELECT b.num FROM ( SELECT CASE DATE_FORMAT(a.create_time, "%Y%m%d") WHEN @startdate THEN @rownum :=@rownum + 1 ELSE @rownum := 1 END as temp, CONCAT( 'CN', DATE_FORMAT(a.create_time, "%Y%m%d"), CASE WHEN @rownum &gt; 999 THEN '' WHEN @rownum &gt; 99 THEN '0' WHEN @rownum &gt; 9 THEN '00' ELSE '000' END, @rownum ) AS num, ( CASE DATE_FORMAT(a.create_time, "%Y%m%d") WHEN @startdate THEN @startdate ELSE @startdate := DATE_FORMAT(a.create_time, "%Y%m%d") END ) AS helpdata, a.id AS id FROM customer a WHERE 1 = 1 ORDER BY p.create_time asc ) AS b WHERE b.id = p.id)WHERE p.locale = '0001'; 可以看到这个SQL超级长，其实不算复杂，其customer_no的赋值就是借助了两个临时变量进行的，可以看到第一个@rownum临时变量，用来进行自增，拼接为0001、0010等等这种形式。 为什么还有个@startdate临时变量呢？ 这个变量主要用来记录上一条的时间跟要更新的这一条的时间是否一致，一致的话@rownum自增1，不一致说明是新的时间，@rownum重新从1开始，这样做的前提保证是数据要根据create_time进行排序，即 ORDER BY p.create_time asc的作用。 同时我们还可以看到我们借助了中间表b实现了对@startdate的赋值。 其实主要思想可以如下分解，更新表里的旧数据，需要将customer_no赋值，则可以先写一个select语句查询出 CN201811120001 这种形式的一张表，通过b.id = p.id实现关联到指定的要更新的数据，然后进行数据update。 这属于一个较复杂的Mysql临时变量的使用例子，临时变量虽然在Mysql查询等语句中使用的不多，但是关键时候还是有很大作用的。 使用并行流时，必须保证操作对象的线程安全性举例： 12345678910111213141516171819public static void main(String[] args) throws Exception&#123; //构造一个源数据List List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for(int i = 0;i&lt;100000;i++)&#123; list.add(i); &#125; //线程同步的ArrayList List&lt;Integer&gt; list1 = Collections.synchronizedList(new ArrayList&lt;&gt;()); //普通的ArrayList List&lt;Integer&gt; list2 = new ArrayList&lt;&gt;(); //并行流处理 list.parallelStream().forEach(e-&gt;&#123; list1.add(e); list2.add(e); &#125;); //输出结果 System.out.println(list1.size()); System.out.println(list2.size());&#125; 输出结果： 1210000099785 可以看到list2 数据不正常，多运行几次，可以发现list2有时候正常，有时候不正常，有时候甚至会出现数组下标越界异常等情况。 而list1的数据结果一直没有问题。 所以在操作ArrayList的不安全操作时（或者其他不安全数据结构），如add，delete等，如果使用并行流，必须保证被操作对象的线程安全性。 上述情况一般有两种解决办法： 使用线程安全的对象，如ArrayList使用Collections.synchronizedList 方法变为同步的List，或者使用Vector等。 抛弃并行流，使用串行流或者其他解决办法。 Java8 lambda表达式无法抛出受检查异常的问题我们知道Java异常分为两类，受检查异常（Checked Exception）跟非受检异常（UnChecked Exception）。 对于受检查异常，代码中必须显式处理该异常，不然编译不通过，如IOException等。 而对于非受检查异常，或称为运行时异常，可以不用处理，如RunTimeException等。 对于一个方法，如果方法内部抛出受检查异常，则方法本身也要显式抛出异常。 我们来看下下面两个例子。 1234567891011121314public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("1"); list.stream().forEach(e-&gt;&#123; throw new RuntimeException("Exception"); &#125;);&#125;public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("1"); list.stream().forEach(e-&gt;&#123; throw new IOException("Exception"); &#125;);&#125; 可以看到第二个main方法是编译不通过的，因为抛出了受检查异常，而它lambda表达式如下 1Consumer consumer = e-&gt;&#123;throw new IOException("Exception");&#125;; 由于Consumer执行的accept方法，如下： 1234@FunctionalInterfacepublic interface Consumer&lt;T&gt; &#123; void accept(T t);&#125; 故需要该方法抛出受检查异常才可以，显然我们是无法修改Java源码的。 对于其它函数式接口lambda表达式亦是如此。 那如何处理这种情况呢？ 在程序代码想使用lambda表达式的情况下，大致有如下两种办法： 将受检查异常包装成非受检查异常。 123list.stream().forEach(e-&gt;&#123; throw new RuntimeException(new IOException("Exception"));&#125;); 这样虽然解决了问题，但是破坏了异常结构，代码也不是很美观，在涉及到一些事务的方法上，还会导致不能正确捕捉异常进行回滚而产生一些问题等。 不推荐使用。 包装泛型异常 12345678910public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("1"); list.stream().forEach(e-&gt;&#123; doThrow(new IOException()); &#125;);&#125;public static &lt;E extends Exception&gt; void doThrow(Exception e) throws E &#123; throw (E)e;&#125; 可以看到我们利用异常泛型包装了异常处理，这时候编译器不能明确异常类型，因此编译通过。 这时候我们在测试，可以看到lambda表达式运行后抛出了我们期望的IOException。 结语以后再有开发相关方面的问题，值得记录和分享的，我会在更新。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PDF.js插件]]></title>
    <url>%2Fblog%2FPDF-js%E6%8F%92%E4%BB%B6.html</url>
    <content type="text"><![CDATA[前言今天我们来学习下一款非常有意思的插件PDF.js，正如它的名字一样，它是由Mozilla开源的，用来在Web浏览器上预览PDF文件的一款插件。 我们知道，目前很多浏览器已经支持了PDF的在线预览及下载功能，但是展示的样式各异，并且一部分手机浏览器并不支持预览，在开发WebApp遇到预览PDF的功能，可能需要下载下来借助手机第三方软件打开，显然用户体验不够友好。 而PDF.js恰恰解决了以上问题。 如果说PDF.js的缺点，那大概就是它不支持IE 8 及以下浏览器。（PS:PDF.js使用了HTML 5的相关技术，如canvas，理论上不支持HTML 5的浏览器均不能使用） 项目地址：https://mozilla.github.io/pdf.js/ 正文分析我们来了解并使用下这款插件。 根据上面地址，我们可以下载PDF.js的预编译版本和源码。 PDF.js提供的预编译版本是可以直接使用的，当然我们也可以使用源码自己编译生成PDF.js插件。 我们来简单说下PDF.js预编译版本的基本构成，截止当前，我使用的是 Stable(v2.0.943) 的预编译版本。 它的结构如下： 12345678910111213141516├── build/│ ├── pdf.js - display layer│ ├── pdf.js.map - display layer&apos;s source map│ ├── pdf.worker.js - core layer│ └── pdf.worker.js.map - core layer&apos;s source map├── web/│ ├── cmaps/ - character maps (required by core)│ ├── compressed.tracemonkey-pldi-09.pdf - PDF file for testing purposes│ ├── debugger.js - helpful debugging features│ ├── images/ - images for the viewer and annotation icons│ ├── locale/ - translation files│ ├── viewer.css - viewer style sheet│ ├── viewer.html - viewer layout│ ├── viewer.js - viewer layer│ └── viewer.js.map - viewer layer&apos;s source map└── LICENSE 主要由build和web包构成，build包里的pdf.worker.js是PDF.js的核心处理包，web包里的viewer.js viewer.html viewer.css 用来在Web页面上展示渲染PDF（边框，工具栏等）。 images文件夹里存放一些工具栏图标等内容，locale文件夹里存放各地区语言包。 debugger.js 是debug相关js，我们在使用时可以开启debug输出某些信息来进行调试。 compressed.tracemonkey-pldi-09.pdf 是一个PDF测试类，当我们没有加载自己定义的PDF时，会默认加载此文件。 使用集成PDF.js插件我们在SpringBoot项目下引入PDF.js插件并简单使用。 我们新建SpringBoot项目，引入Web模块，生成项目后，我们只需在项目的static文件夹下引入预编译版本的PDF.js即可。 如图： PS: 为方便管理，我把PDF.js插件放到了一个pdfViewer文件夹里。 我们启动项目，通过浏览器访问viewer.html，即 http://localhost:8080/pdfViewer/web/viewer.html 可以看到成功打开了我们的测试PDF。 我们如果想打开自己的PDF应该如何操作呢？ 我们可以在上述网络地址上加上file参数指向我们的PDF。 http://localhost:8080/pdfViewer/web/viewer.html?file=..... 的形式。 我们在项目static文件夹中新建pdf包，放入我们的PDF，如下： PDF文件在本项目中，访问有两种方式，网络路径访问和相对路径访问。 相对路径访问。 通过 http://localhost:8080/pdfViewer/web/viewer.html?file=../../pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 进行访问。 可以看到file后面跟的路径是PDF文件相对于viewer.html的路径。 网络路径访问。 通过 http://localhost:8080/pdfViewer/web/viewer.html?file=http://localhost:8080/pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 进行访问。 file后面跟的参数是PDF所在的网络地址。 最后预览效果如下图： 如果PDF文件不在本项目中呢？那一定会出现跨域问题，我们来看一下。 访问 http://localhost:8080/pdfViewer/web/viewer.html?file=http://localhost:8081/pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 可以看到文件不存在。 我们F12查看信息，可以看到PDF.js插件出现如下错误：Uncaught (in promise) Error: file origin does not match viewer’s 说明出现了跨域问题。 如何解决呢？ 毕竟不是所有的PDF文件都会放在项目中的。 这就需要解决跨域问题，通常有两种方法： 我们如果仍使用路径的方式，则需要对PDF.js进行配置，通过刚才报错的信息，我们很容易在viewer.js 里找到如下内容。 这段js很好理解，设置允许跨域的路径，我们把我们的服务器网络路径添加到HOSTED_VIEWER_ORIGINS对象里，http://localhost:8080。 PS: 这里要注意，viewer.js.map 文件里的 HOSTED_VIEWER_ORIGINS 对象也要改变（预编译版本），否则你会看到很奇怪的缓存问题。 但这样仍是不行的，我们尝试访问 http://localhost:8080/pdfViewer/web/viewer.html?file=http://localhost:8081/pdf/IT%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf 可以看到跨域问题仍然存在。 因为要解决跨域问题，服务器也需要进行设置，我们找到8081服务器，添加跨域设置，主要内容如下（SpringBoot项目）： 12345678910111213141516171819202122@Configurationpublic class CorsConfig &#123; @Bean public CorsFilter corsFilter() &#123; UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); String apiAllowOrigins = "http://localhost:8080"; if(StringUtils.isEmpty(apiAllowOrigins))&#123; corsConfiguration.addAllowedOrigin("*"); &#125;else&#123; corsConfiguration.setAllowedOrigins(Arrays.asList(apiAllowOrigins.split(","))); &#125; corsConfiguration.addAllowedHeader("*"); corsConfiguration.setAllowedMethods(Arrays.asList("HEAD","POST", "OPTIONS","GET")); corsConfiguration.setAllowCredentials(true); corsConfiguration.setMaxAge(3600L); //跨域设置 source.registerCorsConfiguration("/**", corsConfiguration); return new CorsFilter(source); &#125;&#125; 这段代码很好理解，apiAllowOrigins设置成我们请求的服务器地址即可。 这时候我们在访问上面的PDF地址，可以看到PDF被加载出来了。 我们也可以使用流的方式来加载远程的PDF文件，当然也需要设置跨域属性。 上面的CorsConfig.java 在8081服务器上保持不变，我们在该项目里新增一个类PDFController，用于解析文件并返回流。 123456789101112131415161718@Controllerpublic class PDFController &#123; @RequestMapping("/getPDF") public void getPDF(@RequestParam("fileName") String fileName, HttpServletRequest request, HttpServletResponse response) throws Exception&#123; response.setStatus(HttpServletResponse.SC_OK); response.setContentType("application/pdf;charset=UTF-8"); //PDF文件在服务器上的位置，可以通过程序获取，略，我直接写死了 try(FileInputStream input =new FileInputStream("E:\\WorkSpace\\spring-cloud\\pdf1-service\\src\\main\\resources\\static\\pdf\\"+fileName); ServletOutputStream out = response.getOutputStream())&#123; byte buffBytes[] = new byte[1024]; int read = 0; while ((read = input.read(buffBytes)) != -1) &#123; out.write(buffBytes, 0, read); &#125; out.flush(); &#125; &#125;&#125; 我这里传入了fileName属性来获取PDF流。 这样在调用PDF.js 插件时，需要先通过ajax获取PDF流拿到PDF，并提供给PDF.js插件。 我们打开viewer.js 找到如下代码。 这个方法是webView初始化方法，如果没有传file属性，就加载默认的PDF文件。 我们把 file = ‘file’ in params ? params.file : _app_options.AppOptions.get(‘defaultUrl’); 这段代码换成如下： 1file = 'file' in params ? params.file : DEFAULT_URL; 很好理解，就是不用它的默认值，我们传入一个DEFAULT_URL值。 我们新建helper.js，如下： 12345678910111213141516171819var DEFAULT_URL = "";var PDFData = "";$.ajax(&#123; type:"post", async:false, mimeType: 'text/plain; charset=x-user-defined', url:"http://localhost:8081/getPDF", data:&#123;fileName:"IT入职指南.pdf"&#125;, success:function(data)&#123; PDFData = data; &#125;&#125;);var rawLength = PDFData.length;//转换成pdf.js能直接解析的Uint8Array类型var array = new Uint8Array(new ArrayBuffer(rawLength));for(i = 0; i &lt; rawLength; i++) &#123; array[i] = PDFData.charCodeAt(i) &amp; 0xff;&#125;DEFAULT_URL = array; 并在PDF.js插件的web包下引入helper.js 和 jquery.js。 PS: jquery.js 的引用就使用了一个ajax，如果由其他的ajax方案可以不引入jquery。 并在viewer.html里引入依赖，需要在viewer.js 之前引入。 12345678.....部分代码略&lt;script src="../build/pdf.js"&gt;&lt;/script&gt;&lt;script src="jquery-3.3.1.min.js"&gt;&lt;/script&gt;&lt;script src="helper.js"&gt;&lt;/script&gt; &lt;script src="viewer.js"&gt;&lt;/script&gt; &lt;/head&gt;.......部分代码略 设置好后，我们重新启动下服务器，可以看到PDF文件被加载了。 PS:根据上面，我们可以看到当不传入file参数时，PDF.js会加载默认配置，我们可以通过js等控制这个默认配置，让PDF.js每次打开的都是 http://localhost:8080/pdfViewer/web/viewer.html ，但是PDF文件变化，以实现不显示file参数的需求。 PDF.js 插件的个性化需求我们使用了PDF.js 插件后，可以看到功能很全，什么工具栏，放大缩小，打印，下载等等功能一应俱全，但现实中我们有可能是不需要这么多的。 尤其在WebApp中，我们可能仅仅可以查看PDF即可，偶尔可以有个放大缩小功能，这样应该如何处理呢。 这项功能的实现是非常简单的，我们找到该功能的按钮的button，直接添加hidden属性即可。 12345678910111213......&lt;button id="sidebarToggle" class="toolbarButton" title="Toggle Sidebar" tabindex="11" data-l10n-id="toggle_sidebar" hidden&gt; &lt;span data-l10n-id="toggle_sidebar_label"&gt;Toggle Sidebar&lt;/span&gt;&lt;/button&gt;&lt;div class="toolbarButtonSpacer"&gt;&lt;/div&gt;&lt;button id="viewFind" class="toolbarButton" title="Find in Document" tabindex="12" data-l10n-id="findbar" hidden&gt; &lt;span data-l10n-id="findbar_label"&gt;Find&lt;/span&gt;&lt;/button&gt;......&lt;button id="secondaryToolbarToggle" class="toolbarButton" title="Tools" tabindex="36" data-l10n-id="tools" hidden&gt; &lt;span data-l10n-id="tools_label"&gt;Tools&lt;/span&gt;&lt;/button&gt;...... PDF.js 踩坑记 关于过大PDF无法显示的问题 曾经遇到过测试环境PDF文件预览正常但是生产环境部分过大PDF文件无法打开的问题，后查看请求发现GET请求的range范围为0-65535，判断可能是服务器对 Range 进行了某些特殊限制。 而PDF.js 插件里有一项参数是可以禁止使用Range的，在viewer.js defaultOptions 对象里，如下： 123456var defaultOptions = &#123; disableRange: &#123; value: false, kind: OptionKind.API &#125;&#125; 我们将属性设置为true得以解决问题。（如果不生效，可能是map文件存在的缘故，需要重新生成viewer.js.map文件） PDF无法正常显示 如果相对路径无法加载，可尝试网络路径。 如果使用的是网络路径扔无法加载，可以对网络路径进行encode编码在返回给前端调用。 如果仍不可以，可以考虑使用流传输的方式。 关于PDF.js 插件的缓存问题 使用PDF.js插件过程中，你会发现它会缓存看过的PDF的阅读位置，这本是一项人性化的设定，但如果你就是想每次打开PDF文件后从头开始看起，请使用 disableHistory 参数。 它也在defaultOptions对象里，默认false，改为true后每次再打开这个PDF文件时就会从第一页看起。 viewer.js defaultOptions 对象 可以看到PDF.js 插件 defaultOptions 对象里还有很多很多的默认参数，我们都是可以对其进行设置以实现相关功能或者禁用相关功能的。 关于它们的用法，可以参考一些相关文档，这儿就不做过多叙述了。 结语今天主要介绍了PDF.js 插件的使用，这是一款非常优秀的插件，在WebApp 预览PDF文件时经常会被使用，而且 Mozilla 以后有意将该插件集成到 FireFox 浏览器里，并且通过该插件立志于打造一项Web浏览PDF文件的标准。 让我们拭目以待吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>PDF.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（3）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%EF%BC%883%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言接 搭建自己的Hexo博客（2） 这篇文章，时隔一段时间，我对自己的博客陆陆续续又进行了部分优化，特地总结了下。 正文图片存储问题原来blog的图片也随静态页面托管在Github Pages 上的，后来页面访问加载速度很慢，而且图片本来较大，上传至Github上也是比较费劲的。 就考虑到将其存储到OSS服务上，由于我的域名没有经过网络备案，因此无法开启CDN服务，开启CDN服务后可以进一步提高图片加载速度。 我的域名在阿里云申请的，进而选择了阿里云的OSS服务。 PS：当然，对于阿里云，CDN服务和OSS服务都是要收取一定费用的。 我们登陆阿里云，选择对象存储OSS服务，如未开通请选择开通，而后新建一个我们blog的Bucket，找到文件管理选项，选择上传文件，上传我们的blog图片，上传成功后，可以通过复制图片URL的方式拿到图片地址，将博文中的图片地址指向此路径即可。 因为上传到OSS图片并在blog中使用后，图片的地址会被暴露在网页上，为了防止图片地址被盗用，我们可以设置只允许我们的域名可以访问图片，以进一步确保安全，如下： 添加相册功能我们为blog添加相册功能，进一步美化blog。 我们可以为相册增加一个标签，如图所示，点击进入到相册页面。 在 next 主题的config文件里添加相册(photos)标签，如下： 1234567891011menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat support: /support/ || support photos: /photos/ || photo PS: 如果不显示中文（相册），只显示photos，需要在next主题里的languages/zh-Hans.yml 里配置 photos: 相册。 添加完标签后，回到hexo的source文件夹，在里面新建photos文件夹，并在该文件夹里新建index.md 文件，内容如下： 1234567title: 相册comments: falsetype: photosnoDate: true---&lt;div class="ImageGrid"&gt;&lt;/div&gt; ImageGrid的div即用来存放相册的一个div。 我们在博客的根目录下新建photosTool.js，如下： 123456789101112131415161718192021222324252627282930313233343536"use strict";const fs = require("fs");const sizeOf = require('image-size');const path = "source/images_backup/photos";const output = "source/photos/photos.json";var dimensions;fs.readdir(path, function(err, files) &#123; if (err) &#123; return; &#125; let arr = []; (function iterator(index) &#123; if (index == files.length) &#123; fs.writeFile(output, JSON.stringify(arr, null, "\t")); return; &#125; fs.stat(path + "/" + files[index], function(err, stats) &#123; if (!files[index].endsWith(".jpg") || files[index].endsWith(".jpeg")) &#123; &#125; if (err) &#123; return; &#125; if (stats.isFile()) &#123; if (files[index].endsWith(".jpg") || files[index].endsWith(".jpeg") || files[index].endsWith(".mp4") || files[index].endsWith(".png") || files[index].endsWith(".gif")) &#123; dimensions = sizeOf(path + "/" + files[index]); console.log(dimensions.width, dimensions.height); arr.push(dimensions.width + '.' + dimensions.height + ' ' + files[index]); &#125; &#125; iterator(index + 1); &#125;) &#125;(0));&#125;); 这个js用于获取图片的长和宽，以便可以获得更好的展示效果，同时，她会生成一个photos.json的文件，里面保存这图片的长和宽，如下： 123456789101112131415161718192021[ photo-1-1.jpg, "3200.2000 photo-10.jpg", "1920.1200 photo-11.jpg", "1920.1080 photo-12.jpg", "1920.1200 photo-13.jpg", "1920.1080 photo-14.jpg", "1920.1080 photo-15.jpg", "1920.1200 photo-16.jpg", "1920.1080 photo-17.jpg", "1920.1186 photo-18.jpg", "1920.1080 photo-19.jpg", "1024.610 photo-2.jpg", "2200.1400 photo-3.jpg", "1200.750 photo-4.jpg", "3500.1898 photo-5.jpg", "1080.1920 photo-6.jpg", "1600.1120 photo-7.jpg", "1728.1080 photo-8.jpg", "1920.1080 photo-9.jpg"] 这个文件我生成在了photos文件夹下。 可以看到它们的图片是从source/images_backup/photos路径下获取的。 为提高相应速度，需要将这些图片放到阿里云OSS上，存放到上面后，我们需要一个photos.js来解析图片并展示。 找到themes/next/source/js/src文件夹，新建photos.js文件，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647photo = &#123; page: 1, offset: 20, init: function() &#123; var that = this; $.getJSON("/photos/photos.json", function(data) &#123; that.render(that.page, data); //that.scroll(data); &#125;); &#125;, render: function(page, data) &#123; var begin = (page - 1) * this.offset; var end = page * this.offset; if (begin &gt;= data.length) return; var html, imgNameWithPattern, imgName, imageSize, imageX, imageY, li = ""; for (var i = begin; i &lt; end &amp;&amp; i &lt; data.length; i++) &#123; imgNameWithPattern = data[i].split(' ')[1]; imgName = imgNameWithPattern.split('.')[0] imageSize = data[i].split(' ')[0]; imageX = imageSize.split('.')[0]; imageY = imageSize.split('.')[1]; li += '&lt;div class="card" style="width:330px"&gt;' + '&lt;div class="ImageInCard" style="height:' + 330 * imageY / imageX + 'px"&gt;' + '&lt;a data-fancybox="gallery" href="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + imgNameWithPattern + '?raw=true" data-caption="' + imgName + '"&gt;' + '&lt;img src="http://sakuratears.oss-cn-beijing.aliyuncs.com/blog/photos/' + imgNameWithPattern + '?raw=true"/&gt;' + '&lt;/a&gt;' + '&lt;/div&gt;' + // '&lt;div class="TextInCard"&gt;' + imgName + '&lt;/div&gt;' + '&lt;/div&gt;' &#125; $(".ImageGrid").append(li); $(".ImageGrid").lazyload(); this.minigrid(); &#125;, minigrid: function() &#123; var grid = new Minigrid(&#123; container: '.ImageGrid', item: '.card', gutter: 12 &#125;); grid.mount(); $(window).resize(function() &#123; grid.mount(); &#125;); &#125;&#125;photo.init(); 可以看到，我们主要是解析原来生成的photos.json文件，获得长和宽，并将其放到card框里，同时图片是通过OSS获取的。 要使该js生效，需要被引用并加载，打开themes/next/layout/_scripts/commons.swig文件，添加photos.js的引用，如下： 123456789101112131415161718192021&#123;% if page.type ===&apos;photos&apos; %&#125;&#123;% set js_commons = [ &apos;src/utils.js&apos;, &apos;src/motion.js&apos;, &apos;src/minigrid.min.js&apos;, &apos;src/photos.js&apos; ]%&#125;&#123;% else %&#125;&#123;% set js_commons = [ &apos;src/utils.js&apos;, &apos;src/motion.js&apos; ]%&#125;&#123;% endif %&#125;&#123;% for common in js_commons %&#125; &lt;script type=&quot;text/javascript&quot; src=&quot;&#123;&#123; url_for(theme.js) &#125;&#125;/&#123;&#123; common &#125;&#125;?v=&#123;&#123; theme.version &#125;&#125;&quot;&gt;&lt;/script&gt;&#123;% endfor %&#125; PS:一些自己想实现的其它功能，如果有新的js文件，也可以做如下的添加方式，另使photos.js生效不仅仅只可以添加到这儿，也可以通过其它的地方引入，考虑到以后维护的方便，我们可以统一添加到了这里。 可以为相册相框添加缩放效果及一些其它效果，我的相框的css样式如下： 12345678910111213141516171819202122232425262728293031323334353637383940//图片鼠标靠近放大效果.fancybox img &#123; display: block; border: 0; width: 100%; transform: scale(1); transition: all 1s ease 0s; -webkit-transform: scale(1); -webkit-transform: all 1s ease 0s;&#125; .fancybox:hover img &#123; transform: scale(1.1); transition: all 1s ease 0s; -webkit-transform: scale(1.1); -webkit-transform: all 1s ease 0s;&#125;.ImageGrid &#123; width: 100%; max-width: 1040px; margin: 0 auto; text-align: center;&#125;.card &#123; overflow: hidden; transition: .3s ease-in-out; border-radius: 8px; background-color: #ddd;&#125;.ImageInCard img &#123; padding: 0 0 0 0;&#125;.TextInCard &#123; line-height: 54px; background-color: #ffffff; font-size: 24px;&#125; 这个css内容我们添加到themes/next/source/css/_custom/custom.styl文件夹里。 最终效果如下： SEO 相关优化这一块的优化还是蛮多的，我挑几个比较重要的说下。 我们知道，我们的blog有了域名后，相当于发布在了外网，为了提高blog阅读量，需要对blog或者其它方面做些工作，这都可以称谓博客的SEO优化。 编写质量高的原创博文这一条基本不用多说，好的文章从不缺少流量。关键在于作者提高自身的写作水平。 更改博文生成结构使用hexo生成博文时，博文的生成方式(permalink)为blog/:title:year:month:day/这种格式，这种是不便于搜索的，因为相当于四级目录，我们可以改为blog/:title.html这种格式。 在hexo 的 config.yml文件里进行修改，并重新运行命令生成博文。 1234url: https://www.sakuratears.toproot: /permalink: blog/:title.html ## SEO优化permalink_defaults: 禁止外链我们在写作博文时，有可能引用到其它博文的文章，会有指向其它博文的链接，这样是不便于SEO的，我们又不能去掉该外链而影响到博文功能。因此我们可以引入 hexo-autonofollow 插件，在package.json文件里配置并安装该插件后，在hexo的config.yml文件里添加如下配置。 12345## nofollow SEO 优化nofollow: enable: true exclude: # 除外的链接 - ‘https://www.sakuratears.top’ 可以使网页爬虫更好的获取我们的页面数据，提高SEO。 人为发布博客外链这个就比较简单了，我们可以在一些平台等地方发布我们的博客地址，提高曝光度，提高SEO。 使百度、谷歌、必应等网站收录你的网站这项是一项漫长而又复杂的过程，但也是效果最明显的一个。 我们如果有百度账号的话，可以登陆百度站长平台，将我们的站点添加百度收录。 如果网站已备案，添加备案号可以加快收录速度。 我们可以在链接提交-自动提交-sitemap里添加我们网站的sitemap.xml并提交，以加快百度对我们网站的收录。 同时在自动推送里，我们看到我们可以使用js进行自动推送，由于hexo已经集成了该功能，我们简单设置下就行。 12345678910111213# ---------------------------------------------------------------# SEO Settings# ---------------------------------------------------------------# Canonical, set a canonical link tag in your hexo, you could use it for your SEO of blog.# See: https://support.google.com/webmasters/answer/139066# Tips: Before you open this tag, remember set up your URL in hexo _config.yml ( ex. url: http://yourdomain.com )canonical: true# Change headers hierarchy on site-subtitle (will be main site description) and on all post/pages titles for better SEO-optimization.seo: true# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true 可以看到hexo里面集成的baidu_push.swig内容和百度提供的js是一样的。 12345678910111213141516&#123;% if theme.baidu_push %&#125;&lt;script&gt;(function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);&#125;)();&lt;/script&gt;&#123;% endif %&#125; 我们可以使用手动抓取功能对我们的网站进行抓取，如下效果图。 PS：由于网络等一些原因，有可能抓取失败。 弄好后，过一段时间，可以看到数据的变化。 我们可以通过 site:www.sakuratears.top来检查百度是否收录了我们的网站，如图。 PS:我在bing上进行测试，发现必应收录的比百度要快。 后来查询了解得知Github屏蔽掉了百度爬虫，所以一些blog作者在国内托管到Coding或者Gitee上，在国外托管到GitHub上，以加快博客访问速度，提高SEO优化。 有关这一块内容，我准备后面在处理。 结语今天差不多就这些了，关于博客的一些其它优化内容我会在整理一下。 一些其它的想法: 原来blog的音乐播放器直接使用的网易云的，但是我们公司居然把网易云墙了，而且考虑到歌曲以后可能放在那边会404等情况，下阶段优化考虑使用Hexo的aplayer去处理，并将音乐放到OSS存储上。 关于托管到Coding或者Gitee上的问题，也要研究下。 每次看博客是都会出现live2D妹子，可能并不是所有人都喜欢，也有时候觉得碍事，可能考虑新增一个关闭按钮，可以主动开启或关闭live2D妹子。 部分页面需要美化下，SEO的部分还要看看有没有需要总结的，毕竟SEO优化是条漫长的道路。 以上。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka简介及简单使用]]></title>
    <url>%2Fblog%2FEureka%E7%AE%80%E4%BB%8B%E5%8F%8A%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言Eureka是Spring Cloud Netflix微服务套件中的一部分，一般与SpringBoot构建的微服务进行整合。它基于Netflix Eureka做了二次封装，主要负责完成微服务架构中的服务治理功能。 服务治理可以说是微服务架构中最为核心和基础的模块。服务治理通常包含服务注册和服务发现两部分。 Eureka包含客户端和服务端部分。 Eureka服务端，我们也称为服务注册中心。它支持高可用配置。它依托于强一致性提供良好的服务实例可用性，可以应对多种不同的故障场景。如果Eureka以集群模式部署，当集群中有分片出现故障时，那么Eureka就转入自我保护模式。它允许在分片故障期间继续提供服务的发现和注册，当故障分片恢复运行时，集群中的其他分片就会把它们的状态再次同步回来。不同可用区域的服务注册中心通过异步模式互相复制各自的状态，这意味着在任意给定的时间点每个实例关于所有服务的状态是有细微差别的。 Eureka客户端，主要处理服务的注册与发现。客户端服务通过注解和参数配置的方式，嵌在客户端应用程序的代码中，在应用程序运行时，Eureka客户端向注册中心注册自身提供的服务并周期性的发送心跳来更新它的服务租约。同时，它也能从服务端查询当前注册的服务信息并把它们缓存到本地并进行周期性的刷新服务状态。 正文我们今天先简单通过例子了解一下Eureka，后面在对其进行一些深入研究。 Eureka服务端我们创建一个Eureka服务端，如下： 新建SpringBoot项目，命名为eureka-server 选择Eureka Server依赖 Eureka Server 的配置 首先我们需要启用EurekaServer，如下： 12345678@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 在application.properties里进行相关配置，如下： 123456789101112131415161718# 应用名称spring.application.name=eureka-server# 实例端口server.port=8001# server地址server.address=172.30.13.173# 实例名称eureka.instance.hostname=test1# 实例IDeureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125;# 是否向注册中心注册自己eureka.client.register-with-eureka=false# 是否需要检索服务eureka.client.fetch-registry=false# 使用IP地址定义主机名eureka.instance.prefer-ip-address=true# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/ 我们启动EurekaServer，通过 http://172.30.13.173:8001/ 进行访问，可以看到如下图，Eureka注册中心中尚未有可以使用的实例。 Eureka 客户端我们再新创建一个项目sakura-service，并为其添加web模块和eureka-client模块，以使其注册到eureka中心上。 启用服务发现，@EnableDiscoveryClient。 123456789@EnableDiscoveryClient@SpringBootApplicationpublic class SakuraServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SakuraServiceApplication.class, args); &#125;&#125; application.properties进行如下相关配置。 123456# 实例端口server.port=9001# 应用名称spring.application.name=sakura-service# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/ 我们在该项目中新建一个SakuraController类，如下： 1234567891011121314151617181920212223242526@RestControllerpublic class SakuraController &#123; // 服务注册 @Qualifier("eurekaRegistration") @Autowired private Registration registration; @Autowired private DiscoveryClient client; @RequestMapping(value = "/hello",method = RequestMethod.GET) public String index()&#123; ServiceInstance instance = serviceInstance(); System.out.println("Host:"+instance.getHost()+";ServiceID:"+instance.getServiceId()); return "Hello World"; &#125; public ServiceInstance serviceInstance() &#123; List&lt;ServiceInstance&gt; list = client.getInstances(registration.getServiceId()); if (list != null &amp;&amp; list.size() &gt; 0) &#123; return list.stream().findAny().get(); &#125; return null; &#125;&#125; 在启动此项目，可以看到此项目已被注册到了注册中心上。 我们在创建一个服务消费者用于消费刚才我们创建的服务Service。 需要添加web模块，eureka-client模块和ribbon模块，Ribbon是一个基于HTTP和TCP的客户端负载均衡器，它可以通过客户端配置的ribbonServerList服务端列表去轮询达到负载均衡的目的，和Eureka联合使用时，Ribbon会从Eureka的注册中心获取服务列表去轮询。关于Ribbon的内容，我们后面在研究。 启用服务发现，@EnableDiscoveryClient。并添加RestTemplate，如下： 123456789101112@EnableDiscoveryClient@SpringBootApplicationpublic class SakuraConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SakuraConsumerApplication.class, args); &#125; @Bean @LoadBalanced RestTemplate restTemplate()&#123; return new RestTemplate(); &#125;&#125; application.properties进行如下相关配置。 123456# 实例端口server.port=7001# 应用名称spring.application.name=sakura-consumer# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/ 我们在项目中新建SakuraController用于对sakura-service的hello服务进行调用，如下： 12345678910@RestControllerpublic class SakuraController &#123; @Autowired RestTemplate restTemplate; @RequestMapping(value = "/consumer",method = RequestMethod.GET) public String hello()&#123; return restTemplate.getForEntity("http://SAKURA-SERVICE/hello",String.class).getBody(); &#125;&#125; 启动此项目，可以发现此项目也被注册到了Eureka注册中心。 我们尝试访问消费者，http://172.30.13.173:7001/consumer 可以看到打印了 sakura-service的 Hello World 字样，同时 sakura-service项目日志中输出相关信息。 1Host:DELL-3020-PC;ServiceID:SAKURA-SERVICE 高可用Eureka注册中心上面的例子，当我们关掉Eureka服务端时，可以发现两个项目之间已经无法进行相互调用了。因此，单节点的服务注册中心是不合理的。 所以作为注册中心，应具备高可用性的特征。 Eureka Server在设计的一开始就考虑了高可用的问题，在Eureka的服务治理设计中，所有节点既是服务提供方，也是服务消费方，服务注册中心也不例外。 我们在配置单节点的注册中心时，设置过如下两个参数，让服务注册中心不注册自己。 1234# 是否向注册中心注册自己eureka.client.register-with-eureka=false# 是否需要检索服务eureka.client.fetch-registry=false Eureka Server 的高可用实际上就是将自己作为服务向其他服务注册中心注册自己，这样就形成一组相互注册的服务注册中心，以实现服务清单的相互同步，达到高可用的效果。 下面我们来尝试搭建高可用的服务注册中心集群，我们构建一个三节点的服务注册中心集群。 在搭建之前，我们需要知道SpringBoot是支持多环境配置的，其命名格式需要满足 application-{profile}.properties的格式，其中{profile}对应环境标识，如下： application-dev.properties: 开发环境 application-test.properties: 测试环境 application-prod.properties: 生产环境 至于要加载那个配置文件，需要在application.properties 文件中通过spring.profiles.active 属性来设置，spring.profiles.active=dev 就会加载 application-dev.properties配置。 另外我们在启动项目时，可以通过 java -jar xxxxx.jar –spring.profiles.active = test 来指定使用哪个配置。 根据上面所述，我们可以在原来eureka-server项目里创建3份properties文件，如下： application-test1.properties 123456789101112131415161718# 应用名称spring.application.name=eureka-server# 实例端口server.port=8001# server地址server.address=172.30.13.173# 实例名称eureka.instance.hostname=test1# 实例IDeureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125;# 是否向注册中心注册自己eureka.client.register-with-eureka=true# 是否需要检索服务eureka.client.fetch-registry=true# 使用IP地址定义主机名eureka.instance.prefer-ip-address=true# eureka服务地址eureka.client.service-url.defaultZone = http://172.30.13.173:8002/eureka/,http://172.30.13.173:8003/eureka/ application-test2.properties 123456789101112131415161718# 应用名称spring.application.name=eureka-server# 实例端口server.port=8002# server地址server.address=172.30.13.173# 实例名称eureka.instance.hostname=test2# 实例IDeureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125;# 是否向注册中心注册自己eureka.client.register-with-eureka=true# 是否需要检索服务eureka.client.fetch-registry=true# 使用IP地址定义主机名eureka.instance.prefer-ip-address=true# eureka服务地址eureka.client.service-url.defaultZone=http://172.30.13.173:8001/eureka/,http://172.30.13.173:8003/eureka/ application-test3.properties 123456789101112131415161718# 应用名称 spring.application.name=eureka-server # 实例端口 server.port=8003 # server地址 server.address=172.30.13.173 # 实例名称 eureka.instance.hostname=test3 # 实例ID eureka.instance.instance-id=$&#123;server.address&#125;:$&#123;server.port&#125; # 是否向注册中心注册自己 eureka.client.register-with-eureka=true # 是否需要检索服务 eureka.client.fetch-registry=true # 使用IP地址定义主机名 eureka.instance.prefer-ip-address=true # eureka服务地址 eureka.client.service-url.defaultZone=http://172.30.13.173:8002/eureka/,http://172.30.13.173:8001/eureka/ 可以看到我们在一台机器上使用了3个不同端口（8001，8002，8003）来搭建了一个Eureka服务注册中心集群。 PS: application.properties 里可以配置 spring.profiles.active=test1 让其默认使用 test1的配置。 我们使用Maven 打包生成 eureka-server-1.0.0-SNAPSHOT.jar 包，并使用 java -jar eureka-server-1.0.0-SNAPSHOT.jar –spring.profiles.active=test1 启动test1 ,相同的方式启动test2和test3。 完成后访问eureka注册中心界面（http://172.30.13.173:8001/ 或者 http://172.30.13.173:8002/ 或者 http://172.30.13.173:8003/），如图： 可以看到Eureka形成了一组相互注册的服务注册中心。 那它的高可用性能被保证吗？ 我们来测试下，我们根据上面所述，对sakura-service做成两个微服务，如下： sakura-service项目的配置文件 application-test1.properties 123server.port=9001spring.application.name=sakura-serviceeureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/,http://172.30.13.173:8002/eureka/,http://172.30.13.173:8003/eureka/ application-test2.properties 123server.port=9002spring.application.name=sakura-serviceeureka.client.service-url.defaultZone = http://172.30.13.173:8001/eureka/,http://172.30.13.173:8002/eureka/,http://172.30.13.173:8003/eureka/ 启动它们，可以看到服务注册中心已经有了它们的实例。 我们启动消费者项目sakura-consumer将其也注册到注册中心。 访问 http://172.30.13.173:7001/consumer 可以看到两个注册的sakura-service交替输出信息，这也是使用ribbon可以做到负载均衡的体现。 这时候我们停止掉 eureka-server 的test1 ，可以看到 使用consumer调用 sakura-service的服务仍然正常。 可以看到test2和test3仍然正常进行服务，我们也可以继续停掉test2，可以看到test3正常服务，保证注册中心的服务正常进行。 总结今天我们研究了Eureka注册中心的使用及如何创建一个高可用的注册中心，后面会结合Eureka的源码详细了解Eureka使用及特点。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
        <tag>Eureka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql数据库知识点总结(1)]]></title>
    <url>%2Fblog%2FMySql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93(1).html</url>
    <content type="text"><![CDATA[前言最近在使用MySql数据库过程中，遇到了一些问题，并抽时间总结了下来，也是结合Java语言和MyBatis的方方面面，在此分享给大家。 正文中文姓名排序问题比如一张有中文姓名的表，我们要对中文姓名按拼音进行排序，在没有姓名拼音字段（插入姓名时顺带插入了姓名拼音）的帮助下，可以使用如下语法。 如果MySql的排序字段采用的GBK字符集，那可以直接使用 order by ‘字段名’ asc 语法。 如果MySql的排序字段采用的不是GBK字符集，那么可以使用如下方法。 1select * from '表名称' order by convert('字段名' using gbk) asc; 使用该语法，姓名字段名无法使用索引（如果有的话），需要注意。 多个字段去重并统计数量这个问题还是比较常见的，比如有一张用户表，有用户姓名 user_name 用户身份证号 id_card_no 等字段，现在我们要把姓名和身份证号一致的认为一个用户，其它情况均为两个用户，则可以使用以下语法。 可以使用DISTINCT，虽然DISTINCT只能去重一行，但是可以使用字段合并功能来进行处理。 1select count(DISTINCT(c.user_name + IFNULL(c.id_card_no,''))) from user c where c.invite_id = 'xxxxx'; 注意： 两列的类型最好为varchar类型，其他类型不推荐使用此方法，比如char和int相加，会得到意想不到的结果从而产生问题。 使用GROUP BY语句，需要有子查询。 1select count(*) from (select * from user u where u.invite_id = 'xxxxx' GROUP BY u.user_name,IFNULL(u.id_card_no,'')) a ; 注意：子查询得到的表必须有别名，不然SQL语句报错。 查询条件字段有多个条件的查询这个问题是这样，比如我们有一群用户在user表，他们有个字段表示所属平台platform，比如有A、B、C、D等几种平台，现在要查询所属平台为A或B平台的用户。 我们可以明显看到用in即可以解决。在MyBatis里，对于这个字段，则需要传入一个List形式的数组，并在xml文件里通过循环赋值给SQL语句进行查询，也是比较常用的方法。 123456&lt;select id="findUser" parameterType="com.xxx.NewUser"&gt; select * from user u where u.invite_id= #&#123;condition.inviteID&#125; and u.platform in &lt;foreach collection="list" item="condition.platform" index="index" open="(" close=")" separator=","&gt; #&#123;condition.platform&#125; &lt;/foreach&gt;&lt;/select&gt; 上面的NewUser对象里的platform字段是个List，最后组成的SQL语句如下。 1select * from user u where u.invite_id= 'xxxxxx' and u.platform in ('A','B'); 我们还可以利用FIND_IN_SET函数，同时在MyBatis里传入的参数直接为String即可，但是A、B平台需要用逗号隔开。 123&lt;select id="findUser" parameterType="com.xxx.User"&gt; select * from user u where u.invite_id= #&#123;condition.inviteID&#125; and FIND_IN_SET(u.platform,#&#123;condition.platform&#125;)&lt;/select&gt; 上面的User对象里的platform字段是个String，平台之间用逗号分割，最后SQL如下。 1select * from user u where u.invite_id= 'xxxxxx' and FIND_IN_SET(u.platform,'A,B'); 这儿需要注意的一点是，FIND_IN_SET无法使用u.platform字段的索引。 虽然无法使用索引，但如果SQL语句本身其它条件索引检索后数据量不大，或者被FIND_IN_SET的字段没有索引，也是可以使用的。 如果传入Mybatis里的实体类字段本身就是逗号分割的，我们在拆成一个一个数据最后再使用in也是非常费事的，可以使用FIND_IN_SET轻松搞定。 使用程序对MySql批量处理数据我们知道，对于MySql批量处理数据，我们可以使用MyBatis的批量处理方法。 大致如下： 1234567&lt;insert id="insertBatch" parameterType="java.util.List"&gt; insert into student (id, stu_no, stu_name, stu_sex, stu_birthday, stu_class) values &lt;foreach collection="list" item="item" index="index" separator=","&gt; (#&#123;item.id,jdbcType=INTEGER&#125;, #&#123;item.stuNo,jdbcType=VARCHAR&#125;, #&#123;item.stuName,jdbcType=VARCHAR&#125;, #&#123;item.stuSex,jdbcType=CHAR&#125;, #&#123;item.stuBirthday,jdbcType=DATE&#125;, #&#123;item.stuClass,jdbcType=CHAR&#125;) &lt;/foreach&gt;&lt;/insert&gt; 这种方法比普通的程序循环一条条插入要快很多，下面是一个例子。 1234567891011121314151617181920212223242526272829303132333435@Testpublic void test3() throws Exception&#123; Thread.sleep(2000); long start1 =System.currentTimeMillis(); for(int i=0;i&lt;1000;i++)&#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i+""); studentModel.setStuClass("1"); studentModel.setStuNo(i+""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); studentMapper.insert(studentModel); &#125; long end1 =System.currentTimeMillis(); System.out.println(end1-start1); Thread.sleep(2000); long start2 =System.currentTimeMillis(); List&lt;StudentModel&gt; list3 = new ArrayList&lt;&gt;(); for(int i=1000;i&lt;2000;i++)&#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i+""); studentModel.setStuClass("1"); studentModel.setStuNo(i+""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); list3.add(studentModel); &#125; studentMapper.insertBatch(list3); long end2 =System.currentTimeMillis(); System.out.println(end2-start2);&#125; 运行结果： 12345672019-02-25 14:46:14.851 INFO 9360 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2019-02-25 14:46:15.031 INFO 9360 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed.1206012602019-02-25 14:48:17.666 INFO 9360 --- [ Thread-2] o.s.w.c.s.GenericWebApplicationContext : Closing org.springframework.web.context.support.GenericWebApplicationContext@2ddc9a9f: startup date [Mon Feb 25 14:46:09 CST 2019]; root of context hierarchy2019-02-25 14:48:17.671 INFO 9360 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...2019-02-25 14:48:17.675 INFO 9360 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. 可以看到批处理要比单条处理快了N倍，这儿不再过多介绍。 Tips：如果数据量过大，如100w，1000w等，可以将数据分成多份循环去批处理插入。 关于批量的处理的另外一点思考： 当我们需要对多张表进行批量处理时，如果想提高运行效率，可以考虑使用多线程处理，比如下面的例子。 我们对4张表插入1w数据，正常的处理逻辑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Testpublic void test4() throws Exception &#123; Thread.sleep(2000); long start1 = System.currentTimeMillis(); List&lt;ScoreModel&gt; list1 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; ScoreModel scoreModel = new ScoreModel(); scoreModel.setId(i); scoreModel.setScore(new BigDecimal(100)); scoreModel.setCouNo(i + ""); scoreModel.setStuNo(i + ""); list1.add(scoreModel); &#125; scoreMapper.insertBatch(list1); List&lt;TeacherModel&gt; list2 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; TeacherModel teacherModel = new TeacherModel(); teacherModel.setId(i); teacherModel.setTeachNo(i + ""); teacherModel.setTeachName(i + ""); teacherModel.setTeachBirthday(new Date()); teacherModel.setTeachDepart(i + ""); teacherModel.setTeachSex("M"); teacherModel.setTeachProf(i + ""); list2.add(teacherModel); &#125; teacherMapper.insertBatch(list2); List&lt;StudentModel&gt; list3 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i + ""); studentModel.setStuClass("1"); studentModel.setStuNo(i + ""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); list3.add(studentModel); &#125; studentMapper.insertBatch(list3); List&lt;CourseModel&gt; list4 = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10000; i++) &#123; CourseModel courseModel = new CourseModel(); courseModel.setId(i); courseModel.setCouName(i + ""); courseModel.setCouNo(i + ""); courseModel.setTeachNo(i + ""); list4.add(courseModel); &#125; courseMapper.insertBatch(list4); long end1 = System.currentTimeMillis(); System.out.println(end1 - start1);&#125; 运行结果： 1234562019-02-25 15:01:23.688 INFO 9576 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2019-02-25 15:01:23.850 INFO 9576 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed.43482019-02-25 15:01:27.689 INFO 9576 --- [ Thread-2] o.s.w.c.s.GenericWebApplicationContext : Closing org.springframework.web.context.support.GenericWebApplicationContext@298a5e20: startup date [Mon Feb 25 15:01:18 CST 2019]; root of context hierarchy2019-02-25 15:01:27.691 INFO 9576 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...2019-02-25 15:01:27.700 INFO 9576 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. 我们用线程池对四张表分别进行批量操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Testpublic void test2() throws Exception&#123; Thread.sleep(2000); long start1 =System.currentTimeMillis(); ExecutorService executorService = Executors.newFixedThreadPool(4); Future&lt;Boolean&gt; future1= executorService.submit(()-&gt;&#123; List&lt;ScoreModel&gt; list1 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; ScoreModel scoreModel =new ScoreModel(); scoreModel.setId(i); scoreModel.setScore(new BigDecimal(100)); scoreModel.setCouNo(i+""); scoreModel.setStuNo(i+""); list1.add(scoreModel); &#125; scoreMapper.insertBatch(list1); return true; &#125;); Future&lt;Boolean&gt; future2= executorService.submit(()-&gt;&#123; List&lt;TeacherModel&gt; list2 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; TeacherModel teacherModel =new TeacherModel(); teacherModel.setId(i); teacherModel.setTeachNo(i+""); teacherModel.setTeachName(i+""); teacherModel.setTeachBirthday(new Date()); teacherModel.setTeachDepart(i+""); teacherModel.setTeachSex("M"); teacherModel.setTeachProf(i+""); list2.add(teacherModel); &#125; teacherMapper.insertBatch(list2); return true; &#125;); Future&lt;Boolean&gt; future3= executorService.submit(()-&gt;&#123; List&lt;StudentModel&gt; list3 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; StudentModel studentModel = new StudentModel(); studentModel.setId(i); studentModel.setStuName(i+""); studentModel.setStuClass("1"); studentModel.setStuNo(i+""); studentModel.setStuBirthday(new Date()); studentModel.setStuSex("M"); list3.add(studentModel); &#125; studentMapper.insertBatch(list3); return true; &#125;); Future&lt;Boolean&gt; future4= executorService.submit(()-&gt;&#123; List&lt;CourseModel&gt; list4 = new ArrayList&lt;&gt;(); for(int i=0;i&lt;10000;i++)&#123; CourseModel courseModel = new CourseModel(); courseModel.setId(i); courseModel.setCouName(i+""); courseModel.setCouNo(i+""); courseModel.setTeachNo(i+""); list4.add(courseModel); &#125; courseMapper.insertBatch(list4); return true; &#125;); future1.get(); future2.get(); future3.get(); future4.get(); long end1 = System.currentTimeMillis(); System.out.println(end1-start1);&#125; 运行结果： 1234562019-02-25 15:04:46.623 INFO 8284 --- [pool-1-thread-3] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2019-02-25 15:04:47.084 INFO 8284 --- [pool-1-thread-3] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed.19932019-02-25 15:04:47.986 INFO 8284 --- [ Thread-2] o.s.w.c.s.GenericWebApplicationContext : Closing org.springframework.web.context.support.GenericWebApplicationContext@298a5e20: startup date [Mon Feb 25 15:04:40 CST 2019]; root of context hierarchy2019-02-25 15:04:47.990 INFO 8284 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated...2019-02-25 15:04:48.004 INFO 8284 --- [ Thread-2] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. 可以看到速度提高了1倍，当数据量更大时，提高效果更明显。 注意 对于上面多线程多表批量插入，如果需要保证事务，单独的每张表的事务是可以保证的（将每张表的批量插入提出来，形成一个方法，并加上事务属性，如果有错误就会回滚），但是如果要同时保证4张表的事务（这几张表要么全成功，要么全不成功），是无法满足的，由于多线程的特殊性。所以这种情况下请使用第一种同步方法，并加上事务，才能保证4张表批处理要么全成功，要么全不成功。 对于一张表的批处理，如果数据量过大时，可以使用多线程同时插入这一张表吗？ 答案是否定的，对于MySQL InnoDB数据库，默认是行锁，前提条件是建立在索引之上的。如果筛选条件没有建立索引，会降级到表锁。即如果where条件中的字段都加了索引，则加的是行锁；否则加的是表锁。 当为表锁时，瓶颈在数据库，多线程是无法提高对同一张表的插入效率的； 当为行锁时，看起来可以insert A行时同时insert B行，确实可以提高效率，但有数据冲突的错误情况，一般也不会使用。 结语我还会在工作学习过程中不断总结，此文章类型也会不断更新，今天就先到这里吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis命令使用参考手册]]></title>
    <url>%2Fblog%2FRedis%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E5%8F%82%E8%80%83%E6%89%8B%E5%86%8C.html</url>
    <content type="text"><![CDATA[前言我们来详细了解下Redis，及其一些命令的具体使用方法，并学会如何使用 Redis 的事务、持久化、复制、Sentinel、集群等功能。 主要命令我们知道Redis支持五种数据类型：string（字符串），hash（哈希表），list（列表），set（集合）及zset(sorted set：有序集合)。先来看下它们的一些操作命令。 字符串SETSET key value [EX seconds] [PX milliseconds] [NX|XX]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将字符串值 value 关联到 key 。 如果 key 已经持有其他值， SET 就覆写旧值， 无视类型。 当 SET 命令对一个带有生存时间（TTL）的键进行设置之后， 该键原有的 TTL 将被清除。 可选参数从 Redis 2.6.12 版本开始， SET 命令的行为可以通过一系列参数来修改： EX seconds ： 将键的过期时间设置为 seconds 秒。 执行 SET key value EX seconds 的效果等同于执行 SETEX key seconds value 。 PX milliseconds ： 将键的过期时间设置为 milliseconds 毫秒。 执行 SET key value PX milliseconds 的效果等同于执行 PSETEX key milliseconds value 。 NX ： 只在键不存在时， 才对键进行设置操作。 执行 SET key value NX 的效果等同于执行 SETNX key value 。 XX ： 只在键已经存在时， 才对键进行设置操作。 Note 因为 SET 命令可以通过参数来实现 SETNX 、 SETEX 以及 PSETEX 命令的效果， 所以 Redis 将来的版本可能会移除并废弃 SETNX 、 SETEX 和 PSETEX 这三个命令。 返回值在 Redis 2.6.12 版本以前， SET 命令总是返回 OK 。 从 Redis 2.6.12 版本开始， SET 命令只在设置操作成功完成时才返回 OK ； 如果命令使用了 NX 或者 XX 选项， 但是因为条件没达到而造成设置操作未执行， 那么命令将返回空批量回复（NULL Bulk Reply）。 代码示例对不存在的键进行设置： 12345redis&gt; SET key "value"OKredis&gt; GET key"value" 对已存在的键进行设置：12345redis&gt; SET key "new-value"OKredis&gt; GET key"new-value" 使用 EX 选项：12345678redis&gt; SET key-with-expire-time "hello" EX 10086OKredis&gt; GET key-with-expire-time"hello"redis&gt; TTL key-with-expire-time(integer) 10069 使用 PX 选项：12345678redis&gt; SET key-with-pexpire-time "moto" PX 123321OKredis&gt; GET key-with-pexpire-time"moto"redis&gt; PTTL key-with-pexpire-time(integer) 111939 使用 NX 选项：1234567891011redis&gt; SET not-exists-key "value" NXOK # 键不存在，设置成功redis&gt; GET not-exists-key"value"redis&gt; SET not-exists-key "new-value" NX(nil) # 键已经存在，设置失败redis&gt; GEt not-exists-key"value" # 维持原值不变 使用 XX 选项：1234567891011121314redis&gt; EXISTS exists-key(integer) 0redis&gt; SET exists-key "value" XX(nil) # 因为键不存在，设置失败redis&gt; SET exists-key "value"OK # 先给键设置一个值redis&gt; SET exists-key "new-value" XXOK # 设置新值成功redis&gt; GET exists-key"new-value" SETNXSETNX key value可用版本： &gt;= 1.0.0 时间复杂度： O(1) 只在键 key 不存在的情况下， 将键 key 的值设置为 value 。 若键 key 已经存在， 则 SETNX 命令不做任何动作。 SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 返回值命令在设置成功时返回 1 ， 设置失败时返回 0 。 代码示例1234567891011redis&gt; EXISTS job # job 不存在(integer) 0redis&gt; SETNX job "programmer" # job 设置成功(integer) 1redis&gt; SETNX job "code-farmer" # 尝试覆盖 job ，失败(integer) 0redis&gt; GET job # 没有被覆盖"programmer" SETEXSETEX key seconds value可用版本： &gt;= 2.0.0 时间复杂度： O(1) 将键 key 的值设置为 value ， 并将键 key 的生存时间设置为 seconds 秒钟。 如果键 key 已经存在， 那么 SETEX 命令将覆盖已有的值。 SETEX 命令的效果和以下两个命令的效果类似： SET key value EXPIRE key seconds # 设置生存时间 SETEX 和这两个命令的不同之处在于 SETEX 是一个原子（atomic）操作， 它可以在同一时间内完成设置值和设置过期时间这两个操作， 因此 SETEX 命令在储存缓存的时候非常实用。 返回值命令在设置成功时返回 OK 。 当 seconds 参数不合法时， 命令将返回一个错误。 代码示例在键 key 不存在的情况下执行 SETEX ：12345678redis&gt; SETEX cache_user_id 60 10086OKredis&gt; GET cache_user_id # 值"10086"redis&gt; TTL cache_user_id # 剩余生存时间(integer) 49 键 key 已经存在， 使用 SETEX 覆盖旧值：1234567891011redis&gt; SET cd "timeless"OKredis&gt; SETEX cd 3000 "goodbye my love"OKredis&gt; GET cd"goodbye my love"redis&gt; TTL cd(integer) 2997 PSETEXPSETEX key milliseconds value可用版本： &gt;= 2.6.0 时间复杂度： O(1) 这个命令和 SETEX 命令相似， 但它以毫秒为单位设置 key 的生存时间， 而不是像 SETEX 命令那样以秒为单位进行设置。 返回值命令在设置成功时返回 OK 。 代码示例12345678redis&gt; PSETEX mykey 1000 "Hello"OKredis&gt; PTTL mykey(integer) 999redis&gt; GET mykey"Hello" GETGET key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回与键 key 相关联的字符串值。 返回值如果键 key 不存在， 那么返回特殊值 nil ； 否则， 返回键 key 的值。 如果键 key 的值并非字符串类型， 那么返回一个错误， 因为 GET 命令只能用于字符串值。 代码示例对不存在的键 key 或是字符串类型的键 key 执行 GET 命令：12345678redis&gt; GET db(nil)redis&gt; SET db redisOKredis&gt; GET db"redis" 对不是字符串类型的键 key 执行 GET 命令：12345678redis&gt; DEL db(integer) 1redis&gt; LPUSH db redis mongodb mysql(integer) 3redis&gt; GET db(error) ERR Operation against a key holding the wrong kind of value GETSETGETSET key value可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将键 key 的值设为 value ， 并返回键 key 在被设置之前的旧值。 返回值返回给定键 key 的旧值。 如果键 key 没有旧值， 也即是说， 键 key 在被设置之前并不存在， 那么命令返回 nil 。 当键 key 存在但不是字符串类型时， 命令返回一个错误。 代码示例1234567891011redis&gt; GETSET db mongodb # 没有旧值，返回 nil(nil)redis&gt; GET db"mongodb"redis&gt; GETSET db redis # 返回旧值 mongodb"mongodb"redis&gt; GET db"redis" STRLENSTRLEN key可用版本： &gt;= 2.2.0 复杂度： O(1) 返回键 key 储存的字符串值的长度。 返回值STRLEN 命令返回字符串值的长度。 当键 key 不存在时， 命令返回 0 。 当 key 储存的不是字符串值时， 返回一个错误。 代码示例获取字符串值的长度：12345redis&gt; SET mykey "Hello world"OKredis&gt; STRLEN mykey(integer) 11 不存在的键的长度为 0 ：12redis&gt; STRLEN nonexisting(integer) 0 APPENDAPPEND key value可用版本： &gt;= 2.0.0 时间复杂度： 平摊O(1) 如果键 key 已经存在并且它的值是一个字符串， APPEND 命令将把 value 追加到键 key 现有值的末尾。 如果 key 不存在， APPEND 就简单地将键 key 的值设为 value ， 就像执行 SET key value 一样。 返回值追加 value 之后， 键 key 的值的长度。 示例代码对不存在的 key 执行 APPEND ：12345redis&gt; EXISTS myphone # 确保 myphone 不存在(integer) 0redis&gt; APPEND myphone "nokia" # 对不存在的 key 进行 APPEND ，等同于 SET myphone "nokia"(integer) 5 # 字符长度 对已存在的字符串进行 APPEND ：12345redis&gt; APPEND myphone " - 1110" # 长度从 5 个字符增加到 12 个字符(integer) 12redis&gt; GET myphone"nokia - 1110" SETRANGESETRANGE key offset value可用版本： &gt;= 2.2.0 时间复杂度：对于长度较短的字符串，命令的平摊复杂度O(1)；对于长度较大的字符串，命令的复杂度为 O(M) ，其中 M 为 value 的长度。 从偏移量 offset 开始， 用 value 参数覆写(overwrite)键 key 储存的字符串值。 不存在的键 key 当作空白字符串处理。 SETRANGE 命令会确保字符串足够长以便将 value 设置到指定的偏移量上， 如果键 key 原来储存的字符串长度比偏移量小(比如字符串只有 5 个字符长，但你设置的 offset 是 10 )， 那么原字符和偏移量之间的空白将用零字节(zerobytes, “\x00” )进行填充。 因为 Redis 字符串的大小被限制在 512 兆(megabytes)以内， 所以用户能够使用的最大偏移量为 2^29-1(536870911) ， 如果你需要使用比这更大的空间， 请使用多个 key 。 Warning 当生成一个很长的字符串时， Redis 需要分配内存空间， 该操作有时候可能会造成服务器阻塞(block)。 在2010年出产的Macbook Pro上， 设置偏移量为 536870911(512MB 内存分配)将耗费约 300 毫秒， 设置偏移量为 134217728(128MB 内存分配)将耗费约 80 毫秒， 设置偏移量 33554432(32MB 内存分配)将耗费约 30 毫秒， 设置偏移量为 8388608(8MB 内存分配)将耗费约 8 毫秒。 返回值SETRANGE 命令会返回被修改之后， 字符串值的长度。 代码示例对非空字符串执行 SETRANGE 命令：12345678redis&gt; SET greeting "hello world"OKredis&gt; SETRANGE greeting 6 "Redis"(integer) 11redis&gt; GET greeting"hello Redis" 对空字符串/不存在的键执行 SETRANGE 命令：12345678redis&gt; EXISTS empty_string(integer) 0redis&gt; SETRANGE empty_string 5 "Redis!" # 对不存在的 key 使用 SETRANGE(integer) 11redis&gt; GET empty_string # 空白处被"\x00"填充"\x00\x00\x00\x00\x00Redis!" GETRANGEGETRANGE key start end可用版本： &gt;= 2.4.0 时间复杂度： O(N)，其中 N 为被返回的字符串的长度。 返回键 key 储存的字符串值的指定部分， 字符串的截取范围由 start 和 end 两个偏移量决定 (包括 start 和 end 在内)。 负数偏移量表示从字符串的末尾开始计数， -1 表示最后一个字符， -2 表示倒数第二个字符， 以此类推。 GETRANGE 通过保证子字符串的值域(range)不超过实际字符串的值域来处理超出范围的值域请求。 Note GETRANGE 命令在 Redis 2.0 之前的版本里面被称为 SUBSTR 命令。 返回值GETRANGE 命令会返回字符串值的指定部分。 代码示例1234567891011121314151617redis&gt; SET greeting "hello, my friend"OKredis&gt; GETRANGE greeting 0 4 # 返回索引0-4的字符，包括4。"hello"redis&gt; GETRANGE greeting -1 -5 # 不支持回绕操作""redis&gt; GETRANGE greeting -3 -1 # 负数索引"end"redis&gt; GETRANGE greeting 0 -1 # 从第一个到最后一个"hello, my friend"redis&gt; GETRANGE greeting 0 1008611 # 值域范围不超过实际字符串，超过部分自动被符略"hello, my friend" INCRBYINCRBY key increment可用版本： &gt;= 1.0.0 时间复杂度： O(1) 为键 key 储存的数字值加上增量 increment 。 如果键 key 不存在， 那么键 key 的值会先被初始化为 0 ， 然后再执行 INCRBY 命令。 如果键 key 储存的值不能被解释为数字， 那么 INCRBY 命令将返回一个错误。 本操作的值限制在 64 位(bit)有符号数字表示之内。 返回值在加上增量 increment 之后， 键 key 当前的值。 代码示例键存在，并且值为数字：12345678redis&gt; SET rank 50OKredis&gt; INCRBY rank 20(integer) 70redis&gt; GET rank"70" 键不存在：12345678redis&gt; EXISTS counter(integer) 0redis&gt; INCRBY counter 30(integer) 30redis&gt; GET counter"30" 键存在，但值无法被解释为数字：12345redis&gt; SET book "long long ago..."OKredis&gt; INCRBY book 200(error) ERR value is not an integer or out of range INCRBYFLOATINCRBYFLOAT key increment可用版本： &gt;= 2.6.0 时间复杂度： O(1) 为键 key 储存的值加上浮点数增量 increment 。 如果键 key 不存在， 那么 INCRBYFLOAT 会先将键 key 的值设为 0 ， 然后再执行加法操作。 如果命令执行成功， 那么键 key 的值会被更新为执行加法计算之后的新值， 并且新值会以字符串的形式返回给调用者。 无论是键 key 的值还是增量 increment ， 都可以使用像 2.0e7 、 3e5 、 90e-2 那样的指数符号(exponential notation)来表示， 但是， 执行 INCRBYFLOAT 命令之后的值总是以同样的形式储存， 也即是， 它们总是由一个数字， 一个（可选的）小数点和一个任意长度的小数部分组成（比如 3.14 、 69.768 ，诸如此类)， 小数部分尾随的 0 会被移除， 如果可能的话， 命令还会将浮点数转换为整数（比如 3.0 会被保存成 3 ）。 此外， 无论加法计算所得的浮点数的实际精度有多长， INCRBYFLOAT 命令的计算结果最多只保留小数点的后十七位。 当以下任意一个条件发生时， 命令返回一个错误： 键 key 的值不是字符串类型(因为 Redis 中的数字和浮点数都以字符串的形式保存，所以它们都属于字符串类型）； 键 key 当前的值或者给定的增量 increment 不能被解释(parse)为双精度浮点数。 返回值在加上增量 increment 之后， 键 key 的值。 代码示例12345678redis&gt; GET decimal"3.0"redis&gt; INCRBYFLOAT decimal 2.56"5.56"redis&gt; GET decimal"5.56" DECRDECR key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 为键 key 储存的数字值减去一。 如果键 key 不存在， 那么键 key 的值会先被初始化为 0 ， 然后再执行 DECR 操作。 如果键 key 储存的值不能被解释为数字， 那么 DECR 命令将返回一个错误。 本操作的值限制在 64 位(bit)有符号数字表示之内。 返回值DECR 命令会返回键 key 在执行减一操作之后的值。 代码示例对储存数字值的键 key 执行 DECR 命令：12345redis&gt; SET failure_times 10OKredis&gt; DECR failure_times(integer) 9 对不存在的键执行 DECR 命令：12345redis&gt; EXISTS count(integer) 0redis&gt; DECR count(integer) -1 DECRBYDECRBY key decrement可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将键 key 储存的整数值减去减量 decrement 。 如果键 key 不存在， 那么键 key 的值会先被初始化为 0 ， 然后再执行 DECRBY 命令。 如果键 key 储存的值不能被解释为数字， 那么 DECRBY 命令将返回一个错误。 本操作的值限制在 64 位(bit)有符号数字表示之内。 返回值DECRBY 命令会返回键在执行减法操作之后的值。 代码示例对已经存在的键执行 DECRBY 命令：12345redis&gt; SET count 100OKredis&gt; DECRBY count 20(integer) 80 对不存在的键执行 DECRBY 命令：12345redis&gt; EXISTS pages(integer) 0redis&gt; DECRBY pages 10(integer) -10 MSETMSET key value [key value …]可用版本： &gt;= 1.0.1 时间复杂度： O(N)，其中 N 为被设置的键数量。 同时为多个键设置值。 如果某个给定键已经存在， 那么 MSET 将使用新值去覆盖旧值， 如果这不是你所希望的效果， 请考虑使用 MSETNX 命令， 这个命令只会在所有给定键都不存在的情况下进行设置。 MSET 是一个原子性(atomic)操作， 所有给定键都会在同一时间内被设置， 不会出现某些键被设置了但是另一些键没有被设置的情况。 返回值MSET 命令总是返回 OK 。 代码示例同时对多个键进行设置：1234567redis&gt; MSET date "2012.3.30" time "11:00 a.m." weather "sunny"OKredis&gt; MGET date time weather1) "2012.3.30"2) "11:00 a.m."3) "sunny" 覆盖已有的值：12345678910redis&gt; MGET k1 k21) "hello"2) "world"redis&gt; MSET k1 "good" k2 "bye"OKredis&gt; MGET k1 k21) "good"2) "bye" MSETNXMSETNX key value [key value …]可用版本： &gt;= 1.0.1 时间复杂度： O(N)， 其中 N 为被设置的键数量。 当且仅当所有给定键都不存在时， 为所有给定键设置值。 即使只有一个给定键已经存在， MSETNX 命令也会拒绝执行对所有键的设置操作。 MSETNX 是一个原子性(atomic)操作， 所有给定键要么就全部都被设置， 要么就全部都不设置， 不可能出现第三种状态。 返回值当所有给定键都设置成功时， 命令返回 1 ； 如果因为某个给定键已经存在而导致设置未能成功执行， 那么命令返回 0 。 代码示例对不存在的键执行 MSETNX 命令：1234567redis&gt; MSETNX rmdbs "MySQL" nosql "MongoDB" key-value-store "redis"(integer) 1redis&gt; MGET rmdbs nosql key-value-store1) "MySQL"2) "MongoDB"3) "redis" 对某个已经存在的键进行设置：12345678redis&gt; MSETNX rmdbs "Sqlite" language "python" # rmdbs 键已经存在，操作失败(integer) 0redis&gt; EXISTS language # 因为 MSETNX 命令没有成功执行(integer) 0 # 所以 language 键没有被设置redis&gt; GET rmdbs # rmdbs 键也没有被修改"MySQL" MGETMGET key [key …]可用版本： &gt;= 1.0.0 时间复杂度： O(N) ，其中 N 为给定键的数量。 返回给定的一个或多个字符串键的值。 如果给定的字符串键里面， 有某个键不存在， 那么这个键的值将以特殊值 nil 表示。 返回值MGET 命令将返回一个列表， 列表中包含了所有给定键的值。 代码示例1234567891011121314redis&gt; SET redis redis.comOKredis&gt; SET mongodb mongodb.orgOKredis&gt; MGET redis mongodb1) "redis.com"2) "mongodb.org"redis&gt; MGET redis mongodb mysql # 不存在的 mysql 返回 nil1) "redis.com"2) "mongodb.org"3) (nil) 哈希表HSETHSET hash field value可用版本： &gt;= 2.0.0 时间复杂度： O(1) 将哈希表 hash 中域 field 的值设置为 value 。 如果给定的哈希表并不存在， 那么一个新的哈希表将被创建并执行 HSET 操作。 如果域 field 已经存在于哈希表中， 那么它的旧值将被新值 value 覆盖。 返回值 当 HSET 命令在哈希表中新创建 field 域并成功为它设置值时， 命令返回 1 ； 如果域 field 已经存在于哈希表， 并且 HSET 命令成功使用新值覆盖了它的旧值， 那么命令返回 0 。 代码示例设置一个新域：12345redis&gt; HSET website google "www.g.cn"(integer) 1redis&gt; HGET website google"www.g.cn" 对一个已存在的域进行更新：12345redis&gt; HSET website google "www.google.com"(integer) 0redis&gt; HGET website google"www.google.com" HSETNXHSETNX hash field value可用版本： &gt;= 2.0.0 时间复杂度： O(1) 当且仅当域 field 尚未存在于哈希表的情况下， 将它的值设置为 value 。 如果给定域已经存在于哈希表当中， 那么命令将放弃执行设置操作。 如果哈希表 hash 不存在， 那么一个新的哈希表将被创建并执行 HSETNX 命令。 返回值HSETNX 命令在设置成功时返回 1 ， 在给定域已经存在而放弃执行设置操作时返回 0 。 代码示例域尚未存在， 设置成功：12345redis&gt; HSETNX database key-value-store Redis(integer) 1redis&gt; HGET database key-value-store"Redis" 域已经存在， 设置未成功， 域原有的值未被改变：12345redis&gt; HSETNX database key-value-store Riak(integer) 0redis&gt; HGET database key-value-store"Redis" HGETHGET hash field可用版本： &gt;= 2.0.0 时间复杂度： O(1) 返回哈希表中给定域的值。 返回值HGET 命令在默认情况下返回给定域的值。 如果给定域不存在于哈希表中， 又或者给定的哈希表并不存在， 那么命令返回 nil 。 代码示例域存在的情况：12345redis&gt; HSET homepage redis redis.com(integer) 1redis&gt; HGET homepage redis"redis.com" 域不存在的情况：12redis&gt; HGET site mysql(nil) HEXISTSHEXISTS hash field可用版本： &gt;= 2.0.0 时间复杂度： O(1) 检查给定域 field 是否存在于哈希表 hash 当中。 返回值HEXISTS 命令在给定域存在时返回 1 ， 在给定域不存在时返回 0 。 代码示例给定域不存在：12redis&gt; HEXISTS phone myphone(integer) 0 给定域存在：12345redis&gt; HSET phone myphone nokia-1110(integer) 1redis&gt; HEXISTS phone myphone(integer) 1 HDELHDEL key field [field …]可用版本：&gt;= 2.0.0 时间复杂度:O(N)， N 为要删除的域的数量。 删除哈希表 key 中的一个或多个指定域，不存在的域将被忽略。 Note 在Redis2.4以下的版本里， HDEL 每次只能删除单个域，如果你需要在一个原子时间内删除多个域，请将命令包含在 MULTI / EXEC 块内。 返回值被成功移除的域的数量，不包括被忽略的域。 代码示例1234567891011121314151617181920212223242526# 测试数据redis&gt; HGETALL abbr1) "a"2) "apple"3) "b"4) "banana"5) "c"6) "cat"7) "d"8) "dog"# 删除单个域redis&gt; HDEL abbr a(integer) 1# 删除不存在的域redis&gt; HDEL abbr not-exists-field(integer) 0# 删除多个域redis&gt; HDEL abbr b c(integer) 2redis&gt; HGETALL abbr1) "d"2) "dog" HLENHLEN key时间复杂度：O(1) 返回哈希表 key 中域的数量。 返回值哈希表中域的数量。 当 key 不存在时，返回 0 。 代码示例1234567891011121314redis&gt; HSET db redis redis.com(integer) 1redis&gt; HSET db mysql mysql.com(integer) 1redis&gt; HLEN db(integer) 2redis&gt; HSET db mongodb mongodb.org(integer) 1redis&gt; HLEN db(integer) 3 HSTRLENHSTRLEN key field可用版本：&gt;= 3.2.0 时间复杂度：O(1) 返回哈希表 key 中， 与给定域 field 相关联的值的字符串长度（string length）。 如果给定的键或者域不存在， 那么命令返回 0 。 返回值一个整数。 代码示例1234567891011redis&gt; HMSET myhash f1 "HelloWorld" f2 "99" f3 "-256"OKredis&gt; HSTRLEN myhash f1(integer) 10redis&gt; HSTRLEN myhash f2(integer) 2redis&gt; HSTRLEN myhash f3(integer) 4 HINCRBYHINCRBY key field increment可用版本：&gt;= 2.0.0 时间复杂度：O(1) 为哈希表 key 中的域 field 的值加上增量 increment 。 增量也可以为负数，相当于对给定域进行减法操作。 如果 key 不存在，一个新的哈希表被创建并执行 HINCRBY 命令。 如果域 field 不存在，那么在执行命令前，域的值被初始化为 0 。 对一个储存字符串值的域 field 执行 HINCRBY 命令将造成一个错误。 本操作的值被限制在 64 位(bit)有符号数字表示之内。 返回值执行 HINCRBY 命令之后，哈希表 key 中域 field 的值。 代码示例1234567891011121314151617181920212223242526272829303132# increment 为正数redis&gt; HEXISTS counter page_view # 对空域进行设置(integer) 0redis&gt; HINCRBY counter page_view 200(integer) 200redis&gt; HGET counter page_view"200"# increment 为负数redis&gt; HGET counter page_view"200"redis&gt; HINCRBY counter page_view -50(integer) 150redis&gt; HGET counter page_view"150"# 尝试对字符串值的域执行HINCRBY命令redis&gt; HSET myhash string hello,world # 设定一个字符串值(integer) 1redis&gt; HGET myhash string"hello,world"redis&gt; HINCRBY myhash string 1 # 命令执行失败，错误。(error) ERR hash value is not an integerredis&gt; HGET myhash string # 原值不变"hello,world" HINCRBYFLOATHINCRBYFLOAT key field increment可用版本：&gt;= 2.6.0 时间复杂度：O(1) 为哈希表 key 中的域 field 加上浮点数增量 increment 。 如果哈希表中没有域 field ，那么 HINCRBYFLOAT 会先将域 field 的值设为 0 ，然后再执行加法操作。 如果键 key 不存在，那么 HINCRBYFLOAT 会先创建一个哈希表，再创建域 field ，最后再执行加法操作。 当以下任意一个条件发生时，返回一个错误： 域 field 的值不是字符串类型(因为 redis 中的数字和浮点数都以字符串的形式保存，所以它们都属于字符串类型） 域 field 当前的值或给定的增量 increment 不能解释(parse)为双精度浮点数(double precision floating point number) 返回值执行加法操作之后 field 域的值。 代码示例1234567891011121314151617181920212223242526272829303132# 值和增量都是普通小数redis&gt; HSET mykey field 10.50(integer) 1redis&gt; HINCRBYFLOAT mykey field 0.1"10.6"# 值和增量都是指数符号redis&gt; HSET mykey field 5.0e3(integer) 0redis&gt; HINCRBYFLOAT mykey field 2.0e2"5200"# 对不存在的键执行 HINCRBYFLOATredis&gt; EXISTS price(integer) 0redis&gt; HINCRBYFLOAT price milk 3.5"3.5"redis&gt; HGETALL price1) "milk"2) "3.5"# 对不存在的域进行 HINCRBYFLOATredis&gt; HGETALL price1) "milk"2) "3.5"redis&gt; HINCRBYFLOAT price coffee 4.5 # 新增 coffee 域"4.5"redis&gt; HGETALL price1) "milk"2) "3.5"3) "coffee"4) "4.5" HMSETHMSET key field value [field value …]可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为 field-value 对的数量。 同时将多个 field-value (域-值)对设置到哈希表 key 中。 此命令会覆盖哈希表中已存在的域。 如果 key 不存在，一个空哈希表被创建并执行 HMSET 操作。 返回值如果命令执行成功，返回 OK 。 当 key 不是哈希表(hash)类型时，返回一个错误。 代码示例12345678redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HGET website google"www.google.com"redis&gt; HGET website yahoo"www.yahoo.com" HMGETHMGET key field [field …]可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为给定域的数量。 返回哈希表 key 中，一个或多个给定域的值。 如果给定的域不存在于哈希表，那么返回一个 nil 值。 因为不存在的 key 被当作一个空哈希表来处理，所以对一个不存在的 key 进行 HMGET 操作将返回一个只带有 nil 值的表。 返回值一个包含多个给定域的关联值的表，表值的排列顺序和给定域参数的请求顺序一样。 代码示例1234567redis&gt; HMSET pet dog "doudou" cat "nounou" # 一次设置多个域OKredis&gt; HMGET pet dog cat fake_pet # 返回值的顺序和传入参数的顺序一样1) "doudou"2) "nounou"3) (nil) HKEYSHKEYS key可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为哈希表的大小。 返回哈希表 key 中的所有域。 返回值一个包含哈希表中所有域的表。当 key 不存在时，返回一个空表。 示例代码1234567891011121314# 哈希表非空redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HKEYS website1) "google"2) "yahoo"# 空哈希表/key不存在redis&gt; EXISTS fake_key(integer) 0redis&gt; HKEYS fake_key(empty list or set) HVALSHVALS key可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为哈希表的大小。 返回哈希表 key 中所有域的值。 返回值一个包含哈希表中所有值的表。 当 key 不存在时，返回一个空表。 代码示例1234567891011121314# 非空哈希表redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HVALS website1) "www.google.com"2) "www.yahoo.com"# 空哈希表/不存在的keyredis&gt; EXISTS not_exists(integer) 0redis&gt; HVALS not_exists(empty list or set) HGETALLHGETALL key可用版本：&gt;= 2.0.0 时间复杂度：O(N)， N 为哈希表的大小。 返回哈希表 key 中，所有的域和值。 在返回值里，紧跟每个域名(field name)之后是域的值(value)，所以返回值的长度是哈希表大小的两倍。 返回值以列表形式返回哈希表的域和域的值。 若 key 不存在，返回空列表。 代码示例1234567891011redis&gt; HSET people jack "Jack Sparrow"(integer) 1redis&gt; HSET people gump "Forrest Gump"(integer) 1redis&gt; HGETALL people1) "jack" # 域2) "Jack Sparrow" # 值3) "gump"4) "Forrest Gump" HSCANHSCAN key cursor [MATCH pattern] [COUNT count]具体信息请参考 SCAN cursor [MATCH pattern] [COUNT count] 命令。 列表LPUSHLPUSH key value [value …]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将一个或多个值 value 插入到列表 key 的表头 如果有多个 value 值，那么各个 value 值按从左到右的顺序依次插入到表头： 比如说，对空列表 mylist 执行命令 LPUSH mylist a b c ，列表的值将是 c b a ，这等同于原子性地执行 LPUSH mylist a 、 LPUSH mylist b 和 LPUSH mylist c 三个命令。 如果 key 不存在，一个空列表会被创建并执行 LPUSH 操作。 当 key 存在但不是列表类型时，返回一个错误。 Note 在Redis 2.4版本以前的 LPUSH 命令，都只接受单个 value 值。 返回值执行 LPUSH 命令后，列表的长度。 代码示例1234567891011121314151617181920# 加入单个元素redis&gt; LPUSH languages python(integer) 1# 加入重复元素redis&gt; LPUSH languages python(integer) 2redis&gt; LRANGE languages 0 -1 # 列表允许重复元素1) "python"2) "python"# 加入多个元素redis&gt; LPUSH mylist a b c(integer) 3redis&gt; LRANGE mylist 0 -11) "c"2) "b"3) "a" LPUSHXLPUSHX key value可用版本： &gt;= 2.2.0 时间复杂度： O(1) 将值 value 插入到列表 key 的表头，当且仅当 key 存在并且是一个列表。 和 LPUSH key value [value …] 命令相反，当 key 不存在时， LPUSHX 命令什么也不做。 返回值LPUSHX 命令执行之后，表的长度。 代码示例1234567891011121314151617# 对空列表执行 LPUSHXredis&gt; LLEN greet # greet 是一个空列表(integer) 0redis&gt; LPUSHX greet "hello" # 尝试 LPUSHX，失败，因为列表为空(integer) 0# 对非空列表执行 LPUSHXredis&gt; LPUSH greet "hello" # 先用 LPUSH 创建一个有一个元素的列表(integer) 1redis&gt; LPUSHX greet "good morning" # 这次 LPUSHX 执行成功(integer) 2redis&gt; LRANGE greet 0 -11) "good morning"2) "hello" RPUSHRPUSH key value [value …]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将一个或多个值 value 插入到列表 key 的表尾(最右边)。 如果有多个 value 值，那么各个 value 值按从左到右的顺序依次插入到表尾：比如对一个空列表 mylist 执行 RPUSH mylist a b c ，得出的结果列表为 a b c ，等同于执行命令 RPUSH mylist a 、 RPUSH mylist b 、 RPUSH mylist c 。 如果 key 不存在，一个空列表会被创建并执行 RPUSH 操作。 当 key 存在但不是列表类型时，返回一个错误。 Note 在 Redis 2.4 版本以前的 RPUSH 命令，都只接受单个 value 值。 返回值执行 RPUSH 操作后，表的长度。 代码示例1234567891011121314151617181920# 添加单个元素redis&gt; RPUSH languages c(integer) 1# 添加重复元素redis&gt; RPUSH languages c(integer) 2redis&gt; LRANGE languages 0 -1 # 列表允许重复元素1) "c"2) "c"# 添加多个元素redis&gt; RPUSH mylist a b c(integer) 3redis&gt; LRANGE mylist 0 -11) "a"2) "b"3) "c" RPUSHXRPUSHX key value可用版本： &gt;= 2.2.0 时间复杂度： O(1) 将值 value 插入到列表 key 的表尾，当且仅当 key 存在并且是一个列表。 和 RPUSH key value [value …] 命令相反，当 key 不存在时， RPUSHX 命令什么也不做。 返回值RPUSHX 命令执行之后，表的长度。 代码示例1234567891011121314151617# key不存在redis&gt; LLEN greet(integer) 0redis&gt; RPUSHX greet "hello" # 对不存在的 key 进行 RPUSHX，PUSH 失败。(integer) 0# key 存在且是一个非空列表redis&gt; RPUSH greet "hi" # 先用 RPUSH 插入一个元素(integer) 1redis&gt; RPUSHX greet "hello" # greet 现在是一个列表类型，RPUSHX 操作成功。(integer) 2redis&gt; LRANGE greet 0 -11) "hi"2) "hello" LPOPLPOP key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 移除并返回列表 key 的头元素。 返回值列表的头元素。 当 key 不存在时，返回 nil 。 代码示例1234567891011redis&gt; LLEN course(integer) 0redis&gt; RPUSH course algorithm001(integer) 1redis&gt; RPUSH course c++101(integer) 2redis&gt; LPOP course # 移除头元素"algorithm001" RPOPRPOP key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 移除并返回列表 key 的尾元素。 返回值列表的尾元素。 当 key 不存在时，返回 nil 。 代码示例123456789101112131415redis&gt; RPUSH mylist "one"(integer) 1redis&gt; RPUSH mylist "two"(integer) 2redis&gt; RPUSH mylist "three"(integer) 3redis&gt; RPOP mylist # 返回被弹出的元素"three"redis&gt; LRANGE mylist 0 -1 # 列表剩下的元素1) "one"2) "two" RPOPLPUSHRPOPLPUSH source destination可用版本： &gt;= 1.2.0 时间复杂度： O(1) 命令 RPOPLPUSH 在一个原子时间内，执行以下两个动作： 将列表 source 中的最后一个元素(尾元素)弹出，并返回给客户端。 将 source 弹出的元素插入到列表 destination ，作为 destination 列表的的头元素。 举个例子，你有两个列表 source 和 destination ， source 列表有元素 a, b, c ， destination 列表有元素 x, y, z ，执行 RPOPLPUSH source destination 之后， source 列表包含元素 a, b ， destination 列表包含元素 c, x, y, z ，并且元素 c 会被返回给客户端。 如果 source 不存在，值 nil 被返回，并且不执行其他动作。 如果 source 和 destination 相同，则列表中的表尾元素被移动到表头，并返回该元素，可以把这种特殊情况视作列表的旋转(rotation)操作。 返回值被弹出的元素。 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# source 和 destination 不同redis&gt; LRANGE alpha 0 -1 # 查看所有元素1) "a"2) "b"3) "c"4) "d"redis&gt; RPOPLPUSH alpha reciver # 执行一次 RPOPLPUSH 看看"d"redis&gt; LRANGE alpha 0 -11) "a"2) "b"3) "c"redis&gt; LRANGE reciver 0 -11) "d"redis&gt; RPOPLPUSH alpha reciver # 再执行一次，证实 RPOP 和 LPUSH 的位置正确"c"redis&gt; LRANGE alpha 0 -11) "a"2) "b"redis&gt; LRANGE reciver 0 -11) "c"2) "d"# source 和 destination 相同redis&gt; LRANGE number 0 -11) "1"2) "2"3) "3"4) "4"redis&gt; RPOPLPUSH number number"4"redis&gt; LRANGE number 0 -1 # 4 被旋转到了表头1) "4"2) "1"3) "2"4) "3"redis&gt; RPOPLPUSH number number"3"redis&gt; LRANGE number 0 -1 # 这次是 3 被旋转到了表头1) "3"2) "4"3) "1"4) "2" 模式： 安全的队列Redis的列表经常被用作队列(queue)，用于在不同程序之间有序地交换消息(message)。一个客户端通过 LPUSH key value [value …] 命令将消息放入队列中，而另一个客户端通过 RPOP key 或者 BRPOP key [key …] timeout 命令取出队列中等待时间最长的消息。 不幸的是，上面的队列方法是『不安全』的，因为在这个过程中，一个客户端可能在取出一个消息之后崩溃，而未处理完的消息也就因此丢失。 使用 RPOPLPUSH 命令(或者它的阻塞版本 BRPOPLPUSH source destination timeout )可以解决这个问题：因为它不仅返回一个消息，同时还将这个消息添加到另一个备份列表当中，如果一切正常的话，当一个客户端完成某个消息的处理之后，可以用 LREM key count value 命令将这个消息从备份表删除。 最后，还可以添加一个客户端专门用于监视备份表，它自动地将超过一定处理时限的消息重新放入队列中去(负责处理该消息的客户端可能已经崩溃)，这样就不会丢失任何消息了。 模式：循环列表通过使用相同的 key 作为 RPOPLPUSH 命令的两个参数，客户端可以用一个接一个地获取列表元素的方式，取得列表的所有元素，而不必像 LRANGE key start stop 命令那样一下子将所有列表元素都从服务器传送到客户端中(两种方式的总复杂度都是 O(N))。 以上的模式甚至在以下的两个情况下也能正常工作： 有多个客户端同时对同一个列表进行旋转(rotating)，它们获取不同的元素，直到所有元素都被读取完，之后又从头开始。 有客户端在向列表尾部(右边)添加新元素。 这个模式使得我们可以很容易实现这样一类系统：有 N 个客户端，需要连续不断地对一些元素进行处理，而且处理的过程必须尽可能地快。一个典型的例子就是服务器的监控程序：它们需要在尽可能短的时间内，并行地检查一组网站，确保它们的可访问性。 注意，使用这个模式的客户端是易于扩展(scala)且安全(reliable)的，因为就算接收到元素的客户端失败，元素还是保存在列表里面，不会丢失，等到下个迭代来临的时候，别的客户端又可以继续处理这些元素了。 LREMLREM key count value可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为列表的长度。 根据参数 count 的值，移除列表中与参数 value 相等的元素。 count 的值可以是以下几种： count &gt; 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。 count &lt; 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。 count = 0 : 移除表中所有与 value 相等的值。 返回值被移除元素的数量。 因为不存在的 key 被视作空表(empty list)，所以当 key 不存在时， LREM 命令总是返回 0 。 代码示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 先创建一个表，内容排列是# morning hello morning helllo morningredis&gt; LPUSH greet "morning"(integer) 1redis&gt; LPUSH greet "hello"(integer) 2redis&gt; LPUSH greet "morning"(integer) 3redis&gt; LPUSH greet "hello"(integer) 4redis&gt; LPUSH greet "morning"(integer) 5redis&gt; LRANGE greet 0 4 # 查看所有元素1) "morning"2) "hello"3) "morning"4) "hello"5) "morning"redis&gt; LREM greet 2 morning # 移除从表头到表尾，最先发现的两个 morning(integer) 2 # 两个元素被移除redis&gt; LLEN greet # 还剩 3 个元素(integer) 3redis&gt; LRANGE greet 0 21) "hello"2) "hello"3) "morning"redis&gt; LREM greet -1 morning # 移除从表尾到表头，第一个 morning(integer) 1redis&gt; LLEN greet # 剩下两个元素(integer) 2redis&gt; LRANGE greet 0 11) "hello"2) "hello"redis&gt; LREM greet 0 hello # 移除表中所有 hello(integer) 2 # 两个 hello 被移除redis&gt; LLEN greet(integer) 0 LLENLLEN key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回列表 key 的长度。 如果 key 不存在，则 key 被解释为一个空列表，返回 0 . 如果 key 不是列表类型，返回一个错误。 返回值列表 key 的长度。 代码示例12345678910111213# 空列表redis&gt; LLEN job(integer) 0# 非空列表redis&gt; LPUSH job "cook food"(integer) 1redis&gt; LPUSH job "have lunch"(integer) 2redis&gt; LLEN job(integer) 2 LINDEXLINDEX key index可用版本： &gt;= 1.0.0 时间复杂度：O(N)， N 为到达下标 index 过程中经过的元素数量。因此，对列表的头元素和尾元素执行 LINDEX 命令，复杂度为O(1)。 返回列表 key 中，下标为 index 的元素。 下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 如果 key 不是列表类型，返回一个错误。 返回值列表中下标为 index 的元素。 如果 index 参数的值不在列表的区间范围内(out of range)，返回 nil 。 代码示例1234567891011121314redis&gt; LPUSH mylist "World"(integer) 1redis&gt; LPUSH mylist "Hello"(integer) 2redis&gt; LINDEX mylist 0"Hello"redis&gt; LINDEX mylist -1"World"redis&gt; LINDEX mylist 3 # index不在 mylist 的区间范围内(nil) LINSERTLINSERT key BEFORE|AFTER pivot value可用版本： &gt;= 2.2.0 时间复杂度: O(N)， N 为寻找 pivot 过程中经过的元素数量。 将值 value 插入到列表 key 当中，位于值 pivot 之前或之后。 当 pivot 不存在于列表 key 时，不执行任何操作。 当 key 不存在时， key 被视为空列表，不执行任何操作。 如果 key 不是列表类型，返回一个错误。 返回值如果命令执行成功，返回插入操作完成之后，列表的长度。 如果没有找到 pivot ，返回 -1 。 如果 key 不存在或为空列表，返回 0 。 代码示例123456789101112131415161718192021222324redis&gt; RPUSH mylist "Hello"(integer) 1redis&gt; RPUSH mylist "World"(integer) 2redis&gt; LINSERT mylist BEFORE "World" "There"(integer) 3redis&gt; LRANGE mylist 0 -11) "Hello"2) "There"3) "World"# 对一个非空列表插入，查找一个不存在的 pivotredis&gt; LINSERT mylist BEFORE "go" "let's"(integer) -1 # 失败# 对一个空列表执行 LINSERT 命令redis&gt; EXISTS fake_list(integer) 0redis&gt; LINSERT fake_list BEFORE "nono" "gogogog"(integer) 0 # 失败 LSETLSET key index value可用版本： &gt;= 1.0.0 时间复杂度：对头元素或尾元素进行 LSET 操作，复杂度为 O(1)。其他情况下，为 O(N)， N 为列表的长度。 将列表 key 下标为 index 的元素的值设置为 value 。 当 index 参数超出范围，或对一个空列表( key 不存在)进行 LSET 时，返回一个错误。 关于列表下标的更多信息，请参考 LINDEX key index 命令。 返回值操作成功返回 ok ，否则返回错误信息。 代码示例1234567891011121314151617181920212223242526# 对空列表(key 不存在)进行 LSETredis&gt; EXISTS list(integer) 0redis&gt; LSET list 0 item(error) ERR no such key# 对非空列表进行 LSETredis&gt; LPUSH job "cook food"(integer) 1redis&gt; LRANGE job 0 01) "cook food"redis&gt; LSET job 0 "play game"OKredis&gt; LRANGE job 0 01) "play game"# index 超出范围redis&gt; LLEN list # 列表长度为 1(integer) 1redis&gt; LSET list 3 'out of range'(error) ERR index out of range LRANGELRANGE key start stop可用版本： &gt;= 1.0.0 时间复杂度: O(S+N)， S 为偏移量 start ， N 为指定区间内元素的数量。 返回列表 key 中指定区间内的元素，区间以偏移量 start 和 stop 指定。 下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 注意LRANGE命令和编程语言区间函数的区别 假如你有一个包含一百个元素的列表，对该列表执行 LRANGE list 0 10 ，结果是一个包含11个元素的列表，这表明 stop 下标也在 LRANGE 命令的取值范围之内(闭区间)，这和某些语言的区间函数可能不一致，比如Ruby的 Range.new 、 Array#slice 和Python的 range() 函数。 超出范围的下标 超出范围的下标值不会引起错误。 如果 start 下标比列表的最大下标 end ( LLEN list 减去 1 )还要大，那么 LRANGE 返回一个空列表。 如果 stop 下标比 end 下标还要大，Redis将 stop 的值设置为 end 。 返回值一个列表，包含指定区间内的元素。 代码示例123456789101112redis&gt; RPUSH fp-language lisp(integer) 1redis&gt; LRANGE fp-language 0 01) "lisp"redis&gt; RPUSH fp-language scheme(integer) 2redis&gt; LRANGE fp-language 0 11) "lisp"2) "scheme" LTRIMLTRIM key start stop可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 为被移除的元素的数量。 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 举个例子，执行命令 LTRIM list 0 2 ，表示只保留列表 list 的前三个元素，其余元素全部删除。 下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 当 key 不是列表类型时，返回一个错误。 LTRIM 命令通常和 LPUSH key value [value …] 命令或 RPUSH key value [value …] 命令配合使用，举个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172LPUSH log newest_logLTRIM log 0 99```bash这个例子模拟了一个日志程序，每次将最新日志 newest_log 放到 log 列表中，并且只保留最新的 100 项。注意当这样使用 LTRIM 命令时，时间复杂度是O(1)，因为平均情况下，每次只有一个元素被移除。**注意LTRIM命令和编程语言区间函数的区别** 假如你有一个包含一百个元素的列表 list ，对该列表执行 LTRIM list 0 10 ，结果是一个包含11个元素的列表，这表明 stop 下标也在 LTRIM 命令的取值范围之内(闭区间)，这和某些语言的区间函数可能不一致，比如Ruby的 Range.new 、 Array#slice 和Python的 range() 函数。**超出范围的下标** 超出范围的下标值不会引起错误。如果 start 下标比列表的最大下标 end ( LLEN list 减去 1 )还要大，或者 start &gt; stop ， LTRIM 返回一个空列表(因为 LTRIM 已经将整个列表清空)。如果 stop 下标比 end 下标还要大，Redis将 stop 的值设置为 end 。#### 返回值命令执行成功时，返回 ok 。#### 代码示例```bash# 情况 1： 常见情况， start 和 stop 都在列表的索引范围之内redis&gt; LRANGE alpha 0 -1 # alpha 是一个包含 5 个字符串的列表1) "h"2) "e"3) "l"4) "l"5) "o"redis&gt; LTRIM alpha 1 -1 # 删除 alpha 列表索引为 0 的元素OKredis&gt; LRANGE alpha 0 -1 # "h" 被删除了1) "e"2) "l"3) "l"4) "o"# 情况 2： stop 比列表的最大下标还要大redis&gt; LTRIM alpha 1 10086 # 保留 alpha 列表索引 1 至索引 10086 上的元素OKredis&gt; LRANGE alpha 0 -1 # 只有索引 0 上的元素 "e" 被删除了，其他元素还在1) "l"2) "l"3) "o"# 情况 3： start 和 stop 都比列表的最大下标要大，并且 start &lt; stopredis&gt; LTRIM alpha 10086 123321OKredis&gt; LRANGE alpha 0 -1 # 列表被清空(empty list or set)# 情况 4： start 和 stop 都比列表的最大下标要大，并且 start &gt; stopredis&gt; RPUSH new-alpha "h" "e" "l" "l" "o" # 重新建立一个新列表(integer) 5redis&gt; LRANGE new-alpha 0 -11) "h"2) "e"3) "l"4) "l"5) "o"redis&gt; LTRIM new-alpha 123321 10086 # 执行 LTRIMOKredis&gt; LRANGE new-alpha 0 -1 # 同样被清空(empty list or set) BLPOPBLPOP key [key …] timeout可用版本： &gt;= 2.0.0 时间复杂度： O(1) BLPOP 是列表的阻塞式(blocking)弹出原语。 它是 LPOP key 命令的阻塞版本，当给定列表内没有任何元素可供弹出的时候，连接将被 BLPOP 命令阻塞，直到等待超时或发现可弹出元素为止。 当给定多个 key 参数时，按参数 key 的先后顺序依次检查各个列表，弹出第一个非空列表的头元素。 非阻塞行为当 BLPOP 被调用时，如果给定 key 内至少有一个非空列表，那么弹出遇到的第一个非空列表的头元素，并和被弹出元素所属的列表的名字一起，组成结果返回给调用者。 当存在多个给定 key 时， BLPOP 按给定 key 参数排列的先后顺序，依次检查各个列表。 假设现在有 job 、 command 和 request 三个列表，其中 job 不存在， command 和 request 都持有非空列表。考虑以下命令： BLPOP job command request 0 BLPOP 保证返回的元素来自 command ，因为它是按”查找 job -&gt; 查找 command -&gt; 查找 request “这样的顺序，第一个找到的非空列表。 123456789101112redis&gt; DEL job command request # 确保key都被删除(integer) 0redis&gt; LPUSH command "update system..." # 为command列表增加一个值(integer) 1redis&gt; LPUSH request "visit page" # 为request列表增加一个值(integer) 1redis&gt; BLPOP job command request 0 # job 列表为空，被跳过，紧接着 command 列表的第一个元素被弹出。1) "command" # 弹出元素所属的列表2) "update system..." # 弹出元素所属的值 阻塞行为如果所有给定 key 都不存在或包含空列表，那么 BLPOP 命令将阻塞连接，直到等待超时，或有另一个客户端对给定 key 的任意一个执行 LPUSH key value [value …] 或 RPUSH key value [value …] 命令为止。 超时参数 timeout 接受一个以秒为单位的数字作为值。超时参数设为 0 表示阻塞时间可以无限期延长(block indefinitely) 。12345678910111213redis&gt; EXISTS job # 确保两个 key 都不存在(integer) 0redis&gt; EXISTS command(integer) 0redis&gt; BLPOP job command 300 # 因为key一开始不存在，所以操作会被阻塞，直到另一客户端对 job 或者 command 列表进行 PUSH 操作。1) "job" # 这里被 push 的是 job2) "do my home work" # 被弹出的值(26.26s) # 等待的秒数redis&gt; BLPOP job command 5 # 等待超时的情况(nil)(5.66s) # 等待的秒数 相同的key被多个客户端同时阻塞 相同的 key 可以被多个客户端同时阻塞。 不同的客户端被放进一个队列中，按『先阻塞先服务』(first-BLPOP，first-served)的顺序为 key 执行 BLPOP 命令。 在MULTI/EXEC事务中的BLPOPBLPOP 可以用于流水线(pipline,批量地发送多个命令并读入多个回复)，但把它用在 MULTI / EXEC 块当中没有意义。因为这要求整个服务器被阻塞以保证块执行时的原子性，该行为阻止了其他客户端执行 LPUSH key value [value …] 或 RPUSH key value [value …] 命令。 因此，一个被包裹在 MULTI / EXEC 块内的 BLPOP 命令，行为表现得就像 LPOP key 一样，对空列表返回 nil ，对非空列表弹出列表元素，不进行任何阻塞操作。 1234567891011121314151617181920212223242526# 对非空列表进行操作redis&gt; RPUSH job programming(integer) 1redis&gt; MULTIOKredis&gt; BLPOP job 30QUEUEDredis&gt; EXEC # 不阻塞，立即返回1) 1) "job" 2) "programming"# 对空列表进行操作redis&gt; LLEN job # 空列表(integer) 0redis&gt; MULTIOKredis&gt; BLPOP job 30QUEUEDredis&gt; EXEC # 不阻塞，立即返回1) (nil) 返回值如果列表为空，返回一个 nil 。 否则，返回一个含有两个元素的列表，第一个元素是被弹出元素所属的 key ，第二个元素是被弹出元素的值。 模式：事件提醒有时候，为了等待一个新元素到达数据中，需要使用轮询的方式对数据进行探查。 另一种更好的方式是，使用系统提供的阻塞原语，在新元素到达时立即进行处理，而新元素还没到达时，就一直阻塞住，避免轮询占用资源。 对于 Redis ，我们似乎需要一个阻塞版的 SPOP key 命令，但实际上，使用 BLPOP 或者 BRPOP key [key …] timeout 就能很好地解决这个问题。 使用元素的客户端(消费者)可以执行类似以下的代码：123456LOOP forever WHILE SPOP(key) returns elements ... process elements ... END BRPOP helper_keyEND 添加元素的客户端(生产者)则执行以下代码：1234MULTI SADD key element LPUSH helper_key xEXEC BRPOPBRPOP key [key …] timeout可用版本： &gt;= 2.0.0 时间复杂度： O(1) BRPOP 是列表的阻塞式(blocking)弹出原语。 它是 RPOP key 命令的阻塞版本，当给定列表内没有任何元素可供弹出的时候，连接将被 BRPOP 命令阻塞，直到等待超时或发现可弹出元素为止。 当给定多个 key 参数时，按参数 key 的先后顺序依次检查各个列表，弹出第一个非空列表的尾部元素。 关于阻塞操作的更多信息，请查看 BLPOP key [key …] timeout 命令， BRPOP 除了弹出元素的位置和 BLPOP key [key …] timeout 不同之外，其他表现一致。 返回值假如在指定时间内没有任何元素被弹出，则返回一个 nil 和等待时长。 反之，返回一个含有两个元素的列表，第一个元素是被弹出元素所属的 key ，第二个元素是被弹出元素的值。 代码示例123456789101112redis&gt; LLEN course(integer) 0redis&gt; RPUSH course algorithm001(integer) 1redis&gt; RPUSH course c++101(integer) 2redis&gt; BRPOP course 301) "course" # 被弹出元素所属的列表键2) "c++101" # 被弹出的元素 BRPOPLPUSHBRPOPLPUSH source destination timeout可用版本： &gt;= 2.2.0 时间复杂度： O(1) BRPOPLPUSH 是 RPOPLPUSH source destination 的阻塞版本，当给定列表 source 不为空时， BRPOPLPUSH 的表现和 RPOPLPUSH source destination 一样。 当列表 source 为空时， BRPOPLPUSH 命令将阻塞连接，直到等待超时，或有另一个客户端对 source 执行 LPUSH key value [value …] 或 RPUSH key value [value …] 命令为止。 超时参数 timeout 接受一个以秒为单位的数字作为值。超时参数设为 0 表示阻塞时间可以无限期延长(block indefinitely) 。 更多相关信息，请参考 RPOPLPUSH source destination 命令。 返回值假如在指定时间内没有任何元素被弹出，则返回一个 nil 和等待时长。 反之，返回一个含有两个元素的列表，第一个元素是被弹出元素的值，第二个元素是等待时长。 代码示例123456789101112131415# 非空列表redis&gt; BRPOPLPUSH msg reciver 500"hello moto" # 弹出元素的值(3.38s) # 等待时长redis&gt; LLEN reciver(integer) 1redis&gt; LRANGE reciver 0 01) "hello moto"# 空列表redis&gt; BRPOPLPUSH msg reciver 1(nil)(1.34s) 模式：安全队列参考 RPOPLPUSH source destination 命令的《安全队列》一节。 模式：循环列表参考 RPOPLPUSH source destination 命令的《循环列表》一节。 集合SADDSADD key member [member …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是被添加的元素的数量。 将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的 member 元素将被忽略。 假如 key 不存在，则创建一个只包含 member 元素作成员的集合。 当 key 不是集合类型时，返回一个错误。 Note在Redis2.4版本以前， SADD 只接受单个 member 值。 返回值被添加到集合中的新元素的数量，不包括被忽略的元素。 代码示例12345678910111213141516# 添加单个元素redis&gt; SADD bbs "discuz.net"(integer) 1# 添加重复元素redis&gt; SADD bbs "discuz.net"(integer) 0# 添加多个元素redis&gt; SADD bbs "tianya.cn" "groups.google.com"(integer) 2redis&gt; SMEMBERS bbs1) "discuz.net"2) "groups.google.com"3) "tianya.cn" SISMEMBERSISMEMBER key member可用版本： &gt;= 1.0.0 时间复杂度: O(1) 判断 member 元素是否集合 key 的成员。 返回值如果 member 元素是集合的成员，返回 1 。 如果 member 元素不是集合的成员，或 key 不存在，返回 0 。 代码示例12345678910redis&gt; SMEMBERS joe's_movies1) "hi, lady"2) "Fast Five"3) "2012"redis&gt; SISMEMBER joe's_movies "bet man"(integer) 0redis&gt; SISMEMBER joe's_movies "Fast Five"(integer) 1 SPOPSPOP key可用版本： &gt;= 1.0.0 时间复杂度: O(1) 移除并返回集合中的一个随机元素。 如果只想获取一个随机元素，但不想该元素从集合中被移除的话，可以使用 SRANDMEMBER key [count] 命令。 返回值被移除的随机元素。 当 key 不存在或 key 是空集时，返回 nil 。 代码示例1234567891011121314151617redis&gt; SMEMBERS db1) "MySQL"2) "MongoDB"3) "Redis"redis&gt; SPOP db"Redis"redis&gt; SMEMBERS db1) "MySQL"2) "MongoDB"redis&gt; SPOP db"MySQL"redis&gt; SMEMBERS db1) "MongoDB" SRANDMEMBERSRANDMEMBER key [count]可用版本： &gt;= 1.0.0 时间复杂度: 只提供 key 参数时为 O(1) 。如果提供了 count 参数，那么为 O(N) ，N 为返回数组的元素个数。 如果命令执行时，只提供了 key 参数，那么返回集合中的一个随机元素。 从 Redis 2.6 版本开始， SRANDMEMBER 命令接受可选的 count 参数： 如果 count 为正数，且小于集合基数，那么命令返回一个包含 count 个元素的数组，数组中的元素各不相同。如果 count 大于等于集合基数，那么返回整个集合。如果 count 为负数，那么命令返回一个数组，数组中的元素可能会重复出现多次，而数组的长度为 count 的绝对值。该操作和 SPOP key 相似，但 SPOP key 将随机元素从集合中移除并返回，而 SRANDMEMBER 则仅仅返回随机元素，而不对集合进行任何改动。 返回值只提供 key 参数时，返回一个元素；如果集合为空，返回 nil 。 如果提供了 count 参数，那么返回一个数组；如果集合为空，返回空数组。 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 添加元素redis&gt; SADD fruit apple banana cherry(integer) 3# 只给定 key 参数，返回一个随机元素redis&gt; SRANDMEMBER fruit"cherry"redis&gt; SRANDMEMBER fruit"apple"# 给定 3 为 count 参数，返回 3 个随机元素# 每个随机元素都不相同redis&gt; SRANDMEMBER fruit 31) "apple"2) "banana"3) "cherry"# 给定 -3 为 count 参数，返回 3 个随机元素# 元素可能会重复出现多次redis&gt; SRANDMEMBER fruit -31) "banana"2) "cherry"3) "apple"redis&gt; SRANDMEMBER fruit -31) "apple"2) "apple"3) "cherry"# 如果 count 是整数，且大于等于集合基数，那么返回整个集合redis&gt; SRANDMEMBER fruit 101) "apple"2) "banana"3) "cherry"# 如果 count 是负数，且 count 的绝对值大于集合的基数# 那么返回的数组的长度为 count 的绝对值redis&gt; SRANDMEMBER fruit -101) "banana"2) "apple"3) "banana"4) "cherry"5) "apple"6) "apple"7) "cherry"8) "apple"9) "apple"10) "banana"# SRANDMEMBER 并不会修改集合内容redis&gt; SMEMBERS fruit1) "apple"2) "cherry"3) "banana"# 集合为空时返回 nil 或者空数组redis&gt; SRANDMEMBER not-exists(nil)redis&gt; SRANDMEMBER not-eixsts 10(empty list or set) SREMSREM key member [member …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 为给定 member 元素的数量。 移除集合 key 中的一个或多个 member 元素，不存在的 member 元素会被忽略。 当 key 不是集合类型，返回一个错误。 Note在 Redis 2.4 版本以前， SREM 只接受单个 member 值。 返回值被成功移除的元素的数量，不包括被忽略的元素。 代码示例123456789101112131415161718192021# 测试数据redis&gt; SMEMBERS languages1) "c"2) "lisp"3) "python"4) "ruby"# 移除单个元素redis&gt; SREM languages ruby(integer) 1# 移除不存在元素redis&gt; SREM languages non-exists-language(integer) 0# 移除多个元素redis&gt; SREM languages lisp python c(integer) 3redis&gt; SMEMBERS languages(empty list or set) SMOVESMOVE source destination member可用版本： &gt;= 1.0.0 时间复杂度: O(1) 将 member 元素从 source 集合移动到 destination 集合。 SMOVE 是原子性操作。 如果 source 集合不存在或不包含指定的 member 元素，则 SMOVE 命令不执行任何操作，仅返回 0 。否则， member 元素从 source 集合中被移除，并添加到 destination 集合中去。 当 destination 集合已经包含 member 元素时， SMOVE 命令只是简单地将 source 集合中的 member 元素删除。 当 source 或 destination 不是集合类型时，返回一个错误。 返回值如果 member 元素被成功移除，返回 1 。 如果 member 元素不是 source 集合的成员，并且没有任何操作对 destination 集合执行，那么返回 0 。 代码示例123456789101112131415redis&gt; SMEMBERS songs1) "Billie Jean"2) "Believe Me"redis&gt; SMEMBERS my_songs(empty list or set)redis&gt; SMOVE songs my_songs "Believe Me"(integer) 1redis&gt; SMEMBERS songs1) "Billie Jean"redis&gt; SMEMBERS my_songs1) "Believe Me" SCARDSCARD key可用版本： &gt;= 1.0.0 时间复杂度: O(1) 返回集合 key 的基数(集合中元素的数量)。 返回值集合的基数。 当 key 不存在时，返回 0 。 代码示例1234567891011redis&gt; SADD tool pc printer phone(integer) 3redis&gt; SCARD tool # 非空集合(integer) 3redis&gt; DEL tool(integer) 1redis&gt; SCARD tool # 空集合(integer) 0 SMEMBERSSMEMBERS key可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 为集合的基数。 返回集合 key 中的所有成员。 不存在的 key 被视为空集合。 返回值集合中的所有成员。 代码示例123456789101112131415# key 不存在或集合为空redis&gt; EXISTS not_exists_key(integer) 0redis&gt; SMEMBERS not_exists_key(empty list or set)# 非空集合redis&gt; SADD language Ruby Python Clojure(integer) 3redis&gt; SMEMBERS language1) "Python"2) "Ruby"3) "Clojure" SSCANSSCAN key cursor [MATCH pattern] [COUNT count]详细信息请参考 SCAN cursor [MATCH pattern] [COUNT count] 命令。 SINTERSINTER key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N * M)， N 为给定集合当中基数最小的集合， M 为给定集合的个数。 返回一个集合的全部成员，该集合是所有给定集合的交集。 不存在的 key 被视为空集。 当给定集合当中有一个空集时，结果也为空集(根据集合运算定律)。 返回值交集成员的列表。 代码示例1234567891011redis&gt; SMEMBERS group_11) "LI LEI"2) "TOM"3) "JACK"redis&gt; SMEMBERS group_21) "HAN MEIMEI"2) "JACK"redis&gt; SINTER group_1 group_21) "JACK" SINTERSTORESINTERSTORE destination key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N * M)， N 为给定集合当中基数最小的集合， M 为给定集合的个数。 这个命令类似于 SINTER key [key …] 命令，但它将结果保存到 destination 集合，而不是简单地返回结果集。 如果 destination 集合已经存在，则将其覆盖。 destination 可以是 key 本身。 返回值结果集中的成员数量。 代码示例12345678910111213redis&gt; SMEMBERS songs1) "good bye joe"2) "hello,peter"redis&gt; SMEMBERS my_songs1) "good bye joe"2) "falling"redis&gt; SINTERSTORE song_interset songs my_songs(integer) 1redis&gt; SMEMBERS song_interset1) "good bye joe" SUNIONSUNION key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 返回一个集合的全部成员，该集合是所有给定集合的并集。 不存在的 key 被视为空集。 返回值并集成员的列表。 代码示例123456789redis&gt; SMEMBERS songs1) "Billie Jean"redis&gt; SMEMBERS my_songs1) "Believe Me"redis&gt; SUNION songs my_songs1) "Billie Jean"2) "Believe Me" SUNIONSTORESUNIONSTORE destination key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 这个命令类似于 SUNION key [key …] 命令，但它将结果保存到 destination 集合，而不是简单地返回结果集。 如果 destination 已经存在，则将其覆盖。 destination 可以是 key 本身。 返回值结果集中的元素数量。 代码示例12345678910111213141516redis&gt; SMEMBERS NoSQL1) "MongoDB"2) "Redis"redis&gt; SMEMBERS SQL1) "sqlite"2) "MySQL"redis&gt; SUNIONSTORE db NoSQL SQL(integer) 4redis&gt; SMEMBERS db1) "MySQL"2) "sqlite"3) "MongoDB"4) "Redis" SDIFFSDIFF key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 返回一个集合的全部成员，该集合是所有给定集合之间的差集。 不存在的 key 被视为空集。 返回值一个包含差集成员的列表。 代码示例12345678910111213redis&gt; SMEMBERS peter's_movies1) "bet man"2) "start war"3) "2012"redis&gt; SMEMBERS joe's_movies1) "hi, lady"2) "Fast Five"3) "2012"redis&gt; SDIFF peter's_movies joe's_movies1) "bet man"2) "start war" SDIFFSTORESDIFFSTORE destination key [key …]可用版本： &gt;= 1.0.0 时间复杂度: O(N)， N 是所有给定集合的成员数量之和。 这个命令的作用和 SDIFF key [key …] 类似，但它将结果保存到 destination 集合，而不是简单地返回结果集。 如果 destination 集合已经存在，则将其覆盖。 destination 可以是 key 本身。 返回值结果集中的元素数量。 代码示例12345678910111213141516redis&gt; SMEMBERS joe's_movies1) "hi, lady"2) "Fast Five"3) "2012"redis&gt; SMEMBERS peter's_movies1) "bet man"2) "start war"3) "2012"redis&gt; SDIFFSTORE joe_diff_peter joe's_movies peter's_movies(integer) 2redis&gt; SMEMBERS joe_diff_peter1) "hi, lady"2) "Fast Five" 有序集合ZADDZADD key score member [[score member] [score member] …]可用版本： &gt;= 1.2.0 时间复杂度: O(M*log(N))， N 是有序集的基数， M 为成功添加的新成员的数量。 将一个或多个 member 元素及其 score 值加入到有序集 key 当中。 如果某个 member 已经是有序集的成员，那么更新这个 member 的 score 值，并通过重新插入这个 member 元素，来保证该 member 在正确的位置上。 score 值可以是整数值或双精度浮点数。 如果 key 不存在，则创建一个空的有序集并执行 ZADD 操作。 当 key 存在但不是有序集类型时，返回一个错误。 Note在 Redis 2.4 版本以前， ZADD 每次只能添加一个元素。 返回值被成功添加的新成员的数量，不包括那些被更新的、已经存在的成员。 代码示例123456789101112131415161718192021222324252627282930313233343536373839# 添加单个元素redis&gt; ZADD page_rank 10 google.com(integer) 1# 添加多个元素redis&gt; ZADD page_rank 9 baidu.com 8 bing.com(integer) 2redis&gt; ZRANGE page_rank 0 -1 WITHSCORES1) "bing.com"2) "8"3) "baidu.com"4) "9"5) "google.com"6) "10"# 添加已存在元素，且 score 值不变redis&gt; ZADD page_rank 10 google.com(integer) 0redis&gt; ZRANGE page_rank 0 -1 WITHSCORES # 没有改变1) "bing.com"2) "8"3) "baidu.com"4) "9"5) "google.com"6) "10"# 添加已存在元素，但是改变 score 值redis&gt; ZADD page_rank 6 bing.com(integer) 0redis&gt; ZRANGE page_rank 0 -1 WITHSCORES # bing.com 元素的 score 值被改变1) "bing.com"2) "6"3) "baidu.com"4) "9"5) "google.com"6) "10" ZSCOREZSCORE key member可用版本： &gt;= 1.2.0 时间复杂度: O(1) 返回有序集 key 中，成员 member 的 score 值。 如果 member 元素不是有序集 key 的成员，或 key 不存在，返回 nil 。 返回值member 成员的 score 值，以字符串形式表示。 代码示例12345678910redis&gt; ZRANGE salary 0 -1 WITHSCORES # 测试数据1) "tom"2) "2000"3) "peter"4) "3500"5) "jack"6) "5000"redis&gt; ZSCORE salary peter # 注意返回值是字符串"3500" ZINCRBYZINCRBY key increment member可用版本： &gt;= 1.2.0 时间复杂度: O(log(N)) 为有序集 key 的成员 member 的 score 值加上增量 increment 。 可以通过传递一个负数值 increment ，让 score 减去相应的值，比如 ZINCRBY key -5 member ，就是让 member 的 score 值减去 5 。 当 key 不存在，或 member 不是 key 的成员时， ZINCRBY key increment member 等同于 ZADD key increment member 。 当 key 不是有序集类型时，返回一个错误。 score 值可以是整数值或双精度浮点数。 返回值member 成员的新 score 值，以字符串形式表示。 代码示例12345redis&gt; ZSCORE salary tom"2000"redis&gt; ZINCRBY salary 2000 tom # tom 加薪啦！"4000" ZCARDZCARD key可用版本： &gt;= 1.2.0 时间复杂度: O(1) 返回有序集 key 的基数。 返回值当 key 存在且是有序集类型时，返回有序集的基数。 当 key 不存在时，返回 0 。 代码示例1234567891011121314151617redis &gt; ZADD salary 2000 tom # 添加一个成员(integer) 1redis &gt; ZCARD salary(integer) 1redis &gt; ZADD salary 5000 jack # 再添加一个成员(integer) 1redis &gt; ZCARD salary(integer) 2redis &gt; EXISTS non_exists_key # 对不存在的 key 进行 ZCARD 操作(integer) 0redis &gt; ZCARD non_exists_key(integer) 0 ZCOUNTZCOUNT key min max可用版本： &gt;= 2.0.0 时间复杂度: O(log(N))， N 为有序集的基数。 返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。 关于参数 min 和 max 的详细使用方法，请参考 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 命令。 返回值score 值在 min 和 max 之间的成员的数量。 代码示例12345678910111213redis&gt; ZRANGE salary 0 -1 WITHSCORES # 测试数据1) "jack"2) "2000"3) "peter"4) "3500"5) "tom"6) "5000"redis&gt; ZCOUNT salary 2000 5000 # 计算薪水在 2000-5000 之间的人数(integer) 3redis&gt; ZCOUNT salary 3000 5000 # 计算薪水在 3000-5000 之间的人数(integer) 2 ZRANGEZRANGE key start stop [WITHSCORES]可用版本： &gt;= 1.2.0 时间复杂度: O(log(N)+M)， N 为有序集的基数，而 M 为结果集的基数。 返回有序集 key 中，指定区间内的成员。 其中成员的位置按 score 值递增(从小到大)来排序。 具有相同 score 值的成员按字典序(lexicographical order )来排列。 如果你需要成员按 score 值递减(从大到小)来排列，请使用 ZREVRANGE key start stop [WITHSCORES] 命令。 下标参数 start 和 stop 都以 0 为底，也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。 你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。 超出范围的下标并不会引起错误。 比如说，当 start 的值比有序集的最大下标还要大，或是 start &gt; stop 时， ZRANGE 命令只是简单地返回一个空列表。 另一方面，假如 stop 参数的值比有序集的最大下标还要大，那么 Redis 将 stop 当作最大下标来处理。 可以通过使用 WITHSCORES 选项，来让成员和它的 score 值一并返回，返回列表以 value1,score1, …, valueN,scoreN 的格式表示。 客户端库可能会返回一些更复杂的数据类型，比如数组、元组等。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例123456789101112131415161718192021222324redis &gt; ZRANGE salary 0 -1 WITHSCORES # 显示整个有序集成员1) "jack"2) "3500"3) "tom"4) "5000"5) "boss"6) "10086"redis &gt; ZRANGE salary 1 2 WITHSCORES # 显示有序集下标区间 1 至 2 的成员1) "tom"2) "5000"3) "boss"4) "10086"redis &gt; ZRANGE salary 0 200000 WITHSCORES # 测试 end 下标超出最大下标时的情况1) "jack"2) "3500"3) "tom"4) "5000"5) "boss"6) "10086"redis &gt; ZRANGE salary 200000 3000000 WITHSCORES # 测试当给定区间不存在于有序集时的情况(empty list or set) ZREVRANGEZREVRANGE key start stop [WITHSCORES]可用版本： &gt;= 1.2.0 时间复杂度: O(log(N)+M)， N 为有序集的基数，而 M 为结果集的基数。 返回有序集 key 中，指定区间内的成员。 其中成员的位置按 score 值递减(从大到小)来排列。 具有相同 score 值的成员按字典序的逆序(reverse lexicographical order)排列。 除了成员按 score 值递减的次序排列这一点外， ZREVRANGE 命令的其他方面和 ZRANGE key start stop [WITHSCORES] 命令一样。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例123456789101112131415redis&gt; ZRANGE salary 0 -1 WITHSCORES # 递增排列1) "peter"2) "3500"3) "tom"4) "4000"5) "jack"6) "5000"redis&gt; ZREVRANGE salary 0 -1 WITHSCORES # 递减排列1) "jack"2) "5000"3) "tom"4) "4000"5) "peter"6) "3500" ZRANGEBYSCOREZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]可用版本： &gt;= 1.0.5 时间复杂度: O(log(N)+M)， N 为有序集的基数， M 为被结果集的基数。 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 具有相同 score 值的成员按字典序(lexicographical order)来排列(该属性是有序集提供的，不需要额外的计算)。 可选的 LIMIT 参数指定返回结果的数量及区间(就像SQL中的 SELECT LIMIT offset, count )，注意当 offset 很大时，定位 offset 的操作可能需要遍历整个有序集，此过程最坏复杂度为 O(N) 时间。 可选的 WITHSCORES 参数决定结果集是单单返回有序集的成员，还是将有序集成员及其 score 值一起返回。 该选项自 Redis 2.0 版本起可用。 区间及无限min 和 max 可以是 -inf 和 +inf ，这样一来，你就可以在不知道有序集的最低和最高 score 值的情况下，使用 ZRANGEBYSCORE 这类命令。 默认情况下，区间的取值使用闭区间 (小于等于或大于等于)，你也可以通过给参数前增加 ( 符号来使用可选的开区间 (小于或大于)。 举个例子：ZRANGEBYSCORE zset (1 5返回所有符合条件 1 &lt; score &lt;= 5 的成员，而 ZRANGEBYSCORE zset (5 (10则返回所有符合条件 5 &lt; score &lt; 10 的成员。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例12345678910111213141516171819202122232425262728redis&gt; ZADD salary 2500 jack # 测试数据(integer) 0redis&gt; ZADD salary 5000 tom(integer) 0redis&gt; ZADD salary 12000 peter(integer) 0redis&gt; ZRANGEBYSCORE salary -inf +inf # 显示整个有序集1) "jack"2) "tom"3) "peter"redis&gt; ZRANGEBYSCORE salary -inf +inf WITHSCORES # 显示整个有序集及成员的 score 值1) "jack"2) "2500"3) "tom"4) "5000"5) "peter"6) "12000"redis&gt; ZRANGEBYSCORE salary -inf 5000 WITHSCORES # 显示工资 &lt;=5000 的所有成员1) "jack"2) "2500"3) "tom"4) "5000"redis&gt; ZRANGEBYSCORE salary (5000 400000 # 显示工资大于 5000 小于等于 400000 的成员1) "peter" ZREVRANGEBYSCOREZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]可用版本： &gt;= 2.2.0 时间复杂度: O(log(N)+M)， N 为有序集的基数， M 为结果集的基数。 返回有序集 key 中， score 值介于 max 和 min 之间(默认包括等于 max 或 min )的所有的成员。有序集成员按 score 值递减(从大到小)的次序排列。 具有相同 score 值的成员按字典序的逆序(reverse lexicographical order )排列。 除了成员按 score 值递减的次序排列这一点外， ZREVRANGEBYSCORE 命令的其他方面和 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 命令一样。 返回值指定区间内，带有 score 值(可选)的有序集成员的列表。 代码示例12345678910111213141516171819redis &gt; ZADD salary 10086 jack(integer) 1redis &gt; ZADD salary 5000 tom(integer) 1redis &gt; ZADD salary 7500 peter(integer) 1redis &gt; ZADD salary 3500 joe(integer) 1redis &gt; ZREVRANGEBYSCORE salary +inf -inf # 逆序排列所有成员1) "jack"2) "peter"3) "tom"4) "joe"redis &gt; ZREVRANGEBYSCORE salary 10000 2000 # 逆序排列薪水介于 10000 和 2000 之间的成员1) "peter"2) "tom"3) "joe" ZRANKZRANK key member可用版本： &gt;= 2.0.0 时间复杂度: O(log(N)) 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。 排名以 0 为底，也就是说， score 值最小的成员排名为 0 。 使用 ZREVRANK key member 命令可以获得成员按 score 值递减(从大到小)排列的排名。 返回值如果 member 是有序集 key 的成员，返回 member 的排名。 如果 member 不是有序集 key 的成员，返回 nil 。 代码示例12345678910redis&gt; ZRANGE salary 0 -1 WITHSCORES # 显示所有成员及其 score 值1) "peter"2) "3500"3) "tom"4) "4000"5) "jack"6) "5000"redis&gt; ZRANK salary tom # 显示 tom 的薪水排名，第二(integer) 1 ZREVRANKZREVRANK key member可用版本： &gt;= 2.0.0 时间复杂度: O(log(N)) 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递减(从大到小)排序。 排名以 0 为底，也就是说， score 值最大的成员排名为 0 。 使用 ZRANK key member 命令可以获得成员按 score 值递增(从小到大)排列的排名。 返回值如果 member 是有序集 key 的成员，返回 member 的排名。 如果 member 不是有序集 key 的成员，返回 nil 。 代码示例12345678910111213redis 127.0.0.1:6379&gt; ZRANGE salary 0 -1 WITHSCORES # 测试数据1) "jack"2) "2000"3) "peter"4) "3500"5) "tom"6) "5000"redis&gt; ZREVRANK salary peter # peter 的工资排第二(integer) 1redis&gt; ZREVRANK salary tom # tom 的工资最高(integer) 0 ZREMZREM key member [member …]可用版本： &gt;= 1.2.0 时间复杂度: O(M*log(N))， N 为有序集的基数， M 为被成功移除的成员的数量。 移除有序集 key 中的一个或多个成员，不存在的成员将被忽略。 当 key 存在但不是有序集类型时，返回一个错误。 Note在 Redis 2.4 版本以前， ZREM 每次只能删除一个元素。 返回值被成功移除的成员的数量，不包括被忽略的成员。 代码示例1234567891011121314151617181920212223242526272829# 测试数据redis&gt; ZRANGE page_rank 0 -1 WITHSCORES1) "bing.com"2) "8"3) "baidu.com"4) "9"5) "google.com"6) "10"# 移除单个元素redis&gt; ZREM page_rank google.com(integer) 1redis&gt; ZRANGE page_rank 0 -1 WITHSCORES1) "bing.com"2) "8"3) "baidu.com"4) "9"# 移除多个元素redis&gt; ZREM page_rank baidu.com bing.com(integer) 2redis&gt; ZRANGE page_rank 0 -1 WITHSCORES(empty list or set)# 移除不存在元素redis&gt; ZREM page_rank non-exists-element(integer) 0 ZREMRANGEBYRANKZREMRANGEBYRANK key start stop可用版本： &gt;= 2.0.0 时间复杂度: O(log(N)+M)， N 为有序集的基数，而 M 为被移除成员的数量。 移除有序集 key 中，指定排名(rank)区间内的所有成员。 区间分别以下标参数 start 和 stop 指出，包含 start 和 stop 在内。 下标参数 start 和 stop 都以 0 为底，也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。 你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。 返回值被移除成员的数量。 代码示例12345678910111213redis&gt; ZADD salary 2000 jack(integer) 1redis&gt; ZADD salary 5000 tom(integer) 1redis&gt; ZADD salary 3500 peter(integer) 1redis&gt; ZREMRANGEBYRANK salary 0 1 # 移除下标 0 至 1 区间内的成员(integer) 2redis&gt; ZRANGE salary 0 -1 WITHSCORES # 有序集只剩下一个成员1) "tom"2) "5000" ZREMRANGEBYSCOREZREMRANGEBYSCORE key min max可用版本： &gt;= 1.2.0 时间复杂度： O(log(N)+M)， N 为有序集的基数，而 M 为被移除成员的数量。 移除有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。 自版本2.1.6开始， score 值等于 min 或 max 的成员也可以不包括在内，详情请参见 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 命令。 返回值被移除成员的数量。 代码示例1234567891011121314redis&gt; ZRANGE salary 0 -1 WITHSCORES # 显示有序集内所有成员及其 score 值1) "tom"2) "2000"3) "peter"4) "3500"5) "jack"6) "5000"redis&gt; ZREMRANGEBYSCORE salary 1500 3500 # 移除所有薪水在 1500 到 3500 内的员工(integer) 2redis&gt; ZRANGE salary 0 -1 WITHSCORES # 剩下的有序集成员1) "jack"2) "5000" ZRANGEBYLEXZRANGEBYLEX key min max [LIMIT offset count]可用版本： &gt;= 2.8.9 时间复杂度：O(log(N)+M)， 其中 N 为有序集合的元素数量， 而 M 则是命令返回的元素数量。 如果 M 是一个常数（比如说，用户总是使用 LIMIT 参数来返回最先的 10 个元素）， 那么命令的复杂度也可以看作是 O(log(N)) 。 当有序集合的所有成员都具有相同的分值时， 有序集合的元素会根据成员的字典序（lexicographical ordering）来进行排序， 而这个命令则可以返回给定的有序集合键 key 中， 值介于 min 和 max 之间的成员。 如果有序集合里面的成员带有不同的分值， 那么命令返回的结果是未指定的（unspecified）。 命令会使用 C 语言的 memcmp() 函数， 对集合中的每个成员进行逐个字节的对比（byte-by-byte compare）， 并按照从低到高的顺序， 返回排序后的集合成员。 如果两个字符串有一部分内容是相同的话， 那么命令会认为较长的字符串比较短的字符串要大。 可选的 LIMIT offset count 参数用于获取指定范围内的匹配元素 （就像 SQL 中的 SELECT LIMIT offset count 语句）。 需要注意的一点是， 如果 offset 参数的值非常大的话， 那么命令在返回结果之前， 需要先遍历至 offset 所指定的位置， 这个操作会为命令加上最多 O(N) 复杂度。 如何指定范围区间合法的 min 和 max 参数必须包含 ( 或者 [ ， 其中 ( 表示开区间（指定的值不会被包含在范围之内）， 而 [ 则表示闭区间（指定的值会被包含在范围之内）。 特殊值 + 和 - 在 min 参数以及 max 参数中具有特殊的意义， 其中 + 表示正无限， 而 - 表示负无限。 因此， 向一个所有成员的分值都相同的有序集合发送命令 ZRANGEBYLEX \&lt;zset> - + ， 命令将返回有序集合中的所有元素。 返回值数组回复：一个列表，列表里面包含了有序集合在指定范围内的成员。 代码示例123456789101112131415161718redis&gt; ZADD myzset 0 a 0 b 0 c 0 d 0 e 0 f 0 g(integer) 7redis&gt; ZRANGEBYLEX myzset - [c1) "a"2) "b"3) "c"redis&gt; ZRANGEBYLEX myzset - (c1) "a"2) "b"redis&gt; ZRANGEBYLEX myzset [aaa (g1) "b"2) "c"3) "d"4) "e"5) "f" ZLEXCOUNTZLEXCOUNT key min max可用版本： &gt;= 2.8.9 时间复杂度： O(log(N))，其中 N 为有序集合包含的元素数量。 对于一个所有成员的分值都相同的有序集合键 key 来说， 这个命令会返回该集合中， 成员介于 min 和 max 范围内的元素数量。 这个命令的 min 参数和 max 参数的意义和 ZRANGEBYLEX key min max [LIMIT offset count] 命令的 min 参数和 max 参数的意义一样。 返回值整数回复：指定范围内的元素数量。 代码示例1234567891011redis&gt; ZADD myzset 0 a 0 b 0 c 0 d 0 e(integer) 5redis&gt; ZADD myzset 0 f 0 g(integer) 2redis&gt; ZLEXCOUNT myzset - +(integer) 7redis&gt; ZLEXCOUNT myzset [b [f(integer) 5 ZREMRANGEBYLEXZREMRANGEBYLEX key min max可用版本： &gt;= 2.8.9 时间复杂度： O(log(N)+M)， 其中 N 为有序集合的元素数量， 而 M 则为被移除的元素数量。 对于一个所有成员的分值都相同的有序集合键 key 来说， 这个命令会移除该集合中， 成员介于 min 和 max 范围内的所有元素。 这个命令的 min 参数和 max 参数的意义和 ZRANGEBYLEX key min max [LIMIT offset count] 命令的 min 参数和 max 参数的意义一样。 返回值整数回复：被移除的元素数量。 代码示例1234567891011121314151617181920212223242526redis&gt; ZADD myzset 0 aaaa 0 b 0 c 0 d 0 e(integer) 5redis&gt; ZADD myzset 0 foo 0 zap 0 zip 0 ALPHA 0 alpha(integer) 5redis&gt; ZRANGE myzset 0 -11) "ALPHA"2) "aaaa"3) "alpha"4) "b"5) "c"6) "d"7) "e"8) "foo"9) "zap"10) "zip"redis&gt; ZREMRANGEBYLEX myzset [alpha [omega(integer) 6redis&gt; ZRANGE myzset 0 -11) "ALPHA"2) "aaaa"3) "zap"4) "zip" ZSCANZSCAN key cursor [MATCH pattern] [COUNT count]详细信息请参考 SCAN cursor [MATCH pattern] [COUNT count] 命令。 ZUNIONSTOREZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX]可用版本：&gt;= 2.0.0 时间复杂度: O(N)+O(M log(M))， N 为给定有序集基数的总和， M 为结果集的基数。 计算给定的一个或多个有序集的并集，其中给定 key 的数量必须以 numkeys 参数指定，并将该并集(结果集)储存到 destination 。 默认情况下，结果集中某个成员的 score 值是所有给定集下该成员 score 值之 和 。 WEIGHTS使用 WEIGHTS 选项，你可以为 每个 给定有序集 分别 指定一个乘法因子(multiplication factor)，每个给定有序集的所有成员的 score 值在传递给聚合函数(aggregation function)之前都要先乘以该有序集的因子。 如果没有指定 WEIGHTS 选项，乘法因子默认设置为 1 。 AGGREGATE使用 AGGREGATE 选项，你可以指定并集的结果集的聚合方式。 默认使用的参数 SUM ，可以将所有集合中某个成员的 score 值之 和 作为结果集中该成员的 score 值；使用参数 MIN ，可以将所有集合中某个成员的 最小 score 值作为结果集中该成员的 score 值；而参数 MAX 则是将所有集合中某个成员的 最大 score 值作为结果集中该成员的 score 值。 返回值保存到 destination 的结果集的基数。 代码示例1234567891011121314151617181920212223242526272829303132redis&gt; ZRANGE programmer 0 -1 WITHSCORES1) "peter"2) "2000"3) "jack"4) "3500"5) "tom"6) "5000"redis&gt; ZRANGE manager 0 -1 WITHSCORES1) "herry"2) "2000"3) "mary"4) "3500"5) "bob"6) "4000"redis&gt; ZUNIONSTORE salary 2 programmer manager WEIGHTS 1 3 # 公司决定加薪。。。除了程序员。。。(integer) 6redis&gt; ZRANGE salary 0 -1 WITHSCORES1) "peter"2) "2000"3) "jack"4) "3500"5) "tom"6) "5000"7) "herry"8) "6000"9) "mary"10) "10500"11) "bob"12) "12000" ZINTERSTOREZINTERSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX]可用版本： &gt;= 2.0.0 时间复杂度: O(N*K)+O(M*log(M))， N 为给定 key 中基数最小的有序集， K 为给定有序集的数量， M 为结果集的基数。 计算给定的一个或多个有序集的交集，其中给定 key 的数量必须以 numkeys 参数指定，并将该交集(结果集)储存到 destination 。 默认情况下，结果集中某个成员的 score 值是所有给定集下该成员 score 值之和. 关于 WEIGHTS 和 AGGREGATE 选项的描述，参见 ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 命令。 返回值保存到 destination 的结果集的基数。 代码示例123456789101112131415161718192021222324redis &gt; ZADD mid_test 70 "Li Lei"(integer) 1redis &gt; ZADD mid_test 70 "Han Meimei"(integer) 1redis &gt; ZADD mid_test 99.5 "Tom"(integer) 1redis &gt; ZADD fin_test 88 "Li Lei"(integer) 1redis &gt; ZADD fin_test 75 "Han Meimei"(integer) 1redis &gt; ZADD fin_test 99.5 "Tom"(integer) 1redis &gt; ZINTERSTORE sum_point 2 mid_test fin_test(integer) 3redis &gt; ZRANGE sum_point 0 -1 WITHSCORES # 显示有序集内所有成员及其 score 值1) "Han Meimei"2) "145"3) "Li Lei"4) "158"5) "Tom"6) "199" HyperLogLogPFADDPFADD key element [element …]可用版本： &gt;= 2.8.9 时间复杂度： 每添加一个元素的复杂度为 O(1) 。 将任意数量的元素添加到指定的 HyperLogLog 里面。 作为这个命令的副作用， HyperLogLog 内部可能会被更新， 以便反映一个不同的唯一元素估计数量（也即是集合的基数）。 如果 HyperLogLog 估计的近似基数（approximated cardinality）在命令执行之后出现了变化， 那么命令返回 1 ， 否则返回 0 。 如果命令执行时给定的键不存在， 那么程序将先创建一个空的 HyperLogLog 结构， 然后再执行命令。 调用 PFADD key element [element …] 命令时可以只给定键名而不给定元素： 如果给定键已经是一个 HyperLogLog ， 那么这种调用不会产生任何效果；但如果给定的键不存在， 那么命令会创建一个空的 HyperLogLog ， 并向客户端返回 1 。要了解更多关于 HyperLogLog 数据结构的介绍知识， 请查阅 PFCOUNT key [key …] 命令的文档。 返回值整数回复： 如果 HyperLogLog 的内部储存被修改了， 那么返回 1 ， 否则返回 0 。 代码示例1234567891011121314151617redis&gt; PFADD databases "Redis" "MongoDB" "MySQL"(integer) 1redis&gt; PFCOUNT databases(integer) 3redis&gt; PFADD databases "Redis" # Redis 已经存在，不必对估计数量进行更新(integer) 0redis&gt; PFCOUNT databases # 元素估计数量没有变化(integer) 3redis&gt; PFADD databases "PostgreSQL" # 添加一个不存在的元素(integer) 1redis&gt; PFCOUNT databases # 估计数量增一4 PFCOUNTPFCOUNT key [key …]可用版本： &gt;= 2.8.9 时间复杂度： 当命令作用于单个 HyperLogLog 时， 复杂度为 O(1) ， 并且具有非常低的平均常数时间。 当命令作用于 N 个 HyperLogLog 时， 复杂度为 O(N) ， 常数时间也比处理单个 HyperLogLog 时要大得多。 当 PFCOUNT key [key …] 命令作用于单个键时， 返回储存在给定键的 HyperLogLog 的近似基数， 如果键不存在， 那么返回 0 。 当 PFCOUNT key [key …] 命令作用于多个键时， 返回所有给定 HyperLogLog 的并集的近似基数， 这个近似基数是通过将所有给定 HyperLogLog 合并至一个临时 HyperLogLog 来计算得出的。 通过 HyperLogLog 数据结构， 用户可以使用少量固定大小的内存， 来储存集合中的唯一元素 （每个 HyperLogLog 只需使用 12k 字节内存，以及几个字节的内存来储存键本身）。 命令返回的可见集合（observed set）基数并不是精确值， 而是一个带有 0.81% 标准错误（standard error）的近似值。 举个例子， 为了记录一天会执行多少次各不相同的搜索查询， 一个程序可以在每次执行搜索查询时调用一次 PFADD key element [element …] ， 并通过调用 PFCOUNT key [key …] 命令来获取这个记录的近似结果。 返回值整数回复： 给定 HyperLogLog 包含的唯一元素的近似数量。 代码示例1234567891011121314151617redis&gt; PFADD databases "Redis" "MongoDB" "MySQL"(integer) 1redis&gt; PFCOUNT databases(integer) 3redis&gt; PFADD databases "Redis" # Redis 已经存在，不必对估计数量进行更新(integer) 0redis&gt; PFCOUNT databases # 元素估计数量没有变化(integer) 3redis&gt; PFADD databases "PostgreSQL" # 添加一个不存在的元素(integer) 1redis&gt; PFCOUNT databases # 估计数量增一4 PFMERGEPFMERGE destkey sourcekey [sourcekey …]可用版本： &gt;= 2.8.9 时间复杂度： O(N) ， 其中 N 为被合并的 HyperLogLog 数量， 不过这个命令的常数复杂度比较高。 将多个 HyperLogLog 合并（merge）为一个 HyperLogLog ， 合并后的 HyperLogLog 的基数接近于所有输入 HyperLogLog 的可见集合（observed set）的并集。 合并得出的 HyperLogLog 会被储存在 destkey 键里面， 如果该键并不存在， 那么命令在执行之前， 会先为该键创建一个空的 HyperLogLog 。 返回值字符串回复：返回 OK 。 代码示例1234567891011redis&gt; PFADD nosql "Redis" "MongoDB" "Memcached"(integer) 1redis&gt; PFADD RDBMS "MySQL" "MSSQL" "PostgreSQL"(integer) 1redis&gt; PFMERGE databases nosql RDBMSOKredis&gt; PFCOUNT databases(integer) 6 地理位置GEOADDGEOADD key longitude latitude member [longitude latitude member …]可用版本： &gt;= 3.2.0 时间复杂度： 每添加一个元素的复杂度为 O(log(N)) ， 其中 N 为键里面包含的位置元素数量。 将给定的空间元素（纬度、经度、名字）添加到指定的键里面。 这些数据会以有序集合的形式被储存在键里面， 从而使得像 GEORADIUS 和 GEORADIUSBYMEMBER 这样的命令可以在之后通过位置查询取得这些元素。 GEOADD 命令以标准的 x,y 格式接受参数， 所以用户必须先输入经度， 然后再输入纬度。 GEOADD 能够记录的坐标是有限的： 非常接近两极的区域是无法被索引的。 精确的坐标限制由 EPSG:900913 / EPSG:3785 / OSGEO:41001 等坐标系统定义， 具体如下： 有效的经度介于 -180 度至 180 度之间。有效的纬度介于 -85.05112878 度至 85.05112878 度之间。当用户尝试输入一个超出范围的经度或者纬度时， GEOADD 命令将返回一个错误。 返回值新添加到键里面的空间元素数量， 不包括那些已经存在但是被更新的元素。 代码示例123456789101112redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEODIST Sicily Palermo Catania"166274.15156960039"redis&gt; GEORADIUS Sicily 15 37 100 km1) "Catania"redis&gt; GEORADIUS Sicily 15 37 200 km1) "Palermo"2) "Catania" GEOPOSGEOPOS key member [member …]可用版本： &gt;= 3.2.0 时间复杂度： 获取每个位置元素的复杂度为 O(log(N)) ， 其中 N 为键里面包含的位置元素数量。 从键里面返回所有给定位置元素的位置（经度和纬度）。 因为 GEOPOS 命令接受可变数量的位置元素作为输入， 所以即使用户只给定了一个位置元素， 命令也会返回数组回复。 返回值GEOPOS 命令返回一个数组， 数组中的每个项都由两个元素组成： 第一个元素为给定位置元素的经度， 而第二个元素则为给定位置元素的纬度。 当给定的位置元素不存在时， 对应的数组项为空值。 代码示例123456789redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEOPOS Sicily Palermo Catania NonExisting1) 1) "13.361389338970184" 2) "38.115556395496299"2) 1) "15.087267458438873" 2) "37.50266842333162"3) (nil) GEODISTGEODIST key member1 member2 [unit]可用版本： &gt;= 3.2.0 复杂度： O(log(N)) 返回两个给定位置之间的距离。 如果两个位置之间的其中一个不存在， 那么命令返回空值。 指定单位的参数 unit 必须是以下单位的其中一个： m 表示单位为米。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 如果用户没有显式地指定单位参数， 那么 GEODIST 默认使用米作为单位。 GEODIST 命令在计算距离时会假设地球为完美的球形， 在极限情况下， 这一假设最大会造成 0.5% 的误差。 返回值计算出的距离会以双精度浮点数的形式被返回。 如果给定的位置元素不存在， 那么命令返回空值。 代码示例1234567891011121314redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEODIST Sicily Palermo Catania"166274.15156960039"redis&gt; GEODIST Sicily Palermo Catania km"166.27415156960038"redis&gt; GEODIST Sicily Palermo Catania mi"103.31822459492736"redis&gt; GEODIST Sicily Foo Bar(nil) GEORADIUSGEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [ASC|DESC] [COUNT count]可用版本： &gt;= 3.2.0 时间复杂度： O(N+log(M))， 其中 N 为指定半径范围内的位置元素数量， 而 M 则是被返回位置元素的数量。 以给定的经纬度为中心， 返回键包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素。 范围可以使用以下其中一个单位： m 表示单位为米。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 在给定以下可选项时， 命令会返回额外的信息： WITHDIST ： 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 距离的单位和用户给定的范围单位保持一致。 WITHCOORD ： 将位置元素的经度和维度也一并返回。 WITHHASH ： 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 命令默认返回未排序的位置元素。 通过以下两个参数， 用户可以指定被返回位置元素的排序方式： ASC ： 根据中心的位置， 按照从近到远的方式返回位置元素。 DESC ： 根据中心的位置， 按照从远到近的方式返回位置元素。 在默认情况下， GEORADIUS 命令会返回所有匹配的位置元素。 虽然用户可以使用 COUNT \&lt;count> 选项去获取前 N 个匹配元素， 但是因为命令在内部可能会需要对所有被匹配的元素进行处理， 所以在对一个非常大的区域进行搜索时， 即使只使用 COUNT 选项去获取少量元素， 命令的执行速度也可能会非常慢。 但是从另一方面来说， 使用 COUNT 选项去减少需要返回的元素数量， 对于减少带宽来说仍然是非常有用的。 返回值GEORADIUS 命令返回一个数组， 具体来说： 在没有给定任何 WITH 选项的情况下， 命令只会返回一个像 [“New York”,”Milan”,”Paris”] 这样的线性（linear）列表。 在指定了 WITHCOORD 、 WITHDIST 、 WITHHASH 等选项的情况下， 命令返回一个二层嵌套数组， 内层的每个子数组就表示一个元素。 在返回嵌套数组时， 子数组的第一个元素总是位置元素的名字。 至于额外的信息， 则会作为子数组的后续元素， 按照以下顺序被返回： 以浮点数格式返回的中心与位置元素之间的距离， 单位与用户指定范围时的单位一致。 geohash 整数。 由两个元素组成的坐标，分别为经度和纬度。 举个例子， GEORADIUS Sicily 15 37 200 km withcoord withdist 这样的命令返回的每个子数组都是类似以下格式的： [“Palermo”,”190.4424”,[“13.361389338970184”,”38.115556395496299”]] 代码示例1234567891011121314151617181920212223242526redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEORADIUS Sicily 15 37 200 km WITHDIST1) 1) "Palermo" 2) "190.4424"2) 1) "Catania" 2) "56.4413"redis&gt; GEORADIUS Sicily 15 37 200 km WITHCOORD1) 1) "Palermo" 2) 1) "13.361389338970184" 2) "38.115556395496299"2) 1) "Catania" 2) 1) "15.087267458438873" 2) "37.50266842333162"redis&gt; GEORADIUS Sicily 15 37 200 km WITHDIST WITHCOORD1) 1) "Palermo" 2) "190.4424" 3) 1) "13.361389338970184" 2) "38.115556395496299"2) 1) "Catania" 2) "56.4413" 3) 1) "15.087267458438873" 2) "37.50266842333162" GEORADIUSBYMEMBERGEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [ASC|DESC] [COUNT count]可用版本： &gt;= 3.2.0 时间复杂度： O(log(N)+M)， 其中 N 为指定范围之内的元素数量， 而 M 则是被返回的元素数量。 这个命令和 GEORADIUS 命令一样， 都可以找出位于指定范围内的元素， 但是 GEORADIUSBYMEMBER 的中心点是由给定的位置元素决定的， 而不是像 GEORADIUS 那样， 使用输入的经度和纬度来决定中心点。 返回值一个数组， 数组中的每个项表示一个范围之内的位置元素。 代码示例123456789redis&gt; GEOADD Sicily 13.583333 37.316667 "Agrigento"(integer) 1redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEORADIUSBYMEMBER Sicily Agrigento 100 km1) "Agrigento"2) "Palermo" GEOHASHGEOHASH key member [member …]可用版本： &gt;= 3.2.0 时间复杂度： 寻找每个位置元素的复杂度为 O(log(N)) ， 其中 N 为给定键包含的位置元素数量。 返回一个或多个位置元素的 Geohash 表示。 返回值一个数组， 数组的每个项都是一个 geohash 。 命令返回的 geohash 的位置与用户给定的位置元素的位置一一对应。 代码示例123456redis&gt; GEOADD Sicily 13.361389 38.115556 "Palermo" 15.087269 37.502669 "Catania"(integer) 2redis&gt; GEOHASH Sicily Palermo Catania1) "sqc8b49rny0"2) "sqdtr74hyu0" 位图SETBITSETBIT key offset value可用版本： &gt;= 2.2.0 时间复杂度: O(1) 对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 位的设置或清除取决于 value 参数，可以是 0 也可以是 1 。 当 key 不存在时，自动生成一个新的字符串值。 字符串会进行伸展(grown)以确保它可以将 value 保存在指定的偏移量上。当字符串值进行伸展时，空白位置以 0 填充。 offset 参数必须大于或等于 0 ，小于 2^32 (bit 映射被限制在 512 MB 之内)。 Warning对使用大的 offset 的 SETBIT 操作来说，内存分配可能造成 Redis 服务器被阻塞。具体参考 SETRANGE key offset value 命令，warning(警告)部分。 返回值指定偏移量原来储存的位。 代码示例12345678redis&gt; SETBIT bit 10086 1(integer) 0redis&gt; GETBIT bit 10086(integer) 1redis&gt; GETBIT bit 100 # bit 默认被初始化为 0(integer) 0 GETBITGETBIT key offset可用版本： &gt;= 2.2.0 时间复杂度： O(1) 对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 当 offset 比字符串值的长度大，或者 key 不存在时，返回 0 。 返回值字符串值指定偏移量上的位(bit)。 代码示例12345678910111213# 对不存在的 key 或者不存在的 offset 进行 GETBIT， 返回 0redis&gt; EXISTS bit(integer) 0redis&gt; GETBIT bit 10086(integer) 0# 对已存在的 offset 进行 GETBITredis&gt; SETBIT bit 10086 1(integer) 0redis&gt; GETBIT bit 10086(integer) 1 BITCOUNTBITCOUNT key [start] [end]可用版本： &gt;= 2.6.0 时间复杂度： O(N) 计算给定字符串中，被设置为 1 的比特位的数量。 一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。 start 和 end 参数的设置和 GETRANGE key start end 命令类似，都可以使用负数值： 比如 -1 表示最后一个字节， -2 表示倒数第二个字节，以此类推。 不存在的 key 被当成是空字符串来处理，因此对一个不存在的 key 进行 BITCOUNT 操作，结果为 0 。 返回值被设置为 1 的位的数量。 代码示例1234567891011121314redis&gt; BITCOUNT bits(integer) 0redis&gt; SETBIT bits 0 1 # 0001(integer) 0redis&gt; BITCOUNT bits(integer) 1redis&gt; SETBIT bits 3 1 # 1001(integer) 0redis&gt; BITCOUNT bits(integer) 2 模式：使用 bitmap 实现用户上线次数统计Bitmap 对于一些特定类型的计算非常有效。 假设现在我们希望记录自己网站上的用户的上线频率，比如说，计算用户 A 上线了多少天，用户 B 上线了多少天，诸如此类，以此作为数据，从而决定让哪些用户参加 beta 测试等活动 —— 这个模式可以使用 SETBIT key offset value 和 BITCOUNT key [start] [end] 来实现。 比如说，每当用户在某一天上线的时候，我们就使用 SETBIT key offset value ，以用户名作为 key ，将那天所代表的网站的上线日作为 offset 参数，并将这个 offset 上的为设置为 1 。 举个例子，如果今天是网站上线的第 100 天，而用户 peter 在今天阅览过网站，那么执行命令 SETBIT peter 100 1 ；如果明天 peter 也继续阅览网站，那么执行命令 SETBIT peter 101 1 ，以此类推。 当要计算 peter 总共以来的上线次数时，就使用 BITCOUNT key [start] [end] 命令：执行 BITCOUNT peter ，得出的结果就是 peter 上线的总天数。 性能前面的上线次数统计例子，即使运行 10 年，占用的空间也只是每个用户 10*365 比特位(bit)，也即是每个用户 456 字节。对于这种大小的数据来说， BITCOUNT key [start] [end] 的处理速度就像 GET key 和 INCR key 这种 O(1) 复杂度的操作一样快。 如果你的 bitmap 数据非常大，那么可以考虑使用以下两种方法： 将一个大的 bitmap 分散到不同的 key 中，作为小的 bitmap 来处理。使用 Lua 脚本可以很方便地完成这一工作。 使用 BITCOUNT key [start] [end] 的 start 和 end 参数，每次只对所需的部分位进行计算，将位的累积工作(accumulating)放到客户端进行，并且对结果进行缓存 (caching)。 BITPOSBITPOS key bit [start] [end]可用版本： &gt;= 2.8.7 时间复杂度： O(N)，其中 N 为位图包含的二进制位数量 返回位图中第一个值为 bit 的二进制位的位置。 在默认情况下， 命令将检测整个位图， 但用户也可以通过可选的 start 参数和 end 参数指定要检测的范围。 返回值整数回复。 代码示例12345678127.0.0.1:6379&gt; SETBIT bits 3 1 # 1000(integer) 0127.0.0.1:6379&gt; BITPOS bits 0(integer) 0127.0.0.1:6379&gt; BITPOS bits 1(integer) 3 BITOPBITOP operation destkey key [key …] 可用版本： &gt;= 2.6.0 时间复杂度： O(N)对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 operation 可以是 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种： BITOP AND destkey key [key ...] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。 BITOP OR destkey key [key ...] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。 BITOP XOR destkey key [key ...] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey 。 BITOP NOT destkey key ，对给定 key 求逻辑非，并将结果保存到 destkey 。 除了 NOT 操作之外，其他操作都可以接受一个或多个 key 作为输入。 处理不同长度的字符串 当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0 。 空的 key 也被看作是包含 0 的字符串序列。 返回值保存到 destkey 的字符串的长度，和输入 key 中最长的字符串长度相等。 NoteBITOP 的复杂度为 O(N) ，当处理大型矩阵(matrix)或者进行大数据量的统计时，最好将任务指派到附属节点(slave)进行，避免阻塞主节点。 代码示例1234567891011121314151617181920212223242526272829redis&gt; SETBIT bits-1 0 1 # bits-1 = 1001(integer) 0redis&gt; SETBIT bits-1 3 1(integer) 0redis&gt; SETBIT bits-2 0 1 # bits-2 = 1011(integer) 0redis&gt; SETBIT bits-2 1 1(integer) 0redis&gt; SETBIT bits-2 3 1(integer) 0redis&gt; BITOP AND and-result bits-1 bits-2(integer) 1redis&gt; GETBIT and-result 0 # and-result = 1001(integer) 1redis&gt; GETBIT and-result 1(integer) 0redis&gt; GETBIT and-result 2(integer) 0redis&gt; GETBIT and-result 3(integer) 1 BITFIELDBITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL]可用版本： &gt;= 3.2.0 时间复杂度： 每个子命令的复杂度为 O(1) 。 BITFIELD 命令可以将一个 Redis 字符串看作是一个由二进制位组成的数组， 并对这个数组中储存的长度不同的整数进行访问 （被储存的整数无需进行对齐）。 换句话说， 通过这个命令， 用户可以执行诸如 “对偏移量 1234 上的 5 位长有符号整数进行设置”、 “获取偏移量 4567 上的 31 位长无符号整数”等操作。 此外， BITFIELD 命令还可以对指定的整数执行加法操作和减法操作， 并且这些操作可以通过设置妥善地处理计算时出现的溢出情况。 BITFIELD 命令可以在一次调用中同时对多个位范围进行操作： 它接受一系列待执行的操作作为参数， 并返回一个数组作为回复， 数组中的每个元素就是对应操作的执行结果。 比如以下命令就展示了如何对位于偏移量 100 的 8 位长有符号整数执行加法操作， 并获取位于偏移量 0 上的 4 位长无符号整数：123&gt; BITFIELD mykey INCRBY i8 100 1 GET u4 01) (integer) 12) (integer) 0 注意： 使用 GET 子命令对超出字符串当前范围的二进制位进行访问（包括键不存在的情况）， 超出部分的二进制位的值将被当做是 0 。 使用 SET 子命令或者 INCRBY 子命令对超出字符串当前范围的二进制位进行访问将导致字符串被扩大， 被扩大的部分会使用值为 0 的二进制位进行填充。 在对字符串进行扩展时， 命令会根据字符串目前已有的最远端二进制位， 计算出执行操作所需的最小长度。 支持的子命令以及数字类型以下是 BITFIELD 命令支持的子命令： GET &lt;type&gt; &lt;offset&gt; —— 返回指定的二进制位范围。 SET &lt;type&gt; &lt;offset&gt; &lt;value&gt; —— 对指定的二进制位范围进行设置，并返回它的旧值。 INCRBY &lt;type&gt; &lt;offset&gt; &lt;increment&gt; —— 对指定的二进制位范围执行加法操作，并返回它的旧值。用户可以通过向 increment 参数传入负值来实现相应的减法操作。 除了以上三个子命令之外， 还有一个子命令， 它可以改变之后执行的 INCRBY 子命令在发生溢出情况时的行为： OVERFLOW [WRAP|SAT|FAIL] 当被设置的二进制位范围值为整数时， 用户可以在类型参数的前面添加 i 来表示有符号整数， 或者使用 u 来表示无符号整数。 比如说， 我们可以使用 u8 来表示 8 位长的无符号整数， 也可以使用 i16 来表示 16 位长的有符号整数。 BITFIELD 命令最大支持 64 位长的有符号整数以及 63 位长的无符号整数， 其中无符号整数的 63 位长度限制是由于 Redis 协议目前还无法返回 64 位长的无符号整数而导致的。 二进制位和位置偏移量在二进制位范围命令中， 用户有两种方法来设置偏移量： 如果用户给定的是一个没有任何前缀的数字， 那么这个数字指示的就是字符串以零为开始（zero-base）的偏移量。 另一方面， 如果用户给定的是一个带有 # 前缀的偏移量， 那么命令将使用这个偏移量与被设置的数字类型的位长度相乘， 从而计算出真正的偏移量。 比如说， 对于以下这个命令来说： BITFIELD mystring SET i8 #0 100 i8 #1 200 命令会把 mystring 键里面， 第一个 i8 长度的二进制位的值设置为 100 ， 并把第二个 i8 长度的二进制位的值设置为 200 。 当我们把一个字符串键当成数组来使用， 并且数组中储存的都是同等长度的整数时， 使用 # 前缀可以让我们免去手动计算被设置二进制位所在位置的麻烦。 溢出控制用户可以通过 OVERFLOW 命令以及以下展示的三个参数， 指定 BITFIELD 命令在执行自增或者自减操作时， 碰上向上溢出（overflow）或者向下溢出（underflow）情况时的行为： WRAP ： 使用回绕（wrap around）方法处理有符号整数和无符号整数的溢出情况。 对于无符号整数来说， 回绕就像使用数值本身与能够被储存的最大无符号整数执行取模计算， 这也是 C 语言的标准行为。 对于有符号整数来说， 上溢将导致数字重新从最小的负数开始计算， 而下溢将导致数字重新从最大的正数开始计算。 比如说， 如果我们对一个值为 127 的 i8 整数执行加一操作， 那么将得到结果 -128 。 SAT ： 使用饱和计算（saturation arithmetic）方法处理溢出， 也即是说， 下溢计算的结果为最小的整数值， 而上溢计算的结果为最大的整数值。 举个例子， 如果我们对一个值为 120 的 i8 整数执行加 10 计算， 那么命令的结果将为 i8 类型所能储存的最大整数值 127 。 与此相反， 如果一个针对 i8 值的计算造成了下溢， 那么这个 i8 值将被设置为 -127 。 FAIL ： 在这一模式下， 命令将拒绝执行那些会导致上溢或者下溢情况出现的计算， 并向用户返回空值表示计算未被执行。 需要注意的是， OVERFLOW 子命令只会对紧随着它之后被执行的 INCRBY 命令产生效果， 这一效果将一直持续到与它一同被执行的下一个 OVERFLOW 命令为止。 在默认情况下， INCRBY 命令使用 WRAP 方式来处理溢出计算。 以下是一个使用 OVERFLOW 子命令来控制溢出行为的例子：123456789101112131415&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 12) (integer) 1&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 22) (integer) 2&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 32) (integer) 3&gt; BITFIELD mykey incrby u2 100 1 OVERFLOW SAT incrby u2 102 11) (integer) 0 -- 使用默认的 WRAP 方式处理溢出2) (integer) 3 -- 使用 SAT 方式处理溢出 而以下则是一个因为 OVERFLOW FAIL 行为而导致子命令返回空值的例子：12&gt; BITFIELD mykey OVERFLOW FAIL incrby u2 102 11) (nil) 作用BITFIELD 命令的作用在于它能够将很多小的整数储存到一个长度较大的位图中， 又或者将一个非常庞大的键分割为多个较小的键来进行储存， 从而非常高效地使用内存， 使得 Redis 能够得到更多不同的应用 —— 特别是在实时分析领域： BITFIELD 能够以指定的方式对计算溢出进行控制的能力， 使得它可以被应用于这一领域。 性能注意事项BITFIELD 在一般情况下都是一个快速的命令， 需要注意的是， 访问一个长度较短的字符串的远端二进制位将引发一次内存分配操作， 这一操作花费的时间可能会比命令访问已有的字符串花费的时间要长。 二进制位的排列BITFIELD 把位图第一个字节偏移量 0 上的二进制位看作是 most significant 位， 以此类推。 举个例子， 如果我们对一个已经预先被全部设置为 0 的位图进行设置， 将它在偏移量 7 的值设置为 5 位无符号整数值 23 （二进制位为 10111 ）， 那么命令将生产出以下这个位图表示：123+--------+--------+|00000001|01110000|+--------+--------+ 当偏移量和整数长度与字节边界进行对齐时， BITFIELD 表示二进制位的方式跟大端表示法（big endian）一致， 但是在没有对齐的情况下， 理解这些二进制位是如何进行排列也是非常重要的。 返回值BITFIELD 命令的返回值是一个数组， 数组中的每个元素对应一个被执行的子命令。 需要注意的是， OVERFLOW 子命令本身并不产生任何回复。 数据库EXISTSEXISTS key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 检查给定 key 是否存在。 返回值若 key 存在，返回 1 ，否则返回 0 。 代码示例1234567891011redis&gt; SET db "redis"OKredis&gt; EXISTS db(integer) 1redis&gt; DEL db(integer) 1redis&gt; EXISTS db(integer) 0 TYPETYPE key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回 key 所储存的值的类型。 返回值none (key不存在) string (字符串) list (列表) set (集合) zset (有序集) hash (哈希表) stream （流） 代码示例1234567891011121314151617181920# 字符串redis&gt; SET weather "sunny"OKredis&gt; TYPE weatherstring# 列表redis&gt; LPUSH book_list "programming in scala"(integer) 1redis&gt; TYPE book_listlist# 集合redis&gt; SADD pat "dog"(integer) 1redis&gt; TYPE patset RENAMERENAME key newkey可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将 key 改名为 newkey 。 当 key 和 newkey 相同，或者 key 不存在时，返回一个错误。 当 newkey 已经存在时， RENAME 命令将覆盖旧值。 返回值改名成功时提示 OK ，失败时候返回一个错误。 代码示例1234567891011121314151617181920212223242526272829303132# key 存在且 newkey 不存在redis&gt; SET message "hello world"OKredis&gt; RENAME message greetingOKredis&gt; EXISTS message # message 不复存在(integer) 0redis&gt; EXISTS greeting # greeting 取而代之(integer) 1# 当 key 不存在时，返回错误redis&gt; RENAME fake_key never_exists(error) ERR no such key# newkey 已存在时， RENAME 会覆盖旧 newkeyredis&gt; SET pc "lenovo"OKredis&gt; SET personal_computer "dell"OKredis&gt; RENAME pc personal_computerOKredis&gt; GET pc(nil)redis:1&gt; GET personal_computer # 原来的值 dell 被覆盖了"lenovo" RENAMENXRENAMENX key newkey可用版本： &gt;= 1.0.0 时间复杂度： O(1) 当且仅当 newkey 不存在时，将 key 改名为 newkey 。 当 key 不存在时，返回一个错误。 返回值修改成功时，返回 1 ； 如果 newkey 已经存在，返回 0 。 代码示例12345678910111213141516171819202122232425# newkey 不存在，改名成功redis&gt; SET player "MPlyaer"OKredis&gt; EXISTS best_player(integer) 0redis&gt; RENAMENX player best_player(integer) 1# newkey存在时，失败redis&gt; SET animal "bear"OKredis&gt; SET favorite_animal "butterfly"OKredis&gt; RENAMENX animal favorite_animal(integer) 0redis&gt; get animal"bear"redis&gt; get favorite_animal"butterfly" MOVEMOVE key db可用版本： &gt;= 1.0.0 时间复杂度： O(1) 将当前数据库的 key 移动到给定的数据库 db 当中。 如果当前数据库(源数据库)和给定数据库(目标数据库)有相同名字的给定 key ，或者 key 不存在于当前数据库，那么 MOVE 没有任何效果。 因此，也可以利用这一特性，将 MOVE 当作锁(locking)原语(primitive)。 返回值移动成功返回 1 ，失败则返回 0 。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# key 存在于当前数据库redis&gt; SELECT 0 # redis默认使用数据库 0，为了清晰起见，这里再显式指定一次。OKredis&gt; SET song "secret base - Zone"OKredis&gt; MOVE song 1 # 将 song 移动到数据库 1(integer) 1redis&gt; EXISTS song # song 已经被移走(integer) 0redis&gt; SELECT 1 # 使用数据库 1OKredis:1&gt; EXISTS song # 证实 song 被移到了数据库 1 (注意命令提示符变成了"redis:1"，表明正在使用数据库 1)(integer) 1# 当 key 不存在的时候redis:1&gt; EXISTS fake_key(integer) 0redis:1&gt; MOVE fake_key 0 # 试图从数据库 1 移动一个不存在的 key 到数据库 0，失败(integer) 0redis:1&gt; select 0 # 使用数据库0OKredis&gt; EXISTS fake_key # 证实 fake_key 不存在(integer) 0# 当源数据库和目标数据库有相同的 key 时redis&gt; SELECT 0 # 使用数据库0OKredis&gt; SET favorite_fruit "banana"OKredis&gt; SELECT 1 # 使用数据库1OKredis:1&gt; SET favorite_fruit "apple"OKredis:1&gt; SELECT 0 # 使用数据库0，并试图将 favorite_fruit 移动到数据库 1OKredis&gt; MOVE favorite_fruit 1 # 因为两个数据库有相同的 key，MOVE 失败(integer) 0redis&gt; GET favorite_fruit # 数据库 0 的 favorite_fruit 没变"banana"redis&gt; SELECT 1OKredis:1&gt; GET favorite_fruit # 数据库 1 的 favorite_fruit 也是"apple" DELDEL key [key …]可用版本： &gt;= 1.0.0 时间复杂度：O(N)， N 为被删除的 key 的数量，其中删除单个字符串类型的 key ，时间复杂度为O(1)；删除单个列表、集合、有序集合或哈希表类型的 key ，时间复杂度为O(M)， M 为以上数据结构内的元素数量。 删除给定的一个或多个 key 。 不存在的 key 会被忽略。 返回值被删除 key 的数量。 代码示例1234567891011121314151617181920212223242526# 删除单个 keyredis&gt; SET name huangzOKredis&gt; DEL name(integer) 1# 删除一个不存在的 keyredis&gt; EXISTS phone(integer) 0redis&gt; DEL phone # 失败，没有 key 被删除(integer) 0# 同时删除多个 keyredis&gt; SET name "redis"OKredis&gt; SET type "key-value store"OKredis&gt; SET website "redis.com"OKredis&gt; DEL name type website(integer) 3 RANDOMKEYRANDOMKEY可用版本： &gt;= 1.0.0 时间复杂度： O(1) 从当前数据库中随机返回(不删除)一个 key 。 返回值当数据库不为空时，返回一个 key 。 当数据库为空时，返回 nil 。 代码示例123456789101112131415161718192021# 数据库不为空redis&gt; MSET fruit "apple" drink "beer" food "cookies" # 设置多个 keyOKredis&gt; RANDOMKEY"fruit"redis&gt; RANDOMKEY"food"redis&gt; KEYS * # 查看数据库内所有key，证明 RANDOMKEY 并不删除 key1) "food"2) "drink"3) "fruit"# 数据库为空redis&gt; FLUSHDB # 删除当前数据库所有 keyOKredis&gt; RANDOMKEY(nil) DBSIZEDBSIZE可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回当前数据库的 key 的数量。 返回值当前数据库的 key 的数量。 代码示例12345678redis&gt; DBSIZE(integer) 5redis&gt; SET new_key "hello_moto" # 增加一个 key 试试OKredis&gt; DBSIZE(integer) 6 KEYSKEYS pattern可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为数据库中 key 的数量。 查找所有符合给定模式 pattern 的 key ， 比如说： KEYS * 匹配数据库中所有 key 。 KEYS h?llo 匹配 hello ， hallo 和 hxllo 等。 KEYS h*llo 匹配 hllo 和 heeeeello 等。 KEYS h[ae]llo 匹配 hello 和 hallo ，但不匹配 hillo 。 特殊符号用 \ 隔开。 WarningKEYS 的速度非常快，但在一个大的数据库中使用它仍然可能造成性能问题，如果你需要从一个数据集中查找特定的 key ，你最好还是用 Redis 的集合结构(set)来代替。 返回值符合给定模式的 key 列表。 代码示例12345678910111213141516171819redis&gt; MSET one 1 two 2 three 3 four 4 # 一次设置 4 个 keyOKredis&gt; KEYS *o*1) "four"2) "two"3) "one"redis&gt; KEYS t??1) "two"redis&gt; KEYS t[w]*1) "two"redis&gt; KEYS * # 匹配数据库内所有 key1) "four"2) "three"3) "two"4) "one" SCANSCAN cursor [MATCH pattern] [COUNT count]可用版本： &gt;= 2.8.0 时间复杂度：增量式迭代命令每次执行的复杂度为 O(1) ， 对数据集进行一次完整迭代的复杂度为 O(N) ， 其中 N 为数据集中的元素数量。 SCAN 命令及其相关的 SSCAN 命令、 HSCAN 命令和 ZSCAN 命令都用于增量地迭代（incrementally iterate）一集元素（a collection of elements）： SCAN 命令用于迭代当前数据库中的数据库键。 SSCAN 命令用于迭代集合键中的元素。 HSCAN 命令用于迭代哈希键中的键值对。 ZSCAN 命令用于迭代有序集合中的元素（包括元素成员和元素分值）。 以上列出的四个命令都支持增量式迭代， 它们每次执行都只会返回少量元素， 所以这些命令可以用于生产环境， 而不会出现像 KEYS 命令、 SMEMBERS 命令带来的问题 —— 当 KEYS 命令被用于处理一个大的数据库时， 又或者 SMEMBERS 命令被用于处理一个大的集合键时， 它们可能会阻塞服务器达数秒之久。 不过， 增量式迭代命令也不是没有缺点的： 举个例子， 使用 SMEMBERS 命令可以返回集合键当前包含的所有元素， 但是对于 SCAN 这类增量式迭代命令来说， 因为在对键进行增量式迭代的过程中， 键可能会被修改， 所以增量式迭代命令只能对被返回的元素提供有限的保证 （offer limited guarantees about the returned elements）。 因为 SCAN 、 SSCAN 、 HSCAN 和 ZSCAN 四个命令的工作方式都非常相似， 所以这个文档会一并介绍这四个命令， 但是要记住： SSCAN 命令、 HSCAN 命令和 ZSCAN 命令的第一个参数总是一个数据库键。 而 SCAN 命令则不需要在第一个参数提供任何数据库键 —— 因为它迭代的是当前数据库中的所有数据库键。SCAN 命令的基本用法SCAN 命令是一个基于游标的迭代器（cursor based iterator）： SCAN 命令每次被调用之后， 都会向用户返回一个新的游标， 用户在下次迭代时需要使用这个新游标作为 SCAN 命令的游标参数， 以此来延续之前的迭代过程。 当 SCAN 命令的游标参数被设置为 0 时， 服务器将开始一次新的迭代， 而当服务器向用户返回值为 0 的游标时， 表示迭代已结束。 以下是一个 SCAN 命令的迭代过程示例：12345678910111213141516171819202122232425redis 127.0.0.1:6379&gt; scan 01) "17"2) 1) "key:12" 2) "key:8" 3) "key:4" 4) "key:14" 5) "key:16" 6) "key:17" 7) "key:15" 8) "key:10" 9) "key:3" 10) "key:7" 11) "key:1"redis 127.0.0.1:6379&gt; scan 171) "0"2) 1) "key:5" 2) "key:18" 3) "key:0" 4) "key:2" 5) "key:19" 6) "key:13" 7) "key:6" 8) "key:9" 9) "key:11" 在上面这个例子中， 第一次迭代使用 0 作为游标， 表示开始一次新的迭代。 第二次迭代使用的是第一次迭代时返回的游标， 也即是命令回复第一个元素的值 —— 17 。 从上面的示例可以看到， SCAN 命令的回复是一个包含两个元素的数组， 第一个数组元素是用于进行下一次迭代的新游标， 而第二个数组元素则是一个数组， 这个数组中包含了所有被迭代的元素。 在第二次调用 SCAN 命令时， 命令返回了游标 0 ， 这表示迭代已经结束， 整个数据集（collection）已经被完整遍历过了。 以 0 作为游标开始一次新的迭代， 一直调用 SCAN 命令， 直到命令返回游标 0 ， 我们称这个过程为一次完整遍历（full iteration）。 SCAN 命令的保证（guarantees）SCAN 命令， 以及其他增量式迭代命令， 在进行完整遍历的情况下可以为用户带来以下保证： 从完整遍历开始直到完整遍历结束期间， 一直存在于数据集内的所有元素都会被完整遍历返回； 这意味着， 如果有一个元素， 它从遍历开始直到遍历结束期间都存在于被遍历的数据集当中， 那么 SCAN 命令总会在某次迭代中将这个元素返回给用户。 然而因为增量式命令仅仅使用游标来记录迭代状态， 所以这些命令带有以下缺点： 同一个元素可能会被返回多次。 处理重复元素的工作交由应用程序负责， 比如说， 可以考虑将迭代返回的元素仅仅用于可以安全地重复执行多次的操作上。 如果一个元素是在迭代过程中被添加到数据集的， 又或者是在迭代过程中从数据集中被删除的， 那么这个元素可能会被返回， 也可能不会， 这是未定义的（undefined）。SCAN 命令每次执行返回的元素数量增量式迭代命令并不保证每次执行都返回某个给定数量的元素。 增量式命令甚至可能会返回零个元素， 但只要命令返回的游标不是 0 ， 应用程序就不应该将迭代视作结束。 不过命令返回的元素数量总是符合一定规则的， 在实际中： 对于一个大数据集来说， 增量式迭代命令每次最多可能会返回数十个元素； 而对于一个足够小的数据集来说， 如果这个数据集的底层表示为编码数据结构（encoded data structure，适用于是小集合键、小哈希键和小有序集合键）， 那么增量迭代命令将在一次调用中返回数据集中的所有元素。最后， 用户可以通过增量式迭代命令提供的 COUNT 选项来指定每次迭代返回元素的最大值。 COUNT 选项虽然增量式迭代命令不保证每次迭代所返回的元素数量， 但我们可以使用 COUNT 选项， 对命令的行为进行一定程度上的调整。 基本上， COUNT 选项的作用就是让用户告知迭代命令， 在每次迭代中应该从数据集里返回多少元素。 虽然 COUNT 选项只是对增量式迭代命令的一种提示（hint）， 但是在大多数情况下， 这种提示都是有效的。 COUNT 参数的默认值为 10 。 在迭代一个足够大的、由哈希表实现的数据库、集合键、哈希键或者有序集合键时， 如果用户没有使用 MATCH 选项， 那么命令返回的元素数量通常和 COUNT 选项指定的一样， 或者比 COUNT 选项指定的数量稍多一些。 在迭代一个编码为整数集合（intset，一个只由整数值构成的小集合）、 或者编码为压缩列表（ziplist，由不同值构成的一个小哈希或者一个小有序集合）时， 增量式迭代命令通常会无视 COUNT 选项指定的值， 在第一次迭代就将数据集包含的所有元素都返回给用户。Note 并非每次迭代都要使用相同的 COUNT 值。 用户可以在每次迭代中按自己的需要随意改变 COUNT 值， 只要记得将上次迭代返回的游标用到下次迭代里面就可以了。 MATCH 选项和 KEYS 命令一样， 增量式迭代命令也可以通过提供一个 glob 风格的模式参数， 让命令只返回和给定模式相匹配的元素， 这一点可以通过在执行增量式迭代命令时， 通过给定 MATCH \&lt;pattern> 参数来实现。 以下是一个使用 MATCH 选项进行迭代的示例：12345678redis 127.0.0.1:6379&gt; sadd myset 1 2 3 foo foobar feelsgood(integer) 6redis 127.0.0.1:6379&gt; sscan myset 0 match f*1) "0"2) 1) "foo" 2) "feelsgood" 3) "foobar" 需要注意的是， 对元素的模式匹配工作是在命令从数据集中取出元素之后， 向客户端返回元素之前的这段时间内进行的， 所以如果被迭代的数据集中只有少量元素和模式相匹配， 那么迭代命令或许会在多次执行中都不返回任何元素。 以下是这种情况的一个例子：123456789101112131415161718192021222324252627282930313233343536redis 127.0.0.1:6379&gt; scan 0 MATCH *11*1) "288"2) 1) "key:911"redis 127.0.0.1:6379&gt; scan 288 MATCH *11*1) "224"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 224 MATCH *11*1) "80"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 80 MATCH *11*1) "176"2) (empty list or set)redis 127.0.0.1:6379&gt; scan 176 MATCH *11* COUNT 10001) "0"2) 1) "key:611" 2) "key:711" 3) "key:118" 4) "key:117" 5) "key:311" 6) "key:112" 7) "key:111" 8) "key:110" 9) "key:113" 10) "key:211" 11) "key:411" 12) "key:115" 13) "key:116" 14) "key:114" 15) "key:119" 16) "key:811" 17) "key:511" 18) "key:11" 如你所见， 以上的大部分迭代都不返回任何元素。 在最后一次迭代， 我们通过将 COUNT 选项的参数设置为 1000 ， 强制命令为本次迭代扫描更多元素， 从而使得命令返回的元素也变多了。 并发执行多个迭代在同一时间， 可以有任意多个客户端对同一数据集进行迭代， 客户端每次执行迭代都需要传入一个游标， 并在迭代执行之后获得一个新的游标， 而这个游标就包含了迭代的所有状态， 因此， 服务器无须为迭代记录任何状态。 中途停止迭代因为迭代的所有状态都保存在游标里面， 而服务器无须为迭代保存任何状态， 所以客户端可以在中途停止一个迭代， 而无须对服务器进行任何通知。 即使有任意数量的迭代在中途停止， 也不会产生任何问题。 使用错误的游标进行增量式迭代使用间断的（broken）、负数、超出范围或者其他非正常的游标来执行增量式迭代并不会造成服务器崩溃， 但可能会让命令产生未定义的行为。 未定义行为指的是， 增量式命令对返回值所做的保证可能会不再为真。 只有两种游标是合法的： 在开始一个新的迭代时， 游标必须为 0 。 增量式迭代命令在执行之后返回的， 用于延续（continue）迭代过程的游标。迭代终结的保证增量式迭代命令所使用的算法只保证在数据集的大小有界（bounded）的情况下， 迭代才会停止， 换句话说， 如果被迭代数据集的大小不断地增长的话， 增量式迭代命令可能永远也无法完成一次完整迭代。 从直觉上可以看出， 当一个数据集不断地变大时， 想要访问这个数据集中的所有元素就需要做越来越多的工作， 能否结束一个迭代取决于用户执行迭代的速度是否比数据集增长的速度更快。 返回值SCAN 命令、 SSCAN 命令、 HSCAN 命令和 ZSCAN 命令都返回一个包含两个元素的 multi-bulk 回复： 回复的第一个元素是字符串表示的无符号 64 位整数（游标）， 回复的第二个元素是另一个 multi-bulk 回复， 这个 multi-bulk 回复包含了本次被迭代的元素。 SCAN 命令返回的每个元素都是一个数据库键。 SSCAN 命令返回的每个元素都是一个集合成员。 HSCAN 命令返回的每个元素都是一个键值对，一个键值对由一个键和一个值组成。 ZSCAN 命令返回的每个元素都是一个有序集合元素，一个有序集合元素由一个成员（member）和一个分值（score）组成。 SORTSORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination]可用版本： &gt;= 1.0.0 时间复杂度： O(N+M*log(M))， N 为要排序的列表或集合内的元素数量， M 为要返回的元素数量。如果只是使用 SORT 命令的 GET 选项获取数据而没有进行排序，时间复杂度 O(N)。 返回或保存给定列表、集合、有序集合 key 中经过排序的元素。 排序默认以数字作为对象，值被解释为双精度浮点数，然后进行比较。 一般 SORT 用法最简单的 SORT 使用方法是 SORT key 和 SORT key DESC ： SORT key 返回键值从小到大排序的结果。 SORT key DESC 返回键值从大到小排序的结果。假设 today_cost 列表保存了今日的开销金额， 那么可以用 SORT 命令对它进行排序： 1234567891011121314151617# 开销金额列表redis&gt; LPUSH today_cost 30 1.5 10 8(integer) 4# 排序redis&gt; SORT today_cost1) "1.5"2) "8"3) "10"4) "30"# 逆序排序redis 127.0.0.1:6379&gt; SORT today_cost DESC1) "30"2) "10"3) "8"4) "1.5" 使用 ALPHA 修饰符对字符串进行排序因为 SORT 命令默认排序对象为数字， 当需要对字符串进行排序时， 需要显式地在 SORT 命令之后添加 ALPHA 修饰符：123456789101112131415161718192021# 网址redis&gt; LPUSH website "www.reddit.com"(integer) 1redis&gt; LPUSH website "www.slashdot.com"(integer) 2redis&gt; LPUSH website "www.infoq.com"(integer) 3# 默认（按数字）排序redis&gt; SORT website1) "www.infoq.com"2) "www.slashdot.com"3) "www.reddit.com"# 按字符排序redis&gt; SORT website ALPHA1) "www.infoq.com"2) "www.reddit.com"3) "www.slashdot.com" 如果系统正确地设置了 LC_COLLATE 环境变量的话，Redis能识别 UTF-8 编码。 使用 LIMIT 修饰符限制返回结果排序之后返回元素的数量可以通过 LIMIT 修饰符进行限制， 修饰符接受 offset 和 count 两个参数： offset 指定要跳过的元素数量。 count 指定跳过 offset 个指定的元素之后，要返回多少个对象。以下例子返回排序结果的前 5 个对象( offset 为 0 表示没有元素被跳过)。1234567891011121314# 添加测试数据，列表值为 1 指 10redis 127.0.0.1:6379&gt; RPUSH rank 1 3 5 7 9(integer) 5redis 127.0.0.1:6379&gt; RPUSH rank 2 4 6 8 10(integer) 10# 返回列表中最小的 5 个值redis 127.0.0.1:6379&gt; SORT rank LIMIT 0 51) "1"2) "2"3) "3"4) "4"5) "5" 可以组合使用多个修饰符。以下例子返回从大到小排序的前 5 个对象。123456redis 127.0.0.1:6379&gt; SORT rank LIMIT 0 5 DESC1) "10"2) "9"3) "8"4) "7"5) "6" 使用外部 key 进行排序可以使用外部 key 的数据作为权重，代替默认的直接对比键值的方式来进行排序。 假设现在有用户数据如下： 以下代码将数据输入到 Redis 中：123456789101112131415161718192021222324252627282930313233343536373839# adminredis 127.0.0.1:6379&gt; LPUSH uid 1(integer) 1redis 127.0.0.1:6379&gt; SET user_name_1 adminOKredis 127.0.0.1:6379&gt; SET user_level_1 9999OK# jackredis 127.0.0.1:6379&gt; LPUSH uid 2(integer) 2redis 127.0.0.1:6379&gt; SET user_name_2 jackOKredis 127.0.0.1:6379&gt; SET user_level_2 10OK# peterredis 127.0.0.1:6379&gt; LPUSH uid 3(integer) 3redis 127.0.0.1:6379&gt; SET user_name_3 peterOKredis 127.0.0.1:6379&gt; SET user_level_3 25OK# maryredis 127.0.0.1:6379&gt; LPUSH uid 4(integer) 4redis 127.0.0.1:6379&gt; SET user_name_4 maryOKredis 127.0.0.1:6379&gt; SET user_level_4 70OK BY 选项默认情况下， SORT uid 直接按 uid 中的值排序：12345redis 127.0.0.1:6379&gt; SORT uid1) "1" # admin2) "2" # jack3) "3" # peter4) "4" # mary 通过使用 BY 选项，可以让 uid 按其他键的元素来排序。 比如说， 以下代码让 uid 键按照 user_level_{uid} 的大小来排序：12345redis 127.0.0.1:6379&gt; SORT uid BY user_level_*1) "2" # jack , level = 102) "3" # peter, level = 253) "4" # mary, level = 704) "1" # admin, level = 9999 user_level_* 是一个占位符， 它先取出 uid 中的值， 然后再用这个值来查找相应的键。 比如在对 uid 列表进行排序时， 程序就会先取出 uid 的值 1 、 2 、 3 、 4 ， 然后使用 user_level_1 、 user_level_2 、 user_level_3 和 user_level_4 的值作为排序 uid 的权重。 GET 选项使用 GET 选项， 可以根据排序的结果来取出相应的键值。 比如说， 以下代码先排序 uid ， 再取出键 user_name_{uid} 的值：12345redis 127.0.0.1:6379&gt; SORT uid GET user_name_*1) "admin"2) "jack"3) "peter"4) "mary" 组合使用 BY 和 GET通过组合使用 BY 和 GET ， 可以让排序结果以更直观的方式显示出来。 比如说， 以下代码先按 user_level_{uid} 来排序 uid 列表， 再取出相应的 user_name_{uid} 的值：12345redis 127.0.0.1:6379&gt; SORT uid BY user_level_* GET user_name_*1) "jack" # level = 102) "peter" # level = 253) "mary" # level = 704) "admin" # level = 9999 现在的排序结果要比只使用 SORT uid BY user_level_* 要直观得多。 获取多个外部键可以同时使用多个 GET 选项， 获取多个外部键的值。 以下代码就按 uid 分别获取 user_level_{uid} 和 user_name_{uid} ：123456789redis 127.0.0.1:6379&gt; SORT uid GET user_level_* GET user_name_*1) "9999" # level2) "admin" # name3) "10"4) "jack"5) "25"6) "peter"7) "70"8) "mary" GET 有一个额外的参数规则，那就是 —— 可以用 # 获取被排序键的值。 以下代码就将 uid 的值、及其相应的 user_level_ 和 user_name_ 都返回为结果：12345678910111213redis 127.0.0.1:6379&gt; SORT uid GET # GET user_level_* GET user_name_*1) "1" # uid2) "9999" # level3) "admin" # name4) "2"5) "10"6) "jack"7) "3"8) "25"9) "peter"10) "4"11) "70"12) "mary" 获取外部键，但不进行排序通过将一个不存在的键作为参数传给 BY 选项， 可以让 SORT 跳过排序操作， 直接返回结果：12345redis 127.0.0.1:6379&gt; SORT uid BY not-exists-key1) "4"2) "3"3) "2"4) "1" 这种用法在单独使用时，没什么实际用处。 不过，通过将这种用法和 GET 选项配合， 就可以在不排序的情况下， 获取多个外部键， 相当于执行一个整合的获取操作（类似于 SQL 数据库的 join 关键字）。 以下代码演示了，如何在不引起排序的情况下，使用 SORT 、 BY 和 GET 获取多个外部键：12345678910111213redis 127.0.0.1:6379&gt; SORT uid BY not-exists-key GET # GET user_level_* GET user_name_*1) "4" # id2) "70" # level3) "mary" # name4) "3"5) "25"6) "peter"7) "2"8) "10"9) "jack"10) "1"11) "9999"12) "admin" 将哈希表作为 GET 或 BY 的参数除了可以将字符串键之外， 哈希表也可以作为 GET 或 BY 选项的参数来使用。 比如说，对于前面给出的用户信息表： 我们可以不将用户的名字和级别保存在 user_name_{uid} 和 user_level_{uid} 两个字符串键中， 而是用一个带有 name 域和 level 域的哈希表 user_info_{uid} 来保存用户的名字和级别信息：1234567891011redis 127.0.0.1:6379&gt; HMSET user_info_1 name admin level 9999OKredis 127.0.0.1:6379&gt; HMSET user_info_2 name jack level 10OKredis 127.0.0.1:6379&gt; HMSET user_info_3 name peter level 25OKredis 127.0.0.1:6379&gt; HMSET user_info_4 name mary level 70OK 之后， BY 和 GET 选项都可以用 key-&gt;field 的格式来获取哈希表中的域的值， 其中 key 表示哈希表键， 而 field 则表示哈希表的域：1234567891011redis 127.0.0.1:6379&gt; SORT uid BY user_info_*-&gt;level1) "2"2) "3"3) "4"4) "1"redis 127.0.0.1:6379&gt; SORT uid BY user_info_*-&gt;level GET user_info_*-&gt;name1) "jack"2) "peter"3) "mary"4) "admin" 保存排序结果默认情况下， SORT 操作只是简单地返回排序结果，并不进行任何保存操作。 通过给 STORE 选项指定一个 key 参数，可以将排序结果保存到给定的键上。 如果被指定的 key 已存在，那么原有的值将被排序结果覆盖。12345678910111213141516171819202122232425262728293031323334# 测试数据redis 127.0.0.1:6379&gt; RPUSH numbers 1 3 5 7 9(integer) 5redis 127.0.0.1:6379&gt; RPUSH numbers 2 4 6 8 10(integer) 10redis 127.0.0.1:6379&gt; LRANGE numbers 0 -11) "1"2) "3"3) "5"4) "7"5) "9"6) "2"7) "4"8) "6"9) "8"10) "10"redis 127.0.0.1:6379&gt; SORT numbers STORE sorted-numbers(integer) 10# 排序后的结果redis 127.0.0.1:6379&gt; LRANGE sorted-numbers 0 -11) "1"2) "2"3) "3"4) "4"5) "5"6) "6"7) "7"8) "8"9) "9"10) "10" 可以通过将 SORT 命令的执行结果保存，并用 EXPIRE key seconds 为结果设置生存时间，以此来产生一个 SORT 操作的结果缓存。 这样就可以避免对 SORT 操作的频繁调用：只有当结果集过期时，才需要再调用一次 SORT 操作。 另外，为了正确实现这一用法，你可能需要加锁以避免多个客户端同时进行缓存重建(也就是多个客户端，同一时间进行 SORT 操作，并保存为结果集)，具体参见 SETNX key value 命令。 返回值没有使用 STORE 参数，返回列表形式的排序结果。 使用 STORE 参数，返回排序结果的元素数量。 FLUSHDBFLUSHDB可用版本： &gt;= 1.0.0 时间复杂度： O(1) 清空当前数据库中的所有 key。 此命令从不失败。 返回值总是返回 OK 。 代码示例12345678redis&gt; DBSIZE # 清空前的 key 数量(integer) 4redis&gt; FLUSHDBOKredis&gt; DBSIZE # 清空后的 key 数量(integer) 0 FLUSHALLFLUSHALL可用版本： &gt;= 1.0.0 时间复杂度： O(N) 清空整个 Redis 服务器的数据(删除所有数据库的所有 key )。 此命令从不失败。 返回值总是返回 OK 。1234567891011121314151617181920redis&gt; DBSIZE # 0 号数据库的 key 数量(integer) 9redis&gt; SELECT 1 # 切换到 1 号数据库OKredis[1]&gt; DBSIZE # 1 号数据库的 key 数量(integer) 6redis[1]&gt; flushall # 清空所有数据库的所有 keyOKredis[1]&gt; DBSIZE # 不但 1 号数据库被清空了(integer) 0redis[1]&gt; SELECT 0 # 0 号数据库(以及其他所有数据库)也一样OKredis&gt; DBSIZE(integer) 0 SELECTSELECT index可用版本： &gt;= 1.0.0 时间复杂度： O(1) 切换到指定的数据库，数据库索引号 index 用数字值指定，以 0 作为起始索引值。 默认使用 0 号数据库。 返回值OK 代码示例12345678910111213141516171819redis&gt; SET db_number 0 # 默认使用 0 号数据库OKredis&gt; SELECT 1 # 使用 1 号数据库OKredis[1]&gt; GET db_number # 已经切换到 1 号数据库，注意 Redis 现在的命令提示符多了个 [1](nil)redis[1]&gt; SET db_number 1OKredis[1]&gt; GET db_number"1"redis[1]&gt; SELECT 3 # 再切换到 3 号数据库OKredis[3]&gt; # 提示符从 [1] 改变成了 [3] SWAPDBSWAPDB db1 db2版本要求： &gt;= 4.0.0 时间复杂度： O(1) 对换指定的两个数据库， 使得两个数据库的数据立即互换。 返回值OK 代码示例123# 对换数据库 0 和数据库 1redis&gt; SWAPDB 0 1OK 自动过期EXPIREEXPIRE key seconds可用版本： &gt;= 1.0.0 时间复杂度： O(1) 为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 在 Redis 中，带有生存时间的 key 被称为『易失的』(volatile)。 生存时间可以通过使用 DEL 命令来删除整个 key 来移除，或者被 SET 和 GETSET 命令覆写(overwrite)，这意味着，如果一个命令只是修改(alter)一个带生存时间的 key 的值而不是用一个新的 key 值来代替(replace)它的话，那么生存时间不会被改变。 比如说，对一个 key 执行 INCR 命令，对一个列表进行 LPUSH 命令，或者对一个哈希表执行 HSET 命令，这类操作都不会修改 key 本身的生存时间。 另一方面，如果使用 RENAME 对一个 key 进行改名，那么改名后的 key 的生存时间和改名前一样。 RENAME 命令的另一种可能是，尝试将一个带生存时间的 key 改名成另一个带生存时间的 another_key ，这时旧的 another_key (以及它的生存时间)会被删除，然后旧的 key 会改名为 another_key ，因此，新的 another_key 的生存时间也和原本的 key 一样。 使用 PERSIST 命令可以在不删除 key 的情况下，移除 key 的生存时间，让 key 重新成为一个『持久的』(persistent) key 。 更新生存时间可以对一个已经带有生存时间的 key 执行 EXPIRE 命令，新指定的生存时间会取代旧的生存时间。 过期时间的精确度在 Redis 2.4 版本中，过期时间的延迟在 1 秒钟之内 —— 也即是，就算 key 已经过期，但它还是可能在过期之后一秒钟之内被访问到，而在新的 Redis 2.6 版本中，延迟被降低到 1 毫秒之内。 Redis 2.1.3 之前的不同之处在 Redis 2.1.3 之前的版本中，修改一个带有生存时间的 key 会导致整个 key 被删除，这一行为是受当时复制(replication)层的限制而作出的，现在这一限制已经被修复。 返回值设置成功返回 1 。 当 key 不存在或者不能为 key 设置生存时间时(比如在低于 2.1.3 版本的 Redis 中你尝试更新 key 的生存时间)，返回 0 。 代码示例1234567891011121314redis&gt; SET cache_page "www.google.com"OKredis&gt; EXPIRE cache_page 30 # 设置过期时间为 30 秒(integer) 1redis&gt; TTL cache_page # 查看剩余生存时间(integer) 23redis&gt; EXPIRE cache_page 30000 # 更新过期时间(integer) 1redis&gt; TTL cache_page(integer) 29996 模式：导航会话假设你有一项 web 服务，打算根据用户最近访问的 N 个页面来进行物品推荐，并且假设用户停止阅览超过 60 秒，那么就清空阅览记录(为了减少物品推荐的计算量，并且保持推荐物品的新鲜度)。 这些最近访问的页面记录，我们称之为『导航会话』(Navigation session)，可以用 INCR 和 RPUSH 命令在 Redis 中实现它：每当用户阅览一个网页的时候，执行以下代码：1234MULTI RPUSH pagewviews.user:&lt;userid&gt; http://..... EXPIRE pagewviews.user:&lt;userid&gt; 60EXEC 如果用户停止阅览超过 60 秒，那么它的导航会话就会被清空，当用户重新开始阅览的时候，系统又会重新记录导航会话，继续进行物品推荐。 EXPIREATEXPIREAT key timestamp可用版本： &gt;= 1.2.0 时间复杂度： O(1) EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置生存时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 返回值如果生存时间设置成功，返回 1 ； 当 key 不存在或没办法设置生存时间，返回 0 。 代码示例12345678redis&gt; SET cache www.google.comOKredis&gt; EXPIREAT cache 1355292000 # 这个 key 将在 2012.12.12 过期(integer) 1redis&gt; TTL cache(integer) 45081860 TTLTTL key可用版本： &gt;= 1.0.0 时间复杂度： O(1) 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 返回值当 key 不存在时，返回 -2 。 当 key 存在但没有设置剩余生存时间时，返回 -1 。 否则，以秒为单位，返回 key 的剩余生存时间。 Note在 Redis 2.8 以前，当 key 不存在，或者 key 没有设置剩余生存时间时，命令都返回 -1 。 代码示例1234567891011121314151617181920# 不存在的 keyredis&gt; FLUSHDBOKredis&gt; TTL key(integer) -2# key 存在，但没有设置剩余生存时间redis&gt; SET key valueOKredis&gt; TTL key(integer) -1# 有剩余生存时间的 keyredis&gt; EXPIRE key 10086(integer) 1redis&gt; TTL key(integer) 10084 PERSISTPERSIST key可用版本： &gt;= 2.2.0 时间复杂度： O(1) 移除给定 key 的生存时间，将这个 key 从“易失的”(带生存时间 key )转换成“持久的”(一个不带生存时间、永不过期的 key )。 返回值当生存时间移除成功时，返回 1 . 如果 key 不存在或 key 没有设置生存时间，返回 0 。 代码示例1234567891011121314redis&gt; SET mykey "Hello"OKredis&gt; EXPIRE mykey 10 # 为 key 设置生存时间(integer) 1redis&gt; TTL mykey(integer) 10redis&gt; PERSIST mykey # 移除 key 的生存时间(integer) 1redis&gt; TTL mykey(integer) -1 PEXPIREPEXPIRE key milliseconds可用版本： &gt;= 2.6.0 时间复杂度： O(1) 这个命令和 EXPIRE 命令的作用类似，但是它以毫秒为单位设置 key 的生存时间，而不像 EXPIRE 命令那样，以秒为单位。 返回值设置成功，返回 1 key 不存在或设置失败，返回 0 代码示例1234567891011redis&gt; SET mykey "Hello"OKredis&gt; PEXPIRE mykey 1500(integer) 1redis&gt; TTL mykey # TTL 的返回值以秒为单位(integer) 2redis&gt; PTTL mykey # PTTL 可以给出准确的毫秒数(integer) 1499 PEXPIREATPEXPIREAT key milliseconds-timestamp可用版本： &gt;= 2.6.0 时间复杂度： O(1) 这个命令和 expireat 命令类似，但它以毫秒为单位设置 key 的过期 unix 时间戳，而不是像 expireat 那样，以秒为单位。 返回值如果生存时间设置成功，返回 1 。 当 key 不存在或没办法设置生存时间时，返回 0 。(查看 EXPIRE key seconds 命令获取更多信息) 代码示例1234567891011redis&gt; SET mykey "Hello"OKredis&gt; PEXPIREAT mykey 1555555555005(integer) 1redis&gt; TTL mykey # TTL 返回秒(integer) 223157079redis&gt; PTTL mykey # PTTL 返回毫秒(integer) 223157079318 PTTLPTTL key可用版本： &gt;= 2.6.0 复杂度： O(1) 这个命令类似于 TTL 命令，但它以毫秒为单位返回 key 的剩余生存时间，而不是像 TTL 命令那样，以秒为单位。 返回值 当 key 不存在时，返回 -2 。 当 key 存在但没有设置剩余生存时间时，返回 -1 。 否则，以毫秒为单位，返回 key 的剩余生存时间。Note 在 Redis 2.8 以前，当 key 不存在，或者 key 没有设置剩余生存时间时，命令都返回 -1 。 代码示例1234567891011121314151617181920# 不存在的 keyredis&gt; FLUSHDBOKredis&gt; PTTL key(integer) -2# key 存在，但没有设置剩余生存时间redis&gt; SET key valueOKredis&gt; PTTL key(integer) -1# 有剩余生存时间的 keyredis&gt; PEXPIRE key 10086(integer) 1redis&gt; PTTL key(integer) 6179 事务MULTIMULTI可用版本：&gt;= 1.2.0 时间复杂度：O(1)。 标记一个事务块的开始。 事务块内的多条命令会按照先后顺序被放进一个队列当中，最后由 EXEC 命令原子性(atomic)地执行。 返回值：总是返回 OK 。 代码示例1234567891011121314151617181920redis&gt; MULTI # 标记事务开始OKredis&gt; INCR user_id # 多条命令按顺序入队QUEUEDredis&gt; INCR user_idQUEUEDredis&gt; INCR user_idQUEUEDredis&gt; PINGQUEUEDredis&gt; EXEC # 执行1) (integer) 12) (integer) 23) (integer) 34) PONG EXECEXEC可用版本：&gt;= 1.2.0 时间复杂度：事务块内所有命令的时间复杂度的总和。 执行所有事务块内的命令。 假如某个(或某些) key 正处于 WATCH 命令的监视之下，且事务块中有和这个(或这些) key 相关的命令，那么 EXEC 命令只在这个(或这些) key 没有被其他命令所改动的情况下执行并生效，否则该事务被打断(abort)。 返回值：事务块内所有命令的返回值，按命令执行的先后顺序排列。当操作被打断时，返回空值 nil 。 代码示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 事务被成功执行redis&gt; MULTIOKredis&gt; INCR user_idQUEUEDredis&gt; INCR user_idQUEUEDredis&gt; INCR user_idQUEUEDredis&gt; PINGQUEUEDredis&gt; EXEC1) (integer) 12) (integer) 23) (integer) 34) PONG# 监视 key ，且事务成功执行redis&gt; WATCH lock lock_timesOKredis&gt; MULTIOKredis&gt; SET lock "huangz"QUEUEDredis&gt; INCR lock_timesQUEUEDredis&gt; EXEC1) OK2) (integer) 1# 监视 key ，且事务被打断redis&gt; WATCH lock lock_timesOKredis&gt; MULTIOKredis&gt; SET lock "joe" # 就在这时，另一个客户端修改了 lock_times 的值QUEUEDredis&gt; INCR lock_timesQUEUEDredis&gt; EXEC # 因为 lock_times 被修改， joe 的事务执行失败(nil) DISCARDDISCARD可用版本： &gt;= 2.0.0 时间复杂度： O(1)。 取消事务，放弃执行事务块内的所有命令。 如果正在使用 WATCH 命令监视某个(或某些) key，那么取消所有监视，等同于执行命令 UNWATCH 。 返回值总是返回 OK 。 代码示例1234567891011redis&gt; MULTIOKredis&gt; PINGQUEUEDredis&gt; SET greeting "hello"QUEUEDredis&gt; DISCARDOK WATCHWATCH key [key …]可用版本：&gt;= 2.2.0 时间复杂度：O(1)。 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 返回值：总是返回 OK 。 代码示例12redis&gt; WATCH lock lock_timesOK UNWATCHUNWATCH可用版本：&gt;= 2.2.0 时间复杂度：O(1) 取消 WATCH 命令对所有 key 的监视。 如果在执行 WATCH 命令之后， EXEC 命令或 DISCARD 命令先被执行了的话，那么就不需要再执行 UNWATCH 了。 因为 EXEC 命令会执行事务，因此 WATCH 命令的效果已经产生了；而 DISCARD 命令在取消事务的同时也会取消所有对 key 的监视，因此这两个命令执行之后，就没有必要执行 UNWATCH 了。 返回值：总是 OK 。 代码示例12345redis&gt; WATCH lock lock_timesOKredis&gt; UNWATCHOK Lua 脚本EVALEVAL script numkeys key [key …] arg [arg …]可用版本： &gt;= 2.6.0 时间复杂度： EVAL 和 EVALSHA 可以在 O(1) 复杂度内找到要被执行的脚本，其余的复杂度取决于执行的脚本本身。 从 Redis 2.6.0 版本开始，通过内置的 Lua 解释器，可以使用 EVAL 命令对 Lua 脚本进行求值。 script 参数是一段 Lua 5.1 脚本程序，它会被运行在 Redis 服务器上下文中，这段脚本不必(也不应该)定义为一个 Lua 函数。 numkeys 参数用于指定键名参数的个数。 键名参数 key [key …] 从 EVAL 的第三个参数开始算起，表示在脚本中所用到的那些 Redis 键(key)，这些键名参数可以在 Lua 中通过全局变量 KEYS 数组，用 1 为基址的形式访问( KEYS[1] ， KEYS[2] ，以此类推)。 在命令的最后，那些不是键名参数的附加参数 arg [arg …] ，可以在 Lua 中通过全局变量 ARGV 数组访问，访问的形式和 KEYS 变量类似( ARGV[1] 、 ARGV[2] ，诸如此类)。 上面这几段长长的说明可以用一个简单的例子来概括：12345&gt; eval "return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;" 2 key1 key2 first second1) "key1"2) "key2"3) "first"4) "second" 其中 “return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}” 是被求值的 Lua 脚本，数字 2 指定了键名参数的数量， key1 和 key2 是键名参数，分别使用 KEYS[1] 和 KEYS[2] 访问，而最后的 first 和 second 则是附加参数，可以通过 ARGV[1] 和 ARGV[2] 访问它们。 在 Lua 脚本中，可以使用两个不同函数来执行 Redis 命令，它们分别是： redis.call() redis.pcall()这两个函数的唯一区别在于它们使用不同的方式处理执行命令所产生的错误，在后面的『错误处理』部分会讲到这一点。 redis.call() 和 redis.pcall() 两个函数的参数可以是任何格式良好(well formed)的 Redis 命令：12&gt; eval "return redis.call('set','foo','bar')" 0OK 需要注意的是，上面这段脚本的确实现了将键 foo 的值设为 bar 的目的，但是，它违反了 EVAL 命令的语义，因为脚本里使用的所有键都应该由 KEYS 数组来传递，就像这样：12&gt; eval "return redis.call('set',KEYS[1],'bar')" 1 fooOK 要求使用正确的形式来传递键(key)是有原因的，因为不仅仅是 EVAL 这个命令，所有的 Redis 命令，在执行之前都会被分析，籍此来确定命令会对哪些键进行操作。 因此，对于 EVAL 命令来说，必须使用正确的形式来传递键，才能确保分析工作正确地执行。除此之外，使用正确的形式来传递键还有很多其他好处，它的一个特别重要的用途就是确保 Redis 集群可以将你的请求发送到正确的集群节点。(对 Redis 集群的工作还在进行当中，但是脚本功能被设计成可以与集群功能保持兼容。)不过，这条规矩并不是强制性的，从而使得用户有机会滥用(abuse) Redis 单实例配置(single instance configuration)，代价是这样写出的脚本不能被 Redis 集群所兼容。 在 Lua 数据类型和 Redis 数据类型之间转换当 Lua 通过 call() 或 pcall() 函数执行 Redis 命令的时候，命令的返回值会被转换成 Lua 数据结构。同样地，当 Lua 脚本在 Redis 内置的解释器里运行时，Lua 脚本的返回值也会被转换成 Redis 协议(protocol)，然后由 EVAL 将值返回给客户端。 数据类型之间的转换遵循这样一个设计原则：如果将一个 Redis 值转换成 Lua 值，之后再将转换所得的 Lua 值转换回 Redis 值，那么这个转换所得的 Redis 值应该和最初时的 Redis 值一样。 换句话说， Lua 类型和 Redis 类型之间存在着一一对应的转换关系。 以下列出的是详细的转换规则： 从 Redis 转换到 Lua ： Redis integer reply -&gt; Lua number / Redis 整数转换成 Lua 数字 Redis bulk reply -&gt; Lua string / Redis bulk 回复转换成 Lua 字符串 Redis multi bulk reply -&gt; Lua table (may have other Redis data types nested) / Redis 多条 bulk 回复转换成 Lua 表，表内可能有其他别的 Redis 数据类型 Redis status reply -&gt; Lua table with a single ok field containing the status / Redis 状态回复转换成 Lua 表，表内的 ok 域包含了状态信息 Redis error reply -&gt; Lua table with a single err field containing the error / Redis 错误回复转换成 Lua 表，表内的 err 域包含了错误信息 Redis Nil bulk reply and Nil multi bulk reply -&gt; Lua false boolean type / Redis 的 Nil 回复和 Nil 多条回复转换成 Lua 的布尔值 false从 Lua 转换到 Redis： Lua number -&gt; Redis integer reply / Lua 数字转换成 Redis 整数 Lua string -&gt; Redis bulk reply / Lua 字符串转换成 Redis bulk 回复 Lua table (array) -&gt; Redis multi bulk reply / Lua 表(数组)转换成 Redis 多条 bulk 回复 Lua table with a single ok field -&gt; Redis status reply / 一个带单个 ok 域的 Lua 表，转换成 Redis 状态回复 Lua table with a single err field -&gt; Redis error reply / 一个带单个 err 域的 Lua 表，转换成 Redis 错误回复 Lua boolean false -&gt; Redis Nil bulk reply / Lua 的布尔值 false 转换成 Redis 的 Nil bulk 回复从 Lua 转换到 Redis 有一条额外的规则，这条规则没有和它对应的从 Redis 转换到 Lua 的规则： Lua boolean true -&gt; Redis integer reply with value of 1 / Lua 布尔值 true 转换成 Redis 整数回复中的 1以下是几个类型转换的例子： 1234567891011&gt; eval "return 10" 0(integer) 10&gt; eval "return &#123;1,2,&#123;3,'Hello World!'&#125;&#125;" 01) (integer) 12) (integer) 23) 1) (integer) 3 2) "Hello World!"&gt; eval "return redis.call('get','foo')" 0"bar" 在上面的三个代码示例里，前两个演示了如何将 Lua 值转换成 Redis 值，最后一个例子更复杂一些，它演示了一个将 Redis 值转换成 Lua 值，然后再将 Lua 值转换成 Redis 值的类型转过程。 脚本的原子性Redis 使用单个 Lua 解释器去运行所有脚本，并且， Redis 也保证脚本会以原子性(atomic)的方式执行：当某个脚本正在运行的时候，不会有其他脚本或 Redis 命令被执行。这和使用 MULTI / EXEC 包围的事务很类似。在其他别的客户端看来，脚本的效果(effect)要么是不可见的(not visible)，要么就是已完成的(already completed)。 另一方面，这也意味着，执行一个运行缓慢的脚本并不是一个好主意。写一个跑得很快很顺溜的脚本并不难，因为脚本的运行开销(overhead)非常少，但是当你不得不使用一些跑得比较慢的脚本时，请小心，因为当这些蜗牛脚本在慢吞吞地运行的时候，其他客户端会因为服务器正忙而无法执行命令。 错误处理前面的命令介绍部分说过， redis.call() 和 redis.pcall() 的唯一区别在于它们对错误处理的不同。 当 redis.call() 在执行命令的过程中发生错误时，脚本会停止执行，并返回一个脚本错误，错误的输出信息会说明错误造成的原因：12345redis&gt; lpush foo a(integer) 1redis&gt; eval "return redis.call('get', 'foo')" 0(error) ERR Error running script (call to f_282297a0228f48cd3fc6a55de6316f31422f5d17): ERR Operation against a key holding the wrong kind of value 和 redis.call() 不同， redis.pcall() 出错时并不引发(raise)错误，而是返回一个带 err 域的 Lua 表(table)，用于表示错误：12redis 127.0.0.1:6379&gt; EVAL "return redis.pcall('get', 'foo')" 0(error) ERR Operation against a key holding the wrong kind of value 带宽和 EVALSHAEVAL 命令要求你在每次执行脚本的时候都发送一次脚本主体(script body)。Redis 有一个内部的缓存机制，因此它不会每次都重新编译脚本，不过在很多场合，付出无谓的带宽来传送脚本主体并不是最佳选择。 为了减少带宽的消耗， Redis 实现了 EVALSHA 命令，它的作用和 EVAL 一样，都用于对脚本求值，但它接受的第一个参数不是脚本，而是脚本的 SHA1 校验和(sum)。 EVALSHA 命令的表现如下： 如果服务器还记得给定的 SHA1 校验和所指定的脚本，那么执行这个脚本 如果服务器不记得给定的 SHA1 校验和所指定的脚本，那么它返回一个特殊的错误，提醒用户使用 EVAL 代替 EVALSHA以下是示例：1234567891011&gt; set foo barOK&gt; eval "return redis.call('get','foo')" 0"bar"&gt; evalsha 6b1bf486c81ceb7edf3c093f4c48582e38c0e791 0"bar"&gt; evalsha ffffffffffffffffffffffffffffffffffffffff 0(error) `NOSCRIPT` No matching script. Please use [EVAL](/commands/eval). 客户端库的底层实现可以一直乐观地使用 EVALSHA 来代替 EVAL ，并期望着要使用的脚本已经保存在服务器上了，只有当 NOSCRIPT 错误发生时，才使用 EVAL 命令重新发送脚本，这样就可以最大限度地节省带宽。 这也说明了执行 EVAL 命令时，使用正确的格式来传递键名参数和附加参数的重要性：因为如果将参数硬写在脚本中，那么每次当参数改变的时候，都要重新发送脚本，即使脚本的主体并没有改变，相反，通过使用正确的格式来传递键名参数和附加参数，就可以在脚本主体不变的情况下，直接使用 EVALSHA 命令对脚本进行复用，免去了无谓的带宽消耗。 脚本缓存Redis 保证所有被运行过的脚本都会被永久保存在脚本缓存当中，这意味着，当 EVAL 命令在一个 Redis 实例上成功执行某个脚本之后，随后针对这个脚本的所有 EVALSHA 命令都会成功执行。 刷新脚本缓存的唯一办法是显式地调用 SCRIPT FLUSH 命令，这个命令会清空运行过的所有脚本的缓存。通常只有在云计算环境中，Redis 实例被改作其他客户或者别的应用程序的实例时，才会执行这个命令。 缓存可以长时间储存而不产生内存问题的原因是，它们的体积非常小，而且数量也非常少，即使脚本在概念上类似于实现一个新命令，即使在一个大规模的程序里有成百上千的脚本，即使这些脚本会经常修改，即便如此，储存这些脚本的内存仍然是微不足道的。 事实上，用户会发现 Redis 不移除缓存中的脚本实际上是一个好主意。比如说，对于一个和 Redis 保持持久化链接(persistent connection)的程序来说，它可以确信，执行过一次的脚本会一直保留在内存当中，因此它可以在流水线中使用 EVALSHA 命令而不必担心因为找不到所需的脚本而产生错误(稍候我们会看到在流水线中执行脚本的相关问题)。 SCRIPT 命令Redis 提供了以下几个 SCRIPT 命令，用于对脚本子系统(scripting subsystem)进行控制： SCRIPT FLUSH ：清除所有脚本缓存 SCRIPT EXISTS sha1 [sha1 …] ：根据给定的脚本校验和，检查指定的脚本是否存在于脚本缓存 SCRIPT LOAD script ：将一个脚本装入脚本缓存，但并不立即运行它 SCRIPT KILL ：杀死当前正在运行的脚本纯函数脚本在编写脚本方面，一个重要的要求就是，脚本应该被写成纯函数(pure function)。 也就是说，脚本应该具有以下属性： 对于同样的数据集输入，给定相同的参数，脚本执行的 Redis 写命令总是相同的。脚本执行的操作不能依赖于任何隐藏(非显式)数据，不能依赖于脚本在执行过程中、或脚本在不同执行时期之间可能变更的状态，并且它也不能依赖于任何来自 I/O 设备的外部输入。 使用系统时间(system time)，调用像 RANDOMKEY 那样的随机命令，或者使用 Lua 的随机数生成器，类似以上的这些操作，都会造成脚本的求值无法每次都得出同样的结果。 为了确保脚本符合上面所说的属性， Redis 做了以下工作： Lua 没有访问系统时间或者其他内部状态的命令 Redis 会返回一个错误，阻止这样的脚本运行： 这些脚本在执行随机命令之后(比如 RANDOMKEY 、 SRANDMEMBER key [count] 或 TIME 等)，还会执行可以修改数据集的 Redis 命令。如果脚本只是执行只读操作，那么就没有这一限制。注意，随机命令并不一定就指那些带 RAND 字眼的命令，任何带有非确定性的命令都会被认为是随机命令，比如 TIME 命令就是这方面的一个很好的例子。 每当从 Lua 脚本中调用那些返回无序元素的命令时，执行命令所得的数据在返回给 Lua 之前会先执行一个静默(slient)的字典序排序(lexicographical sorting)。举个例子，因为 Redis 的 Set 保存的是无序的元素，所以在 Redis 命令行客户端中直接执行 SMEMBERS key ，返回的元素是无序的，但是，假如在脚本中执行 redis.call(“smembers”, KEYS[1]) ，那么返回的总是排过序的元素。 对 Lua 的伪随机数生成函数 math.random 和 math.randomseed 进行修改，使得每次在运行新脚本的时候，总是拥有同样的 seed 值。这意味着，每次运行脚本时，只要不使用 math.randomseed ，那么 math.random 产生的随机数序列总是相同的。 尽管有那么多的限制，但用户还是可以用一个简单的技巧写出带随机行为的脚本(如果他们需要的话)。 假设现在我们要编写一个 Redis 脚本，这个脚本从列表中弹出 N 个随机数。一个 Ruby 写的例子如下：1234567891011121314151617require 'rubygems'require 'redis'r = Redis.newRandomPushScript = &lt;&lt;EOF local i = tonumber(ARGV[1]) local res while (i &gt; 0) do res = redis.call('lpush',KEYS[1],math.random()) i = i-1 end return resEOFr.del(:mylist)puts r.eval(RandomPushScript,[:mylist],[10,rand(2**32)]) 这个程序每次运行都会生成带有以下元素的列表：1234567891011&gt; lrange mylist 0 -11) "0.74509509873814"2) "0.87390407681181"3) "0.36876626981831"4) "0.6921941534114"5) "0.7857992587545"6) "0.57730350670279"7) "0.87046522734243"8) "0.09637165539729"9) "0.74990198051087"10) "0.17082803611217" 上面的 Ruby 程序每次都只生成同样的列表，用途并不是太大。那么，该怎样修改这个脚本，使得它仍然是一个纯函数(符合 Redis 的要求)，但是每次调用都可以产生不同的随机元素呢？ 一个简单的办法是，为脚本添加一个额外的参数，让这个参数作为 Lua 的随机数生成器的 seed 值，这样的话，只要给脚本传入不同的 seed ，脚本就会生成不同的列表元素。 以下是修改后的脚本：12345678910111213RandomPushScript = &lt;&lt;EOF local i = tonumber(ARGV[1]) local res math.randomseed(tonumber(ARGV[2])) while (i &gt; 0) do res = redis.call('lpush',KEYS[1],math.random()) i = i-1 end return resEOFr.del(:mylist)puts r.eval(RandomPushScript,1,:mylist,10,rand(2**32)) 尽管对于同样的 seed ，上面的脚本产生的列表元素是一样的(因为它是一个纯函数)，但是只要每次在执行脚本的时候传入不同的 seed ，我们就可以得到带有不同随机元素的列表。 Seed 会在复制(replication link)和写 AOF 文件时作为一个参数来传播，保证在载入 AOF 文件或附属节点(slave)处理脚本时， seed 仍然可以及时得到更新。 注意，Redis 实现保证 math.random 和 math.randomseed 的输出和运行 Redis 的系统架构无关，无论是 32 位还是 64 位系统，无论是小端(little endian)还是大端(big endian)系统，这两个函数的输出总是相同的。 全局变量保护为了防止不必要的数据泄漏进 Lua 环境， Redis 脚本不允许创建全局变量。如果一个脚本需要在多次执行之间维持某种状态，它应该使用 Redis key 来进行状态保存。 企图在脚本中访问一个全局变量(不论这个变量是否存在)将引起脚本停止， EVAL 命令会返回一个错误：12redis 127.0.0.1:6379&gt; eval 'a=10' 0(error) ERR Error running script (call to f_933044db579a2f8fd45d8065f04a8d0249383e57): user_script:1: Script attempted to create global variable 'a' Lua 的 debug 工具，或者其他设施，比如打印（alter）用于实现全局保护的 meta table ，都可以用于实现全局变量保护。 实现全局变量保护并不难，不过有时候还是会不小心而为之。一旦用户在脚本中混入了 Lua 全局状态，那么 AOF 持久化和复制（replication）都会无法保证，所以，请不要使用全局变量。 避免引入全局变量的一个诀窍是：将脚本中用到的所有变量都使用 local 关键字定义为局部变量。 库Redis 内置的 Lua 解释器加载了以下 Lua 库： base table string math debug cjson cmsgpack其中 cjson 库可以让 Lua 以非常快的速度处理 JSON 数据，除此之外，其他别的都是 Lua 的标准库。 每个 Redis 实例都保证会加载上面列举的库，从而确保每个 Redis 脚本的运行环境都是相同的。 使用脚本散发 Redis 日志在 Lua 脚本中，可以通过调用 redis.log 函数来写 Redis 日志(log)：1redis.log(loglevel, message) 其中， message 参数是一个字符串，而 loglevel 参数可以是以下任意一个值： redis.LOG_DEBUG redis.LOG_VERBOSE redis.LOG_NOTICE redis.LOG_WARNING上面的这些等级(level)和标准 Redis 日志的等级相对应。 对于脚本散发(emit)的日志，只有那些和当前 Redis 实例所设置的日志等级相同或更高级的日志才会被散发。 以下是一个日志示例：1redis.log(redis.LOG_WARNING, "Something is wrong with this script.") 执行上面的函数会产生这样的信息：1[32343] 22 Mar 15:21:39 # Something is wrong with this script. 沙箱(sandbox)和最大执行时间脚本应该仅仅用于传递参数和对 Redis 数据进行处理，它不应该尝试去访问外部系统(比如文件系统)，或者执行任何系统调用。 除此之外，脚本还有一个最大执行时间限制，它的默认值是 5 秒钟，一般正常运作的脚本通常可以在几分之几毫秒之内完成，花不了那么多时间，这个限制主要是为了防止因编程错误而造成的无限循环而设置的。 最大执行时间的长短由 lua-time-limit 选项来控制(以毫秒为单位)，可以通过编辑 redis.conf 文件或者使用 CONFIG GET parameter 和 CONFIG SET parameter value 命令来修改它。 当一个脚本达到最大执行时间的时候，它并不会自动被 Redis 结束，因为 Redis 必须保证脚本执行的原子性，而中途停止脚本的运行意味着可能会留下未处理完的数据在数据集(data set)里面。 因此，当脚本运行的时间超过最大执行时间后，以下动作会被执行： Redis 记录一个脚本正在超时运行 Redis 开始重新接受其他客户端的命令请求，但是只有 SCRIPT KILL 和 SHUTDOWN NOSAVE 两个命令会被处理，对于其他命令请求， Redis 服务器只是简单地返回 BUSY 错误。 可以使用 SCRIPT KILL 命令将一个仅执行只读命令的脚本杀死，因为只读命令并不修改数据，因此杀死这个脚本并不破坏数据的完整性 如果脚本已经执行过写命令，那么唯一允许执行的操作就是 SHUTDOWN NOSAVE ，它通过停止服务器来阻止当前数据集写入磁盘流水线(pipeline)上下文(context)中的 EVALSHA在流水线请求的上下文中使用 EVALSHA 命令时，要特别小心，因为在流水线中，必须保证命令的执行顺序。 一旦在流水线中因为 EVALSHA 命令而发生 NOSCRIPT 错误，那么这个流水线就再也没有办法重新执行了，否则的话，命令的执行顺序就会被打乱。 为了防止出现以上所说的问题，客户端库实现应该实施以下的其中一项措施： 总是在流水线中使用 EVAL 命令 检查流水线中要用到的所有命令，找到其中的 EVAL 命令，并使用 SCRIPT EXISTS sha1 [sha1 …] 命令检查要用到的脚本是不是全都已经保存在缓存里面了。如果所需的全部脚本都可以在缓存里找到，那么就可以放心地将所有 EVAL 命令改成 EVALSHA 命令，否则的话，就要在流水线的顶端(top)将缺少的脚本用 SCRIPT LOAD script 命令加上去。 EVALSHAEVALSHA sha1 numkeys key [key …] arg [arg …]可用版本： &gt;= 2.6.0 时间复杂度： 根据脚本的复杂度而定。 根据给定的 sha1 校验码，对缓存在服务器中的脚本进行求值。 将脚本缓存到服务器的操作可以通过 SCRIPT LOAD script 命令进行。 这个命令的其他地方，比如参数的传入方式，都和 EVAL script numkeys key [key …] arg [arg …] 命令一样。12345redis&gt; SCRIPT LOAD "return 'hello moto'""232fd51614574cf0867b83d384a5e898cfd24e5a"redis&gt; EVALSHA "232fd51614574cf0867b83d384a5e898cfd24e5a" 0"hello moto" SCRIPT LOADSCRIPT LOAD script可用版本： &gt;= 2.6.0 时间复杂度： O(N) , N 为脚本的长度(以字节为单位)。 将脚本 script 添加到脚本缓存中，但并不立即执行这个脚本。 EVAL script numkeys key [key …] arg [arg …] 命令也会将脚本添加到脚本缓存中，但是它会立即对输入的脚本进行求值。 如果给定的脚本已经在缓存里面了，那么不做动作。 在脚本被加入到缓存之后，通过 EVALSHA 命令，可以使用脚本的 SHA1 校验和来调用这个脚本。 脚本可以在缓存中保留无限长的时间，直到执行 SCRIPT FLUSH 为止。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值给定 script 的 SHA1 校验和。 代码示例12345redis&gt; SCRIPT LOAD "return 'hello moto'""232fd51614574cf0867b83d384a5e898cfd24e5a"redis&gt; EVALSHA 232fd51614574cf0867b83d384a5e898cfd24e5a 0"hello moto" SCRIPT EXISTSSCRIPT EXISTS sha1 [sha1 …]可用版本： &gt;= 2.6.0 时间复杂度： O(N) , N 为给定的 SHA1 校验和的数量。 给定一个或多个脚本的 SHA1 校验和，返回一个包含 0 和 1 的列表，表示校验和所指定的脚本是否已经被保存在缓存当中。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值一个列表，包含 0 和 1 ，前者表示脚本不存在于缓存，后者表示脚本已经在缓存里面了。 列表中的元素和给定的 SHA1 校验和保持对应关系，比如列表的第三个元素的值就表示第三个 SHA1 校验和所指定的脚本在缓存中的状态。 代码示例1234567891011redis&gt; SCRIPT LOAD "return 'hello moto'" # 载入一个脚本"232fd51614574cf0867b83d384a5e898cfd24e5a"redis&gt; SCRIPT EXISTS 232fd51614574cf0867b83d384a5e898cfd24e5a1) (integer) 1redis&gt; SCRIPT FLUSH # 清空缓存OKredis&gt; SCRIPT EXISTS 232fd51614574cf0867b83d384a5e898cfd24e5a1) (integer) 0 SCRIPT FLUSHSCRIPT FLUSH可用版本： &gt;= 2.6.0 复杂度： O(N) ， N 为缓存中脚本的数量。 清除所有 Lua 脚本缓存。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值总是返回 OK 代码示例12redis&gt; SCRIPT FLUSHOK SCRIPT KILLSCRIPT KILL可用版本： &gt;= 2.6.0 时间复杂度： O(1) 杀死当前正在运行的 Lua 脚本，当且仅当这个脚本没有执行过任何写操作时，这个命令才生效。 这个命令主要用于终止运行时间过长的脚本，比如一个因为 BUG 而发生无限 loop 的脚本，诸如此类。 SCRIPT KILL 执行之后，当前正在运行的脚本会被杀死，执行这个脚本的客户端会从 EVAL script numkeys key [key …] arg [arg …] 命令的阻塞当中退出，并收到一个错误作为返回值。 另一方面，假如当前正在运行的脚本已经执行过写操作，那么即使执行 SCRIPT KILL ，也无法将它杀死，因为这是违反 Lua 脚本的原子性执行原则的。在这种情况下，唯一可行的办法是使用 SHUTDOWN NOSAVE 命令，通过停止整个 Redis 进程来停止脚本的运行，并防止不完整(half-written)的信息被写入数据库中。 关于使用 Redis 对 Lua 脚本进行求值的更多信息，请参见 EVAL script numkeys key [key …] arg [arg …] 命令。 返回值执行成功返回 OK ，否则返回一个错误。 代码示例12345678910111213# 没有脚本在执行时redis&gt; SCRIPT KILL(error) ERR No scripts in execution right now.# 成功杀死脚本时redis&gt; SCRIPT KILLOK(1.30s)# 尝试杀死一个已经执行过写操作的脚本，失败redis&gt; SCRIPT KILL(error) ERR Sorry the script already executed write commands against the dataset. You can either wait the script termination or kill the server in an hard way using the SHUTDOWN NOSAVE command.(1.69s) 以下是脚本被杀死之后，返回给执行脚本的客户端的错误：123redis&gt; EVAL "while true do end" 0(error) ERR Error running script (call to f_694a5fe1ddb97a4c6a1bf299d9537c7d3d0f84e7): Script killed by user with SCRIPT KILL...(5.00s) 持久化SAVESAVE可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为要保存到数据库中的 key 的数量。 SAVE 命令执行一个同步保存操作，将当前 Redis 实例的所有数据快照(snapshot)以 RDB 文件的形式保存到硬盘。 一般来说，在生产环境很少执行 SAVE 操作，因为它会阻塞所有客户端，保存数据库的任务通常由 BGSAVE 命令异步地执行。然而，如果负责保存数据的后台子进程不幸出现问题时， SAVE 可以作为保存数据的最后手段来使用。 返回值保存成功时返回 OK 。 代码示例12redis&gt; SAVEOK BGSAVEBGSAVE可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为要保存到数据库中的 key 的数量。 在后台异步(Asynchronously)保存当前数据库的数据到磁盘。 BGSAVE 命令执行之后立即返回 OK ，然后 Redis fork 出一个新子进程，原来的 Redis 进程(父进程)继续处理客户端请求，而子进程则负责将数据保存到磁盘，然后退出。 客户端可以通过 LASTSAVE 命令查看相关信息，判断 BGSAVE 命令是否执行成功。 请移步 持久化文档 查看更多相关细节。 返回值反馈信息。 代码示例12redis&gt; BGSAVEBackground saving started BGREWRITEAOFBGREWRITEAOF可用版本： &gt;= 1.0.0 时间复杂度： O(N)， N 为要追加到 AOF 文件中的数据数量。 执行一个 AOF文件 重写操作。重写会创建一个当前 AOF 文件的体积优化版本。 即使 BGREWRITEAOF 执行失败，也不会有任何数据丢失，因为旧的 AOF 文件在 BGREWRITEAOF 成功之前不会被修改。 重写操作只会在没有其他持久化工作在后台执行时被触发，也就是说： 如果 Redis 的子进程正在执行快照的保存工作，那么 AOF 重写的操作会被预定(scheduled)，等到保存工作完成之后再执行 AOF 重写。在这种情况下， BGREWRITEAOF 的返回值仍然是 OK ，但还会加上一条额外的信息，说明 BGREWRITEAOF 要等到保存操作完成之后才能执行。在 Redis 2.6 或以上的版本，可以使用 INFO [section] 命令查看 BGREWRITEAOF 是否被预定。如果已经有别的 AOF 文件重写在执行，那么 BGREWRITEAOF 返回一个错误，并且这个新的 BGREWRITEAOF 请求也不会被预定到下次执行。从 Redis 2.4 开始， AOF 重写由 Redis 自行触发， BGREWRITEAOF 仅仅用于手动触发重写操作。 请移步 持久化文档(英文) 查看更多相关细节。 返回值反馈信息。 代码示例12redis&gt; BGREWRITEAOFBackground append only file rewriting started LASTSAVELASTSAVE可用版本： &gt;= 1.0.0 时间复杂度： O(1) 返回最近一次 Redis 成功将数据保存到磁盘上的时间，以 UNIX 时间戳格式表示。 返回值一个 UNIX 时间戳。 代码示例12redis&gt; LASTSAVE(integer) 1324043588 发布与订阅PUBLISHPUBLISH channel message可用版本： &gt;= 2.0.0 时间复杂度： O(N+M)，其中 N 是频道 channel 的订阅者数量，而 M 则是使用模式订阅(subscribed patterns)的客户端的数量。 将信息 message 发送到指定的频道 channel 。 返回值接收到信息 message 的订阅者数量。 代码示例1234567891011# 对没有订阅者的频道发送信息redis&gt; publish bad_channel "can any body hear me?"(integer) 0# 向有一个订阅者的频道发送信息redis&gt; publish msg "good morning"(integer) 1# 向有多个订阅者的频道发送信息redis&gt; publish chat_room "hello~ everyone"(integer) 3 SUBSCRIBESUBSCRIBE channel [channel …]可用版本： &gt;= 2.0.0 时间复杂度： O(N)，其中 N 是订阅的频道的数量。 订阅给定的一个或多个频道的信息。 返回值接收到的信息(请参见下面的代码说明)。 代码示例123456789101112131415161718192021# 订阅 msg 和 chat_room 两个频道# 1 - 6 行是执行 subscribe 之后的反馈信息# 第 7 - 9 行才是接收到的第一条信息# 第 10 - 12 行是第二条redis&gt; subscribe msg chat_roomReading messages... (press Ctrl-C to quit)1) "subscribe" # 返回值的类型：显示订阅成功2) "msg" # 订阅的频道名字3) (integer) 1 # 目前已订阅的频道数量1) "subscribe"2) "chat_room"3) (integer) 21) "message" # 返回值的类型：信息2) "msg" # 来源(从那个频道发送过来)3) "hello moto" # 信息内容1) "message"2) "chat_room"3) "testing...haha" PSUBSCRIBEPSUBSCRIBE pattern [pattern …]可用版本： &gt;= 2.0.0 时间复杂度： O(N)， N 是订阅的模式的数量。 订阅一个或多个符合给定模式的频道。 每个模式以 作为匹配符，比如 it 匹配所有以 it 开头的频道( it.news 、 it.blog 、 it.tweets 等等)， news.* 匹配所有以 news. 开头的频道( news.it 、 news.global.today 等等)，诸如此类。 返回值接收到的信息(请参见下面的代码说明)。 代码示例1234567891011121314151617181920212223242526272829303132333435# 订阅 news.* 和 tweet.* 两个模式# 第 1 - 6 行是执行 psubscribe 之后的反馈信息# 第 7 - 10 才是接收到的第一条信息# 第 11 - 14 是第二条# 以此类推。。。redis&gt; psubscribe news.* tweet.*Reading messages... (press Ctrl-C to quit)1) "psubscribe" # 返回值的类型：显示订阅成功2) "news.*" # 订阅的模式3) (integer) 1 # 目前已订阅的模式的数量1) "psubscribe"2) "tweet.*"3) (integer) 21) "pmessage" # 返回值的类型：信息2) "news.*" # 信息匹配的模式3) "news.it" # 信息本身的目标频道4) "Google buy Motorola" # 信息的内容1) "pmessage"2) "tweet.*"3) "tweet.huangz"4) "hello"1) "pmessage"2) "tweet.*"3) "tweet.joe"4) "@huangz morning"1) "pmessage"2) "news.*"3) "news.life"4) "An apple a day, keep doctors away" UNSUBSCRIBEUNSUBSCRIBE [channel [channel …]]可用版本： &gt;= 2.0.0 时间复杂度： O(N) ， N 是客户端已订阅的频道的数量。 指示客户端退订给定的频道。 如果没有频道被指定，也即是，一个无参数的 UNSUBSCRIBE 调用被执行，那么客户端使用 SUBSCRIBE 命令订阅的所有频道都会被退订。在这种情况下，命令会返回一个信息，告知客户端所有被退订的频道。 返回值这个命令在不同的客户端中有不同的表现。 PUNSUBSCRIBEPUNSUBSCRIBE [pattern [pattern …]]可用版本： &gt;= 2.0.0 时间复杂度： O(N+M) ，其中 N 是客户端已订阅的模式的数量， M 则是系统中所有客户端订阅的模式的数量。 指示客户端退订所有给定模式。 如果没有模式被指定，也即是，一个无参数的 PUNSUBSCRIBE 调用被执行，那么客户端使用 PSUBSCRIBE pattern [pattern …] 命令订阅的所有模式都会被退订。在这种情况下，命令会返回一个信息，告知客户端所有被退订的模式。 返回值这个命令在不同的客户端中有不同的表现。 PUBSUBPUBSUB [argument [argument …]]可用版本：&gt;= 2.8.0 PUBSUB 是一个查看订阅与发布系统状态的内省命令， 它由数个不同格式的子命令组成， 以下将分别对这些子命令进行介绍。 PUBSUB CHANNELS [pattern]复杂度： O(N) ， N 为活跃频道的数量（对于长度较短的频道和模式来说，将进行模式匹配的复杂度视为常数）。列出当前的活跃频道。 活跃频道指的是那些至少有一个订阅者的频道， 订阅模式的客户端不计算在内。 pattern 参数是可选的： 如果不给出 pattern 参数，那么列出订阅与发布系统中的所有活跃频道。 如果给出 pattern 参数，那么只列出和给定模式 pattern 相匹配的那些活跃频道。返回值一个由活跃频道组成的列表。 代码示例1234567891011121314151617181920212223242526272829303132# client-1 订阅 news.it 和 news.sport 两个频道client-1&gt; SUBSCRIBE news.it news.sportReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.sport"3) (integer) 2# client-2 订阅 news.it 和 news.internet 两个频道client-2&gt; SUBSCRIBE news.it news.internetReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.internet"3) (integer) 2# 首先， client-3 打印所有活跃频道# 注意，即使一个频道有多个订阅者，它也只输出一次，比如 news.itclient-3&gt; PUBSUB CHANNELS1) "news.sport"2) "news.internet"3) "news.it"# 接下来， client-3 打印那些与模式 news.i* 相匹配的活跃频道# 因为 news.sport 不匹配 news.i* ，所以它没有被打印redis&gt; PUBSUB CHANNELS news.i*1) "news.internet"2) "news.it" PUBSUB NUMSUB [channel-1 … channel-N]复杂度： O(N) ， N 为给定频道的数量。 返回给定频道的订阅者数量， 订阅模式的客户端不计算在内。 返回值一个多条批量回复（Multi-bulk reply），回复中包含给定的频道，以及频道的订阅者数量。 格式为：频道 channel-1 ， channel-1 的订阅者数量，频道 channel-2 ， channel-2 的订阅者数量，诸如此类。 回复中频道的排列顺序和执行命令时给定频道的排列顺序一致。 不给定任何频道而直接调用这个命令也是可以的， 在这种情况下， 命令只返回一个空列表。 代码示例123456789101112131415161718192021222324252627282930# client-1 订阅 news.it 和 news.sport 两个频道client-1&gt; SUBSCRIBE news.it news.sportReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.sport"3) (integer) 2# client-2 订阅 news.it 和 news.internet 两个频道client-2&gt; SUBSCRIBE news.it news.internetReading messages... (press Ctrl-C to quit)1) "subscribe"2) "news.it"3) (integer) 11) "subscribe"2) "news.internet"3) (integer) 2# client-3 打印各个频道的订阅者数量client-3&gt; PUBSUB NUMSUB news.it news.internet news.sport news.music1) "news.it" # 频道2) "2" # 订阅该频道的客户端数量3) "news.internet"4) "1"5) "news.sport"6) "1"7) "news.music" # 没有任何订阅者8) "0" PUBSUB NUMPAT复杂度： O(1) 。 返回订阅模式的数量。 注意， 这个命令返回的不是订阅模式的客户端的数量， 而是客户端订阅的所有模式的数量总和。 返回值一个整数回复（Integer reply）。 代码示例1234567891011121314151617181920212223242526272829303132# client-1 订阅 news.* 和 discount.* 两个模式client-1&gt; PSUBSCRIBE news.* discount.*Reading messages... (press Ctrl-C to quit)1) "psubscribe"2) "news.*"3) (integer) 11) "psubscribe"2) "discount.*"3) (integer) 2# client-2 订阅 tweet.* 一个模式client-2&gt; PSUBSCRIBE tweet.*Reading messages... (press Ctrl-C to quit)1) "psubscribe"2) "tweet.*"3) (integer) 1# client-3 返回当前订阅模式的数量为 3client-3&gt; PUBSUB NUMPAT(integer) 3# 注意，当有多个客户端订阅相同的模式时，相同的订阅也被计算在 PUBSUB NUMPAT 之内# 比如说，再新建一个客户端 client-4 ，让它也订阅 news.* 频道client-4&gt; PSUBSCRIBE news.*Reading messages... (press Ctrl-C to quit)1) "psubscribe"2) "news.*"3) (integer) 1# 这时再计算被订阅模式的数量，就会得到数量为 4client-3&gt; PUBSUB NUMPAT(integer) 4 复制SLAVEOFSLAVEOF host port可用版本： &gt;= 1.0.0 时间复杂度： SLAVEOF host port 的复杂度为 O(N)，其中 N 为要同步的数据数量； SLAVEOF NO ONE 命令的复杂度为 O(1) 。 SLAVEOF 命令用于在 Redis 运行时动态地修改复制(replication)功能的行为。 通过执行 SLAVEOF host port 命令，可以将当前服务器转变为指定服务器的从属服务器(slave server)。 如果当前服务器已经是某个主服务器(master server)的从属服务器，那么执行 SLAVEOF host port 将使当前服务器停止对旧主服务器的同步，丢弃旧数据集，转而开始对新主服务器进行同步。 另外，对一个从属服务器执行命令 SLAVEOF NO ONE 将使得这个从属服务器关闭复制功能，并从从属服务器转变回主服务器，原来同步所得的数据集不会被丢弃。 利用“SLAVEOF NO ONE 不会丢弃同步所得数据集”这个特性，可以在主服务器失败的时候，将从属服务器用作新的主服务器，从而实现无间断运行。 返回值总是返回 OK 。 代码示例12345redis&gt; SLAVEOF 127.0.0.1 6379OKredis&gt; SLAVEOF NO ONEOK ROLEROLE可用版本： &gt;= 2.8.12 时间复杂度： O(1) 返回实例在复制中担任的角色， 这个角色可以是 master 、 slave 或者 sentinel 。 除了角色之外， 命令还会返回与该角色相关的其他信息， 其中： 主服务器将返回属下从服务器的 IP 地址和端口。 从服务器将返回自己正在复制的主服务器的 IP 地址、端口、连接状态以及复制偏移量。 Sentinel 将返回自己正在监视的主服务器列表。返回值ROLE 命令将返回一个数组。 代码示例主服务器123456781) "master"2) (integer) 31296593) 1) 1) "127.0.0.1" 2) "9001" 3) "3129242" 2) 1) "127.0.0.1" 2) "9002" 3) "3129543" 从服务器123451) "slave"2) "127.0.0.1"3) (integer) 90004) "connected"5) (integer) 3167038 Sentinel123451) "sentinel"2) 1) "resque-master" 2) "html-fragments-master" 3) "stats-master" 4) "metadata-master" 客户端与服务器AUTHAUTH password可用版本： &gt;= 1.0.0 时间复杂度： O(1) 通过设置配置文件中 requirepass 项的值(使用命令 CONFIG SET requirepass password )，可以使用密码来保护 Redis 服务器。 如果开启了密码保护的话，在每次连接 Redis 服务器之后，就要使用 AUTH 命令解锁，解锁之后才能使用其他 Redis 命令。 如果 AUTH 命令给定的密码 password 和配置文件中的密码相符的话，服务器会返回 OK 并开始接受命令输入。 另一方面，假如密码不匹配的话，服务器将返回一个错误，并要求客户端需重新输入密码。 Warning因为 Redis 高性能的特点，在很短时间内尝试猜测非常多个密码是有可能的，因此请确保使用的密码足够复杂和足够长，以免遭受密码猜测攻击。 返回值密码匹配时返回 OK ，否则返回一个错误。 代码示例12345678910111213141516171819202122232425262728# 设置密码redis&gt; CONFIG SET requirepass secret_password # 将密码设置为 secret_passwordOKredis&gt; QUIT # 退出再连接，让新密码对客户端生效[huangz@mypad]$ redisredis&gt; PING # 未验证密码，操作被拒绝(error) ERR operation not permittedredis&gt; AUTH wrong_password_testing # 尝试输入错误的密码(error) ERR invalid passwordredis&gt; AUTH secret_password # 输入正确的密码OKredis&gt; PING # 密码验证成功，可以正常操作命令了PONG# 清空密码redis&gt; CONFIG SET requirepass "" # 通过将密码设为空字符来清空密码OKredis&gt; QUIT$ redis # 重新进入客户端redis&gt; PING # 执行命令不再需要密码，清空密码操作成功PONG QUITQUIT可用版本： &gt;= 1.0.0 时间复杂度： O(1) 请求服务器关闭与当前客户端的连接。 一旦所有等待中的回复(如果有的话)顺利写入到客户端，连接就会被关闭。 返回值总是返回 OK (但是不会被打印显示，因为当时 Redis-cli 已经退出)。 代码示例123$ redisredis&gt; QUIT$ INFOINFO [section]可用版本： &gt;= 1.0.0 时间复杂度： O(1) 以一种易于解释（parse）且易于阅读的格式，返回关于 Redis 服务器的各种信息和统计数值。 通过给定可选的参数 section ，可以让命令只返回某一部分的信息： server 部分记录了 Redis 服务器的信息，它包含以下域： redis_version : Redis 服务器版本 redis_git_sha1 : Git SHA1 redis_git_dirty : Git dirty flag os : Redis 服务器的宿主操作系统 arch_bits : 架构（32 或 64 位） multiplexing_api : Redis 所使用的事件处理机制 gcc_version : 编译 Redis 时所使用的 GCC 版本 process_id : 服务器进程的 PID run_id : Redis 服务器的随机标识符（用于 Sentinel 和集群） tcp_port : TCP/IP 监听端口 uptime_in_seconds : 自 Redis 服务器启动以来，经过的秒数 uptime_in_days : 自 Redis 服务器启动以来，经过的天数 lru_clock : 以分钟为单位进行自增的时钟，用于 LRU 管理 clients 部分记录了已连接客户端的信息，它包含以下域： connected_clients : 已连接客户端的数量（不包括通过从属服务器连接的客户端） client_longest_output_list : 当前连接的客户端当中，最长的输出列表 client_longest_input_buf : 当前连接的客户端当中，最大输入缓存 blocked_clients : 正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 memory 部分记录了服务器的内存信息，它包含以下域： used_memory : 由 Redis 分配器分配的内存总量，以字节（byte）为单位 used_memory_human : 以人类可读的格式返回 Redis 分配的内存总量 used_memory_rss : 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致。 used_memory_peak : Redis 的内存消耗峰值（以字节为单位） used_memory_peak_human : 以人类可读的格式返回 Redis 的内存消耗峰值 used_memory_lua : Lua 引擎所使用的内存大小（以字节为单位） mem_fragmentation_ratio : used_memory_rss 和 used_memory 之间的比率 mem_allocator : 在编译时指定的， Redis 所使用的内存分配器。可以是 libc 、 jemalloc 或者 tcmalloc 。 在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。 当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。 内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。 当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。 1Because Redis does not have control over how its allocations are mapped to memory pages, high used_memory_rss is often the result of a spike in memory usage. 当 Redis 释放内存时，分配器可能会，也可能不会，将内存返还给操作系统。 如果 Redis 释放了内存，却没有将内存返还给操作系统，那么 used_memory 的值可能和操作系统显示的 Redis 内存占用并不一致。 查看 used_memory_peak 的值可以验证这种情况是否发生。 persistence 部分记录了跟 RDB 持久化和 AOF 持久化有关的信息，它包含以下域： loading : 一个标志值，记录了服务器是否正在载入持久化文件。 rdb_changes_since_last_save : 距离最近一次成功创建持久化文件之后，经过了多少秒。 rdb_bgsave_in_progress : 一个标志值，记录了服务器是否正在创建 RDB 文件。 rdb_last_save_time : 最近一次成功创建 RDB 文件的 UNIX 时间戳。 rdb_last_bgsave_status : 一个标志值，记录了最近一次创建 RDB 文件的结果是成功还是失败。 rdb_last_bgsave_time_sec : 记录了最近一次创建 RDB 文件耗费的秒数。 rdb_current_bgsave_time_sec : 如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数。 aof_enabled : 一个标志值，记录了 AOF 是否处于打开状态。 aof_rewrite_in_progress : 一个标志值，记录了服务器是否正在创建 AOF 文件。 aof_rewrite_scheduled : 一个标志值，记录了在 RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作。 aof_last_rewrite_time_sec : 最近一次创建 AOF 文件耗费的时长。 aof_current_rewrite_time_sec : 如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数。 aof_last_bgrewrite_status : 一个标志值，记录了最近一次创建 AOF 文件的结果是成功还是失败。 如果 AOF 持久化功能处于开启状态，那么这个部分还会加上以下域： aof_current_size : AOF 文件目前的大小。 aof_base_size : 服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小。 aof_pending_rewrite : 一个标志值，记录了是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行。 aof_buffer_length : AOF 缓冲区的大小。 aof_rewrite_buffer_length : AOF 重写缓冲区的大小。 aof_pending_bio_fsync : 后台 I/O 队列里面，等待执行的 fsync 调用数量。 aof_delayed_fsync : 被延迟的 fsync 调用数量。 stats 部分记录了一般统计信息，它包含以下域： total_connections_received : 服务器已接受的连接请求数量。 total_commands_processed : 服务器已执行的命令数量。 instantaneous_ops_per_sec : 服务器每秒钟执行的命令数量。 rejected_connections : 因为最大客户端数量限制而被拒绝的连接请求数量。 expired_keys : 因为过期而被自动删除的数据库键数量。 evicted_keys : 因为最大内存容量限制而被驱逐（evict）的键数量。 keyspace_hits : 查找数据库键成功的次数。 keyspace_misses : 查找数据库键失败的次数。 pubsub_channels : 目前被订阅的频道数量。 pubsub_patterns : 目前被订阅的模式数量。 latest_fork_usec : 最近一次 fork() 操作耗费的毫秒数。 replication : 主/从复制信息 role : 如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器。 如果当前服务器是一个从服务器的话，那么这个部分还会加上以下域： master_host : 主服务器的 IP 地址。 master_port : 主服务器的 TCP 监听端口号。 master_link_status : 复制连接当前的状态， up 表示连接正常， down 表示连接断开。 master_last_io_seconds_ago : 距离最近一次与主服务器进行通信已经过去了多少秒钟。 master_sync_in_progress : 一个标志值，记录了主服务器是否正在与这个从服务器进行同步。 如果同步操作正在进行，那么这个部分还会加上以下域： master_sync_left_bytes : 距离同步完成还缺少多少字节数据。 master_sync_last_io_seconds_ago : 距离最近一次因为 SYNC 操作而进行 I/O 已经过去了多少秒。 如果主从服务器之间的连接处于断线状态，那么这个部分还会加上以下域： master_link_down_since_seconds : 主从服务器连接断开了多少秒。 以下是一些总会出现的域： connected_slaves : 已连接的从服务器数量。 对于每个从服务器，都会添加以下一行信息： slaveXXX : ID、IP 地址、端口号、连接状态 cpu 部分记录了 CPU 的计算量统计信息，它包含以下域： used_cpu_sys : Redis 服务器耗费的系统 CPU 。 used_cpu_user : Redis 服务器耗费的用户 CPU 。 used_cpu_sys_children : 后台进程耗费的系统 CPU 。 used_cpu_user_children : 后台进程耗费的用户 CPU 。 commandstats 部分记录了各种不同类型的命令的执行统计信息，比如命令执行的次数、命令耗费的 CPU 时间、执行每个命令耗费的平均 CPU 时间等等。对于每种类型的命令，这个部分都会添加一行以下格式的信息： cmdstat_XXX:calls=XXX,usec=XXX,usecpercall=XXX cluster 部分记录了和集群有关的信息，它包含以下域： cluster_enabled : 一个标志值，记录集群功能是否已经开启。 keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库已经被删除的过期键数量等。对于每个数据库，这个部分都会添加一行以下格式的信息： dbXXX:keys=XXX,expires=XXX 除上面给出的这些值以外， section 参数的值还可以是下面这两个： all : 返回所有信息 default : 返回默认选择的信息 当不带参数直接调用 INFO 命令时，使用 default 作为默认参数。 Note不同版本的 Redis 可能对返回的一些域进行了增加或删减。 因此，一个健壮的客户端程序在对 INFO [section] 命令的输出进行分析时，应该能够跳过不认识的域，并且妥善地处理丢失不见的域。 返回值具体请参见下面的测试代码。 代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889redis&gt; INFO# Serverredis_version:2.9.11redis_git_sha1:937384d0redis_git_dirty:0redis_build_id:8e9509442863f22redis_mode:standaloneos:Linux 3.13.0-35-generic x86_64arch_bits:64multiplexing_api:epollgcc_version:4.8.2process_id:4716run_id:26186aac3f2380aaee9eef21cc50aecd542d97dctcp_port:6379uptime_in_seconds:362uptime_in_days:0hz:10lru_clock:1725349config_file:# Clientsconnected_clients:1client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0# Memoryused_memory:508536used_memory_human:496.62Kused_memory_rss:7974912used_memory_peak:508536used_memory_peak_human:496.62Kused_memory_lua:33792mem_fragmentation_ratio:15.68mem_allocator:jemalloc-3.2.0# Persistenceloading:0rdb_changes_since_last_save:6rdb_bgsave_in_progress:0rdb_last_save_time:1411011131rdb_last_bgsave_status:okrdb_last_bgsave_time_sec:-1rdb_current_bgsave_time_sec:-1aof_enabled:0aof_rewrite_in_progress:0aof_rewrite_scheduled:0aof_last_rewrite_time_sec:-1aof_current_rewrite_time_sec:-1aof_last_bgrewrite_status:okaof_last_write_status:ok# Statstotal_connections_received:2total_commands_processed:4instantaneous_ops_per_sec:0rejected_connections:0sync_full:0sync_partial_ok:0sync_partial_err:0expired_keys:0evicted_keys:0keyspace_hits:0keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:0migrate_cached_sockets:0# Replicationrole:masterconnected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0# CPUused_cpu_sys:0.21used_cpu_user:0.17used_cpu_sys_children:0.00used_cpu_user_children:0.00# Clustercluster_enabled:0# Keyspacedb0:keys=2,expires=0,avg_ttl=0 SHUTDOWNSHUTDOWN [SAVE|NOSAVE]可用版本： &gt;= 1.0.0 时间复杂度： O(N)，其中 N 为关机时需要保存的数据库键数量。 SHUTDOWN 命令执行以下操作： 停止所有客户端 如果有至少一个保存点在等待，执行 SAVE 命令 如果 AOF 选项被打开，更新 AOF 文件 关闭 redis 服务器(server) 如果持久化被打开的话， SHUTDOWN 命令会保证服务器正常关闭而不丢失任何数据。 另一方面，假如只是单纯地执行 SAVE 命令，然后再执行 QUIT 命令，则没有这一保证 —— 因为在执行 SAVE 之后、执行 QUIT 之前的这段时间中间，其他客户端可能正在和服务器进行通讯，这时如果执行 QUIT 就会造成数据丢失。 SAVE 和 NOSAVE 修饰符通过使用可选的修饰符，可以修改 SHUTDOWN 命令的表现。比如说： 执行 SHUTDOWN SAVE 会强制让数据库执行保存操作，即使没有设定(configure)保存点 执行 SHUTDOWN NOSAVE 会阻止数据库执行保存操作，即使已经设定有一个或多个保存点(你可以将这一用法看作是强制停止服务器的一个假想的 ABORT 命令)返回值执行失败时返回错误。 执行成功时不返回任何信息，服务器和客户端的连接断开，客户端自动退出。 代码示例123456789redis&gt; PINGPONGredis&gt; SHUTDOWN$$ redisCould not connect to Redis at: Connection refusednot connected&gt; TIMETIME可用版本： &gt;= 2.6.0 时间复杂度： O(1) 返回当前服务器时间。 返回值一个包含两个字符串的列表： 第一个字符串是当前时间(以 UNIX 时间戳格式表示)，而第二个字符串是当前这一秒钟已经逝去的微秒数。 代码示例1234567redis&gt; TIME1) "1332395997"2) "952581"redis&gt; TIME1) "1332395997"2) "953148" CLIENTCLIENT GETNAME可用版本： &gt;= 2.6.9 时间复杂度： O(1) 返回 CLIENT SETNAME 命令为连接设置的名字。 因为新创建的连接默认是没有名字的， 对于没有名字的连接， CLIENT GETNAME 返回空白回复。 返回值如果连接没有设置名字，那么返回空白回复； 如果有设置名字，那么返回名字。 代码示例1234567891011# 新连接默认没有名字redis 127.0.0.1:6379&gt; CLIENT GETNAME(nil)# 设置名字redis 127.0.0.1:6379&gt; CLIENT SETNAME hello-world-connectionOK# 返回名字redis 127.0.0.1:6379&gt; CLIENT GETNAME"hello-world-connection" CLIENT KILL ip:port可用版本： &gt;= 2.4.0 时间复杂度： O(N) ， N 为已连接的客户端数量。 关闭地址为 ip:port 的客户端。 ip:port 应该和 CLIENT LIST 命令输出的其中一行匹配。 因为 Redis 使用单线程设计，所以当 Redis 正在执行命令的时候，不会有客户端被断开连接。 如果要被断开连接的客户端正在执行命令，那么当这个命令执行之后，在发送下一个命令的时候，它就会收到一个网络错误，告知它自身的连接已被关闭。 返回值当指定的客户端存在，且被成功关闭时，返回 OK 。 代码示例123456789101112# 列出所有已连接客户端redis 127.0.0.1:6379&gt; CLIENT LISTaddr=127.0.0.1:43501 fd=5 age=10 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client# 杀死当前客户端的连接redis 127.0.0.1:6379&gt; CLIENT KILL 127.0.0.1:43501OK# 之前的连接已经被关闭，CLI 客户端又重新建立了连接# 之前的端口是 43501 ，现在是 43504redis 127.0.0.1:6379&gt; CLIENT LISTaddr=127.0.0.1:43504 fd=5 age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client CLIENT LIST可用版本： &gt;= 2.4.0 时间复杂度： O(N) ， N 为连接到服务器的客户端数量。 以人类可读的格式，返回所有连接到服务器的客户端信息和统计数据。1234redis&gt; CLIENT LISTaddr=127.0.0.1:43143 fd=6 age=183 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=clientaddr=127.0.0.1:43163 fd=5 age=35 idle=15 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=pingaddr=127.0.0.1:43167 fd=7 age=24 idle=6 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=get 返回值命令返回多行字符串，这些字符串按以下形式被格式化： 每个已连接客户端对应一行（以 LF 分割） 每行字符串由一系列 属性=值 形式的域组成，每个域之间以空格分开 以下是域的含义： addr ： 客户端的地址和端口 fd ： 套接字所使用的文件描述符 age ： 以秒计算的已连接时长 idle ： 以秒计算的空闲时长 flags ： 客户端 flag （见下文） db ： 该客户端正在使用的数据库 ID sub ： 已订阅频道的数量 psub ： 已订阅模式的数量 multi ： 在事务中被执行的命令数量 qbuf ： 查询缓冲区的长度（字节为单位， 0 表示没有分配查询缓冲区） qbuf-free ： 查询缓冲区剩余空间的长度（字节为单位， 0 表示没有剩余空间） obl ： 输出缓冲区的长度（字节为单位， 0 表示没有分配输出缓冲区） oll ： 输出列表包含的对象数量（当输出缓冲区没有剩余空间时，命令回复会以字符串对象的形式被入队到这个队列里） omem ： 输出缓冲区和输出列表占用的内存总量 events ： 文件描述符事件（见下文） cmd ： 最近一次执行的命令 客户端 flag 可以由以下部分组成： O ： 客户端是 MONITOR 模式下的附属节点（slave） S ： 客户端是一般模式下（normal）的附属节点 M ： 客户端是主节点（master） x ： 客户端正在执行事务 b ： 客户端正在等待阻塞事件 i ： 客户端正在等待 VM I/O 操作（已废弃） d ： 一个受监视（watched）的键已被修改， EXEC 命令将失败 c : 在将回复完整地写出之后，关闭链接 u : 客户端未被阻塞（unblocked） A : 尽可能快地关闭连接 N : 未设置任何 flag 文件描述符事件可以是： r : 客户端套接字（在事件 loop 中）是可读的（readable） w : 客户端套接字（在事件 loop 中）是可写的（writeable）Note 为了 debug 的需要，经常会对域进行添加和删除，一个安全的 Redis 客户端应该可以对 CLIENT LIST 的输出进行相应的处理（parse），比如忽略不存在的域，跳过未知域，诸如此类。 CLIENT SETNAME connection-name可用版本： &gt;= 2.6.9 时间复杂度： O(1) 为当前连接分配一个名字。 这个名字会显示在 CLIENT LIST 命令的结果中， 用于识别当前正在与服务器进行连接的客户端。 举个例子， 在使用 Redis 构建队列（queue）时， 可以根据连接负责的任务（role）， 为信息生产者（producer）和信息消费者（consumer）分别设置不同的名字。 名字使用 Redis 的字符串类型来保存， 最大可以占用 512 MB 。 另外， 为了避免和 CLIENT LIST 命令的输出格式发生冲突， 名字里不允许使用空格。 要移除一个连接的名字， 可以将连接的名字设为空字符串 “” 。 使用 CLIENT GETNAME 命令可以取出连接的名字。 新创建的连接默认是没有名字的。 Tip在 Redis 应用程序发生连接泄漏时，为连接设置名字是一种很好的 debug 手段。 返回值设置成功时返回 OK 。 代码示例1234567891011121314151617181920212223242526272829# 新连接默认没有名字redis 127.0.0.1:6379&gt; CLIENT GETNAME(nil)# 设置名字redis 127.0.0.1:6379&gt; CLIENT SETNAME hello-world-connectionOK# 返回名字redis 127.0.0.1:6379&gt; CLIENT GETNAME"hello-world-connection"# 在客户端列表中查看redis 127.0.0.1:6379&gt; CLIENT LISTaddr=127.0.0.1:36851fd=5name=hello-world-connection # &lt;- 名字age=51...# 清除名字redis 127.0.0.1:6379&gt; CLIENT SETNAME # 只用空格是不行的！(error) ERR Syntax error, try CLIENT (LIST | KILL ip:port)redis 127.0.0.1:6379&gt; CLIENT SETNAME "" # 必须双引号显示包围OKredis 127.0.0.1:6379&gt; CLIENT GETNAME # 清除完毕(nil) 配置选项CONFIG SETCONFIG SET parameter value可用版本： &gt;= 2.0.0 时间复杂度：O(1) CONFIG SET 命令可以动态地调整 Redis 服务器的配置(configuration)而无须重启。 你可以使用它修改配置参数，或者改变 Redis 的持久化(Persistence)方式。 CONFIG SET 可以修改的配置参数可以使用命令 CONFIG GET * 来列出，所有被 CONFIG SET 修改的配置参数都会立即生效。 关于 CONFIG SET 命令的更多消息，请参见命令 CONFIG GET 的说明。 关于如何使用 CONFIG SET 命令修改 Redis 持久化方式，请参见 Redis Persistence 。 返回值当设置成功时返回 OK ，否则返回一个错误。 代码示例12345678910redis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "1024"redis&gt; CONFIG SET slowlog-max-len 10086OKredis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "10086" CONFIG GETCONFIG GET parameter可用版本： &gt;= 2.0.0 时间复杂度： O(N)，其中 N 为命令返回的配置选项数量。 CONFIG GET 命令用于取得运行中的 Redis 服务器的配置参数(configuration parameters)，在 Redis 2.4 版本中， 有部分参数没有办法用 CONFIG GET 访问，但是在最新的 Redis 2.6 版本中，所有配置参数都已经可以用 CONFIG GET 访问了。 CONFIG GET 接受单个参数 parameter 作为搜索关键字，查找所有匹配的配置参数，其中参数和值以“键-值对”(key-value pairs)的方式排列。 比如执行 CONFIG GET s* 命令，服务器就会返回所有以 s 开头的配置参数及参数的值： 1234567891011redis&gt; CONFIG GET s*1) "save" # 参数名：save2) "900 1 300 10 60 10000" # save 参数的值3) "slave-serve-stale-data" # 参数名： slave-serve-stale-data4) "yes" # slave-serve-stale-data 参数的值5) "set-max-intset-entries" # ...6) "512"7) "slowlog-log-slower-than"8) "1000"9) "slowlog-max-len"10) "1000" 如果你只是寻找特定的某个参数的话，你当然也可以直接指定参数的名字： 123redis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "1000" 使用命令 CONFIG GET * ，可以列出 CONFIG GET 命令支持的所有参数：12345678910111213141516171819202122redis&gt; CONFIG GET *1) "dir"2) "/var/lib/redis"3) "dbfilename"4) "dump.rdb"5) "requirepass"6) (nil)7) "masterauth"8) (nil)9) "maxmemory"10) "0"11) "maxmemory-policy"12) "volatile-lru"13) "maxmemory-samples"14) "3"15) "timeout"16) "0"17) "appendonly"18) "no"# ...49) "loglevel"50) "verbose" 所有被 CONFIG SET 所支持的配置参数都可以在配置文件 redis.conf 中找到，不过 CONFIG GET 和 CONFIG SET 使用的格式和 redis.conf 文件所使用的格式有以下两点不同： 10kb 、 2gb 这些在配置文件中所使用的储存单位缩写，不可以用在 CONFIG 命令中， CONFIG SET 的值只能通过数字值显式地设定。 像 CONFIG SET xxx 1k 这样的命令是错误的，正确的格式是 CONFIG SET xxx 1000 。save 选项在 redis.conf 中是用多行文字储存的，但在 CONFIG GET 命令中，它只打印一行文字。 以下是 save 选项在 redis.conf 文件中的表示：123save 900 1save 300 10save 60 10000 但是 CONFIG GET 命令的输出只有一行：123redis&gt; CONFIG GET save1) "save"2) "900 1 300 10 60 10000" 上面 save 参数的三个值表示：在 900 秒内最少有 1 个 key 被改动，或者 300 秒内最少有 10 个 key 被改动，又或者 60 秒内最少有 1000 个 key 被改动，以上三个条件随便满足一个，就触发一次保存操作。 返回值给定配置参数的值。 CONFIG RESETSTATCONFIG RESETSTAT可用版本： &gt;= 2.0.0 时间复杂度： O(1) 重置 INFO 命令中的某些统计数据，包括： Keyspace hits (键空间命中次数) Keyspace misses (键空间不命中次数) Number of commands processed (执行命令的次数) Number of connections received (连接服务器的次数) Number of expired keys (过期key的数量) Number of rejected connections (被拒绝的连接数量) Latest fork(2) time(最后执行 fork(2) 的时间) The aof_delayed_fsync counter(aof_delayed_fsync 计数器的值)返回值总是返回 OK 。 代码示例重置前1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465redis 127.0.0.1:6379&gt; INFO# Serverredis_version:2.5.3redis_git_sha1:d0407c2dredis_git_dirty:0arch_bits:32multiplexing_api:epollgcc_version:4.6.3process_id:11095run_id:ef1f6b6c7392e52d6001eaf777acbe547d1192e2tcp_port:6379uptime_in_seconds:6uptime_in_days:0lru_clock:1205426# Clientsconnected_clients:1client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0# Memoryused_memory:331076used_memory_human:323.32Kused_memory_rss:1568768used_memory_peak:293424used_memory_peak_human:286.55Kused_memory_lua:16384mem_fragmentation_ratio:4.74mem_allocator:jemalloc-2.2.5# Persistenceloading:0aof_enabled:0changes_since_last_save:0bgsave_in_progress:0last_save_time:1333260015last_bgsave_status:okbgrewriteaof_in_progress:0# Statstotal_connections_received:1total_commands_processed:0instantaneous_ops_per_sec:0rejected_connections:0expired_keys:0evicted_keys:0keyspace_hits:0keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:0# Replicationrole:masterconnected_slaves:0# CPUused_cpu_sys:0.01used_cpu_user:0.00used_cpu_sys_children:0.00used_cpu_user_children:0.00# Keyspacedb0:keys=20,expires=0 重置12redis 127.0.0.1:6379&gt; CONFIG RESETSTATOK 重置后1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465redis 127.0.0.1:6379&gt; INFO# Serverredis_version:2.5.3redis_git_sha1:d0407c2dredis_git_dirty:0arch_bits:32multiplexing_api:epollgcc_version:4.6.3process_id:11095run_id:ef1f6b6c7392e52d6001eaf777acbe547d1192e2tcp_port:6379uptime_in_seconds:134uptime_in_days:0lru_clock:1205438# Clientsconnected_clients:1client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0# Memoryused_memory:331076used_memory_human:323.32Kused_memory_rss:1568768used_memory_peak:330280used_memory_peak_human:322.54Kused_memory_lua:16384mem_fragmentation_ratio:4.74mem_allocator:jemalloc-2.2.5# Persistenceloading:0aof_enabled:0changes_since_last_save:0bgsave_in_progress:0last_save_time:1333260015last_bgsave_status:okbgrewriteaof_in_progress:0# Statstotal_connections_received:0total_commands_processed:1instantaneous_ops_per_sec:0rejected_connections:0expired_keys:0evicted_keys:0keyspace_hits:0keyspace_misses:0pubsub_channels:0pubsub_patterns:0latest_fork_usec:0# Replicationrole:masterconnected_slaves:0# CPUused_cpu_sys:0.05used_cpu_user:0.02used_cpu_sys_children:0.00used_cpu_user_children:0.00# Keyspacedb0:keys=20,expires=0 CONFIG REWRITECONFIG REWRITE可用版本： &gt;= 2.8.0 时间复杂度：O(N)，其中 N 为被重写的配置选项数量。 CONFIG REWRITE 命令对启动 Redis 服务器时所指定的 redis.conf 文件进行改写： 因为 CONFIG_SET 命令可以对服务器的当前配置进行修改， 而修改后的配置可能和 redis.conf 文件中所描述的配置不一样， CONFIG REWRITE 的作用就是通过尽可能少的修改， 将服务器当前所使用的配置记录到 redis.conf 文件中。 重写会以非常保守的方式进行： 原有 redis.conf 文件的整体结构和注释会被尽可能地保留。 如果一个选项已经存在于原有 redis.conf 文件中 ， 那么对该选项的重写会在选项原本所在的位置（行号）上进行。 如果一个选项不存在于原有 redis.conf 文件中， 并且该选项被设置为默认值， 那么重写程序不会将这个选项添加到重写后的 redis.conf 文件中。 如果一个选项不存在于原有 redis.conf 文件中， 并且该选项被设置为非默认值， 那么这个选项将被添加到重写后的 redis.conf 文件的末尾。 未使用的行会被留白。 比如说， 如果你在原有 redis.conf 文件上设置了数个关于 save 选项的参数， 但现在你将这些 save 参数的一个或全部都关闭了， 那么这些不再使用的参数原本所在的行就会变成空白的。 即使启动服务器时所指定的 redis.conf 文件已经不再存在， CONFIG REWRITE 命令也可以重新构建并生成出一个新的 redis.conf 文件。 另一方面， 如果启动服务器时没有载入 redis.conf 文件， 那么执行 CONFIG REWRITE 命令将引发一个错误。 原子性重写对 redis.conf 文件的重写是原子性的， 并且是一致的： 如果重写出错或重写期间服务器崩溃， 那么重写失败， 原有 redis.conf 文件不会被修改。 如果重写成功， 那么 redis.conf 文件为重写后的新文件。 返回值一个状态值：如果配置重写成功则返回 OK ，失败则返回一个错误。 代码示例以下是执行 CONFIG REWRITE 前， 被载入到 Redis 服务器的 redis.conf 文件中关于 appendonly 选项的设置：1234567891011121314151617181920212223# ... 其他选项appendonly no# ... 其他选项在执行以下命令之后：redis&gt; CONFIG GET appendonly # appendonly 处于关闭状态1) "appendonly"2) "no"redis&gt; CONFIG SET appendonly yes # 打开 appendonlyOKredis&gt; CONFIG GET appendonly1) "appendonly"2) "yes"redis&gt; CONFIG REWRITE # 将 appendonly 的修改写入到 redis.conf 中OK重写后的 redis.conf 文件中的 appendonly 选项将被改写：# ... 其他选项appendonly yes# ... 其他选项 调试PINGPING可用版本： &gt;= 1.0.0 时间复杂度： O(1) 使用客户端向 Redis 服务器发送一个 PING ，如果服务器运作正常的话，会返回一个 PONG 。 通常用于测试与服务器的连接是否仍然生效，或者用于测量延迟值。 返回值如果连接正常就返回一个 PONG ，否则返回一个连接错误。 代码示例1234567# 客户端和服务器连接正常redis&gt; PINGPONG# 客户端和服务器连接不正常(网络不正常或服务器未能正常运行)redis 127.0.0.1:6379&gt; PINGCould not connect to Redis at 127.0.0.1:6379: Connection refused ECHOECHO message可用版本： &gt;= 1.0.0 时间复杂度： O(1) 打印一个特定的信息 message ，测试时使用。 返回值message 自身。 代码示例12345redis&gt; ECHO "Hello Moto""Hello Moto"redis&gt; ECHO "Goodbye Moto""Goodbye Moto" OBJECTOBJECT subcommand [arguments [arguments]]可用版本： &gt;= 2.2.3 时间复杂度： O(1) OBJECT 命令允许从内部察看给定 key 的 Redis 对象， 它通常用在除错(debugging)或者了解为了节省空间而对 key 使用特殊编码的情况。 当将Redis用作缓存程序时，你也可以通过 OBJECT 命令中的信息，决定 key 的驱逐策略(eviction policies)。 OBJECT 命令有多个子命令： OBJECT REFCOUNT \&lt;key> 返回给定 key 引用所储存的值的次数。此命令主要用于除错。 OBJECT ENCODING \&lt;key> 返回给定 key 锁储存的值所使用的内部表示(representation)。 OBJECT IDLETIME \&lt;key> 返回给定 key 自储存以来的空闲时间(idle， 没有被读取也没有被写入)，以秒为单位。对象可以以多种方式编码： 字符串可以被编码为 raw (一般字符串)或 int (为了节约内存，Redis 会将字符串表示的 64 位有符号整数编码为整数来进行储存）。 列表可以被编码为 ziplist 或 linkedlist 。 ziplist 是为节约大小较小的列表空间而作的特殊表示。 集合可以被编码为 intset 或者 hashtable 。 intset 是只储存数字的小集合的特殊表示。 哈希表可以编码为 zipmap 或者 hashtable 。 zipmap 是小哈希表的特殊表示。 有序集合可以被编码为 ziplist 或者 skiplist 格式。 ziplist 用于表示小的有序集合，而 skiplist 则用于表示任何大小的有序集合。 假如你做了什么让 Redis 没办法再使用节省空间的编码时(比如将一个只有 1 个元素的集合扩展为一个有 100 万个元素的集合)，特殊编码类型(specially encoded types)会自动转换成通用类型(general type)。 返回值REFCOUNT 和 IDLETIME 返回数字。 ENCODING 返回相应的编码类型。 代码示例1234567891011121314151617181920212223242526272829redis&gt; SET game "COD" # 设置一个字符串OKredis&gt; OBJECT REFCOUNT game # 只有一个引用(integer) 1redis&gt; OBJECT IDLETIME game # 等待一阵。。。然后查看空闲时间(integer) 90redis&gt; GET game # 提取game， 让它处于活跃(active)状态"COD"redis&gt; OBJECT IDLETIME game # 不再处于空闲状态(integer) 0redis&gt; OBJECT ENCODING game # 字符串的编码方式"raw"redis&gt; SET big-number 23102930128301091820391092019203810281029831092 # 非常长的数字会被编码为字符串OKredis&gt; OBJECT ENCODING big-number"raw"redis&gt; SET small-number 12345 # 而短的数字则会被编码为整数OKredis&gt; OBJECT ENCODING small-number"int" SLOWLOGSLOWLOG subcommand [argument]可用版本： &gt;= 2.2.12 时间复杂度： O(1) 什么是 SLOWLOGSlow log 是 Redis 用来记录查询执行时间的日志系统。 查询执行时间指的是不包括像客户端响应(talking)、发送回复等 IO 操作，而单单是执行一个查询命令所耗费的时间。 另外，slow log 保存在内存里面，读写速度非常快，因此你可以放心地使用它，不必担心因为开启 slow log 而损害 Redis 的速度。 设置 SLOWLOGSlow log 的行为由两个配置参数(configuration parameter)指定，可以通过改写 redis.conf 文件或者用 CONFIG GET 和 CONFIG SET 命令对它们动态地进行修改。 第一个选项是 slowlog-log-slower-than ，它决定要对执行时间大于多少微秒(microsecond，1秒 = 1,000,000 微秒)的查询进行记录。 比如执行以下命令将让 slow log 记录所有查询时间大于等于 100 微秒的查询：1CONFIG SET slowlog-log-slower-than 100 而以下命令记录所有查询时间大于 1000 微秒的查询：1CONFIG SET slowlog-log-slower-than 1000 另一个选项是 slowlog-max-len ，它决定 slow log 最多能保存多少条日志， slow log 本身是一个 FIFO 队列，当队列大小超过 slowlog-max-len 时，最旧的一条日志将被删除，而最新的一条日志加入到 slow log ，以此类推。 以下命令让 slow log 最多保存 1000 条日志：1CONFIG SET slowlog-max-len 1000 使用 CONFIG GET 命令可以查询两个选项的当前值：1234567redis&gt; CONFIG GET slowlog-log-slower-than1) "slowlog-log-slower-than"2) "1000"redis&gt; CONFIG GET slowlog-max-len1) "slowlog-max-len"2) "1000" 查看 slow log要查看 slow log ，可以使用 SLOWLOG GET 或者 SLOWLOG GET number 命令，前者打印所有 slow log ，最大长度取决于 slowlog-max-len 选项的值，而 SLOWLOG GET number 则只打印指定数量的日志。 最新的日志会最先被打印：12345678910111213141516171819202122232425# 为测试需要，将 slowlog-log-slower-than 设成了 10 微秒redis&gt; SLOWLOG GET1) 1) (integer) 12 # 唯一性(unique)的日志标识符 2) (integer) 1324097834 # 被记录命令的执行时间点，以 UNIX 时间戳格式表示 3) (integer) 16 # 查询执行时间，以微秒为单位 4) 1) "CONFIG" # 执行的命令，以数组的形式排列 2) "GET" # 这里完整的命令是 CONFIG GET slowlog-log-slower-than 3) "slowlog-log-slower-than"2) 1) (integer) 11 2) (integer) 1324097825 3) (integer) 42 4) 1) "CONFIG" 2) "GET" 3) "*"3) 1) (integer) 10 2) (integer) 1324097820 3) (integer) 11 4) 1) "CONFIG" 2) "GET" 3) "slowlog-log-slower-than"# ... 日志的唯一 id 只有在 Redis 服务器重启的时候才会重置，这样可以避免对日志的重复处理(比如你可能会想在每次发现新的慢查询时发邮件通知你)。 查看当前日志的数量使用命令 SLOWLOG LEN 可以查看当前日志的数量。 请注意这个值和 slower-max-len 的区别，它们一个是当前日志的数量，一个是允许记录的最大日志的数量。12redis&gt; SLOWLOG LEN(integer) 14 清空日志使用命令 SLOWLOG RESET 可以清空 slow log 。12345678redis&gt; SLOWLOG LEN(integer) 14redis&gt; SLOWLOG RESETOKredis&gt; SLOWLOG LEN(integer) 0 返回值取决于不同命令，返回不同的值。 MONITORMONITOR可用版本： &gt;= 1.0.0 时间复杂度： O(N) 实时打印出 Redis 服务器接收到的命令，调试用。 返回值总是返回 OK 。 代码示例12345678910111213127.0.0.1:6379&gt; MONITOROK# 以第一个打印值为例# 1378822099.421623 是时间戳# [0 127.0.0.1:56604] 中的 0 是数据库号码， 127... 是 IP 地址和端口# "PING" 是被执行的命令1378822099.421623 [0 127.0.0.1:56604] "PING"1378822105.089572 [0 127.0.0.1:56604] "SET" "msg" "hello world"1378822109.036925 [0 127.0.0.1:56604] "SET" "number" "123"1378822140.649496 [0 127.0.0.1:56604] "SADD" "fruits" "Apple" "Banana" "Cherry"1378822154.117160 [0 127.0.0.1:56604] "EXPIRE" "msg" "10086"1378822257.329412 [0 127.0.0.1:56604] "KEYS" "*"1378822258.690131 [0 127.0.0.1:56604] "DBSIZE" DEBUG OBJECTDEBUG OBJECT key可用版本： &gt;= 1.0.0 时间复杂度： O(1) DEBUG OBJECT 是一个调试命令，它不应被客户端所使用。 查看 OBJECT 命令获取更多信息。 返回值当 key 存在时，返回有关信息。 当 key 不存在时，返回一个错误。 代码示例12345redis&gt; DEBUG OBJECT my_pcValue at:0xb6838d20 refcount:1 encoding:raw serializedlength:9 lru:283790 lru_seconds_idle:150redis&gt; DEBUG OBJECT your_mac(error) ERR no such key DEBUG SEGFAULTDEBUG SEGFAULT可用版本： &gt;= 1.0.0 时间复杂度： O(1) 执行一个不合法的内存访问从而让 Redis 崩溃，仅在开发时用于 BUG 模拟。 返回值无 代码示例1234redis&gt; DEBUG SEGFAULTCould not connect to Redis at: Connection refusednot connected&gt; 内部命令MIGRATEMIGRATE host port key destination-db timeout [COPY] [REPLACE]可用版本： &gt;= 2.6.0 时间复杂度： 这个命令在源实例上实际执行 DUMP 命令和 DEL 命令，在目标实例执行 RESTORE 命令，查看以上命令的文档可以看到详细的复杂度说明。 key 数据在两个实例之间传输的复杂度为 O(N) 。 将 key 原子性地从当前实例传送到目标实例的指定数据库上，一旦传送成功， key 保证会出现在目标实例上，而当前实例上的 key 会被删除。 这个命令是一个原子操作，它在执行的时候会阻塞进行迁移的两个实例，直到以下任意结果发生：迁移成功，迁移失败，等待超时。 命令的内部实现是这样的：它在当前实例对给定 key 执行 DUMP 命令 ，将它序列化，然后传送到目标实例，目标实例再使用 RESTORE 对数据进行反序列化，并将反序列化所得的数据添加到数据库中；当前实例就像目标实例的客户端那样，只要看到 RESTORE 命令返回 OK ，它就会调用 DEL 删除自己数据库上的 key 。 timeout 参数以毫秒为格式，指定当前实例和目标实例进行沟通的最大间隔时间。这说明操作并不一定要在 timeout 毫秒内完成，只是说数据传送的时间不能超过这个 timeout 数。 MIGRATE 命令需要在给定的时间规定内完成 IO 操作。如果在传送数据时发生 IO 错误，或者达到了超时时间，那么命令会停止执行，并返回一个特殊的错误： IOERR 。 当 IOERR 出现时，有以下两种可能： key 可能存在于两个实例 key 可能只存在于当前实例 唯一不可能发生的情况就是丢失 key ，因此，如果一个客户端执行 MIGRATE 命令，并且不幸遇上 IOERR 错误，那么这个客户端唯一要做的就是检查自己数据库上的 key 是否已经被正确地删除。 如果有其他错误发生，那么 MIGRATE 保证 key 只会出现在当前实例中。（当然，目标实例的给定数据库上可能有和 key 同名的键，不过这和 MIGRATE 命令没有关系）。 可选项COPY ：不移除源实例上的 key 。 REPLACE ：替换目标实例上已存在的 key 。 返回值迁移成功时返回 OK ，否则返回相应的错误。 代码示例先启动两个 Redis 实例，一个使用默认的 6379 端口，一个使用 7777 端口。123456789$ ./redis-server &amp;[1] 3557...$ ./redis-server --port 7777 &amp;[2] 3560... 然后用客户端连上 6379 端口的实例，设置一个键，然后将它迁移到 7777 端口的实例上：12345678910111213$ ./redis-cliredis 127.0.0.1:6379&gt; flushdbOKredis 127.0.0.1:6379&gt; SET greeting "Hello from 6379 instance"OKredis 127.0.0.1:6379&gt; MIGRATE 127.0.0.1 7777 greeting 0 1000OKredis 127.0.0.1:6379&gt; EXISTS greeting # 迁移成功后 key 被删除(integer) 0 使用另一个客户端，查看 7777 端口上的实例：1234$ ./redis-cli -p 7777redis 127.0.0.1:7777&gt; GET greeting"Hello from 6379 instance" DUMPDUMP key可用版本： &gt;= 2.6.0 时间复杂度：查找给定键的复杂度为 O(1) ，对键进行序列化的复杂度为 O(N*M) ，其中 N 是构成 key 的 Redis 对象的数量，而 M 则是这些对象的平均大小。如果序列化的对象是比较小的字符串，那么复杂度为 O(1) 。 序列化给定 key ，并返回被序列化的值，使用 RESTORE 命令可以将这个值反序列化为 Redis 键。 序列化生成的值有以下几个特点： 它带有 64 位的校验和，用于检测错误， RESTORE 在进行反序列化之前会先检查校验和。 值的编码格式和 RDB 文件保持一致。 RDB 版本会被编码在序列化值当中，如果因为 Redis 的版本不同造成 RDB 格式不兼容，那么 Redis 会拒绝对这个值进行反序列化操作。 序列化的值不包括任何生存时间信息。 返回值如果 key 不存在，那么返回 nil 。 否则，返回序列化之后的值。 代码示例12345678redis&gt; SET greeting "hello, dumping world!"OKredis&gt; DUMP greeting"\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"redis&gt; DUMP not-exists-key(nil) RESTORERESTORE key ttl serialized-value [REPLACE]可用版本： &gt;= 2.6.0 时间复杂度： 查找给定键的复杂度为 O(1) ，对键进行反序列化的复杂度为 O(N*M) ，其中 N 是构成 key 的 Redis 对象的数量，而 M 则是这些对象的平均大小。 有序集合(sorted set)的反序列化复杂度为 O(N*M*log(N)) ，因为有序集合每次插入的复杂度为 O(log(N)) 。 如果反序列化的对象是比较小的字符串，那么复杂度为 O(1) 。 反序列化给定的序列化值，并将它和给定的 key 关联。 参数 ttl 以毫秒为单位为 key 设置生存时间；如果 ttl 为 0 ，那么不设置生存时间。 RESTORE 在执行反序列化之前会先对序列化值的 RDB 版本和数据校验和进行检查，如果 RDB 版本不相同或者数据不完整的话，那么 RESTORE 会拒绝进行反序列化，并返回一个错误。 如果键 key 已经存在， 并且给定了 REPLACE 选项， 那么使用反序列化得出的值来代替键 key 原有的值； 相反地， 如果键 key 已经存在， 但是没有给定 REPLACE 选项， 那么命令返回一个错误。 更多信息可以参考 DUMP 命令。 返回值如果反序列化成功那么返回 OK ，否则返回一个错误。 代码示例12345678910111213141516171819202122232425# 创建一个键，作为 DUMP 命令的输入redis&gt; SET greeting "hello, dumping world!"OKredis&gt; DUMP greeting"\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"# 将序列化数据 RESTORE 到另一个键上面redis&gt; RESTORE greeting-again 0 "\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"OKredis&gt; GET greeting-again"hello, dumping world!"# 在没有给定 REPLACE 选项的情况下，再次尝试反序列化到同一个键，失败redis&gt; RESTORE greeting-again 0 "\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde"(error) ERR Target key name is busy.# 给定 REPLACE 选项，对同一个键进行反序列化成功redis&gt; RESTORE greeting-again 0 "\x00\x15hello, dumping world!\x06\x00E\xa0Z\x82\xd8r\xc1\xde" REPLACEOK# 尝试使用无效的值进行反序列化，出错redis&gt; RESTORE fake-message 0 "hello moto moto blah blah"(error) ERR DUMP payload version or checksum are wrong SYNCSYNC可用版本： &gt;= 1.0.0 时间复杂度： O(N) 用于复制功能(replication)的内部命令。 更多信息请参考 Redis 官网的 Replication 章节 。 返回值序列化数据。 代码示例123redis&gt; SYNC"REDIS0002\xfe\x00\x00\auser_id\xc0\x03\x00\anumbers\xc2\xf3\xe0\x01\x00\x00\tdb_number\xc0\x00\x00\x04name\x06huangz\x00\anew_key\nhello_moto\x00\bgreeting\nhello moto\x00\x05my_pc\bthinkpad\x00\x04lock\xc0\x01\x00\nlock_times\xc0\x04\xfe\x01\t\x04info\x19\x02\x04name\b\x00zhangyue\x03age\x02\x0022\xff\t\aooredis,\x03\x04name\a\x00ooredis\aversion\x03\x001.0\x06author\x06\x00huangz\xff\x00\tdb_number\xc0\x01\x00\x05greet\x0bhello world\x02\nmy_friends\x02\x05marry\x04jack\x00\x04name\x05value\xfe\x02\x0c\x01s\x12\x12\x00\x00\x00\r\x00\x00\x00\x02\x00\x00\x01a\x03\xc0f'\xff\xff"(1.90s) PSYNCPSYNC master_run_id offset可用版本： &gt;= 2.8.0 时间复杂度： 不明确 用于复制功能(replication)的内部命令。 更多信息请参考 复制（Replication） 文档。 返回值序列化数据。 代码示例12127.0.0.1:6379&gt; PSYNC ? -1"REDIS0006\xfe\x00\x00\x02kk\x02vv\x00\x03msg\x05hello\xff\xc3\x96P\x12h\bK\xef" 转载出处Redis命令参考]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Java发送邮件]]></title>
    <url>%2Fblog%2F%E4%BD%BF%E7%94%A8Java%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6.html</url>
    <content type="text"><![CDATA[前言在使用Java开发项目过程中我们通常会遇到发送邮件的需求，那么系统中是如何使用Java发送邮件的呢？今天我们来了解下使用Java发送邮件的两种方式。 它们分别是使用 apache-commons-email 包来发送邮件、使用javax.mail 包来发送邮件。 我们分别对它们进行了解。 正文JavaMail我们来看下使用JavaMail发送一封邮件的Java代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * @author zwt * @detail 使用JavaxMail 发送邮件 * @date 2019/1/23 * @since 1.0 */public class SendMail&#123; //SMTP主机设置 private static final String MAIL_HOST_NAME = "smtp.qq.com"; //发送邮件的邮箱用户名 private static final String MAIL_SYSTEM_USER = "test@qq.com"; //发送邮件的邮箱密码 private static final String MAIL_SYSTEM_PASSWORD = "SMTP授权码"; /** * 发送邮件 * @param subject * @param receivers * @param copys * @param msg * @param attachName * @return */ public static boolean sendMail(String subject, List&lt;String&gt; receivers, List&lt;String&gt; copys, String msg, String attachName)&#123; if (CollectionUtils.isEmpty(receivers) || (subject == null) || (msg == null)) &#123; throw new SendMailException("Email receivers or subject or content must be not null"); &#125; boolean isSuccess = false; try &#123; // 获取系统属性 Properties properties = System.getProperties(); // 设置邮件服务器 properties.setProperty("mail.smtp.host", MAIL_HOST_NAME); //传输协议SMTP properties.setProperty("mail.transport.protocol", "smtp"); //开启SMTP认证 properties.setProperty("mail.smtp.auth", "true"); //SMTP端口号 properties.setProperty("mail.smtp.port","465"); //开启ssl properties.setProperty("mail.smtp.ssl.enable","true"); //用户名密码认证 properties.setProperty("mail.user",MAIL_SYSTEM_USER ); properties.setProperty("mail.password", MAIL_SYSTEM_PASSWORD); // 获取默认的 Session 对象。 Session session = Session.getDefaultInstance(properties); // 创建默认的 MimeMessage 对象。 MimeMessage message = new MimeMessage(session); // 发信人 message.setFrom(new InternetAddress(MAIL_SYSTEM_USER)); // 收信人 for(String receiver:receivers)&#123; message.addRecipient(Message.RecipientType.TO, new InternetAddress(receiver)); &#125; //抄送 if(!CollectionUtils.isEmpty(copys))&#123; for(String copy:copys)&#123; message.addRecipient(Message.RecipientType.CC, new InternetAddress(copy)); &#125; &#125; //邮件主题 message.setSubject(subject); //有附件创建附件发送 if(!StringUtils.isEmpty(attachName))&#123; // 创建消息部分 BodyPart messageBodyPart = new MimeBodyPart(); // 消息 messageBodyPart.setText(msg); // 创建多重消息 Multipart multipart = new MimeMultipart(); // 设置文本消息部分 multipart.addBodyPart(messageBodyPart); // 附件部分 messageBodyPart = new MimeBodyPart(); DataSource source = new FileDataSource(attachName); messageBodyPart.setDataHandler(new DataHandler(source)); messageBodyPart.setFileName("附件"); multipart.addBodyPart(messageBodyPart); // 发送完整消息 message.setContent(multipart); //setContent方法可以插入html标签生成Html邮件 &#125;else&#123; message.setText(msg); &#125; // 发送消息 Transport.send(message); isSuccess = true; &#125;catch (MessagingException e)&#123; e.printStackTrace(); throw new SendMailException(e.getMessage()); &#125; return isSuccess; &#125; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("test1@qq.com"); sendMail("test",list,null,"测试一下",null); &#125;&#125; 可以看到，要发送一封邮件，Java的处理主要有以下几步： ①系统参数及属性设置，包括使用的协议（Pop3、SMTP等），是否开启ssl加密、发送邮件邮箱的用户名和密码等 ②设置邮件属性，比如主题、内容、附件、接收人、抄送人等等 ③发送邮件 我这儿使用的是QQ邮箱，要注意使用SMTP传输协议时，要开启SMTP，使用SSL，邮箱的认证密码不是邮箱密码，而是授权码，需要在QQ邮箱进行设置，如下： PS：在发送邮件时，如果想插入图片，可以设置成Html的内联图片，然后通过message.setContent添加这个Html文档。还有一些其它的功能（比如给某个人发送回执等）都可以通过设置实现，有兴趣的可以看下。 整个发送邮件的代码基本如上，还是比较易于理解的，我们不再对此做过多介绍。 apache-commons-email再来看看Apache提供的apache-commons-email这个包。 这个包就是对JavaMail进行了一层封装，我们可以看到这个工具包本质还是使用了JavaMail去发送邮件。 我们来看一个使用apache-commons-email包去发送邮件的例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class SendMail &#123; //SMTP主机设置 private static final String MAIL_HOST_NAME = "smtp.qq.com"; //发送邮件的邮箱用户名 private static final String MAIL_SYSTEM_USER = "test@qq.com"; //发送邮件的邮箱密码 private static final String MAIL_SYSTEM_PASSWORD = "SMTP授权码"; /** * 发送邮件 * @param subject 邮件主题 * @param receivers 接受者邮件地址 * @param copys 要抄送的邮件地址 * @param msg 内容 * @param attachName 附件地址 * @return * @throws Exception */ public static boolean sendMail(String subject, List&lt;String&gt; receivers,List&lt;String&gt; copys, String msg, String attachName)&#123; if (CollectionUtils.isEmpty(receivers) || (subject == null) || (msg == null)) &#123; throw new SendMailException("Email receivers or subject or content must be not null"); &#125; boolean isSuccess = false; try &#123; //SimpleEmail email = new SimpleEmail();//创建简单邮件,不可添加附件、HTML文本等 //MultiPartEmail email = new MultiPartEmail();//创建能加附件的邮件,可多个、网络附件亦可 //HtmlEmail email = new HtmlEmail();//创建能加附件内容为HTML文本的邮件、HTML直接内联图片但必须用setHtmlMsg()传邮件内容 MultiPartEmail email = new MultiPartEmail(); //设置smtp 主机 email.setHostName(MAIL_HOST_NAME); email.setSmtpPort(465); email.setSSL(true); //邮箱登录认证（保证邮件服务器POP3/SMTP服务开启） email.setAuthentication(MAIL_SYSTEM_USER,MAIL_SYSTEM_PASSWORD); //收信人列表 for(String receiver:receivers)&#123; email.addTo(receiver, receiver); &#125; //抄送人列表 if(!CollectionUtils.isEmpty(copys))&#123; for(String copy:copys)&#123; email.addCc(copy,copy); &#125; &#125; //发信人 email.setFrom(MAIL_SYSTEM_USER, MAIL_SYSTEM_USER); //邮件主题 email.setSubject(MimeUtility.encodeText(subject)); //邮件内容 email.setMsg(msg); //邮件附件 if ((attachName != null) &amp;&amp; (attachName.trim().length() &gt; 0)) &#123; EmailAttachment attachment = new EmailAttachment(); attachment.setPath(attachName); attachment.setDisposition("attachment"); email.attach(attachment); &#125; //发送邮件 email.send(); isSuccess = true; &#125; catch (EmailException e) &#123; throw new SendMailException(e.getMessage()); &#125; catch (UnsupportedEncodingException e) &#123; throw new SendMailException(e.getMessage()); &#125; return isSuccess; &#125; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("test1@qq.com"); sendMail("test",list,null,"测试一下",null); &#125;&#125; 对比两个程序，可以了解apache-commons-email这个包把JavaMail的Session给我们隐藏了，使得我们更专注于API调用，同时它分出了三种邮件类型，供我们使用。 使用apache-commons-email，要发送不同类型的邮件，可以创建不同的mail class，如下： //SimpleEmail email = new SimpleEmail();//创建简单邮件,不可添加附件、HTML文本等 //MultiPartEmail email = new MultiPartEmail();//创建能加附件的邮件,可多个、网络附件亦可 //HtmlEmail email = new HtmlEmail();//创建能加附件内容为HTML文本的邮件、HTML直接内联图片 上面基本上是使用Java发送邮件的两种方式。 我测试了一下发邮件的代码，可以成功收到邮件。如下图： 总结在后台应用系统中邮件系统还是比较常用的，可以用来提供系统邮件预警、发送客户邮件进行活动推广等等一系列的事情。SpringBoot里整合了JavaMail，创建SpringBoot项目时可以直接引入，是十分方便的。 在application.properties里的配置均以spring.mail开头。 1234567891011# JavaMailSender 邮件发送的配置spring.mail.host=smtp.qq.comspring.mail.username=qq邮箱#QQ邮箱的授权码spring.mail.password=授权码spring.mail.properties.mail.smtp.auth=truespring.mail.properties.mail.smtp.starttls.enable=truespring.mail.properties.mail.smtp.starttls.required=truespring.mail.default-encoding=UTF-8spring.mail.protocol=smtpspring.mail.port=465]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaMail</tag>
        <tag>Java邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JWT简介及使用]]></title>
    <url>%2Fblog%2FJWT%E7%AE%80%E4%BB%8B%E5%8F%8A%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言JWT（JSON Web Tokens）是目前比较流行的跨域认证解决方案，遵循RFC 7519 标准，我们可以使用JWT在用户和服务器之间传递安全可靠的信息。 我们可以在 JWT的官网 了解到更多关于JWT的信息。 正文构成JWT主要由头部、载荷与签名三部分构成，三部分的信息用英文逗号“.”分割。 一个完整的JWT如下： 它的头部部分（Header）为红色标注部分，载荷（Payload）为蓝色部分，签名（Signature）为橘色部分。 它们均使用Base64进行编码，我们将上述Base64解码后可以看到如下： Header部分Json： 1234&#123; "typ": "JWT", "alg": "HS256"&#125; 这儿Header声明了使用的加密算法（HS256）及token类型（JWT）。 Payload部分Json如下： 1234567&#123; "phone": "1888888888", "sessionId": "111111111111", "exp": 1548052800, "userId": "1433223", "platform": "APP"&#125; 其中除exp字段其它都是我自定义的字段。JWT 规定了7个官方字段，供我们选用，如下： iss (issuer)：签发人 exp (expiration time)：过期时间 sub (subject)：主题 aud (audience)：受众 nbf (Not Before)：生效时间 iat (Issued At)：签发时间 jti (JWT ID)：编号 PS：可以看到信息仅仅使用了Base64进行了一下编码，非加密，如果我们想是信息更安全可以对JWT生成的token进行可逆加密。 Signature部分: 会对上面两部分进行签名，通常使用RSA，Hmac或者ECDSA等签名方式。用于防止上面两部分的数据遭到篡改。 源码要想在Java项目里使用JWT，需要引入以下依赖。 123456&lt;!--JSON Web Tokens--&gt;&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 我们来简单的看一下它的源码部分。 先看一下包的结构： algorithm：各种签名的包。 exceptions：自定义异常类的包。 interfaces和impl：JWT的接口和实现类包。 在algorithm包里我们可以看到我们刚才描述的几种签名算法（RSA，HMAC，ECDSA）。 在JWTCreator类中，我们可以看到Signature部分是通过Header，Payload经过签名算法得来的。 同时载荷Payload里JWT规定的几个可使用字段也能看到。 生成签名，放入Header信息及Payload信息，使用指定签名算法生成JWT token。 JWTDecoder类为解密token的类，可以看到它的处理方法，获取headerJson和payloadJson，还是比较好理解的。 应用我们使用JWT生成token并使用。 我们知道，前后端使用token进行交互，服务器端可以不用保存session状态，减轻压力。 我们定义一个Vo，用于存放一些用户数据，如下： 123456789101112131415161718192021222324@Data@Accessors(chain = true)public class UserVo &#123; /** * 用户手机号 */ private String phone; /** * 用户唯一Id */ private String userId; /** * sessionId */ private String sessionId; /** * 过期时间 */ private long expiresAt; /** * 用户所属平台 */ private String platform;&#125; 可以认为这些为公共部分，用户登录后应该携带这些信息。 这样我们就可以使用JWT在该用户登录后生成一个有效token，为保证信息安全，我们可以对生成的token进行加密，如下： 我们使用AES算法对token进行加解密。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394@Slf4jpublic class AESUtils &#123; private final static String ENCODING_UTF8 = "utf-8"; private static final String KEY_ALGORITHM = "AES"; /**默认的加密算法*/ private static final String CIPHER_ALGORITHM = "AES/CBC/PKCS5Padding"; private String secretKeySeed = null; private String ivParameterSeed = null; public AESUtils(String secretKeySeed,String ivParameterSeed) &#123; this.secretKeySeed = secretKeySeed; if (ivParameterSeed.length() != 16) &#123; throw new RuntimeException("iv向量长度必须为16"); &#125; this.ivParameterSeed=ivParameterSeed; &#125; public AESUtils(String secretKeySeed) &#123; if (secretKeySeed.length() != 16) &#123; throw new RuntimeException("iv向量长度必须为16"); &#125; this.secretKeySeed = secretKeySeed; this.ivParameterSeed=secretKeySeed; &#125; /** * AES加密 * @param content * @return String */ public String aesEncrypt(String content) throws Exception &#123; // AES加密 byte[] encryptStr = encrypt(content, secretKeySeed,ivParameterSeed); // BASE64位加密 return Base64.encodeBase64String(encryptStr); &#125; /** * AES解密 * @param encryptStr * @return String */ public String aesDecrypt(String encryptStr) throws Exception &#123; // BASE64位解密 byte[] decodeBase64 = Base64.decodeBase64(encryptStr); // AES解密 return new String(decrypt(decodeBase64, secretKeySeed,ivParameterSeed),ENCODING_UTF8); &#125; /** * 生成加密秘钥 * @return */ private static SecretKeySpec getSecretKeySpec(final String secretKeySeed) &#123; try &#123; return new SecretKeySpec(secretKeySeed.getBytes(), "AES"); &#125; catch (Exception ex) &#123; log.error("生成加密密钥异常",ex); &#125; return null; &#125; /** * 生成向量秘钥 * @return */ private static IvParameterSpec getIvParameterSpec(final String ivParameterSeed) &#123; //使用CBC模式，需要一个向量iv，可增加加密算法的强度 return new IvParameterSpec(ivParameterSeed.getBytes()); &#125; /** * 加密 * @param content * @return byte[] */ private static byte[] encrypt(String content, String secretKeySeed,String ivParameterSeed) throws Exception &#123; // 创建密码器 Cipher cipher = Cipher.getInstance(CIPHER_ALGORITHM); // 初始化为加密模式的密码器 cipher.init(Cipher.ENCRYPT_MODE, getSecretKeySpec(secretKeySeed),getIvParameterSpec(ivParameterSeed)); // 加密 return cipher.doFinal(content.getBytes(ENCODING_UTF8)); &#125; /** * 解密 * @param content * @return byte[] */ private static byte[] decrypt(byte[] content, String secretKeySeed,String ivParameterSeed) throws Exception &#123; // 创建密码器 Cipher cipher = Cipher.getInstance(CIPHER_ALGORITHM); // 初始化为加密模式的密码器 cipher.init(Cipher.DECRYPT_MODE, getSecretKeySpec(secretKeySeed),getIvParameterSpec(ivParameterSeed)); // 解密 return cipher.doFinal(content); &#125;&#125; 同时，根据刚才我们的说明创建一个JWT帮助类用于生成token，大致如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105@Slf4jpublic class JWTUtils &#123; //JWT header private String JWT_HEADER = ""; //签名算法 private Algorithm algorithm; //默认token过期时间 private long expireTimeMillis=2*60*60*1000; private AtomicBoolean initState = new AtomicBoolean(false); private static AESUtils aesUtils=null; private static class SingletonHolder &#123; static final JWTUtils instance = new JWTUtils(); &#125; public static JWTUtils getInstance() &#123; return JWTUtils.SingletonHolder.instance; &#125; /** * 初始化jwt * @param jwtSecretKey * @param jwtExpireTimeSeconds * @param aesSecretKeySeed * @param aesIvParameterSeed * @throws UnsupportedEncodingException */ public void init(String jwtSecretKey,long jwtExpireTimeSeconds,String aesSecretKeySeed,String aesIvParameterSeed) throws UnsupportedEncodingException &#123; if (initState.compareAndSet(false, true)) &#123; this.algorithm = Algorithm.HMAC256(jwtSecretKey); if(jwtExpireTimeSeconds&gt;0) &#123; this.expireTimeMillis = jwtExpireTimeSeconds * 1000; &#125; this.JWT_HEADER = StringUtils.substringBefore(JWT.create().sign(Algorithm.HMAC256(jwtSecretKey)), ".")+"."; aesUtils = new AESUtils(aesSecretKeySeed, aesIvParameterSeed); &#125;else&#123; log.error("重复初始化jwt"); &#125; &#125; /** * 使用jwt生成token * @param userVo * @return */ public String encodeJWT(UserVo userVo) &#123; return encodeJWT(userVo,this.expireTimeMillis); &#125; /** * 生成token * @param userVo * @param expireTimeMillis * @return */ public String encodeJWT(UserVo userVo,long expireTimeMillis)&#123; String token = JWT.create() .withExpiresAt(new Date(System.currentTimeMillis()+expireTimeMillis)) .withClaim("phone",userVo.getPhone()) .withClaim("userId",userVo.getUserId()) .withClaim("sessionId",userVo.getSessionId()) .withClaim("platform",userVo.getPlatform()) .sign(algorithm); System.out.println("token----&gt; "+token); try &#123; token = aesUtils.aesEncrypt(StringUtils.removeStart(token, JWT_HEADER)); &#125; catch (Exception ex) &#123; log.error("加密异常",ex); token = ""; &#125; return token; &#125; /** * 解密token * @param token * @return */ public UserVo decodeJWT(String token) &#123; UserVo userVo =new UserVo(); try &#123; if (StringUtils.isBlank(token)) &#123; throw new RuntimeException("无效token"); &#125; String decryptJwtToken = aesUtils.aesDecrypt(token); JWTVerifier verifier = JWT.require(algorithm).build(); DecodedJWT jwt = verifier.verify(StringUtils.join(JWT_HEADER,decryptJwtToken)); long expiresAt = jwt.getExpiresAt()==null?0:jwt.getExpiresAt().getTime(); if(System.currentTimeMillis()&gt;expiresAt)&#123; throw new RuntimeException("token有效期超期"); &#125; Map&lt;String, Claim&gt; claims = jwt.getClaims(); userVo.setPhone(claims.get("phone")==null?"":claims.get("phone").asString()); userVo.setUserId(claims.get("userId")==null?"":claims.get("userId").asString()); userVo.setSessionId(claims.get("sessionId")==null?"":claims.get("sessionId").asString()); userVo.setPlatform(claims.get("platform")==null?"":claims.get("platform").asString()); userVo.setExpiresAt(expiresAt); &#125; catch (Exception exception)&#123; throw new RuntimeException("无效token"); &#125; return userVo; &#125;&#125; 这样，一个简单的JWT工具类就搞定了，可以用于生成token。 测试我们测试一下效果，新建方法如下： 1234567891011public static void main(String[] args) throws Exception&#123; JWTUtils jwtUtils = JWTUtils.getInstance(); jwtUtils.init("sakuratears",1000,"1234567891111111","test111111111111"); UserVo userVo = new UserVo(); userVo.setUserId("1433223").setPhone("1888888888").setPlatform("APP").setSessionId("111111111111"); String token = jwtUtils.encodeJWT(userVo); System.out.println("加密token----&gt; "+token); Thread.sleep(1); UserVo vo = jwtUtils.decodeJWT(token); System.out.println("解密token得到结果----&gt; "+vo.toString());&#125; 运行结果如下： 我们尝试缩短token失效时间，增加线程等待时间。如下： 123jwtUtils.init("sakuratears",1,"1234567891111111","test111111111111");......Thread.sleep(10000); 再次运行，可以看到token已失效。 所以我们在为客户端颁发token后，应该设置合理的token失效时间，当token失效后，再次请求，应告诉用户需要重新登录了。 总结经过上面的一些描述，我们可以知道JWT的一些特点。 JWT默认是不加密的，为了保证安全，可以对JWT（token）进行可逆加密处理。 JWT可以用于信息交换，比如上面UserVo里面的手机号，这样我们不用在使用userId在对用户进行数据库查询，提高系统性能。 可以看到，JWT一旦签发生成token，如果不设置超时时间或者设置不合理（过长），在有效期内，token始终是有效的，除非服务器进行额外的处理。所以如果token泄露或被盗用，将是十分危险的，故应当设置合理的过期时间。 为了减少泄露或者盗用风险，JWT一般使用HTTPS协议进行传输。若使用HTTP协议，务必对token进行可逆加密处理后在进行传输。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JWT</tag>
        <tag>Java Web Token</tag>
        <tag>Token</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot多数据源配置]]></title>
    <url>%2Fblog%2FSpringBoot%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E9%85%8D%E7%BD%AE.html</url>
    <content type="text"><![CDATA[前言之前研究了下SpringBoot下多数据源的配置处理，感觉蛮有意思的，今天特地总结分享下。 我们知道，当一些项目较大时，有可能出现分库等技术操作，这时候就有可能需要使用多数据源了，为保证程序SQL能在多个数据源之间来回切换并正常执行，就需要对数据源代码上进行一些处理，这里我已SpringBoot下的配置为例。 正文如何配置是简单的，但是要了解如何配置，我们应该下些功夫。 在spring-jdbc这个jar包里，我们可以找到这个抽象类AbstractRoutingDataSource.class，这个类是处理多数据源配置的关键类，我们来看一下。 这个类的主要作用是是的我们可以切换数据源key以切换数据源，key值的切换一般通过线程Context实现。 targetDataSources： 是一个Map集合，用于存放多个数据源。 resolvedDataSources：dataSource的Map集合，用于确定多个数据源。 defaultTargetDataSource：默认使用的数据源。 lenientFallback：可以用来回滚到原来的数据源的设置。 dataSourceLookup：数据源循环查看实现，有多个实现。 resolvedDefaultDataSource：确定的要使用哪一个数据源。 我们来看一些相关代码。 先看属性设置完成后执行的这个方法。 可以看到，resolvedDataSources从targetDataSources拿到值并转换为DataSource集合，resolvedDefaultDataSource会对defaultTargetDataSource进行转化处理。 再看这个决定使用哪个数据源的方法。 可以看到会先拿到当前的LookupKey，即当前要使用的数据源的key，拿到key后在尝试拿到当前key对应的数据源，如果没有数据源并且设置了回滚属性，会继续使用默认的数据源而不切换，如果没有设置回滚又拿不到数据源，dataSource==null，那么就会抛出异常，如果拿到数据源了，dataSource!=null，那么就会进行数据源的切换，返回一个切换后的数据源。 这儿lenientFallback（仁慈回滚）的作用，如果设置为false，拿不到数据源会出错，便于我们分析问题。 再看下 determineCurrentLookupKey 这个方法，就是获取要使用的数据源的key，是个抽象方法，需要我们进行实现。（具体什么时候切换数据源，就改变key值，我们一般使用线程绑定的上下文来对key值进行控制） 我们再来看下把把我们的对象转为数据源对象的这个方法。 可以看到如果传入的是String，会使用DataSourceLookup去获取数据源，默认使用JndiDataSourceLookup。 DataSourceLookup有4种实现，可以看下。 BeanFactoryDataSourceLookup是从SpringBean里获取DataSource。 JndiDataSourceLookup是通过JndiTemplate获取DataSource。 MapDataSourceLookup可以自定义DataSourceMap，然后根据Key值获取。下图中的dataSources就是我们可以提前主动set进去的数据。 SingleDataSourceLookup指的单一数据源，一般很少使用。这个dataSource我们也是可以主动设置的。 可以看到，我们有多种方式去放置数据源，targetDataSources这个Map集合可以直接放置数据源集合，也可以放置数据源bean名字（但需要指定DataSourceLookup为BeanFactoryDataSourceLookup去解析），也可以放置自定义数据源集合的key（但需要指定DataSourceLookup为MapDataSourceLookup去解析）等等。 因此我们要实现多数据源，需要对AbstractRoutingDataSource进行实现。 多数据源配置新建DynamicDataSource类，内容如下： 123456public class DynamicDataSource extends AbstractRoutingDataSource &#123; @Override protected Object determineCurrentLookupKey() &#123; return DatabaseContextHolder.getDatabaseType(); &#125;&#125; 看一下lookupKey的实现，我们新创建一个DatabaseContextHolder类，用一个ThreadLocal来对当前正在使用的数据源key进行管理。如下： 123456789101112131415161718192021222324252627282930public class DatabaseContextHolder &#123; /** * 使用ThreadLocal管理数据源 */ private static final ThreadLocal&lt;DatabaseType&gt; contextHolder = new ThreadLocal&lt;&gt;(); /** * 当前可以使用的所有数据源key值 */ public static Set&lt;DatabaseType&gt; databaseTypes = new HashSet&lt;&gt;(); /** * 设置数据源类型 * @param type */ public static void setDatabaseType(DatabaseType type) &#123; contextHolder.set(type); &#125; /** * 获取数据源类型 * @return */ public static DatabaseType getDatabaseType() &#123; return contextHolder.get(); &#125; /** * 清除数据源类型 */ public static void clearDatabaseType()&#123; contextHolder.remove(); &#125;&#125; databaseTypes 这个set我们后面可以用来判断是否有此数据源。 DatabaseType枚举如下（用静态String等也可以代替）： 123public enum DatabaseType &#123; MASTER,SLAVE&#125; 可以看到有两个数据源key值，MASTER和SLAVE。 然后我们要使多数据源生效，需要配置多数据源，如下： 1234567891011121314151617181920212223242526@Configuration@MapperScan(basePackages = "com.zwt.frameworkdatasource.mapper")public class DynamicDataSourceConfig &#123; /** * 主数据源 * @return */ @Bean(name = "masterDataSource") @Primary @ConfigurationProperties(prefix = "spring.datasource.multiple.master-datasource") public DataSource masterDataSource() &#123; return DataSourceBuilder.create().build(); &#125; /** * 从数据源 * @return */ @Bean(name = "slaveDataSource") @ConfigurationProperties(prefix = "spring.datasource.multiple.slave-datasource") public DataSource slaveDataSource() &#123; return DataSourceBuilder.create().build(); &#125; ......&#125; 配置文件内容如下： 123456789101112131415spring.datasource.multiple.master-datasource.driverClassName=com.mysql.cj.jdbc.Driverspring.datasource.multiple.master-datasource.jdbcUrl=jdbc:mysql://localhost:3306/test1?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8spring.datasource.multiple.master-datasource.username=rootspring.datasource.multiple.master-datasource.password=rootspring.datasource.multiple.master-datasource.initialSize=5spring.datasource.multiple.master-datasource.minIdle=1spring.datasource.multiple.master-datasource.maxActive=50spring.datasource.multiple.slave-datasource.driverClassName=com.mysql.cj.jdbc.Driverspring.datasource.multiple.slave-datasource.jdbcUrl=jdbc:mysql://localhost:3306/test2?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=GMT%2B8spring.datasource.multiple.slave-datasource.username=rootspring.datasource.multiple.slave-datasource.password=rootspring.datasource.multiple.slave-datasource.initialSize=5spring.datasource.multiple.slave-datasource.minIdle=1spring.datasource.multiple.slave-datasource.maxActive=50 我们说下这个Config类。masterDataSource和slaveDataSource很好理解，就是获取配置文件属性生成相应的Bean，这儿要注意@Primary注解，这个注解指定默认使用哪个Bean，因为DataSource有两个实现，master和slave，如果不指定，Spring会不知道选用哪一个。 要使用多数据源，就需要对两个数据源进行管理，我们在Config这个类里创建一个返回DynamicDataSource的方法，如下: 1234567891011121314151617181920/** * 动态数据源 * @param masterDataSource * @param slaveDataSource * @return */@Beanpublic DynamicDataSource dataSource(@Qualifier("masterDataSource") DataSource masterDataSource,@Qualifier("slaveDataSource") DataSource slaveDataSource) &#123; Map&lt;Object, Object&gt; targetDataSources = new HashMap&lt;&gt;(); targetDataSources.put(DatabaseType.MASTER, masterDataSource); targetDataSources.put(DatabaseType.SLAVE, slaveDataSource); DatabaseContextHolder.databaseTypes.add(DatabaseType.MASTER); DatabaseContextHolder.databaseTypes.add(DatabaseType.SLAVE); DynamicDataSource dataSource = new DynamicDataSource(); dataSource.setTargetDataSources(targetDataSources); dataSource.setDefaultTargetDataSource(masterDataSource); return dataSource;&#125; 可以看到我们把MASTER数据源和SLAVE数据源放到了targetDataSources 里面，同时在DatabaseContextHolder的databaseTypes里面保存一份key值，用于一些处理。整体上这个类还是比较好理解的。 如何让程序执行SQL使用使用动态数据源呢？这时候就要处理SqlSessionFactory了，我们创建获取SqlSessionFactory的Bean的方法，传入动态数据源获取SqlSessionFactory，如下： 1234567891011121314/** * 多数据源切换关键类 * @param dynamicDataSource * @return * @throws Exception */@Beanpublic SqlSessionFactory sqlSessionFactory(DynamicDataSource dynamicDataSource) throws Exception&#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dynamicDataSource); sqlSessionFactoryBean.setTypeAliasesPackage("com.zwt.frameworkdatasource.model"); sqlSessionFactoryBean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources("classpath:sqlmap/mapper/*.xml")); return sqlSessionFactoryBean.getObject();&#125; 这个类是是数据源切换的关键类。 为了管理支持多数据源事务，我们的事务Bean也是要传入动态数据源的，如下： 123456789/** * 事务管理 * @param dynamicDataSource * @return */ @Bean public PlatformTransactionManager transactionManager(DynamicDataSource dynamicDataSource)&#123; return new DataSourceTransactionManager(dynamicDataSource); &#125; 这样随着数据源的切换，事务也是切换后的数据源的。 如何切换数据源？其实只要控制DatabaseContextHolder里面ThreadLocal的值就可以控制数据源切换，但是我们想切换数据源时，总不能每个方法都在方法开始前设置，在结束前清除吧，那样太麻烦了。 我们可以借助Aspect切面来处理这个问题。我们定义一个TargetDataSource注解，只要标注这个注解，并指定数据源key便可以进行数据源切换，这个注解一定是作用在方法上的，如下： 12345678910@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface TargetDataSource &#123; /** * 数据源key * @return */ DatabaseType database() default DatabaseType.MASTER;&#125; 切面的话，只要检测到这个方法上有这个注解，那就拿到注解里的数据源key进行切换，如下： 1234567891011121314151617181920212223242526272829303132333435@Aspect@Order(-1)//保证在@Transactional之前执行@Componentpublic class DynamicDataSourceAspect &#123; private static final Logger logger = LoggerFactory.getLogger(DynamicDataSourceAspect.class); /** * 改变数据源 * @param joinPoint * @param targetDataSource */ @Before("@annotation(targetDataSource)") public void changeDataSource(JoinPoint joinPoint, TargetDataSource targetDataSource) &#123; DatabaseType type = targetDataSource.database(); if(DatabaseContextHolder.databaseTypes.contains(type))&#123; logger.info("使用数据源："+ type); DatabaseContextHolder.setDatabaseType(type); &#125;else&#123; logger.error("未配置数据源，使用默认数据源：MASTER"); DatabaseContextHolder.setDatabaseType(DatabaseType.MASTER); &#125; &#125; /** * 清除数据源 * @param joinPoint * @param targetDataSource */ @After("@annotation(targetDataSource)") public void clearDataSource(JoinPoint joinPoint, TargetDataSource targetDataSource) &#123; logger.info("清除数据源 " + targetDataSource.database()); DatabaseContextHolder.clearDatabaseType(); &#125;&#125; @Order注解设置为-1保证此过程在事务之前执行，然后在执行事务。@Before注解表示在在动作之前执行，我们设置为在有此注解的方法之前执行changeDataSource方法，这个方法对数据源进行切换，@After注解表示方法执行完之后要处理的事情，这儿我们用clearDataSource方法来清除使用过的数据源信息。 讲到这儿，SpringBoot的多数据源配置也基本完成了，我们进行必要的测试。 测试创建一个Service类，代码如下： 1234567891011121314151617181920212223242526272829@Servicepublic class TestServiceImpl implements TestService &#123; @Autowired private ScoreMapper scoreMapper; boolean flag = false; @Override @TargetDataSource(database = DatabaseType.MASTER) @Transactional(rollbackFor = Exception.class,propagation = Propagation.REQUIRED) public void doSomething1() &#123; ScoreModel scoreModel = new ScoreModel(); scoreModel.setId(1234); scoreMapper.insert(scoreModel); if(!flag)&#123; throw new RuntimeException("出现异常"); &#125; &#125; @Override @TargetDataSource(database = DatabaseType.SLAVE) @Transactional(rollbackFor = Exception.class,propagation = Propagation.REQUIRED) public void doSomething2() &#123; ScoreModel scoreModel = new ScoreModel(); scoreModel.setId(12345); scoreMapper.insert(scoreModel); if(flag)&#123; throw new RuntimeException("出现异常"); &#125; &#125;&#125; 注：这儿略掉了Score表和Mapper文件的创建。 上面的TestServiceImpl，我们应该期望的结果是 doSomething1执行成功入库test1，doSomething2执行失败不能入库test2（因为抛出异常事务要回滚），我们看下执行结果: 新建Test，执行我们的方法。 1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTestpublic class FrameworkDatasourceApplicationTests &#123; @Autowired TestService testService; @Test public void contextLoads() &#123; try&#123; testService.doSomething1(); &#125;catch(Exception e)&#123; System.out.println(e); &#125; try&#123; testService.doSomething2(); &#125;catch(Exception e)&#123; System.out.println(e); &#125; &#125;&#125; 可以看到如下输出： 查看数据库。 符合我们期望值。我们把数据库数据清除后，把TestServiceImpl里的flag设置为false。执行刚才的测试方法，结果如下： 可以看到也符合我们的期望结果，成功完成了数据源切换及事务支持。 总结通过对SpringBoot多数据源的配置和理解，了解了多数据源的配置和使用，及多数据源实现的一些简单原理，也是蛮不错的一次学习过程。 本节源代码详见： framework-datasource]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>多数据源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Java自制一张雷达图]]></title>
    <url>%2Fblog%2F%E7%94%A8Java%E8%87%AA%E5%88%B6%E4%B8%80%E5%BC%A0%E9%9B%B7%E8%BE%BE%E5%9B%BE.html</url>
    <content type="text"><![CDATA[前言今天整理资料，看到了之前自己用Java画了张雷达图，遂又拿来研究了下，感觉比较好玩，特地分享一下。 众所周知，Java在绘制图像方面的能力是比较差（la）劲（ji）的。自带的主要的一些图像处理类有Java2D中绘图类Graphics，图像流处理类ImageIO等。 有一些基于Java的图像处理开源包也仅仅是对原图像进行缩放、变化、水印等操作，使用Java进行绘图的少之又少。 今天我们用Java自带的图像处理类（Graphics、ImageIO等）来绘制一张雷达图吧。 正文为什么要画雷达图？而不是用Java绘制动漫人物？ 咳咳……因为雷达图应用广泛（就不要为难Java画动漫人物了TAT）。 我们开始吧，先随便看一张雷达图。 可知其主要内容： 外层环、圆环数量、圆环半径、分类名称、各个部分的数值、各种颜色等等很多很多属性。 这里，我们不妨绘制一个考试分数雷达图，这样更结合实际。 我们需要一个Java Bean，里面存放雷达图的一些属性参数。如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 雷达图数据bean * @author zwt * ------------------关于雷达图，以下的说明--------------------- * 默认圆环最大半径为200 * 生成的图片为 宽600*高700 默认 * 雷达图中心点的位置为 300,300 默认 * 线及文字大概占 600*100 默认 * 这些参数都可以调整，但调整后应尽量保证生成图的美观性 * ------------------------------------------------------------- */public class RadarMapInit &#123; // 圆环数量 private int cirNum=2; // 圆的最大半径 private int r=200; // 圆心坐标 private Point point=new Point(300,300); // 圆环颜色 private Color cirColor=new Color(24,165,255); // 射线起始角度 默认顺时针旋转 private float startAngle=-90; // 射线颜色 private Color spoColor=new Color(0,0,255,100); // 绘图参数g 画图时该参数不需要设置，设置了也没用 private Graphics2D g; //填充色 private Color fillColor=new Color(146,199,234,200); //中部文字颜色 private Color cenColor=Color.WHITE; //中部文字 private String cenText; //中部文字字体样式 private Font cenFont=new Font("宋体", Font.TYPE1_FONT, 50); // 外围文字颜色 private Color outColor=Color.BLUE; //外围文字样式 有默认值 private Font outFont=new Font("宋体", Font.TYPE1_FONT,12); //生成图片路径 无默认值，必填 private String imgPath; //生成图片格式 默认jpg 支持 jpg和png 其他格式没测 private String imgType="jpg"; //图片大小设置 宽 默认600*700 像素 private int picWidth=600; //图片大小设置 高 private int picHeight=700; //文字及线段的x方向距图片边的距离 private int shiftX=100; //文字距雷达图y方向的距离 private int shiftY=100; //生成的图片的背景颜色 默认白色 private Color picBackColor=Color.WHITE; //又要开始添加参数了(⊙o⊙).....下面的主要是那一道线的参数 //线的最大值 private int lineMaxVal=1000; //线的当前值 private int lineCurVal=-1; //线的起始值 private int lineStaVal=0; //线的已达到的颜色 private Color lineArrColor=new Color(24,165,255); //线的未达到的颜色 private Color lineNotArrColor=Color.GRAY; //线的粗细 private BasicStroke lineStroke=new BasicStroke(5.0f,BasicStroke.CAP_ROUND,BasicStroke.JOIN_MITER); //下面的一般说明字体 一般说明文字写死吧，参数太多了，不想写了.... private Font genLineFont=new Font("宋体", Font.TYPE1_FONT,12); //下面的特殊说明字体 private Font speLineFont=new Font("宋体", Font.TYPE1_FONT,25); //特殊说明字体百分数 private String speLineValue; //特殊说明自己文字 private String speLineText; //一般说明字体颜色 private Color genTextColor=Color.BLACK; //特殊说明字体颜色 private Color speTextColor=Color.BLUE; ...... //Get Set 及构造方法略&#125; 上面省略掉了get set和构造方法。 另外我里面还有一些参数设置没提到的，主要是用来展示效果让其更人性化（应付客户使其更加满意）的，比如底部的分数击败直线，雷达图中央的分数显示等，一会儿大家可以看到效果。 参数很多，没办法…… 对于每个分类，我们也新建一个属于它们的Java Bean，用来存储它们的属性。（PS：因为到底有多少分类是不确定的，故应该在画图时传入一个Bean List）。 如下： 1234567891011121314public class RadarMapData &#123; //数值 private String value; //该维度数值最大值 private String maxValue; //分组 private String group; //文字 private String textValue; //图片路径 private String picPath; ...... //Get Set 及构造方法略&#125; 同样省略了Get Set 和构造方法，我又增加了图片路径可以在分类的文字部分添加小图标，还可以对分类进行分组（比如数学是理科分组，语文英语是文科分组），可以说很Nice了（以应对莫名其妙的需求）。 好了，开始使用我们的Graphics类进行绘图等操作了。 首先，这个关键类要使用我们刚才的那两个Bean，应该如下代码，同样Get，Set和构造方法略。 123456789101112public class RadarMap &#123; // 日志记录 private Logger logger = LoggerFactory.getLogger(RadarMap.class); // 数据集合 private List&lt;RadarMapData&gt; dataList; // 初始化参数 private RadarMapInit init; ...... //Get Set 及构造方法略 &#125; 开始绘制逻辑，首先有个画圆环方法，可以画出数个同心圆，如下： 1234567891011121314151617/** * 画圆环 * */private void drawCircles() &#123; Graphics2D g = init.getG(); // 根据圆心，半径，画出同心圆 g.setColor(init.getCirColor()); // 获取圆心 int x = init.getPoint().x; int y = init.getPoint().y; // 获取各个圆的直径并画圆环 for (int i = 1; i &lt;= init.getCirNum(); i++) &#123; int d = 2 * i * init.getR() / init.getCirNum(); g.drawOval(x - d / 2, y - d / 2, d, d); &#125;&#125; 主要方法就是g.drawOval画圆，不在详细解释。 然后我们以圆心绘制分类的每条射线，同时拿到每个分类的最大数值和这个人的数值并标记，然后连接这个人的各个数值，并将这个多边形内部填充起来。 如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 以圆心画射线 * */private void drawSpokes() &#123; Graphics2D g = init.getG(); Point point = init.getPoint(); // 解析数组 int num = dataList.size(); // 设置颜色 g.setColor(init.getSpoColor()); // 每个角度 float angle = init.getStartAngle(); float angleStep = 360 / num; for (int i = 1; i &lt;= num; i++) &#123; Point pt = getMappedPoint(init.getR(), angle, point); g.drawLine(point.x, point.y, pt.x, pt.y); addCirWordPic(dataList.get(i - 1).getTextValue(), pt, angle, dataList.get(i - 1).getPicPath()); g.setColor(init.getSpoColor()); angle += angleStep; &#125; // 获取各个点的坐标 Polygon p = new Polygon(); g.setColor(init.getFillColor()); float angle1 = init.getStartAngle(); Point ptNext = new Point(); for (int i = 1; i &lt;= num; i++) &#123; int myR = (int)(Double.parseDouble(dataList.get(i - 1).getValue())*(double)init.getR()/Double.parseDouble(dataList.get(i - 1).getMaxValue())); Point pt = getMappedPoint(myR, angle1, point); p.addPoint(pt.x, pt.y); if (i &lt; num) &#123; angle1 += angleStep; int myRNext = (int)(Double.parseDouble(dataList.get(i).getValue())*(double)init.getR()/Double.parseDouble(dataList.get(i).getMaxValue())); ptNext = getMappedPoint(myRNext, angle1, point); &#125; else &#123; angle1 += angleStep; int myRNext = (int)(Double.parseDouble(dataList.get(0).getValue())*(double)init.getR()/Double.parseDouble(dataList.get(0).getMaxValue())); ptNext = getMappedPoint(myRNext, angle1, point); &#125; //g.drawLine(pt.x, pt.y, ptNext.x, ptNext.y); &#125; g.drawPolygon(p); g.fillPolygon(p);&#125; 这个方法首先解析分类数组，有几个就画几条射线（根据角度，这里逆时针旋转，起始角度-90，也就是从正上方开始旋转），从圆心与角度点之间画直线，并为这条直线的终点（角度点）添加文字和图片（分类文字及图片，方法addCirWordPic）。 完成后对于各个实际值点（实际分，比如语文100分，实际78分，78就是这儿的实际值），绘制成多边形。（PS： Polygon p = new Polygon();绘制多边形，p.addPoint为多边形添加指定点，g.drawPolygon(p)为绘制多边形，g.fillPolygon(p)为多边形填充颜色） 上面方法调用了两个方法getMappedPoint和addCirWordPic，大家可以看下，分别为寻找绘图点方法和添加分类图片文字方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 根据点，角度，半径获取另一个点坐标 * * @param r * @param angle * @param point * @return */private Point getMappedPoint(int r, float angle, Point point) &#123; // 计算坐标 Point pt = new Point(); pt.x = (int) (r * Math.cos(angle * Math.PI / 180) + point.x); pt.y = (int) (r * Math.sin(angle * Math.PI / 180) + point.y); return pt;&#125; /** * 添加圆外文字图片 * * @param point * 绘制起始点 * @param picValue * 图片路径 */private void addCirWordPic(String wordValue, Point point, float angle, String picValue) &#123; Graphics2D g = init.getG(); double x = 0; double y = 0; double dx = 0; double dy = 0; if (wordValue == null || "".equals(wordValue)) &#123; logger.info(angle + "角度的点没有配置圆外文字"); Map&lt;String, Double&gt; map = setDxDy(angle, x, y, dx, dy); dx = map.get("dx"); dy = map.get("dy"); &#125; else &#123; FontRenderContext context = g.getFontRenderContext(); Font font = init.getOutFont(); g.setColor(init.getOutColor()); g.setFont(font); Rectangle2D bounds = font.getStringBounds(wordValue, context); x = (bounds.getWidth()) / 2; y = (bounds.getHeight()) / 2; Map&lt;String, Double&gt; map = setDxDy(angle, x, y, dx, dy); dx = map.get("dx"); dy = map.get("dy"); g.drawString(wordValue, (int) (point.x + dx), (int) (point.y + dy)); &#125; if (picValue == null || "".equals(picValue)) &#123; logger.info(angle + "角度的点没有配置圆外图片"); return; &#125; // 图片放到文字上面 ImageIcon imgIcon = new ImageIcon(picValue); Image img = imgIcon.getImage(); g.drawImage(img, (int) (point.x + dx), (int) (point.y + dy - 5 * y), null);&#125; /** * 设置偏移量 圈外文字及图片使用 * * @param dx * @param dy */private Map&lt;String, Double&gt; setDxDy(float angle, double x, double y, double dx, double dy) &#123; double cosVal = Math.cos(angle * Math.PI / 180); double sinVal = Math.sin(angle * Math.PI / 180); if (cosVal &gt; 0 &amp;&amp; sinVal &gt; 0) &#123; dx = 5; dy = 2 * y + 20; // 第一象限额外偏移15用于放置图片 &#125; else if (cosVal &gt; 0 &amp;&amp; sinVal &lt; 0) &#123; dx = 5; dy = -5; // 第三象限 &#125; else if (cosVal &lt; 0 &amp;&amp; sinVal &gt; 0) &#123; dx = -2 * x - 5; dy = 2 * y + 20; // 第二象限额外偏移15用于放置图片 &#125; else if (cosVal &lt; 0 &amp;&amp; sinVal &lt; 0) &#123; dx = -2 * x - 5; dy = 5; // 第四象限 &#125; Map&lt;String, Double&gt; map = new HashMap&lt;String, Double&gt;(); map.put("dx", dx); map.put("dy", dy); return map;&#125; 绘制分类图片及文字时，由于要考虑文字大小，图片宽度等要求，故对图片及文字位置做了微调，会显得代码多些。 然后我们再对圆心加上一些文字，比如总分多少分什么的，如下： 1234567891011121314151617181920/** * 添加圆心文字 * */private void addWord() &#123; Graphics2D g = init.getG(); String value = init.getCenText(); if (value == null || "".equals(value)) &#123; logger.info("雷达图没有配置圆心文字，不添加圆心文字"); return; &#125; FontRenderContext context = g.getFontRenderContext(); Font font = init.getCenFont(); g.setColor(init.getCenColor()); g.setFont(font); Rectangle2D bounds = font.getStringBounds(value, context); double x = (bounds.getWidth()) / 2; double y = (bounds.getHeight()) / 2; g.drawString(value, (int) (init.getPoint().x - x), (int) (init.getPoint().y + y - 10));&#125; 总的来说就是找到圆心位置，根据字体大小，对字体位置进行调整。 然后我们再来绘制下面的跑分直线部分，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 添加雷达图下面的线 */private void drawLineAndWord() &#123; int curVal=init.getLineCurVal(); //-1为开始设置的值，默认为没有填写，直接跳过线条打印 if(curVal==-1) &#123; return; &#125; int staVal=init.getLineStaVal(); int maxVal=init.getLineMaxVal(); Graphics2D g = init.getG(); int width=init.getPicWidth(); //图片定的为 600*700 目前 雷达图占地 500 * 500 这边从y=600开始画 r+pointy+100 //可以将这些参数改为变量，目前不想改 //x方向前后都要空100单位 int shiftX=init.getShiftX(); //y方向距离雷达图100单位 int shiftY=init.getR()+init.getPoint().y+init.getShiftY(); //起点 Point point=new Point(shiftX,shiftY); //两边空50 实际绘图区域为500 int lineCur=(width-2*shiftX)*curVal/maxVal; int lineMax=(width-2*shiftX)*maxVal/maxVal; Point p1=getMappedPoint(lineCur,0,point); Point p2=getMappedPoint(lineMax,0,point); g.setColor(init.getLineArrColor()); g.setStroke(init.getLineStroke()); g.drawLine(point.x,point.y,p1.x,p1.y); g.setColor(init.getLineNotArrColor()); g.drawLine(p1.x,p1.y,p2.x,p2.y); //画完线后标记起点和结束点 drawWord(g,init.getGenLineFont(),init.getGenTextColor(),point,staVal+"",-10,0); drawWord(g,init.getGenLineFont(),init.getGenTextColor(),p2,maxVal+"",20,0); drawWord(g,init.getGenLineFont(),init.getSpeTextColor(),p1,curVal+"",0,15); //画小箭头 drawArrow(g,init.getLineArrColor(),p1); //写文字 //特殊文字起点 Point speTextP=new Point(shiftX-5,shiftY+60); drawWordNormal(g,init.getSpeLineFont(),init.getGenTextColor(),init.getSpeTextColor(),speTextP,init.getSpeLineText()+"的分数击败了全班",init.getSpeLineValue(),"的童鞋！");&#125; 这个线就是比分用的，比如小明总分500分，击败了全班80%的童鞋，这种直线及文字。也没啥内容，主要就是绘制直线，要求变换直线宽度及颜色，然后在线的两端添加文字，然后在底部添加必要文字。（PS：这儿的底部文字可以设置的，我直接写死了，其实也可以当参数传进来） 这个方法里面也用到了几个小方法，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 写字居中偏移方法 * @param g 绘图参数 * @param font 字体 * @param color 字体颜色 * @param point 绘制点 * @param value 绘制文本内容 * @param dx x偏移量 * @param dy y偏移量 如果x，y偏移量不设置理论上位于绘点中心 */private void drawWord(Graphics2D g,Font font,Color color,Point point,String value,double dx,double dy) &#123; FontRenderContext context = g.getFontRenderContext(); Rectangle2D bounds = font.getStringBounds(value, context); g.setFont(font); g.setColor(color); double x = (bounds.getWidth()) / 2; double y = (bounds.getHeight()) / 2; g.drawString(value, (int) (point.x - x+dx), (int) (point.y + y+dy));&#125;/** * 特殊文字的文字绘制 */private void drawWordNormal(Graphics2D g,Font font,Color genColor,Color speColor,Point point,String firstValue,String cenVal,String endValue) &#123; FontRenderContext context = g.getFontRenderContext(); Rectangle2D bounds1 = font.getStringBounds(firstValue, context); Rectangle2D bounds2 = font.getStringBounds(cenVal, context); //Rectangle2D bounds3 = font.getStringBounds(endValue, context); double x1 = bounds1.getWidth(); double x2 = bounds2.getWidth(); g.setFont(font); g.setColor(genColor); g.drawString(firstValue, (int) (point.x), (int) (point.y)); //画完后 再画特殊文字 再补上后面的内容 g.setColor(speColor); g.drawString(cenVal, (int) (point.x+x1), (int) (point.y)); g.setColor(genColor); g.drawString(endValue, (int) (point.x+x1+x2), (int) (point.y));&#125;/** * 绘制小箭头 * @param g 绘图参数 * @param color 箭头填充色 * @param point 箭头顶点 */private void drawArrow(Graphics2D g,Color color,Point point) &#123; g.setColor(color); Polygon p = new Polygon(); p.addPoint(point.x,point.y); p.addPoint(point.x-10,point.y-20); p.addPoint(point.x,point.y-15); p.addPoint(point.x+10,point.y-20); g.fillPolygon(p);&#125; 相当于对一些细节的操作优化，这儿就不一一介绍了。 然后在程序出图之前，我们先用Java中比较古老的JFrame看看图片的效果，方便调试“修图”。 在RadarMap里添加如下方法，这个方法只有JFrame测试时用到，当图调好了就可以删了，如下： 123456789/** * 画雷达图主方法 这个方法主要是为了显示在jpanel上用，实际中主调 */public void drawRadarMap() &#123; drawCircles(); drawSpokes(); addWord(); drawLineAndWord();&#125; 很简单，依次调用上面的几个方法。 我们新建一个JFrame Test类，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * 测试类，可以把生成的图放到Jpanel可视化界面上，便于调试 * * @author zwt * */public class MyTest extends JFrame &#123; MyPanel mp = null; public static void main(String[] args) &#123; MyTest demo01 = new MyTest(); &#125; public MyTest() &#123; mp = new MyPanel(); this.add(mp); this.setSize(600, 700); this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); this.setVisible(true); &#125;&#125;// 定义一个MyPanel面板，用于绘图区域class MyPanel extends JPanel &#123; // 覆盖JPanel // Graphics 是绘图的重要类，可以理解成一支画笔 public void paint(Graphics g) &#123; // 1. 调用父类函数完成初始化任务 // 这句话不可以少 super.paint(g); List&lt;RadarMapData&gt; dataList = new ArrayList&lt;RadarMapData&gt;(); RadarMapData map1 = new RadarMapData("100","150", "", "数学", ""); RadarMapData map2 = new RadarMapData("120","150", "", "语文", ""); RadarMapData map3 = new RadarMapData("90","150", "", "英语", ""); RadarMapData map4 = new RadarMapData("80","100", "", "物理", ""); RadarMapData map5 = new RadarMapData("95","100", "", "化学", ""); RadarMapData map6 = new RadarMapData("88","100","", "生物", ""); RadarMapData map7 = new RadarMapData("66","100", "", "历史", ""); RadarMapData map8 = new RadarMapData("77","100", "", "政治", ""); RadarMapData map9 = new RadarMapData("45","100", "", "地理", ""); RadarMapData map10 = new RadarMapData("88","100", "", "音乐", ""); RadarMapData map11 = new RadarMapData("80","100", "", "体育", ""); RadarMapData map12 = new RadarMapData("100","100", "", "美术", ""); dataList.add(map1); dataList.add(map2); dataList.add(map3); dataList.add(map4); dataList.add(map5); dataList.add(map6); dataList.add(map7); dataList.add(map8); dataList.add(map9); dataList.add(map10); dataList.add(map11); dataList.add(map12); int lineMaxValue = 0; int currentValue = 0; for(RadarMapData data:dataList)&#123; lineMaxValue+=Integer.valueOf(data.getMaxValue()); currentValue+=Integer.valueOf(data.getValue()); &#125; RadarMapInit init = new RadarMapInit(); init.setCirNum(5);//20分一档，可以自己设置 init.setCenText(currentValue+"分"); init.setSpeLineValue("60%");//击败多少童鞋先瞎传一个数 init.setG((Graphics2D) g); //init.setImgPath("/Users/zhangwentong/Desktop/image.jpg"); init.setLineCurVal(currentValue); init.setLineMaxVal(lineMaxValue); RadarMap test = new RadarMap(dataList, init); test.drawRadarMap(); //test.createImage(); &#125;&#125; 这个Test主要就是给一些参数赋值。然后测试，可以看到结果如下图所所示，哈哈，还是蛮不错的。 这我们只在JFrame上测试了，我们的任务是生成图片，然后开始吧，在RadarMap里添加如下方法： 123456789101112131415161718192021222324252627282930313233/** * 生成图片文件 */public boolean createImage() &#123; boolean flag = false; try &#123; File file = new File(init.getImgPath()); int width=init.getPicWidth(); int height=init.getPicHeight(); BufferedImage bi = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D g2 = (Graphics2D) bi.getGraphics(); //设置抗锯齿属性，不然图片锯齿化很模糊 g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING, RenderingHints.VALUE_ANTIALIAS_ON); g2.setRenderingHint(RenderingHints.KEY_TEXT_ANTIALIASING, RenderingHints.VALUE_TEXT_ANTIALIAS_ON); g2.setBackground(init.getPicBackColor()); g2.clearRect(0, 0, width, height); init.setG(g2); //画圆 drawCircles(); //画射线及添加圆边的文字图片 drawSpokes(); //添加中心文字 addWord(); //添加下面那条线及内容 drawLineAndWord(); flag = ImageIO.write(bi, init.getImgType(), file); &#125; catch (Exception e) &#123; flag = false; e.printStackTrace(); &#125; return flag;&#125; 这个方法主要就是利用了BufferedImage来生成一张图片。我们把刚才JFrame里的两行注掉的代码开启。 12//init.setImgPath("/Users/zhangwentong/Desktop/image.jpg");//test.createImage(); 再次启动JFrame类，可以看到生成图片啦。 哈哈，蛮不错的…… 然后我们把你的分数……这句话的你动态传入姓名（比如小红……）。然后新建分数MapTest如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * @Author: zwt * @Description: 总分雷达图 * @Name: ScoreMapTest * @Date: 2019/1/5 11:49 PM * @Version: 1.0 */public class ScoreMapTest &#123; private static final String [] names = &#123;"小一","小二","小三","小四","小五","小六","小七","小八","小九","小十"&#125;; //随机分数使用 private static int randomValue(int maxValue)&#123; Random random = new Random(); return random.nextInt(maxValue); &#125; public static void drawMapPic()&#123; for(String name:names)&#123; List&lt;RadarMapData&gt; dataList = new ArrayList&lt;RadarMapData&gt;(); RadarMapData map1 = new RadarMapData(randomValue(150)+"","150", "", "数学", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map2 = new RadarMapData(randomValue(150)+"","150", "", "语文", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map3 = new RadarMapData(randomValue(150)+"","150", "", "英语", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map4 = new RadarMapData(randomValue(100)+"","100", "", "物理", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map5 = new RadarMapData(randomValue(100)+"","100", "", "化学", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map6 = new RadarMapData(randomValue(100)+"","100","", "生物", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map7 = new RadarMapData(randomValue(100)+"","100", "", "历史", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map8 = new RadarMapData(randomValue(100)+"","100", "", "政治", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map9 = new RadarMapData(randomValue(100)+"","100", "", "地理", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map10 = new RadarMapData(randomValue(100)+"","100", "", "音乐", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map11 = new RadarMapData(randomValue(100)+"","100", "", "体育", "/Users/zhangwentong/Desktop/book.png"); RadarMapData map12 = new RadarMapData(randomValue(100)+"","100", "", "美术", "/Users/zhangwentong/Desktop/book.png"); dataList.add(map1); dataList.add(map2); dataList.add(map3); dataList.add(map4); dataList.add(map5); dataList.add(map6); dataList.add(map7); dataList.add(map8); dataList.add(map9); dataList.add(map10); dataList.add(map11); dataList.add(map12); int lineMaxValue = 0; int currentValue = 0; for(RadarMapData data:dataList)&#123; lineMaxValue+=Integer.valueOf(data.getMaxValue()); currentValue+=Integer.valueOf(data.getValue()); &#125; RadarMapInit init = new RadarMapInit(); init.setCirNum(5);//20分一档，可以自己设置 init.setCenText(currentValue+"分"); init.setSpeLineText(name); init.setSpeLineValue("80%");//这个数可以计算，我这儿就不计算了（因为分数是随机的，计算比较蛋疼） init.setImgPath("/Users/zhangwentong/Desktop/map/image"+name+".jpg"); init.setLineCurVal(currentValue); init.setLineMaxVal(lineMaxValue); RadarMap test = new RadarMap(dataList, init); test.createImage(); &#125; &#125; public static void main(String[] args) &#123; drawMapPic(); &#125;&#125; 同时添加图标（我这里只添加了一个相同的，不同的也是可以的），新建map文件夹（用于存放雷达图）。如下： 运行测试类。打开map文件夹。 随便选一张查看。 总结通过使用Java2D绘制雷达图，学到了Java2D的一些用途吧，虽然Java2D使用的很少，而且以后估计用的概率也不大，但是，我们就当一次对于Java程序的自娱自乐吧！ PS：Java图像处理方面确实很差，因为它的着重点不是这儿，而是大型Web项目，这篇文章的目的也不在于去理解Java2D的一些用途，而是通过一些学习，让我们知道，Java也是可以做一些莫名其妙的事情的，虽然不尽人意，也是，世界上哪有一种编程语言是完美的呢？]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>雷达图</tag>
        <tag>Graphics2D</tag>
        <tag>Java绘图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之注册中心]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83.html</url>
    <content type="text"><![CDATA[前言Zookeeper是可以实现注册中心相关功能的。 何为注册中心？ 我们知道，随着系统业务扩张，为了满足业务规模要求，引入了分布式、微服务等相关技术。引入它们后，相关的进程通信变为了网络通信。进而出现了服务调用方和服务被调用方，由于被调用方提供的服务地址在分布式环境下不是唯一的，因而需要对它们进行统一管理，这个管理的模块称之为注册中心。 可以参照如下图。 Provider代表服务提供方，由于有多个服务地址（分布式情形下，这里简化为一个），均需要向Registry模块注册自己的信息，Invoker代表服务调用方，它通过注册中心的通知或者主动订阅获得服务信息，进而发起请求。其中Registry模块即为注册中心。 注册中心作为服务框架核心模块，它是服务框架唯一核心链路上的一个集中点，所以它的好坏也影响着整个服务框架的可用性以及稳定性。 注册中心的话大部分公司都使用的开源实现，Dubbo体系中使用Zookeeper的居多，SpringCloud体系中使用Eureka的居多。 我们今天来看一下一些好的开源实现吧。 正文我们使用Spring Initializr 初始化一个集成了Zookeeper注册中心的SpringBoot项目。 如图，选择Zookeeper作为注册中心及服务发现。 完成后在pom文件里会有如下依赖。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zookeeper-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 我们来分析下Spring团队是如何使用Zookeeper作为注册中心的。 找到这个jar包，目录结构如下图，可以清楚的明白。 discovery是用于服务发现的包。 serviceregistry是用于服务注册的包。 support包包含一些工具类等信息。 先来看看serviceregistry包下的类。 ServiceInstanceRegistration类是一个服务注册信息bean。其他带AutoConfiguration的是结合properties文件进行自动配置的类，也不做过多介绍。我们来看下ZookeeperServiceRegistry这个服务注册关键类。 它的主要方法如下： 可以看到实现了register（服务注册），getServiceDiscovery（获取服务发现者），deregister（服务解绑）等方法。 继续查看两个关键方法服务注册和解绑。 可以看到它是通过一个叫ServiceDiscoveryImpl类进行实现的。 可以发现这个类在 curator-x-discovery包下。 这个包就是对Zookeeper实现服务注册与发现的一套封装。 这个jar包可以看到internalRegisterService这个方法，如果在缓存里不存在此服务的话就会去创建一个，可以看到它的逻辑，最大重试三次创建节点，可以选择创建节点的类型（服务类型），如果出现异常要删除创建的节点。 也可以看到这个类使用了ConcurrentMap作为数据缓存。 这个类里面的start方法会注册所有服务。 而后最开始的ZookeeperServiceRegistry会调用此方法进而注册服务。 再来看一下discovery（服务发现）包。 这个包里面内容较多，我们只看下比较关键的类ZookeeperDiscoveryClient即可。 先看看它的getInstances方法，通过serviceId获取一个Service实例。 在ServiceDiscoveryImpl中的方法如下： 可以看到会调用queryForInstances方法，通过name拿到Zookeeper的path，然后获取该path上的所有instancesIds，在通过name和id调用queryForInstances方法查询具体的实例信息。 我们创建一个hello项目，加入Web及Zookeeper Discovery 依赖后，进行必要配置，启动项目，便可以在Zookeeper里看到我们注册的服务信息。 application.properties配置 12345spring.cloud.zookeeper.discovery.register=truespring.cloud.zookeeper.discovery.root=/testspring.cloud.zookeeper.connect-string=127.0.0.1:2181spring.application.name=testserver.port=8081 HelloController内容 1234567RestControllerpublic class HelloController &#123; @RequestMapping("/hello") public String hello()&#123; return "hello"; &#125;&#125; 启用服务发现。 启动项目后通过客户端连接到Zookeeper可以看到服务节点信息。 PS：启动过程中如果出现错误，可能是Zookeeper客户端版本与服务端版本不一致，一般为客户端版本较高出现的问题，应当注意。 我们继续新建一个Test类，如下： 1234567891011121314151617@RunWith(SpringRunner.class)@SpringBootTestpublic class HelloApplicationTest &#123; @Autowired RestTemplate restTemplate; @Autowired DiscoveryClient client; @Test public void contextLoads() &#123; List&lt;String&gt; list=client.getServices(); for(String str:list)&#123; ServiceInstance instance=client.getInstances(str).get(0); String result=restTemplate.getForEntity(instance.getUri(),String.class).getBody(); System.out.println(result); &#125; &#125;&#125; 可以看到获得的ServiceInstance信息如下。 使用restTemplate发送请求，获得结果。 使用RestTemplate时，应当在启动时声明这个Bean，如下。 1234@BeanRestTemplate restTemplate()&#123; return new RestTemplate();&#125; 如果服务调用时出现异常，可能是host的问题，比如我上面的host为DELL-3020-PC，如果不在hosts文件里指定其解析为127.0.0.1，那么请求会出现异常，应当注意。 说到这里，其实Zookeeper作为注册中心的本质，就是把服务的一些信息（host，port，URI，serviceName等等）以顺序临时节点的形式存储在Zookeeper上，通过主动拉取或者通知的方式获取服务信息，服务调用者拿到信息后进行调用服务提供方。 因此我们也可以自己基于Zookeeper实现一个注册中心，但是根据上面我们可以知道，注册中心可以说是系统中的关键部分，如果出现问题，可能导致系统服务不可用，出现严重生产事故，因此，注册中心必须保证高可用性、高性能、实时性（如有服务出现问题，应该从注册中心剔除）等特点。 因此我们现在使用一些开源代码，如Erueka Discovery、Zookeeper Discovery等，毕竟这些代码已经经过了无数人的测验。 我们也应该多学习关于注册中心这一方面的知识，争取可以造一个属于自己的“轮子”。 总结通过对 Spring Cloud 里 Zookeeper Discovery代码的部分分析，了解了注册中心的大致原理，明白了Zookeeper作为注册中心的原理，对于我们来说，也是蛮不错的一次学习过程。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>注册中心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2Fblog%2FHTTP%E5%8D%8F%E8%AE%AE.html</url>
    <content type="text"><![CDATA[前言超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，所有的WWW文件都必须遵守这个标准。 今天我们来详细了解下它的一些特点及Java里对此协议的解析处理。 正文我们发送一个HTTP 请求，在浏览器中可以看到HTTP协议的一些信息。 Request Headers： POST /test HTTP/1.1 POST 表示HTTP的请求方法类型。 /test 表示请求URI。 HTTP/1.1 表示协议和协议版本。 说明：HTTP协议里目前定义了8中请求方法类型。 OPTIONS：返回服务器针对特定资源所支持的HTTP请求方法，也可以利用向web服务器发送“*”的请求来测试服务器的功能性。 HEAD：服务器返回与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以再不必传输整个响应内容的情况下，就可以获取包含在响应小消息头中的元信息。 GET：向特定的资源发出请求。它本质就是发送一个请求来取得服务器上的某一资源。 POST：向指定资源提交数据进行处理请求，数据被包含在请求体中，POST请求可能会导致新的资源的建立或已有资源的修改。 PUT：向指定资源位置上传其最新内容。 DELETE：请求服务器删除指定的资源。 TRACE：回显服务器收到的请求，主要用于测试或诊断 CONNECT：HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 Host: localhost:8080 表示服务器域名地址 Connection: keep-alive 表示连接类型，一直保持连接。 Content-Length: 11 表示参数长度 Accept: application/json, text/javascript, */*; q=0.01 表示接受的数据类型，逗号分隔，优先支持程度从前到后，q表示权重， 0&lt;=q&lt;=1， 权重越大，分号前面这一项支持度越好，0表示不支持，不指定q值默认为1。 Origin: http://localhost:8080 表示源数据地址。 X-Requested-With: XMLHttpRequest XMLHttpRequest对象，可以实现局部更新，ajax使用。 User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1 表示用户代理信息，浏览器信息等。 Content-Type: application/x-www-form-urlencoded; charset=UTF-8 提交数据类型，编码格式。 Referer: http://localhost:8080/ 表示引用源地址 Accept-Encoding: gzip, deflate, br 表示浏览器支持的压缩编码格式。当前浏览器支持 gzip, deflate, br 三种压缩算法。 Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 可以接受的语言。 Cookie: Idea-6ad4ef07=ced8796b-12ab-4db6-9bd7-556be6c5132b; JSESSIONID=3F1698BD54B15FFE91EC948154761956 Cookie信息。 Status Code: 200 状态码。 目前总共有41种状态码。 在HttpServletResponse由定义，有兴趣的可以看看。 主要状态码及含义如下： 状态码 含义 100 表示客户端可以继续 101 表示服务器正在根据升级头切换协议 200 表示请求成功 201 表示请求成功，并在服务器上创建了一个新资源 202 表示服务器接受了处理请求，但未完成 203 表示客户端提供的元信息并非来自服务器 204 表示请求成功，但没有返回新信息 205 表示代理可能被重置导致发送了文档视图 206 表示服务器已经完成了对资源的部分GET请求 300 表示所请求的资源对应一组资源，组里每一种表示都有其特定的位置 301 表示资源已经永久地移动到一个新的位置，并且将来的请求应使用一个新的URI 302 表示资源暂时移动到其它的URI下 303 表示可以在其它的URI下找到对请求的响应 304 表示GET操作发现资源可用且未修改 305 表示资源必须通过代理访问 307 表示资源暂时在其它的URI下，但是这个URI地址应该在响应的时候给出 400 表示客户端发送的请求语法错误 401 表示请求需要HTTP身份验证 402 保留以备将来使用 403 表示服务器接受了请求，但拒绝执行 404 表示请求的资源不可用或者找不到 405 表示对于要请求的资源，请求中指定的方法是不允许的 406 表示请求标识的资源仅能够根据请求中发送的accept标头生成具有不可接受的内容特征的响应实体 407 表示客户端必须先通过代理进行身份验证 408 表示客户端在服务器准备等待的时间内没有生成请求 409 表示由于与资源的当前状态发生冲突，请求无法完成 410 表示该资源在服务器上不再可用，且不知道转发地址 411 表示如果没有定义Content-Length，则无法处理请求 412 表示在服务器上测试时，在一个或多个请求头字段中给出的前提条件评估不通过 413 表示服务器拒绝处理请求，因为请求实体大于服务器能够处理的大小 414 表示服务器拒绝服务请求，因为request-uri长度超过了服务器能够处理的长度 415 表示服务器拒绝服务请求，因为请求的实体的格式不受请求方法的请求资源支持 416 表示服务器不能为请求的字节范围提供服务 417 表示服务器不能满足Expect请求头中给出的期望 500 表示服务器内部的一个错误，它阻止了服务器完成请求 501 表示服务器不支持满足请求所需的功能 502 表示服务器作为代理或网关服务器时接收到了无效响应 503 表示服务器暂时过载，无法处理请求 504 表示服务器作为网关或代理服务器时没有及时收到来自上游服务器的响应 505 表示服务器不支持或拒绝支持请求消息中使用的HTTP协议版本 我们来看一下后端Java部分的一些处理吧。 找到HttpServlet，可以看到它定义的几种请求方法。 同时GET，POST，PUT，DELETE都需要被实现。HEAD、OPTIONS、TRACE已被实现。 我们根据上面的知识，可以自己实现一个HttpServlet，同时可以实现其它的方法，如下： 我们新建HttpTestServlet 抽象类，新增方法 doCompress。 这样我们写一个测试Servlet，实现doCompress方法。 发送ajax，type 为 COMPRESS。 如下：可以看到请求方法变成了COMPRESS。 总结通过对HTTP协议的理解，加深了对HTTP协议的认识与认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之分布式锁]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81.html</url>
    <content type="text"><![CDATA[前言Zookeeper是可以用来实现分布式锁的。 要了解它，我们先简单说下分布式锁吧。 我们知道，在系统中，当我们访问公共资源并对资源进行一些操作时，为防止出现问题，需要对公共资源依次访问，如常见的多线程售票模型等。由于在一个系统中，我们可以使用锁（如ReentrantLock）或者synchronized关键字等Java方法处理。 但是，当系统逐渐由单系统转换为分布式系统、微服务时，情况就变得复杂了，比如有某共享资源，比如有1个奖品，一个应用查询到有奖品并尝试发给用户A，另一个应用也查询到有奖品并尝试发给用户B，这样A，B均显示有奖品，实际上我们的奖品数量是不足的。如常见的秒杀系统，抽奖系统等。 这时候就需要一种全局的互斥机制来控制应用对共享资源的访问，这就是所谓的分布式锁。 PS：分布式锁的实现也可以基于缓存（如Redis）实现，亦可以通过数据库（乐观锁等）实现，实际中要确实使用到分布式锁，基于缓存的实现还是要偏多一些的。 根据上面所述，下面的图是比较好理解的。 我们再来说下分布式锁应具备的一些特点。 同一时间只允许一台机器（服务）的一个线程执行。 为整个系统必要业务提供服务，应当是高可用的。 性能应得以保证，不能在获取锁和释放锁过程中浪费太多资源或时间。 分布式锁应当具备失效机制，避免死锁发生。 应当具有可重入特性。 应当有非阻塞的特点，某个服务没有获取到锁，应返回获取失败，不能阻塞。 下面我们来用Zookeeper实现我们的分布式锁。 正文我们在之前封装framework-zookeeper时，用到了下面这个依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 这里面有一个类InterProcessMutex，这是分布式锁使用的关键类。 PS：其实它已经实现了分布式锁，我们来使用下它吧。 我们创建一个Test，如下： 1234567891011121314151617181920212223242526@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; ExecutorService executorService = Executors.newCachedThreadPool(); static int TEST = 5; @Test public void test1() throws Exception&#123; for(int i=0;i&lt;10;i++)&#123; executorService.execute(()-&gt;&#123; try&#123; if(TEST&gt;0)&#123; doSomething(); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125; Thread.sleep(100000);&#125;public void doSomething() throws Exception&#123; Thread.sleep(1000); TEST--; System.out.println(Thread.currentThread().getId()+"--"+Thread.currentThread().getName()+"执行,TEST="+TEST); &#125;&#125; 考虑到多应用分布式比较麻烦，我这里的Test使用多线程模拟分布式请求，用了线程池管理了10个线程，假设TEST静态变量为共享资源，如果TEST数量大于0的时候，我们执行doSomething方法，假设这个方法执行需要一秒，执行后TEST减一。 我们运行Test后，可以看到输出结果。 可以看到出现了负数。 我们引入InterProcessMutex，在判断TEST之前对其加分布式锁，锁的zk基路径我们定为/zwt/lock。调用acquire获取锁，完成业务逻辑后调用release方法释放锁。 1234567891011121314151617181920@AutowiredCuratorZKClient client;@Testpublic void test1() throws Exception&#123; for(int i=0;i&lt;10;i++)&#123; executorService.execute(()-&gt;&#123; try&#123; InterProcessMutex interProcessMutex = new InterProcessMutex(client.getCuratorFramework(),"/zwt/lock"); interProcessMutex.acquire(); if(TEST&gt;0)&#123; doSomething(); &#125; interProcessMutex.release(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125; Thread.sleep(100000);&#125; 运行Test查看结果。 可以看到结果有序的减一最后到0结束。说明了InterProcessMutex实现了我们的分布式锁的功能。 它是如何实现的呢？ 我们在上面的doSomething代码里加一些输出。如下： 12345678910public void doSomething() throws Exception&#123; Thread.sleep(1000); List&lt;String&gt; list=client.getNodes("/zwt/lock"); list.forEach((e)-&gt;&#123; System.out.print(e+" "); &#125;); System.out.println(); TEST--; System.out.println(Thread.currentThread().getId()+"--"+Thread.currentThread().getName()+"执行,TEST="+TEST);&#125; 继续测试，如下结果： 数据大致如下： 可以看到，Zookeeper创建了10个临时顺序节点，每次会找到最小的节点并删除。其实这就是InterProcessMutex这个类的实现分布式锁的原理。 我们可以看下它的相关代码。 我们从acquire方法看起，调用了internalLock方法，而后调用了attemptLock方法，这个方法会通过createsTheLock去创建锁。 可以看到createsTheLock方法里创建了临时有序节点。 再看下internalLockLoop这个方法，有些大。 可以看到会拿到有序的子节点，getSortedChildren。 然后尝试去获取锁（从最小的节点开始），getsTheLock会先获取比自己小的节点，要是自己是最小的节点就会获得锁。 拿到后就设置haveTheLock为true，没有拿到，就添加watcher，监听比自己小的节点。然后根据设置的等待时间判断是否超时从而进行等待或者退出。 最后，如果到了时间或者出现异常，doDelete为true，就会删除节点。 再来看下release方法，里面的主要方法releaseLock，可以看到它调用了上面的deleteOurPath方法删除创建的临时节点。 在锁的获取和释放方法里可以看到下面这些地方，它可以保证我们的分布式锁具有可重入的性质。其通过lockCount（AtomicInteger ）实现的，统计重入次数。 1final AtomicInteger lockCount = new AtomicInteger(1); Zookeeper分布式锁的基本内容就是这些了，我们来总结下Zookeeper分布式锁的步骤： 指定一个存放锁目录（这儿我们指定的/zwt/lock）。 线程A想要获取锁，就需要在该目录下创建临时有序节点。 获取该目录下的所有子节点，然后获取比自己小的兄弟节点，如果不存在，说明自己是最小节点，那么就去获得锁。 线程B同线程A，创建好节点后获取目录下所有子节点，判断自己不是最小的，就会对获得锁的节点添加监听。 线程A处理完后释放锁，删除自己的节点，并通知，线程B监听后判断自己是不是最小节点，是的话会获取锁，不是的话在添加对当前获得锁的线程的监听。 通过上面我们可以看到Zookeeper分布式锁的一些优点，如高可用性（由Zookeeper保证）、可重入性、不会出现死锁（临时节点程序出现异常断开连接后会被删除也就失去了锁）等。 和一些缺点，如需要创建临时节点、删除临时节点，性能上肯定有一些影响。 其它我们可以在对其进行简单封装形成自己的分布式锁工具类。 相关代码如下： 提供一个分布式锁的接口。 1234567891011121314151617181920public interface DistributedReentrantLock &#123; /** * 获得锁 * @return * @throws InterruptedException */ boolean tryLock() throws InterruptedException; /** * 获得锁 * @param timeout * @param unit * @return * @throws InterruptedException */ boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException; /** * 解除锁 */ void unlock();&#125; 同时我们使用Zookeeper的InterProcessMutex去完成相关实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public class ZKDistributedReentrantLock implements DistributedReentrantLock &#123; private static final Logger logger = LoggerFactory.getLogger(ZKDistributedReentrantLock.class); /** * 线程池 */ private static final ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor( 10, new BasicThreadFactory.Builder().namingPattern("scheduled-pool-%d").build() ); /** * 所有锁的根节点 */ public static final String ROOT_PATH = "/LOCK/"; /** * 每次延迟清理PERSISTENT节点的时间 毫秒 */ private static final long DELAY_TIME_FOR_CLEAN = 1000; /** * zk 共享锁实现 */ private InterProcessMutex interProcessMutex; /** * 锁的ID,对应zk一个PERSISTENT节点,下挂EPHEMERAL节点. */ private String path; /** * zk的客户端 */ private CuratorFramework client; private volatile boolean isLockSuccess; public ZKDistributedReentrantLock(CuratorFramework client, String lockId) &#123; this.client = client; this.path = ROOT_PATH + lockId; interProcessMutex = new InterProcessMutex(client, this.path); &#125; public ZKDistributedReentrantLock(CuratorZKClient zkClient, String lockId) &#123; this.client = zkClient.getCuratorFramework(); this.path = ROOT_PATH + lockId; interProcessMutex = new InterProcessMutex(client, this.path); &#125; /** * 获取锁 * @return * @throws InterruptedException */ @Override public boolean tryLock() throws InterruptedException &#123; return tryLock(-1, null); &#125; /** * 获取锁 * @param timeout * @param unit * @return * @throws InterruptedException */ @Override public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; try &#123; isLockSuccess = interProcessMutex.acquire(timeout, unit); logger.debug("&#123;&#125; lock result:&#123;&#125;",this.path,isLockSuccess); return isLockSuccess; &#125; catch (InterruptedException e) &#123; throw e; &#125; catch (Exception e) &#123; logger.error(e.getMessage(),e); throw new RuntimeException(e.getMessage(),e); &#125; &#125; /** * 释放锁 */ @Override public void unlock() &#123; if(isLockSuccess) &#123; try &#123; isLockSuccess = false; interProcessMutex.release(); &#125; catch (Throwable e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; executorService.schedule(new Cleaner(client, path), DELAY_TIME_FOR_CLEAN, TimeUnit.MILLISECONDS); &#125; logger.debug("&#123;&#125; success unlock.",this.path); &#125; &#125; static class Cleaner implements Runnable &#123; private CuratorFramework client; private String path; public Cleaner(CuratorFramework client, String path) &#123; this.client = client; this.path = path; &#125; @Override public void run() &#123; try &#123; List list = client.getChildren().forPath(path); if (list == null || list.isEmpty()) &#123; client.delete().forPath(path); &#125; &#125; catch (KeeperException.NoNodeException | KeeperException.NotEmptyException e1) &#123; //nothing &#125; catch (Exception e) &#123; //准备删除时,正好有线程创建锁 logger.error(e.getMessage(), e); &#125; &#125; &#125;&#125; 实现还是比较简单的，tryLock方法主要就是使用了interProcessMutex的acquire方法，成功后记录isLockSuccess状态，失败后除了调用release方法、把isLockSuccess变为false外，还应尝试清除刚才已经创建的业务lockId节点(线程池)。 测试相关代码： 12345678910111213ZKConfig zkConfig = new ZKConfig();//....CuratorZKClient zkClient=new CuratorZKClient(zkConfig);DistributedReentrantLock lock = new ZKDistributedReentrantLock(zkClient,"test");try&#123; if(lock.tryLock())&#123; //doSomething &#125;&#125;catch (Exception e)&#123; //...&#125;finally &#123; lock.unlock();&#125; 总结通过对Zookeeper实现分布式锁的学习理解，我们又看到了Zookeeper的另外一个用途，对Zookeeper有了更深入的理解，也是蛮不错的一次学习体验。 有时间我会在研究下其它方式实现的分布式锁及其一些特点。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>分布式锁</tag>
        <tag>InterProcessMutex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用之配置中心]]></title>
    <url>%2Fblog%2FZookeeper%E5%BA%94%E7%94%A8%E4%B9%8B%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83.html</url>
    <content type="text"><![CDATA[前言根据Zookeeper的一些特点，它是可以作为配置中心使用的。 何为配置中心？ 我们在项目开发维护过程中会有很多公共变量或资源，需要统一管理，以前我们把它们写在程序公共类或者配置文件中，可是这样以后有变动，程序就需要重新部署，很是不方便，而且分布式、微服务等技术出现，修改维护多个项目管理也变得复杂。 为了解决以上问题，实现一次打包多地部署需求，减少项目管理及安全风险，我们需要将可变变量外移并通过页面统一可视化管理，基于此，我们统一建设了配置中心。 引入Zookeeper后，我们把数据存放在Zookeeper节点Znode上，可以选择主动轮询查询或者等待Zookeeper通知。当数据发生变化时，我们可以直接通过通知去执行某些业务操作。一般为了数据准确性，我们会主动轮询查询或者通知+轮询的方式。 PS：当然使用数据库保存这些数据也是可以的，采用定期查询的方式，而且有的配置中心就与之类似，我们在这儿不做更广泛讨论。 分析注册中心可以分为服务端和客户端两部分。 服务端一般用于存储配置数据，提供数据管理等等服务。客户端一般为业务端调用数据提供API服务等。 当然现在有一些开源的配置中心，如spring-cloud-config，diamond，disconf，apollo 等，我们以后有接触在具体介绍研究它们。 今天我们基于Zookeeper实现自己的一个简单的注册中心。 如下的配置中心流程图也就比较好理解了。 正文先来实现基于Zookeeper的配置中心客户端吧。 PS：了解这篇文章之前可以先看看 Zookeeper Java客户端Curator 先来了解下curator-recipes 包下的一个类PathChildrenCache。 该类是从本地缓存ZK路径的所有子路径中保存所有数据的一个工具类，它将监视ZK路径、响应更新/创建/删除事件、下拉数据等等，此类不能保证事务处理时的强同步。 这个类有一个全参构造方法： 1public PathChildrenCache(CuratorFramework client, String path, boolean cacheData)&#123;...&#125; Client是我们的ZKClient需要创建，path指要监控的路径，cacheData指是否缓存数据。 同时我们可以为其添加Listener，当节点/子节点数据有变化时，可以进行通知等。 使用该方法： 1pathChildrenCache.getListenable().addListener(pathChildrenCacheListener); 我们想实现自己的配置中心客户端，与SpringBoot进行集成，其目录结构如下创建： ConfigCenterAutoConfig：SpringBoot自动配置类，会提供相应的Bean。 ConfigCenterConfiguration：自动配置类，从properties文件中获取配置属性。 ConfigCenterException：异常处理类。 ConfigCenterListener：配置中心监听listener。 ConfigCenterListenerAdapter：考虑到监听可以有多个，这个类用来处理它们。 LocalCacheService：主要用来定时轮询Zookeeper的配置。 ZKConfigService：主要用来创建Zookeeper连接及添加监听等。 ConfigUtil：工具类。 CacheNodeVo：节点Vo。 ConfigCenterClient：配置中心客户端。 先从配置类说起吧，连接配置文件properties的类ConfigCenterConfiguration。 1234567891011121314@ConfigurationProperties("spring.zookeeper.config-center")public class ConfigCenterConfiguration &#123; //zk地址 private String zkAddress; //业务名称 private String sysName; //连接超时时间 private Integer connectTimeoutMs = 60000; //session过期时间 private Integer sessionTimeoutMs = 60000; //重试间隔 private Integer retryInterval = 1000; ......&#125; 这个类不过多介绍了，就是Zookeeper的配置信息，连接Zookeeper时使用。 我们引入之前封装的framework-zookeeper包，通过自动配置拿到client。 12345&lt;dependency&gt; &lt;groupId&gt;com.zwt&lt;/groupId&gt; &lt;artifactId&gt;framework-zookeeper&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; ConfigCenterAutoConfig部分代码如下，也比较好理解，就是Spring启动后自动配置CuratorClient。 12345678910111213141516171819202122232425262728293031323334353637@Configuration@ConditionalOnClass(CuratorZKClient.class)@EnableConfigurationProperties(ConfigCenterConfiguration.class)public class ConfigCenterAutoConfig implements ApplicationContextAware &#123; private static final Logger log= LoggerFactory.getLogger(ConfigCenterAutoConfig.class); @Autowired private ConfigCenterConfiguration properties; @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = "spring.zookeeper.config-center", name = "enabled", havingValue = "true") CuratorZKClient curatorZKClient ()&#123; ZKConfig config = new ZKConfig(); config.setConnectString(properties.getZkAddress()); config.setNameSpace(properties.getSysName()); config.setSessionTimeoutMs(properties.getSessionTimeoutMs()); config.setConnectTimeoutMs(properties.getConnectTimeoutMs()); config.setRetryInterval(properties.getRetryInterval()); CuratorZKClient zkClient = new CuratorZKClient(config); zkClient.addConnectionListener((state) -&gt; &#123; log.debug("ZKConfigService connectionListener state：" + state); if (state == ZKConstants.State.CONNECTED || state == ZKConstants.State.RECONNECTED) &#123; log.info("ZKConfigService zookeeper is connected..."); &#125; &#125;); zkClient.start(); return zkClient; &#125; private static ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125; //根据class类型返回bean public static &lt;T&gt; T getBean(Class&lt;T&gt; requireType)&#123; return applicationContext.getBean(requireType); &#125;&#125; 有了CuratorClient，我们创建ZKConfigService，主要为指定节点添加希望的监听。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ZKConfigService&#123; private static final Logger log= LoggerFactory.getLogger(ZKConfigService.class); private static CuratorZKClient zkClient = null; final static String configRootPath = ConfigUtil.getConfigCenterPath(); /** * ZKService初始化 */ public static void init() &#123; //拿到zkClient zkClient = ConfigCenterAutoConfig.getBean(CuratorZKClient.class); try &#123; PathChildrenCache pathChildrenCache = new PathChildrenCache(zkClient.getCuratorFramework(), configRootPath, true); PathChildrenCacheListener pathChildrenCacheListener = (client,event) -&gt; &#123; log.debug("pathChildrenCacheListener eventType：" + event.getType()); ChildData data = event.getData(); if(data!=null)&#123; String dataStr = new String(data.getData(), "UTF-8"); String key = StringUtils.substringAfterLast(data.getPath(), ConfigUtil.SEP_STRING); switch (event.getType()) &#123; case CHILD_ADDED: LocalCacheService.put(key,dataStr); break; case CHILD_UPDATED: LocalCacheService.put(key,dataStr); ConfigCenterListenerAdapter.onChange(key,dataStr); break; case CHILD_REMOVED: LocalCacheService.remove(key); break; default: break; &#125; &#125; &#125;; pathChildrenCache.getListenable().addListener(pathChildrenCacheListener); pathChildrenCache.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; log.info("spring-boot-config ZKConfigService init success."); &#125; /** * 根据key获取值 * @param key * @return */ public static String getKey(String key) &#123; return zkClient.getStringData(ConfigUtil.joinPath(configRootPath, key)); &#125;&#125; 可以看到用到了我们刚才说的PathChildrenCache类，启动后，添加一个Listener，监听节点变化，一方面，我们需要一个类，对节点变化进行通知；另一方面，我们应把数据缓存在本地，如果数据变化后ZK没有通知或者其它情况，我们可以轮询查询后与本地缓存比较，有变化后继续进行我们节点变化的通知。 这就是我们的ConfigCenterListenerAdapter监听处理类和LocalCacheService本地缓存服务。 先说ConfigCenterListenerAdapter吧，可以看到上面代码节点有变化时触发了onChange事件。 由于我们业务可能需要多个监听类，故，我们提供一个监听接口，相关业务类实现这个接口，在注册一下监听即可使用岂不美哉。 考虑到此，我们存储监听类的集合应是静态的。如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ConfigCenterListenerAdapter &#123; private static Logger log= LoggerFactory.getLogger(ConfigCenterListenerAdapter.class); //key和要监听此节点的监听者列表 private static ConcurrentHashMap&lt;String, List&lt;ConfigCenterListener&gt;&gt; someKeyListenerMap = new ConcurrentHashMap&lt;&gt;(); private static List&lt;ConfigCenterListener&gt; allKeyListeners = new CopyOnWriteArrayList&lt;&gt;(); public static boolean addListener(String key, ConfigCenterListener configCenterListener) &#123; if (configCenterListener == null) &#123; return false; &#125; if (key == null || key.trim().length() == 0) &#123; allKeyListeners.add(configCenterListener); return true; &#125; else &#123; List&lt;ConfigCenterListener&gt; listeners = someKeyListenerMap.get(key); if (listeners == null) &#123; listeners = new ArrayList&lt;&gt;(); someKeyListenerMap.put(key, listeners); &#125; listeners.add(configCenterListener); return true; &#125; &#125; public static void onChange(String key, String value) &#123; if (key == null || key.trim().length() == 0) &#123; return; &#125; List&lt;ConfigCenterListener&gt; keyListeners = someKeyListenerMap.get(key); if (keyListeners != null &amp;&amp; keyListeners.size() &gt; 0) &#123; for(ConfigCenterListener listener : keyListeners) &#123; try &#123; listener.onChange(key, value); &#125; catch (Exception e) &#123; log.error(e.getMessage(), e); &#125; &#125; &#125; if (allKeyListeners.size() &gt; 0) &#123; for(ConfigCenterListener confListener : allKeyListeners) &#123; try &#123; confListener.onChange(key, value); &#125; catch (Exception e) &#123; log.error(e.getMessage(), e); &#125; &#125; &#125; &#125;&#125; 12345678public interface ConfigCenterListener &#123; /** * 配置的key有变化触发事件 * @param key * @param value */ void onChange(String key, String value) ;&#125; 我们再来看看我们的主动轮询服务。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class LocalCacheService &#123; private static final Logger log= LoggerFactory.getLogger(LocalCacheService.class); //配置缓存信息map private static final ConcurrentHashMap&lt;String, CacheNodeVo&gt; LOCAL_CONFIG_CACHE_MAP = new ConcurrentHashMap&lt;&gt;(); //刷新线程状态 private static boolean refreshThreadStop = false; //一个单线程的线程池（刷新缓存使用） private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder().setNameFormat("spring-boot-config-%d").setDaemon(true).build(); private static ExecutorService singleThreadPool = Executors.newFixedThreadPool(1, namedThreadFactory); //一分钟刷新一次 public static void init() &#123; // refresh thread singleThreadPool.submit(()-&gt;&#123; while (!refreshThreadStop) &#123; try &#123; TimeUnit.SECONDS.sleep(60); reloadAll(); log.debug("spring-boot-config, refresh thread reloadAll success."); &#125; catch (Exception e) &#123; log.error("spring-boot-config, refresh thread error."); log.error(e.getMessage(), e); &#125; &#125; log.info("spring-boot-config, refresh thread stopped."); &#125;); log.info("spring-boot-config LocalCacheService init success."); &#125; /** * 通过key获取值 * @param key * @return */ public static String get(String key) &#123; CacheNodeVo cacheNodeVo = LOCAL_CONFIG_CACHE_MAP.get(key); if (cacheNodeVo != null) &#123; return cacheNodeVo.getValue(); &#125; return null; &#125; /** * 放入值 * @param key * @param value */ public static void put(String key, String value) &#123; LOCAL_CONFIG_CACHE_MAP.put(key, new CacheNodeVo(key, value)); &#125; /** * 移除某个值 * @param key */ public static void remove(String key) &#123; LOCAL_CONFIG_CACHE_MAP.remove(key); &#125; /** * 重新加载全部 */ private static void reloadAll() &#123; Set&lt;String&gt; keySet = LOCAL_CONFIG_CACHE_MAP.keySet(); if (keySet.size() &gt; 1) &#123; for(String key : keySet) &#123; String zkValue = ZKConfigService.getKey(key); CacheNodeVo cacheNodeVo = LOCAL_CONFIG_CACHE_MAP.get(key); if (cacheNodeVo != null &amp;&amp; cacheNodeVo.getValue() != null &amp;&amp; cacheNodeVo.getValue().equals(zkValue)) &#123; log.debug("refresh key:&#123;&#125; no changed ", key); &#125; else &#123; LOCAL_CONFIG_CACHE_MAP.put(key, new CacheNodeVo(key, zkValue)); ConfigCenterListenerAdapter.onChange(key, zkValue); &#125; &#125; &#125; &#125;&#125; 可以看到借助了一个定长线程池，每隔60s重载一下数据，有变化会对监听者进行通知。它是通过一个静态的ConcurrentHashMap 来保存数据的。 这儿看到刚才的也会主动通知监听者，这儿也通知监听者，它们会通知两次吗？ 我们可以看到主动通知的时候，也会先把ConcurrentHashMap的值先改变在进行通知，要是出现通知两次的情况，会是概率极低的。要是要求只能通知一次，且业务监听无法重复处理两次数据变化请求，可以在向ConcurrentHashMap里放值时，再检查一下它的当前值，或使用其它方法处理。 两个服务ZK通知和主动轮询处理完成后，提供一个配置中心Client，用于获取值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ConfigCenterClient &#123; private static final Logger log= LoggerFactory.getLogger(ConfigCenterClient.class); static &#123; LocalCacheService.init(); ZKConfigService.init(); &#125; public static String getString(String key, String defaultValue) &#123; //查询本地缓存 String value = LocalCacheService.get(key); if (value != null) &#123; log.debug("get config &#123;&#125; from cache",key); return value; &#125; //没有命中，查询zk中的值，并加入到缓存中，并加watcher value = ZKConfigService.getKey(key); if (value != null) &#123; log.debug("get config &#123;&#125; from zookeeper",key); LocalCacheService.put(key,value); return value; &#125; return defaultValue; &#125; public static String getString(String key) &#123; return getString(key, null); &#125; private static void checkNull(String key,String value) &#123; if (value == null) &#123; throw new ConfigCenterException(String.format("config key [%s] does not exist",key)); &#125; &#125; public static long getLong(String key) &#123; String value = getString(key, null); checkNull(key, value); return Long.valueOf(value); &#125; public static int getInt(String key) &#123; String value = getString(key, null); checkNull(key, value); return Integer.valueOf(value); &#125; public static boolean getBoolean(String key) &#123; String value = getString(key, null); checkNull(key, value); return Boolean.valueOf(value); &#125; public static boolean addListener(String key, ConfigCenterListener configCenterListener)&#123; return ConfigCenterListenerAdapter.addListener(key, configCenterListener); &#125;&#125; 首先应加载LocalCacheService和ZKConfigService，然后实现主要的方法getString，直接去缓存里取，拿不到去Zookeeper里取并放到缓存里，在提供一个addListener方法，可以让用户自己定义想监听的节点。 至此，我们一个简单的配置中心的客户端就完成了，我们把它打包引入一个demo项目测试一下。 创建一个demo项目，引入我们的jar包。 12345&lt;dependency&gt; &lt;groupId&gt;com.zwt&lt;/groupId&gt; &lt;artifactId&gt;config-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 我们创建一个Listener实现。 1234567public class MyListener implements ConfigCenterListener &#123; private static final Logger log= LoggerFactory.getLogger(MyListener.class); @Override public void onChange(String key, String value) &#123; log.info(key+ " changed "+ value); &#125;&#125; 创建测试类，我们循环10次改变节点的值，测试一下我们的程序。 1234567891011121314151617@RunWith(SpringRunner.class)@SpringBootTestpublic class DemoApplicationTests &#123; @Autowired CuratorZKClient client; @Test public void contextLoads() throws Exception&#123; //client.createNode("/config/test","Hello World"); ConfigCenterClient.addListener("test",new MyListener()); for(int i = 0;i&lt;10;i++)&#123; Thread.sleep(5000); String str = ConfigCenterClient.getString("test"); System.out.println(str); client.setData("/config/test","Hello World"+i); &#125; &#125;&#125; 可以看到执行结果。 我们还要实现一个配置中心的服务端。 服务端基本上是对Zookeeper数据节点的增删改查这几个逻辑，其核心是Zookeeper保存在节点上的数据。 为了方便对Zookeeper数据进行操作，我们一般创建一个可视化后台管理系统。如下： 这个系统是比较好实现的，引入我们的framework-zookeeper包，里面封装了Zookeeper的增删改查，当然需要创建一个web项目。 这一块就不再过多介绍了，当明白了Zookeeper的增删改查节点数据后，实现起来是比较容易的。 总结今天我们通过Zookeeper实现了一个配置中心，简单了解了它的原理，也对Zookeeper有了一些更深刻的理解。 现在很多开源的配置中心也相当的不错，也是可以学习和理解的，我后面可能也会讲解一些关于这方面的知识。 framework-zookeeper 和 config-spring-boot-starter 的相关代码已上传GitHub，大家如果有兴趣在实践中遇到问题可以过去参考下代码，如有疑问也欢迎与我交流探讨。 配置中心服务端（web项目）由于个人原因和时间原因，只写了个大概，也没有提交Github，后续应该会补上。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>配置中心</tag>
        <tag>ConfigCenter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql 使用Explain分析SQL语句及索引]]></title>
    <url>%2Fblog%2FMySql-%E4%BD%BF%E7%94%A8Explain%E5%88%86%E6%9E%90SQL%E8%AF%AD%E5%8F%A5%E5%8F%8A%E7%B4%A2%E5%BC%95.html</url>
    <content type="text"><![CDATA[前言我们经常会用到数据库，并可能写一些查询语句，大多数情况下，可能都是一些比较简单的增删改查语句。但某些时候，也可能遇到一些较复杂的语句。 在系统数据量不太大的情况下，可能我们SQL语句的好坏对系统性能并不会造成太大影响，但是随着系统数据量增大，数据库压力增大，除一些其它策略（使用缓存数据库、分库分表等等），良好的SQL语句也是提高数据库性能的一个重要指标。 SQL语句我们基本是按照，能精确查询就精确查询，能简化就简化，能使用索引就使用索引（必要条件下）的原则去编写。 我们今天通过Explain这个关键字来分析及优化我们的SQL语句。 正文那就先说下Explain吧。 打开数据库，随便找一张数据表。如下： 可以看到，当我们在我们写的SQL语句前加上Explain后，会得到如下的结果表。 PS：如果SQL语句关联其它表，会有多行结果，后续。 我们先来简单说下结果各个字段的含义。 id 暂且叫主键或者顺序吧，如果不止一次查询搜索（比如关联其它表），会按照顺序列出来。序号越大越先被执行。 select_type 查询类型，有下面几种： SIMPLE：简单SELECT(不使用UNION或子查询等) PRIMARY：最外面的SELECT UNION：UNION中的第二个或后面的SELECT语句 DEPENDENT UNION：UNION中的第二个或后面的SELECT语句，取决于外面的查询 UNION RESULT：UNION的结果。 SUBQUERY：子查询中的第一个SELECT DEPENDENT SUBQUERY：子查询中的第一个SELECT，取决于外面的查询 DERIVED：导出表的SELECT(FROM子句的子查询) table 这行数据和哪张表有关联。 partitions 匹配的哪个分区。 type 这列最重要，显示了连接使用了哪种类别，有无使用索引，是使用Explain命令分析性能瓶颈的关键项之一。 结果值从好到坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL 一般来说，得保证查询至少达到range级别，最好能达到ref，否则就可能会出现性能问题。 possible_keys MySQL可能使用哪个索引在该表中找到行。 key MySQL实际决定使用的键（索引）。如果没有选择索引，值是NULL。 key_len MySQL决定使用的索引长度。如果key是NULL，则key_len为NULL。在不损失精确性的情况下，长度越短越好。 ref 使用哪个列或常数与key一起从表中选择行。 rows MySQL认为它执行查询时必须检查的行数。正常情况下这个值越小越好。 Extra MySQL解决查询的详细信息，也是关键参考项之一。 Distinct：一旦MYSQL找到了与行相联合匹配的行，就不再搜索了。 Not exists：MYSQL 优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了。 Range checked for each Record：没有找到理想的索引，因此对于从前面表中来的每一个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。 Using filesort：MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行。出现这种情况应当优化SQL语句。 Using index：列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候。 Using temporary：说明MySQL需要新建一个临时表来存储结果，当生成的临时表较大时，应当优化SQL语句。 Using where：使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，就会出现这种情况。 Using union：使用了合并，有可能合并索引、表等。一般使用or连接条件时可能会出现。 我们创建一张学生表。 123456789CREATE TABLE `student` ( `id` int(32) NOT NULL AUTO_INCREMENT COMMENT '自增主键', `stu_no` varchar(20) DEFAULT NULL COMMENT '学生编号', `stu_name` varchar(50) DEFAULT NULL COMMENT '学生姓名', `stu_sex` char(2) DEFAULT NULL, `stu_birthday` date DEFAULT NULL COMMENT '学生生日', `stu_class` char(2) DEFAULT NULL COMMENT '学生所在班级', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=50077 DEFAULT CHARSET=utf8; 向表中加入100w数据。（过程略） 关于索引我们统计5班学生的学生数量。 可以看到耗时2s之久。 使用Explain分析可以看到mysql需要检查994511行才能得到结果，已经相当于全表扫描了。Type是ALL，为最差的一种。 我们在stu_class班级这一列上加上索引。 继续查询及分析，可以看到耗时0.07s。 Explain分析可知type为ref满足我们要求，mysql执行这个查询认为只要扫描191502条数据即可，使用了index_1索引。 是不是感觉到索引很厉害？ 我们再来看一下。 这回我们查询5年级所有学生的学生名。如下： 先不使用索引，可以看到，耗时2s左右。 我们给年级加上索引，继续测试。如下： 可以看到耗时竟达到5s左右。 为什么会这样呢？ 一般我们认为，这种属于很特殊的情况，一般在硬盘上，数据是连续存储的，但是引入索引后，数据连续性变差，随机波动性要大，在数据量大的情况下更是。 这种情况下，我们读取大量数据（99497条已接近1/10数据总量），由于随机性变大，故读取时间会变大。因此索引在查询一些小数据量数据的情况下效果明显，或者索引带来的优化已经完全超过了数据读取时间。 关于上面，我们可以看下有索引和无索引SQL执行情况。 有索引情况下: 无索引情况下: 可以看到它们明显的差距就在返回数据这儿了，有索引情况下读取数据浪费了太多时间。 关于索引带来的优化已经完全超过了数据读取时间这个，我们可以看下这个例子。 我们在添加一条姓名索引。 我们查询5班所有姓张的童鞋。 有索引情况下: 无索引情况下: 明显的有索引的完爆无索引查询时间啦，这里就不在具体分析了。 我们可以看下这儿的like，可以直接看下面的图。 明显得到的结论就是当有like查询时，%或者_在前面的是无法使用当前字段索引的（如果有），会进行全表扫描，这一点需要注意。 SQL优化我们接下来分析一些SQL的优化。 1.我想查询生日在1998年至2010年之间的所有学生数量。（生日已建立索引index_3） 可以这样，如下图。 Explain分析仅为index级别，虽然用到了索引index_3。 如何优化下呢？ 我们可以这样处理： 通过Explain分析type也达到了range级别。 PS： 通过上面例子，我们知道我们应该尽量减少对字段的计算操作，以便使其可以正常利用索引，也就是尽量使条件数据与字段数据去匹配，而不是对字段数据进行操作后去匹配条件数据。 2.我想查询生日在98年后且是5班的且姓张的童鞋。 有生日索引index_3，姓名索引index_2，班级索引index_1。 运行上述SQL后我们看到type级别为range，还可以，但是却只用到了index_2索引，时间还可以0.006s。 还有好一点的方法吗？ 我们试着删除上面三个索引，创建一个联合索引。如下： 进行测试。 可以看到时间有提高（反复测试是比单索引要好的），type级别还是range。 PS：联合索引只有在一些特定的情况下能发挥很好的作用，一般单索引可以满足大多需求，所以联合索引的创建是需要慎重的。 联合索引有一些特点需要注意，以免被坑。对于上面的联合索引，index_1(stu_name, stu_birthday, stu_class)，遵循最左原则。 最左原则可以看下面7个例子。 例子1:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01'; Explain分析结果如下： 例子2:1EXPLAIN select * from student s where s.stu_name like '张%'; Explain分析结果如下： 例子3:1EXPLAIN select * from student s where s.stu_class = '5'; Explain分析结果如下： 例子4:1EXPLAIN select * from student s where s.stu_class = '5' and s.stu_birthday &gt;= '1998-01-01'; Explain分析结果如下： 例子5:1EXPLAIN select * from student s where s.stu_class = '5' and s.stu_name like '张%'; Explain分析结果如下： 例子6:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01' and s.stu_name like '张%'; Explain分析结果如下： 例子7:1EXPLAIN select * from student s where s.stu_birthday &gt;= '1998-01-01' and s.stu_name like '张%' and s.stu_class = '5'; Explain分析结果如下： 可以看到，可以使用索引的查询有 （stu_name）（stu_name, stu_birthday）（stu_name, stu_birthday, stu_class）（stu_name, stu_class），也就是必须有stu_name这个条件。这就是联合索引的最左原则。 3.我们统计姓名相同的学生的学生名及数量。1select s.stu_name,count(1) from student s GROUP BY s.stu_name HAVING count(1)&gt;1 ORDER BY s.stu_name; 这儿就不在贴图了，当stu_name有索引时，查询效率会有明显提升。 在有索引Explain分析后，我们可以看到，Mysql认为这个查询需要检索全部数据（rows 99万）。 这是Mysql认为要检索的行数，实际由于stu_name有索引，是没有检索这么多的。 因为查询语句中出现了group by，对于某张表，正常情况的的认知肯定是检索完全表后才能进行分组归类。所以，在一些经常被group by 或者 order by 的字段上，数据量较大的情况下，且该字段比较多样性（比如性别这种字段顶多有男、女、未知三种，加不加索引都可以），向表中常用的创建时间、修改时间等字段，还是有必要添加索引的。 4.查询所有学生生日，性别，班级相同的学生数量。 我们假设建立了index_1(stu_birthday)索引，index_2(stu_sex)索引，index_3(stu_class)索引。 你可能会想到如下SQL。1select count(1) from student a,(select stu_birthday,stu_class,stu_sex from student) b where a.stu_birthday = b.stu_birthday and a.stu_class=b.stu_class and a.stu_sex=b.stu_sex; 嗯，这个SQL是不正确的，而且耗时无法忍受。 其实不用Explain，单独看它就可以知道它会扫描全表，无论有无索引。而且我们统计学生数量，如果有2个以上情况相同呢？显然上面SQL的3个等于条件会出现问题。 其实我们可以根据分组数量来处理这个问题，如下：1select count(1) from student s ,(select a.stu_birthday,a.stu_sex,a.stu_class from student a GROUP BY a.stu_birthday,a.stu_sex,a.stu_class HAVING count(1)&gt;1) b where s.stu_birthday=b.stu_birthday and s.stu_sex=b.stu_sex and s.stu_class=b.stu_class; 通过学生生日，性别，班级相同，他们的数量要大于1，从而进行数量统计。 耗时在8s左右。我们Explain分析下： 先从id为2的看起，生成了一个临时表a，但一个索引也没用到，向上看它用了学生生日，性别，班级这三个临时表的值做了auto_key（可以理解为临时索引），然后对s表全表扫描得到结果。 那我们的三个索引岂不是创建的是毫无意义的…… 我们可以将它的auto_key变成一个联合索引，index_4(stu_birthday, stu_sex, stu_class)。 这时候查询时间减少到5s左右。 可以看到使用到了index_4，且Extra字段原来使用Using temporary现在变为了Using index。 总结好了，基本上就这些内容，其实也有想写一个多表联查复杂的SQL语句的优化，但是无奈例子比较难找（自己不想再建新表，再插入数据，再想例子），在这儿就不在举例了。 其实实际中SQL语句的编写，复杂的还是比较少的（毕竟有程序结合，一个不行可以拆开嘛）。 这一节主要的内容其实是讲Explain这个关键字的，实际中写完复杂SQL语句可以用它分析下SQL执行情况。 上面也看到了有些索引不是建了就会减少查询时间的，索引的选择还是要认真对待，处理好能提升不少查询效率，另外索引创建也应找到问题的关键点。比如该字段经常被group by，order by ，该字段业务中经常使用等等。 使用Explain可以帮我们分析到索引的利用情况。 另外，Mysql除BTree索引外，还有一种Hash索引。 BTree索引具有范围查找和前缀查找的能力，一个N节点的BTree，检索一条记录时间复杂度为O(LogN)，相当于二分查找。 Hash索引只能做等值查找，但查找时间复杂度为O(1)。 如果值的差异性大，并且以等值查找（=、 &lt;、&gt;、in）为主，Hash索引是更好的选择。如果值的差异性相对较差，并且以范围查找为主，BTree是更好的选择。 一般使用BTree索引的场合较多。 结语结束之前，我们可以看看索引的一些失效场景。 索引列查询出现 is not null 情况，会进行全表扫描不走索引。 列类型是字符串，数据需要引号引起来，否则会进行全表扫描不走索引。 模糊查询like的各种情况，不再详述。 联合索引的各种情况，不再详述。 or的情况，当用or连接条件时，应保证条件都有索引才能使用索引，否则索引无效。 可以看到由于stu_name没有索引，故进行了全表扫描。 stu_sex和stu_birthday均有索引，使用了索引。 如果MySQL觉得全表扫描要比使用索引要快，它也不会使用索引。 如下：由于stu_sex=0 或者 stu_class =1 数据很多，且都要返回，故没有使用它们的索引。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>Explain</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的2018年度总结]]></title>
    <url>%2Fblog%2F%E6%88%91%E7%9A%842018%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[我的2018年度总结本来计划月底写的，可是啊，看到有时力不从心的自己，还是决定提前写了吧。 写这个文章，算是对自己今年一整年的总结吧。 怎么说呢？ 先整体上总结一下吧，感觉自己比17年状态要好，懂得了时间的宝贵吧，自制力上也有些许提高。 语无伦次了呢…… 哎，谁让我是个偏内向的男孩啊。啊啊啊啊啊啊啊，老男人，哈哈。 不知道3、5年后，在看到这篇文章，会想到些什么呢？ 如果文章还在的话…… 不扯那么多矫情的话了…… 我的2018年度总结开篇吧。 先说下自己的一些目标及愿望吧，我比较喜欢列清单的方式。 主要也是由于制定的比较晚吧，可能目标/愿望较少，不过也基本上能代表2018年的一些自己主要的计划/想法了。 总的来说一些目标也太模糊，一些目标也很随意，毕竟第一年制定这么个清单。 比如坚持锻炼，可能那时定个坚持锻炼7天，坚持15天……一点点来，自己就不会太迷茫，开始就感觉没戏一样。 而且总感觉对自己下不去狠心，惩罚写的很潦草，奖励写的不现实。 PS：所以这儿就不贴上面给大家看啦，哈哈。 PS：而且还有一些比较个人隐私的目标和愿望没写到上面。 PS：如果你看到这篇文章，也可以自己制定个计划List，不一定要一年，一个月，半年也是可以的。有的时候越长远，反而越不容易实现。 人都是有惰性的嘛。 总会越来越好的。 2018年的自己，怎么说呢？ 也自信也迷茫，也努力也彷徨，也欢喜也哀伤，也年轻也沧桑。 感觉自己也是蛮幸运的吧。 也受到过不少负面的影响。 有时甚至无聊到想探寻人类的本质…… 有人说，出生那一刻，人命就定了，曾经年少轻狂，以为自己可以拯救世界，却发现世界根本不在乎你…… 哈哈，越长大越“认命”吗？ 罗曼·罗兰有句话说的好啊。 世界上只有一种真正的英雄主义，那就是在认清生活的真相后依然热爱生活。 对于人生，我觉得，或许会认命，但绝不会认输。 好吧，我自己这都是写了啥啊…… 哎，跑题了…… 就这样吧，就这样吧。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper Java客户端Curator]]></title>
    <url>%2Fblog%2FZookeeper-Java%E5%AE%A2%E6%88%B7%E7%AB%AFCurator.html</url>
    <content type="text"><![CDATA[前言我们今天来讲讲Zookeeper Java客户端的一些使用吧。 之前我们说到Zookeeper的安装及简单使用，要介绍它的一些应用场景，要明白它的应用场景，要先理解它客户端的一些操作方法。 Zookeeper的Java客户端，最常使用的便是Apache Curator了，它是Netflix公司开源的一个Zookeeper客户端，与Zookeeper提供的原生客户端ZooKeeper相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量，而且Curator的功能更加强大。 正文要使用Curator客户端，需要下面的两个依赖。 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; curator-recipes包一般能满足我们的需要，要是封装更简便的底层功能的话，curator-framework包必不可少。 创建并启动客户端使用程序创建一个客户端client并启动（连接到Zookeeper）。 Builder模式创建一个客户端。 1234567CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder() .connectString(zkConfig.getConnectString()) .namespace(zkConfig.getNameSpace()) .retryPolicy(new ExponentialBackoffRetry(zkConfig.getRetryInterval(), Integer.MAX_VALUE)) .connectionTimeoutMs(zkConfig.getConnectTimeoutMs()) .sessionTimeoutMs(zkConfig.getSessionTimeoutMs()); client = builder.build(); 客户端启动。 1client.start(); 简单说下连接时的参数: connectString：服务器列表，逗号隔开（host1:port1,host2:port2……） namespace：命名空间，可以用来进行业务区分。 retryPolicy：重试策略,有以下4种重试策略，也可以自己实现重试策略（实现RetryPolicy接口）。RetryOneTime：重试一次。 RetryNTimes：重试N次（需要传入重试间隔参数sleepMsBetweenRetries，及尝试次数n），它继承了抽象类SleepingRetry（每休眠一段时间重试一次）。 RetryForever：一直重试（需要传入重试间隔retryIntervalMs参数）。 BoundedExponentialBackoffRetry：重试次数固定，但每次重试的时间间隔会不断变大（如果一直连不上），需要传入初始等待重试时间baseSleepTimeMs，重试次数maxRetries，及最大等待重试时间maxSleepTimeMs 参数，这个类继承ExponentialBackoffRetry（它又继承SleepingRetry）抽象类。 sessionTimeoutMs：会话超时时间，单位毫秒，默认60000ms。 connectionTimeoutMs：连接创建超时时间，单位毫秒，默认60000ms。 创建数据节点。Zookeeper节点有4种，上篇文章已介绍。 创建持久化节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path, data); 使用create方法，creatingParentsIfNeeded这个方法保证如果有父节点也会一起创建，这在原生客户端是无法实现的。CreateMode 有4种，跟Zookeeper的节点类型对应。forPath方法可以认为最终操作，path表示节点路径，data表示节点数据。data是byte数组，其它类型的数据应转换为byte数组。 注：如果不设置withMode方法，默认创建持久化节点，不设置data，节点默认内容为空。 如下： 1client.create().forPath(path); 创建顺序节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath(path, data); 创建临时节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(path, data); 创建临时顺序节点 1client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath(path, data); 设置节点数据1client.setData().forPath(path, bytes); 直接调用setData方法即可，返回一个Stat（节点信息类）。 获取节点数据1client.getData().forPath(path); 使用getData方法，返回byte数组。 获取子节点1client.getChildren().forPath(path); 使用getChildren方法，返回一个子节点List &lt;String&gt; 列表，数据为各个子节点名称。 删除节点1client.delete().guaranteed().forPath(path); 使用delete方法，guaranteed方法可以保证一定删除。如果某个节点删除失败，会抛出异常，但是如果使用了guaranteed，它会在后台继续进行删除直到删除成功。 删除节点（包括子节点）1client.delete().guaranteed().deletingChildrenIfNeeded().forPath(path); deletingChildrenIfNeeded方法可以保证如果有子节点的话一并删除，原生client是无法实现此功能的（需要我们写方法处理）。 判断节点是否存在1234567891011public boolean checkNodeExist(String path) &#123; boolean exits = false; try &#123; Stat stat = client.checkExists().forPath(path); if (stat != null) &#123; exits = true; &#125; &#125; catch (Exception e) &#123; &#125; return exits;&#125; 使用checkExists方法，最终返回一个Stat，如果Stat为空就说明不存在。 PS：由此我们可以创建一个 createOrUpdate方法，无节点时创建，有节点时更新内容。 1234567891011public void createOrUpdateNode(String path, byte[] data) &#123; try &#123; if(checkNodeExist(path))&#123; setData(path,data); &#125;else&#123; createNode(path, data); &#125; &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125;&#125; 异步处理上面的操作方法，都可以使用异步进行处理的，主要使用了inBackground方法。 如下： 1client.create().inBackground().forPath(path, data); 该方法全参函数如下，且重载了多个方法。 1public T inBackground(BackgroundCallback callback, Object context, Executor executor); BackgroundCallback callback：异步回调函数，处理完成后会回调此函数进行某些逻辑。 Object context：上下文对象。 Executor executor：异步处理的线程，不指定的话将使用内部默认线程处理。 我们可以看下BackgroundCallback 方法 会有两个参数。 123public interface BackgroundCallback &#123; void processResult(CuratorFramework var1, CuratorEvent var2) throws Exception;&#125; 第二个参数CuratorEvent里面包含了此次处理结果的所有信息，包括节点信息等。 123456789101112public interface CuratorEvent &#123; CuratorEventType getType(); int getResultCode(); String getPath(); Object getContext(); Stat getStat(); byte[] getData(); String getName(); List&lt;String&gt; getChildren(); List&lt;ACL&gt; getACLList(); WatchedEvent getWatchedEvent();&#125; CuratorEventType表示事件类型，表示此次操作的事件类型。可以看到它与CuratorFramework里的方法是一一对应的。 getResultCode返回处理结果码。可以在这个枚举里查看各个状态码。 添加watcher12client.getData().usingWatcher(watcher).forPath(path);client.getChildren().usingWatcher(watcher).forPath(path); 使用usingWatcher结合getData或者getChildren方法可以为指定节点或者子节点添加watcher。 Watcher可以为CuratorWatcher或者Zookeeper自带的Watcher。它们有一个event参数。 可以拿到Zookeeper的状态 KeeperState和 事件类型 EventType，从而进行某些必要的操作。 KeeperState枚举和EventType枚举如下图。 事务支持Zookeeper一些操作是支持事务的。 主要用到的方法有inTransaction、and、commit等方法。举例如下： 123456789101112131415161718192021222324252627282930/** * 开启事务 */public CuratorTransaction startTransaction() &#123; return client.inTransaction();&#125;/** * 事务中添加create操作 */public CuratorTransactionFinal addCreateToTransaction(CuratorTransaction transaction, String path) throws Exception &#123; return transaction.create().forPath(path, new byte[0]).and();&#125;/** * 事务中添加delete操作 */public CuratorTransactionFinal addDeleteToTransaction(CuratorTransaction transaction, String path) throws Exception &#123; return transaction.delete().forPath(path).and();&#125;/** * 事务中添加seData操作 */public CuratorTransactionFinal addSetDataToTransaction(CuratorTransaction transaction, String path, byte[] data) throws Exception &#123; return transaction.setData().forPath(path, data).and();&#125;/** * 提交事务 */public Collection&lt;CuratorTransactionResult&gt; commitTransaction(CuratorTransactionFinal transaction) throws Exception &#123; return transaction.commit();&#125; 检查连接情况1client.getZookeeperClient().isConnected(); 关闭连接1client.close(); 提升对Curator客户端有简单理解后，我们把它进行简单功能的封装。 PS：Curator的强大之处在于其增强功能部分，我们会在后面结合Zookeeper应用讨论。 创建项目framework-zookeeper，搭建如下结构： 接口ZKClient，里面有一些Zookeeper客户端的协议，大致如下： 12345678910public interface ZKClient &#123; void start();//启动 boolean isConnected();//连接情况 void close();//关闭 void createNode(String path, byte[] data);//创建永久节点 void createOrUpdateNode(String path, byte[] data); //创建或者更新节点 void createEphemeralNode(String path, byte[] data); String createSequenceNode(String path) ; ......&#125; CuratorZKClient是Curator对接口ZKClient的实现，BaseZKClient是原生客户端对接口ZKClient的实现。 我们来写下CuratorZKClient的一些关键代码。 12345678910111213public class CuratorZKClient implements ZKClient &#123; //zk客户端 private CuratorFramework client; //关闭状态 private volatile boolean closed = false; //zk配置 private ZKConfig zkConfig; //连接状态监听 private Set&lt;ConnectionListener&gt; connectionListeners = new CopyOnWriteArraySet&lt;&gt;(); //节点变化监听及相关Watcher private final ConcurrentMap&lt;String, ConcurrentMap&lt;NodeListener, CuratorWatcher&gt;&gt; nodeListeners = new ConcurrentHashMap&lt;&gt;();......&#125; 提供一个连接监听接口，以便我们可以监听Zookeeper的连接状态并且执行某些操作。 123public interface ConnectionListener &#123; void stateChanged(ZKConstants.ConnectState state);&#125; 及节点变化接口，监测节点变化进行某些操作。 12345public interface NodeListener &#123; void nodeChanged(String path, List&lt;String&gt; nodes);//节点改变 void nodeDelete(String path);//节点删除 void dataChanged(String path, byte[] data);//节点数据改变&#125; ZKConfig是Zookeeper客户端连接的配置，属性值可以配置在properties等配置文件里。 12345678public class ZKConfig &#123; private String connectString; private String nameSpace; private int retryInterval = 1000; private int connectTimeoutMs = 60000; private int sessionTimeoutMs = 60000; ......&#125; CuratorZKClient 里实现接口的start方法是，部分代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940...... public void start() &#123; ....... CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder() .connectString(zkConfig.getConnectString()) .namespace(zkConfig.getNameSpace()) //重试指定的次数, 且每一次重试之间停顿的时间逐渐增加 .retryPolicy(new ExponentialBackoffRetry(zkConfig.getRetryInterval(), Integer.MAX_VALUE)) .connectionTimeoutMs(zkConfig.getConnectTimeoutMs()) .sessionTimeoutMs(zkConfig.getSessionTimeoutMs()); client = builder.build(); //添加Zookeeper状态监听 client.getConnectionStateListenable().addListener(new ConnectionStateListener() &#123; @Override public void stateChanged(CuratorFramework client, ConnectionState zkConnectionState) &#123; ZKConstants.ConnectState state = toConnectionListenerState(zkConnectionState); if (state != null) &#123; for(ConnectionListener connectionListener : connectionListeners) &#123; connectionListener.stateChanged(state); &#125; &#125; &#125; private ZKConstants.ConnectState toConnectionListenerState(ConnectionState zkConnectionState) &#123; switch (zkConnectionState) &#123; case LOST: return ZKConstants.ConnectState.DISCONNECTED; case SUSPENDED: return ZKConstants.ConnectState.DISCONNECTED; case CONNECTED: return ZKConstants.ConnectState.CONNECTED; case RECONNECTED: return ZKConstants.ConnectState.RECONNECTED; default: return null; &#125; &#125; &#125;); client.start(); &#125;..... 这样我们暴露了Zookeeper的连接状态监听接口，以后想监听它的连接状态进行某些操作，直接实现接口，并通过addConnectionListener添加进来即可。 12345678@Overridepublic void addConnectionListener(ConnectionListener listener) &#123; connectionListeners.add(listener); &#125;@Overridepublic void removeConnectionListener(ConnectionListener listener) &#123; connectionListeners.remove(listener);&#125; 其它的方法，比如createNode、deleteNode等，我们拿到client后，按照上面讲述的各个操作便可以写出代码，这里不再赘述。以下是createNode的例子。 12345678910@Overridepublic void createNode(String path, byte[] data) &#123; try &#123; client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(path, data); &#125; catch (KeeperException.NodeExistsException e) &#123; log.warn(String.format("create node is exist:%s", path)); &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125;&#125; 再说一下需要实现的NodeListener方法，节点发生变化，主要通过watcher通知。实现一个watcher。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758private class CuratorWatcherImpl implements CuratorWatcher &#123; private volatile NodeListener listener; public CuratorWatcherImpl(NodeListener listener) &#123; this.listener = listener; &#125; public void unWatch() &#123; this.listener = null; &#125; @Override public void process(WatchedEvent event) throws Exception &#123; if (listener != null) &#123; log.debug(event.getPath() + " with event " + event.getType()); switch (event.getType()) &#123; //节点数据变化，调用listener指定方法 case NodeDataChanged: try &#123; byte[] data = client.getData().usingWatcher(this).forPath(event.getPath()); log.debug(event.getPath() + " data after change: " + new String(data)); listener.dataChanged(event.getPath(), data); &#125; catch (Exception e) &#123; log.warn(e.getMessage(), e); &#125; break; //节点删除 case NodeDeleted: //节点创建 case NodeCreated: log.error(event.getPath()); //自己点改变 case NodeChildrenChanged: try &#123; if (event.getType().equals(Watcher.Event.EventType.NodeDeleted)) &#123; listener.nodeDelete(event.getPath()); &#125; else &#123; List&lt;String&gt; nodes = getNodes(event.getPath()); if (nodes != null) &#123; client.getChildren().usingWatcher(this).forPath(event.getPath()); &#125; //监控子节点数据变化 //for(String node : nodes) &#123; // client.getData().usingWatcher(this).forPath( ZKPathMgr.joinPath(event.getPath(),node)); //&#125; log.debug(event.getPath() + " nodes after change: " + nodes); listener.nodeChanged(event.getPath(), nodes); &#125; &#125; catch (KeeperException.NoNodeException e) &#123; log.warn(e.getMessage()); &#125; catch (Exception e) &#123; log.warn(e.getMessage(), e); &#125; break; case None: default: break; &#125; &#125; &#125; &#125; 然后实现NodeListener的添加移除。 123456789101112131415161718192021222324252627282930313233343536@Override public void addNodeListener(String path, NodeListener listener) &#123; ConcurrentMap&lt;NodeListener, CuratorWatcher&gt; listeners = nodeListeners.get(path); if (listeners == null) &#123; nodeListeners.putIfAbsent(path, new ConcurrentHashMap&lt;NodeListener, CuratorWatcher&gt;()); listeners = nodeListeners.get(path); &#125; CuratorWatcher watcher = listeners.get(listener); if (watcher == null) &#123; listeners.putIfAbsent(listener, new CuratorWatcherImpl(listener)); watcher = listeners.get(listener); &#125; addChildrenCuratorWatcher(path, watcher); &#125; @Override public void removeNodeListener(String path, NodeListener listener) &#123; ConcurrentMap&lt;NodeListener, CuratorWatcher&gt; listeners = nodeListeners.get(path); if (listeners != null) &#123; CuratorWatcher watcher = listeners.remove(listener); if (watcher != null) &#123; ((CuratorWatcherImpl) watcher).unWatch(); &#125; &#125; &#125; //添加目录watcher private void addChildrenCuratorWatcher(final String path, CuratorWatcher watcher) &#123; try &#123; client.getData().usingWatcher(watcher).forPath(path); client.getChildren().usingWatcher(watcher).forPath(path); &#125; catch (KeeperException.NoNodeException e) &#123; log.warn(String.format("add watcher node not exist:%s", path)); &#125; catch (Exception e) &#123; throw new IllegalStateException(e.getMessage(), e); &#125; &#125; 可以看到主要是是维护一个ConcurrentHashMap，listener为key，watcher为value，节点有变化，通知到listener。 好。到这里基本上一个Zookeeper工具客户端就OK了，BaseZKClient的实现与CuratorZKClient类似，有兴趣的可以自己看看。 测试我们测试下我们的代码。 12345678910111213141516171819public static void main(String[] args) throws Exception&#123; ZKConfig config = new ZKConfig(); config.setConnectString("127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"); config.setConnectTimeoutMs(60000); config.setNameSpace("zwt"); config.setRetryInterval(1000); config.setSessionTimeoutMs(60000); ZKClient client = new CuratorZKClient(config); client.addConnectionListener(new ConnectionListener() &#123; @Override public void stateChanged(ZKConstants.ConnectState state) &#123; System.out.println("ZKState state "+ state.name()); &#125; &#125;); client.start(); client.createNode("/mytest","Hello World"); System.out.println(new String(client.getData("/mytest"),"UTF-8")); client.close();&#125; 运行可以看到输出： 我们使用命令行也可以看到我们新增的test节点及其属性。 PS：可以看到nameSpace 业务命名空间相当于新增一个根节点以区分不同业务，避免节点冲突等作用。 我们在client启动后添加watcher。 12345678910111213141516171819202122......client.start();client.addNodeListener("/mytest", new NodeListener() &#123; @Override public void nodeChanged(String path, List&lt;String&gt; nodes) &#123; System.out.println(path+" node changed"); &#125; @Override public void nodeDelete(String path) &#123; System.out.println(path+" node delete"); &#125; @Override public void dataChanged(String path, byte[] data) &#123; System.out.println(path+" data changed "+ data); &#125;&#125;);System.out.println(new String(client.getData("/mytest"),"UTF-8"));client.setData("/mytest","World");System.out.println(new String(client.getData("/mytest"),"UTF-8"));client.createNode("/mytest/test");client.deleteNodeWithChildren("/mytest");...... 继续测试，结果如下。 到这里，我们基本把客户端操作的基本说完了。关于其它一些Zookeeper客户端，这里就不在过多介绍了，有兴趣的可以继续实现ZKClient接口去完成。 PS：BaseZKClient类是我写的一个原生Zookeeper客户端的集成工具，但有些小问题未处理。 总结通过使用Zookeeper客户端的一些例子，更对Zookeeper有了更深入的了解。 下面的文章我们将结合Curator的一些高级功能及Zookeeper的一些应用来了解Zookeeper的强大之处。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Zookeeper</tag>
        <tag>Curator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper初探]]></title>
    <url>%2Fblog%2FZookeeper%E5%88%9D%E6%8E%A2.html</url>
    <content type="text"><![CDATA[前言前不久研究了下Zookeeper，感觉挺不错的，这几天把成果总结下，方便记忆。 今天先简单介绍下它的安装使用和一些特性吧。 ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 正文Zookeeper的安装与配置首先需要下载安装包。 Zookeeper官网地址 点击这里 我们可以通过镜像的方式将压缩包下载下来。 解压到合适目录，进入到zookeeper目录下的conf目录，需要新建zoo.cfg文件，可以看到里面有一个zoo_sample.cfg文件，可以作为我们的配置参考。 我们来看下它里面的一些常用参数： tickTime=2000 指Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。单位毫秒 initLimit=10 此配置表示，允许 follower（相对于 leader 而言）连接并同步到 leader 的初始化连接时间，它以 tickTime 的倍数来表示。当超过设置倍数的 tickTime 时间，则连接失败。 syncLimit=5 此配置表示，leader 与 follower 之间发送消息，请求和应答时间长度。如果 follower 在设置的时间内不能与leader 进行通信，那么此 follower 将被丢弃。 dataDir=D:/zookeeper1/zookeeper/data 数据存储快照的地址 dataLogDir=D:/zookeeper1/zookeeper/log 数据库事务日志地址，如果不设置此项，默认dataDir的地址 clientPort=2181 供客户端连接的端口，对外提供的端口 server.10=127.0.0.1:2888:3888 集群模式下需要的配置（单机不需要） server.A = B:C:D A：表示这是第几号服务器 B：服务器的 IP 地址 C：集群内机器通讯使用（Leader监听此端口） D：一旦集群中的 Leader 服务器挂了，需要一个端口重新进行选举，选出一个新的 Leader Zookeeper的启动和使用看完上面配置后，我们先配置单机zookeeper对其简单使用。 进入到zookeeper bin目录下，我们可以看到一些脚本，分为cmd脚本和sh脚本。 zkCleanup 清理Zookeeper历史数据脚本，包括日志文件和快照数据文件 zkCli Zookeeper的客户端脚本 zkEnv 设置Zookeeper的环境变量脚本 zkServer Zookeeper服务器的启动、停止、和重启脚本 启动ZookeeperWindows操作： 进入到bin目录下，执行如下命令： zkServer 可以启动zookeeper服务。（需要注意windows下dataDir路径支持/和\\，但使用\会出现问题。） 下图是我公司电脑（Windows）上的zookeeper启动图。 Linux/Mac/Unix操作： 进入到bin目录下，执行如下命令： ./zkServer.sh start 可以启动zookeeper服务。 停止ZookeeperWindows操作： 直接可以Ctrl+C结束命令。 Linux/Mac/Unix操作： ./zkServer.sh stop PS:杀进程也可以结束，不推荐 连接ZookeeperWindows操作： 进入到bin目录下，执行如下命令： zkCli -server localhost:2181 可以连接到zookeeper服务。 下图是Windows电脑上连接到zookeeper服务的图。 Linux/Mac/Unix操作： 进入到bin目录下，执行如下命令： ./zkCli.sh -server localhost:2181 可以连接到zookeeper服务。 Zookeeper常用命令在zookeeper客户端，使用help可以查看可以使用的命令。 我们说几个比较常用的命令。 创建节点： create [-s] [-e] path data acl 使用此命令可以创建一个zookeeper节点。 其中，-s或-e分别指定节点特性，顺序或临时节点，若不指定，则表示持久节点；acl用来进行权限控制。 create -s /zk-test test 表示创建顺序节点zk-test。 可以看到创建的zk-test节点后面添加了一串数字以示区别顺序。 create -e /zk-temp test 表示创建临时节点。临时节点在客户端断开连接后会自动删除。 可以看到创建临时节点重新登陆后节点已被删除。 create /zk-permanent test 创建永久节点zk-permanent。 读取节点： ls path [watch] 可以列出节点下面的所有子节点。 get path [watch] 可以获取根节点数据内容和属性信息。 ls2 path [watch] 列出节点信息。 如下图： 更新节点： set path data [version] 我们将zk-permanent节点下的数据test变为test1。 可以看到dataVersion也发生了变化，用来表示已经进行了更新。 删除节点： delete path [version] 我们删除刚才创建的zk-permanent节点。 可以看到节点已经被删除。 PS：若删除节点存在子节点，那么无法删除该节点，必须先删除子节点，再删除父节点。 Zookeeper部署模式Zookeeper的部署模式一般分为3种，单机模式、伪集群模式、集群模式。 单机模式 单个Zookeeper服务，如上面所述。 伪集群模式 所谓伪集群, 是指在单台机器中启动多个zookeeper进程, 并组成一个集群。 以3个zookeeper服务进程为例。 需要有3个zookeeper项目。 三个项目配置特殊的地方： zookeeper1配置：1234567dataDir=D:/zookeeper1/zookeeper/datadataLogDir=D:/zookeeper1/zookeeper/logclientPort=2181server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 1 zookeeper2配置：1234567dataDir=D:/zookeeper2/zookeeper/datadataLogDir=D:/zookeeper2/zookeeper/logclientPort=2182server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 2 zookeeper3配置：1234567dataDir=D:/zookeeper3/zookeeper/datadataLogDir=D:/zookeeper3/zookeeper/logclientPort=2183server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882 server.3=127.0.0.1:2883:3883#同时在dataDir路径文件夹里需要新建myid文件，里面写入服务器的编号 3 在同一台机器上，3者的端口号不能相同，不然出现端口冲突。 集群模式 集群模式下配置和伪集群相似。 服务器1配置：123456789tickTime=2000initLimit=10syncLimit=5dataDir=D:/zookeeper1/zookeeper/datadataLogDir=D:/zookeeper1/zookeeper/logclientPort=2181server.1=10.1.100.1:2888:3888server.2=10.1.100.2:2888:3888server.3=10.1.100.3:2888:3888 由于不在一个服务器上，故服务器2，服务器3也如上面的配置即可，它们3个唯一不同的地方是dataDir文件夹里面的myid文件，代表着服务器各自的编号。 Zookeeper特性由上面内容，我们汇总说下Zookeeper的一些性质。 类文件系统（数据节点） Zookeeper有类似于Unix文件系统的节点。它可以自由创建、修改和删除子目录，这些目录项被称为znode，znode可以存储数据。 Zookeeper有4种类型的znode： 1. PERSISTENT 持久化目录节点，客户端与zookeeper断开连接后，该节点依旧存在 2. PERSISTENT_SEQUENTIAL 持久化顺序编号目录节点，客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 3. EPHEMERAL 临时目录节点，客户端与zookeeper断开连接后，该节点被删除 4. EPHEMERAL_SEQUENTIAL 临时顺序编号目录节点，客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 通知机制 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。这是非常重要的一条性质。 高可用性 zookeeper的高可用性通过zookeeper集群实现。 Zookeeper一般有三种集群角色： Leader（领导者） Follower（跟随者） Observer（观察者） 一个 ZooKeeper 集群同一时刻只会有一个 Leader，其他都是 Follower 或 Observer。 ZooKeeper 默认只有 Leader 和 Follower 两种角色，没有 Observer 角色。为了使用 Observer 模式，在任何想变成Observer的节点的配置文件中加入 peerType=observer 并在所有 server 的配置文件中，配置成 observer 模式的 server 的那行配置追加 :observer即可。 ZooKeeper 集群的所有机器通过一个 Leader 选举过程来选定一台机器作为Leader，Leader服务器为客户端提供读和写服务。Follower 和 Observer 都能提供读服务，不能提供写服务。两者唯一的区别在于，Observer机器不参与 Leader 选举过程，也不参与写操作的过半写成功策略，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。 我们简单来看下： 我在自己的Mac机器上创建了4个zookeeper服务并启动（伪集群），其中一个是Observer，另三个其中一个是Leader，其它两个Follower。（3台机器中谁是Leader是选举出来的，一般情况下先启动的会成为Leader）。 配置如下(关键配置)：12345678910111213141516171819202122232425262728# zookeeper1的配置，myid文件里值为1clientPort=2181server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper2的配置，myid文件里值为2clientPort=2182server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper3的配置，myid文件里值为3clientPort=2183server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer# zookeeper4的配置，myid文件里值为4clientPort=2184peerType=observerserver.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883server.4=127.0.0.1:2884:3884:observer 启动后查看它们的角色： 可以看到zookeeper2是leader，我们停掉zookeeper2. 可以看到leader变成了zookeeper3. 我们继续把zookeeper3停掉。 可以发现zookeeper集群已经不能正常运行了。 zookeeper在执行增删改操作时，只要有半数以上服务器通过，就可以成功执行（不包括observer服务器） 半数以上投票通过：可以这样理解，客户端的增删改操作无论访问到了哪台zookeeper服务器，最终都会被转发给leader服务器，再由leader服务器分给zookeeper集群中所有follower服务器去投票（投票指的是在内存中做增删改操作），半数投票通过就被认为操作可执行（commit），否则不可执行。 由于在增删改操作中需要半数以上服务器通过，来分析以下情况。 2台服务器，至少2台正常运行才行，正常运行1台服务器都不允许挂掉。 3台服务器，至少2台正常运行才行，正常运行可以允许1台服务器挂掉。 4台服务器，至少3台正常运行才行，正常运行可以允许1台服务器挂掉。 ...... 故上面例子当挂掉两台服务器时，zookeeper服务已经不能正常运行了。 由上面可以知道，2n-1台服务器和2n台服务器都最多允许n-1台服务器挂掉（n&gt;=2）。 一般为了节省资源，环境基本配置奇数2n-1台服务器（不包括observer）。 Zookeeper应用场景Zookeeper的应用场景还是很多的，如： 1.命名服务 2.分布式协调服务/通知 3.数据发布与订阅（配置中心） 4.Master选举 5.分布式锁 等。 我会在后面结合例子研究zookeeper的一些应用场景。 结语通过对Zookeeper的一些学习，明白了Zookeeper的一些特点用途，了解了一些简单操作，还是蛮不错的一次学习过程。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot集成RocketMQ]]></title>
    <url>%2Fblog%2FSpringBoot%E9%9B%86%E6%88%90RocketMQ.html</url>
    <content type="text"><![CDATA[前言在开始之前，大家可以先了解RocketMQ的一些特性。 RocketMQ简介 今天我们接上之前说的，对RocketMQ进行简单使用。主要的也是讲如何在SpringBoot项目中使用RocketMQ。 环境RocketMQ安装我们先在RocketMQ官网上下载最新的MQ版本并进行安装。 可以通过镜像进行下载。 将压缩包解压并放在一个指定文件夹下。（这里要注意的是文件夹路径中尽量不要有空格，像Program Files这种，有可能导致mq无法正常启动） RocketMQ启动通过命令行进入到bin目录下，使用 mqnamesrv -n localhost:9876 （windows）可以启动mq的namesrv。如下图： 使用 mqbroker -n localhost:9876 （windows）可以启动mqbroker。如下图： 注意：上图表示RocketMQ的namesrv和broker启动成功，RocketMQ若正常使用应保证namesrv和broker均启动成功。 与Java集成使用主要依赖于rocketmq-client的jar包，在与SpringBoot进行集成时，应当引入该jar包。 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;$&#123;version.rocketmq&#125;&lt;/version&gt;&lt;/dependency&gt; 例子我们简单使用下该jar包创建消费者和生产者进行消费，来了解下它们的一些参数。 生产者： 12345678910111213141516171819public class ProducerTest &#123; public static void main(String[] args) throws Exception &#123; //producerGroup 生产者组名称 DefaultMQProducer producer = new DefaultMQProducer("producer1"); //设置NamesrvAddr producer.setNamesrvAddr("127.0.0.1:9876"); //设置自动创建Topic producer.setCreateTopicKey("AUTO_CREATE_TOPIC_KEY"); //调用start()方法启动一个producer实例 producer.start(); System.out.println("Producer started"); Message message=new Message(); message.setTopic("Test"); message.setTags("123"); message.setBody(new String("Hello").getBytes()); SendResult result=producer.send(message); System.out.println(result); &#125;&#125; 消费者： 123456789101112131415161718192021222324252627282930313233public class ConsumerTest &#123; public static void main(String[] args) throws Exception&#123; //需要consumerGroup DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("consumer1"); //设置NameServer consumer.setNamesrvAddr("127.0.0.1:9876"); //消费策略 //CONSUME_FROM_LAST_OFFSET 默认策略，从该队列最尾开始消费，即跳过历史消息 //CONSUME_FROM_FIRST_OFFSET 从队列最开始开始消费，即历史消息（还储存在broker的）全部消费一遍 //CONSUME_FROM_TIMESTAMP 从某个时间点开始消费，和setConsumeTimestamp()配合使用，默认是半个小时以前 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET); //Topic和Tag，*代表全部的Tag consumer.subscribe("Test", "*"); //设置一个Listener，主要进行消息的逻辑处理 consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; msgs.forEach((a)-&gt;&#123; System.out.println(new String(a.getBody())); &#125;); //返回消费状态 //CONSUME_SUCCESS 消费成功 //RECONSUME_LATER 消费失败，需要稍后重新消费 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); //启动 consumer.start(); System.out.println("Consumer started"); &#125;&#125; 运行这两个类进行测试。 生产者结果： 消费者结果： 备注：如果启动中出现异常com.alibaba.rocketmq.client.exception.MQClientException: No route info of this topic, TestTopic可能是没有开启AUTO_CREATE_TOPIC，我们可以在启动broker的时候加上该参数，mqbroker -n localhost:9876&amp;autoCreateTopicEnable=true 可以保证使用时自动创建Topic。 生产等环境一般需要什么Topic就配置什么，不会开启这个参数让程序自动创建。 简要说明我们对上面例子里的一些参数等做些说明，以便于我们可以更好的封装功能。 可与之前RocketMQ的简介结合理解。 DefaultMQProducer部分参数 producerGroup：生产者组名称。 namesrvAddr：生产者NameSrvAddr的服务地址。 createTopicKey：可以创建指定的Topic。AUTO_CREATE_TOPIC_KEY为自动创建Topic。 其它参数略。 发送Message时，需要设置Message的Topic和Tag，并能收到发送状态结果。 DefaultMQPushConsumer部分参数 consumerGroup：消费者组名称。 namesrvAddr：消费者NameSrvAddr的服务地址。 ConsumeFromWhere：消费策略。 ---&gt; CONSUME_FROM_LAST_OFFSET 默认策略，从该队列最尾开始消费，即跳过历史消息 ---&gt; CONSUME_FROM_FIRST_OFFSET 从队列最开始开始消费，即历史消息（还储存在broker的）全部消费一遍 ---&gt; CONSUME_FROM_TIMESTAMP 从某个时间点开始消费，和setConsumeTimestamp()配合使用，默认是半个小时以前 subscribe方法：使消费者订阅指定的Topic和Tag。 registerMessageListener方法：注册消费者监听，用于消费消息，这里面也是我们业务逻辑的主要内容。有顺序消费和并行消费两种模式。 分别需要实现MessageListenerOrderly接口和MessageListenerConcurrently接口。 我们的例子是并行消费处理的。 小试牛刀每次处理时都要这样写会有很多的与业务无关的代码，也不美观。 我们对其进行必要封装，之前集成了SpringBoot的一个自己封装的starter插件，今天我们把RocketMQ也与SpringBoot集成下。 A 首先新建SpringBoot项目，引入RocketMQ 的jar依赖，不在详述。 然后在项目下创建必要的package。 producer：用来存放我们构建producer的类的包。 consumer：用来存放我们构建consumer的类的包。 listener:用来存放我们构建listener的类的包 factory:用来构建生产者和消费者群组的包。 config：存放SpringBoot配置类的包。 autoware:存放启动配置生效的类的包。 annotation：用来存放注解的包。 先说下简单思路吧。 首先这个生产者和消费者是可以有多个的，然后我们怎么管理它们？生产者可以发送顺序消息和并发消息，消费者可以处理顺序消息和并发消息，同时我们可能有两种业务要使用同一个Listener，如何解耦呢？ 关于管理：我们可以管理生产者和消费者的一个集合来解决。 关于解耦：可以提供一个接口，业务类实现这个接口拿到Message，进行处理。那如何知道这个业务类需要哪个listener呢？自然需要customerId或者listenerId。 好了开始工作。 B 先从配置入手。 下面是生产者和消费者的配置Bean。 由于可以配置多个生产者或者消费者，故使用List处理它们。部分代码如下： C 然后从消费者和生产者的提供入手。也是比较简单的，主要是根据参数生成一个生产者或者消费者，然后暴露一些方法，如start，stop等方法。 提供生产者的类，部分代码如下。 备注：生产环境一般不设置AUTO_CREATE_TOPIC_KEY，需要什么Topic要手动创建加入管理。 创建消费者的类，部分代码如下。 我们认为两个consumerId相等则获取的是一个Consumer，因此需要重写equals方法。 D 创建消费者监听，为处理消息提供一个接口。 使用抽象类部分实现这个接口。 这一步的目的是由于不同的业务逻辑可能用到一个监听，这样可以两个业务逻辑写到两个不同的类中，只需实现IProcessor。 写两个并行处理监听和顺序处理监听，对其进行实现。 并行监听： 1234567891011121314151617181920public class ConcurrentlyRocketMQMessageListener extends AbstractRocketMQMessageListener implements MessageListenerConcurrently &#123; private static final transient Logger logger = LoggerFactory.getLogger(ConcurrentlyRocketMQMessageListener.class); @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; logger.debug("ConcurrentlyRocketMQMessageListener receive message begin,length:&#123;&#125;" , msgs.size()); for(MessageExt msg:msgs ) &#123; for (IProcessor processor : processorList) &#123; try &#123; // 处理消息 process(processor, msg); &#125; catch (Exception ex) &#123; logger.error("ConcurrentlyRocketMQMessageListener error",ex); return ConsumeConcurrentlyStatus.RECONSUME_LATER; &#125; &#125; &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;&#125; 顺序监听： 12345678910111213141516171819public class OrderlyRocketMQMessageListener extends AbstractRocketMQMessageListener implements MessageListenerOrderly &#123; private static final transient Logger logger = LoggerFactory.getLogger(OrderlyRocketMQMessageListener.class); @Override public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; logger.debug("OrderlyRocketMQMessageListener receive message begin,length:&#123;&#125;" , msgs.size()); for(MessageExt msg:msgs ) &#123; for (IProcessor processor : processorList) &#123; try &#123; process(processor, msg); &#125; catch (Exception ex) &#123; logger.error("OrderlyRocketMQMessageListener error",ex); return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT; &#125; &#125; &#125; return ConsumeOrderlyStatus.SUCCESS; &#125;&#125; 然后需要对实现IProcesser接口的类添加如下注解，用来查看调用的哪个Listener。 1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface RocketMQProcessor &#123; /** * 该程序归哪个消费者监听处理 * @return */ String consumerId() default "";&#125; E 好，我们开始构建factory。 使用ConcurrentHashMap存储生产者和消费者集合。 1234//用于存放生产者的map组private static Map&lt;String, RocketMQMessageProducer&gt; producers=new ConcurrentHashMap&lt;&gt;();//用于存放消费者的map组private static Map&lt;String, RocketMQMessageConsumer&gt; consumers=new ConcurrentHashMap&lt;&gt;(); 创建生产者： 1234567891011121314151617181920212223242526272829/** * 创建一个生产者 * @param configuration * @return */public RocketMQMessageProducer createProducer(MqProducerConfiguration configuration)&#123; //如果map里存在这个实例，直接返回 if(producers.get(configuration.getProducerId())!=null)&#123; return producers.get(configuration.getProducerId()); &#125; //创建一个生产者 RocketMQMessageProducer producer=new RocketMQMessageProducer(configuration.getGroupName(), configuration.getNamesrvAddr()); if(configuration.getSendMsgTimeout()!=null)&#123; producer.setSendMsgTimeout(configuration.getSendMsgTimeout()); &#125; if(configuration.getMaxMessageSize()!=null)&#123; producer.setMaxMessageSize(configuration.getMaxMessageSize()); &#125; try &#123; //启动生产者并放入map进行管理 producer.start(); producers.put(configuration.getProducerId(), producer); logger.info("MqProducer start success "+configuration.toString()); &#125; catch (MQClientException e) &#123; logger.error("MqProducer start error "+configuration.toString(),e); throw new RuntimeException(e); &#125; return producer;&#125; 创建消费者： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 创建一个消费者 * @param mqConsumerConfiguration * @return */public RocketMQMessageConsumer createConsumer(MqConsumerConfiguration mqConsumerConfiguration, List&lt;IProcessor&gt; list) &#123; //如果map里存在，直接返回 if (consumers.get(mqConsumerConfiguration.getConsumerId()) != null) &#123; return consumers.get(mqConsumerConfiguration.getConsumerId()); &#125; try &#123; RocketMQMessageConsumer consumer = new RocketMQMessageConsumer(mqConsumerConfiguration.getConsumerId(), mqConsumerConfiguration.getGroupName(), mqConsumerConfiguration.getNamesrvAddr()); consumer.subscribe(mqConsumerConfiguration.getTopicAndTagMap()); //设置消费者其它参数 if(!CollectionUtils.isEmpty(mqConsumerConfiguration.getOptions()))&#123; String consumeFromWhere = mqConsumerConfiguration.getOptions().get("consumeFromWhere"); String consumeThreadMin = mqConsumerConfiguration.getOptions().get("consumeThreadMin"); String consumeThreadMax = mqConsumerConfiguration.getOptions().get("consumeThreadMax"); String pullThresholdForQueue = mqConsumerConfiguration.getOptions().get("pullThresholdForQueue"); String consumeMessageBatchMaxSize = mqConsumerConfiguration.getOptions().get("consumeMessageBatchMaxSize"); String pullBatchSize = mqConsumerConfiguration.getOptions().get("pullBatchSize"); String pullInterval = mqConsumerConfiguration.getOptions().get("pullInterval"); if (StringUtils.isNotBlank(consumeFromWhere)) &#123; if (StringUtils.equals(consumeFromWhere, "CONSUME_FROM_LAST_OFFSET")) &#123; consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET); &#125; else if (StringUtils.equals(consumeFromWhere, "CONSUME_FROM_FIRST_OFFSET")) &#123; consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); &#125; &#125; if (StringUtils.isNotBlank(consumeThreadMin)) &#123; consumer.setConsumeThreadMin(Integer.parseInt(consumeThreadMin)); &#125; if (StringUtils.isNotBlank(consumeThreadMax)) &#123; consumer.setConsumeThreadMax(Integer.parseInt(consumeThreadMax)); &#125; if (StringUtils.isNotBlank(pullThresholdForQueue)) &#123; consumer.setPullThresholdForQueue(Integer.parseInt(pullThresholdForQueue)); &#125; if (StringUtils.isNotBlank(consumeMessageBatchMaxSize)) &#123; consumer.setConsumeMessageBatchMaxSize(Integer.parseInt(consumeMessageBatchMaxSize)); &#125; if (StringUtils.isNotBlank(pullBatchSize)) &#123; consumer.setPullBatchSize(Integer.parseInt(pullBatchSize)); &#125; if (StringUtils.isNotBlank(pullInterval)) &#123; consumer.setPullInterval(Integer.parseInt(pullInterval)); &#125; &#125; //设置消费者监听 if(mqConsumerConfiguration.isOrderly())&#123; OrderlyRocketMQMessageListener orderlyRocketMQMessageListener=new OrderlyRocketMQMessageListener(); orderlyRocketMQMessageListener.setProcessorList(list); consumer.registerMessageListener(orderlyRocketMQMessageListener); &#125;else&#123; ConcurrentlyRocketMQMessageListener concurrentlyRocketMQMessageListener=new ConcurrentlyRocketMQMessageListener(); concurrentlyRocketMQMessageListener.setProcessorList(list); consumer.registerMessageListener(concurrentlyRocketMQMessageListener); &#125; consumer.start(); consumers.put(mqConsumerConfiguration.getConsumerId(), consumer); logger.info("MqConsumer start success "+mqConsumerConfiguration.toString()); logger.info("MqConsumer processors size "+list.size()); return consumer; &#125; catch (Exception e) &#123; logger.error("MqConsumer start error", e); throw new RuntimeException(e); &#125;&#125; 它们的其它方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344 /** * 获取一个生产者 * @param producerId * @return */ public RocketMQMessageProducer getProducer(String producerId)&#123; if(producers.get(producerId)!=null)&#123; return producers.get(producerId); &#125; return null; &#125; /** * 停止某个生产者 * @param producerId */ public void stopProducer(String producerId)&#123; if(producers.get(producerId)!=null)&#123; producers.get(producerId).shutdown(); producers.remove(producerId); logger.info("MqProducer "+producerId+" is shutdown!"); &#125; &#125;/** * 获取一个消费者 * @param customerId * @return */ public RocketMQMessageConsumer getConsumer(String customerId)&#123; if(consumers.get(customerId)!=null)&#123; return consumers.get(customerId); &#125; return null; &#125; /** * 停止某个消费者 * @param customerId */ public void stopConsumer(String customerId)&#123; if(consumers.get(customerId)!=null)&#123; consumers.get(customerId).shutdown(); consumers.remove(customerId); logger.info("MqConsumer "+customerId+" is shutdown!"); &#125; &#125; F 然后到了我们关键的自动配置部分了。 让这个类实现ApplicationContextAware可以拿到applicationContext。 同时生成一个FactoryBean。 主要方法： 12345678910111213141516171819202122232425262728293031323334353637@Bean@ConditionalOnMissingBean@ConditionalOnProperty(prefix = "spring.rocketmq.config", name = "enabled", havingValue = "true")RocketMQFactory rocketMQFactory()&#123; RocketMQFactory rocketMQFactory=new RocketMQFactory(); //处理生产者 if(mqConfigurations.getMqProducerConfigurations()!=null&amp;&amp;mqConfigurations.getMqProducerConfigurations().size()&gt;0)&#123; mqConfigurations.getMqProducerConfigurations().forEach(producerConfiguration -&gt;&#123; rocketMQFactory.createProducer(producerConfiguration); &#125;); &#125; if(mqConfigurations.getMqConsumerConfigurations()!=null&amp;&amp;mqConfigurations.getMqConsumerConfigurations().size()&gt;0)&#123; //处理消费者 mqConfigurations.getMqConsumerConfigurations().forEach(consumerConfiguration-&gt;&#123; //处理Processor final Map&lt;String, Object&gt; annotationMap = applicationContext.getBeansWithAnnotation(RocketMQProcessor.class); List&lt;IProcessor&gt; list = new ArrayList&lt;&gt;(); if(annotationMap!=null)&#123; annotationMap.forEach((key,value)-&gt;&#123; RocketMQProcessor annotation = value.getClass().getAnnotation(RocketMQProcessor.class); if(consumerConfiguration.getConsumerId().equals(annotation.consumerId()))&#123; try&#123; list.add((IProcessor) value); &#125;catch (Exception e)&#123; throw new RuntimeException(e); &#125; &#125; &#125;); &#125; rocketMQFactory.createConsumer(consumerConfiguration,list); &#125;); &#125; return rocketMQFactory;&#125; 总的来说就是拿到配置生产生产者组，生成消费者组。 在生成消费者的时候需要注册监听，一个监听可以有很多业务类，通过注解拿到业务类，放到处理器列表里，再把该监听注册到指定的customerId上。 G 配置spring.factories。 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.zwt.rocketmqspringboot.autoware.RocketMqAutoConfig H 然后我们添加application.properties进行测试。 12345678910111213141516171819202122232425262728## 是否启用RocketMQspring.rocketmq.config.enabled=true## Producer1配置spring.rocketmq.config.mq-producer-configurations[0].group-name=rocketmq-producerspring.rocketmq.config.mq-producer-configurations[0].namesrv-addr=localhost:9876spring.rocketmq.config.mq-producer-configurations[0].producer-id=001## Consumer1配置spring.rocketmq.config.mq-consumer-configurations[0].namesrv-addr=localhost:9876spring.rocketmq.config.mq-consumer-configurations[0].group-name=rocketmq-consumerspring.rocketmq.config.mq-consumer-configurations[0].consumer-id=001spring.rocketmq.config.mq-consumer-configurations[0].topic-and-tag-map.123=123spring.rocketmq.config.mq-consumer-configurations[0].orderly=falseserver.port=8001## Producer2配置spring.rocketmq.config.mq-producer-configurations[1].group-name=rocketmq-producer1spring.rocketmq.config.mq-producer-configurations[1].namesrv-addr=localhost:9876spring.rocketmq.config.mq-producer-configurations[1].producer-id=002## Consumer2配置spring.rocketmq.config.mq-consumer-configurations[1].namesrv-addr=localhost:9876spring.rocketmq.config.mq-consumer-configurations[1].group-name=rocketmq-consumer1spring.rocketmq.config.mq-consumer-configurations[1].consumer-id=002spring.rocketmq.config.mq-consumer-configurations[1].topic-and-tag-map.1234=1234spring.rocketmq.config.mq-consumer-configurations[1].orderly=false 编写一些测试类进行测试。 123456789@Service@RocketMQProcessor(consumerId = "001")public class TestConsumer implements IProcessor &#123; private static final Logger logger = LoggerFactory.getLogger(TestConsumer.class); @Override public void handleMessage(MessageExt msg) throws Exception &#123; System.out.println(new String(msg.getBody())+"TestConsumer"); &#125;&#125; 123456789101112131415161718@Controllerpublic class TestProducer &#123; @Autowired RocketMQFactory rocketMQFactory; @RequestMapping("/test") public String doSomething() throws Exception&#123; Message message=new Message(); message.setTopic("123"); message.setTags("123"); message.setBody(new String("Hello World").getBytes()); SendResult result=rocketMQFactory.getProducer("001").sendMessage(message); System.out.println(result); return result.toString(); &#125;&#125; 可以看到正常运行。 实战我们按照上篇文章那样把它封装成jar包。 完成后进行测试。 新建一个test的SpringBoot项目。作为消费者。 引入我们的包。 PS：这儿没有命名为xxxx-spring-boot-starter的形式。 创建一个消费者。 再新建一个test1的SpringBoot项目。作为生产者。 引入我们的包并进行配置。 启动test消费者，同时使用test1生产者发送一条消息。 可以看到。 生产者发送成功： 消费者处理成功： 总结通过对RocketMQ的集成封装使用，更好地学会了如何使用RocketMQ，及对其的更多理解。 消息中间件在我们软件开发中具有重要作用，应当好好理解。 如果觉得properties配置太繁琐可以改用yml配置，会更简介好看些。 如下： 1234567891011121314151617181920212223spring: rocketmq: config: enabled: true # 是否启用RocketMQ mq-producer-configurations: # 生产者配置 - producerId: TestProducer1 # 生产者1的id groupName: producer1 # 生产者1的组名称 namesrvAddr: 127.0.0.1:9876 # 生产者1的namesrvAddr - producerId: TestProducer2 # 生产者2的id groupName: producer2 # 生产者2的组名称 namesrvAddr: 127.0.0.1:9876 # 生产者2的namesrvAddr mq-consumer-configurations: # 消费者配置 - consumerId: TestConsumer1 # 消费者1的id groupName: consumer1 # 消费者1的组名称 namesrvAddr: 127.0.0.1:9876 # 消费者1的namesrvAddr topicAndTagMap: &#123; TestTopic1:TestTag1 &#125; # 消费者1监听的topic和tag orderly: false # 消费者1是否顺序消费消息 - consumerId: TestConsumer2 # 消费者2的id groupName: consumer2 # 消费者2的组名称 namesrvAddr: 127.0.0.1:9876 # 消费者2的namesrvAddr topicAndTagMap: &#123; TestTopic2:TestTag2 &#125; # 消费者2监听的topic和tag orderly: true # 消费者2是否顺序消费消息 代码地址：rocketmq-spring-boot]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建一个自定义的spring-boot-starter]]></title>
    <url>%2Fblog%2F%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84spring-boot-starter.html</url>
    <content type="text"><![CDATA[前言现在很多项目都使用SpringBoot构建了，不知道大家在使用SpringBoot项目时有没有注意到它的一些jar包的引用。如下图： 我们引入相关starter后，在application.properties或者application.yml里面进行相关配置，便可以尽情使用相关功能了，十分方便。 今天我们构建一个自己的spring-boot-starter，来体验下spring-boot的魅力和乐趣。 正文我之前有一篇文章是ElasticSearch的介绍并且对它的Java客户端封装了下，那个例子中我们最后的配置文件叫property/es-config.properties 不知道大家还有没有印象。要使用那个jar包，我们需要配置一个es-config.properties文件，显然，我想去掉这个文件，改为application.properties的那种配置，和各种spring-boot-starter一样。 好了，我们开始吧。 创建项目首先，我们需要创建一个名为 elasticsearch-spring-boot-starter 的SpringBoot项目（除springboot基础外不需要其他dependencies）。 这儿有一点要注意的，就是项目（后面生成jar包）命名。 备注：Spring官方的starter命名一般会采用 spring-boot-starter-xxxx的形式，而官方建议非官方的starter命名可以遵循 xxxx-spring-boot-starter的形式。 引入依赖然后我们引入我们要编写的starter的一些依赖。（这里对以前封装的ElasticSearch 客户端jar包进行继续优化封装） 备注：dependency里的optional表示可选依赖，因为spring-boot父项目有日志的相关jar，我们不必再次引用。 这样配置的优点大致如下： 节约空间（不用两个jar包重复引用） 避免license许可问题（两个jar版本不同可能出现问题） 避免jar包出现冲突等问题（两个jar版本不同可能出现问题） 构建（改造）项目然后我们把原来的项目改造下。 我们移除了配置解析的一些方法及类，改用自动配置。 使用@ConfigurationProperties注解，指定要进行属性配置的Bean。 使用该注解，需要引入下面的jar包。 这个jar包的@ConfigurationProperties注解可以帮我们把application.properties的属性值映射到Bean里。 这个jar的作用是编译时生成 spring-configuration-metadata.json ，此文件主要给IDE使用，可以进行友好的提示。 我们还需要创建一个自动配置类，名为ElasticSearchStarterAutoConfigure，如下： 123456789101112131415@Configuration@ConditionalOnClass(ElasticSearchClientFactory.class)@EnableConfigurationProperties(ElasticSearchConfiguration.class)public class ElasticSearchStarterAutoConfigure &#123; @Autowired private ElasticSearchConfiguration properties; @Bean @ConditionalOnMissingBean @ConditionalOnProperty(prefix = "spring.elasticserch.zwt", name = "enabled", havingValue = "true") ElasticSearchUtil elasticSearchUtil ()&#123; ElasticSearchClientFactory elasticSearchClientFactory=new ElasticSearchClientFactory(properties); return new ElasticSearchUtil(elasticSearchClientFactory); &#125;&#125; @Configuration注解，我们可以看到它被标识为 @Component注解，可以认为它最终也会成为spring容器里的一个Bean。 @ConditionalOnClass 当指定class存在时进行Bean创建。 @EnableConfigurationProperties 为指定class启用属性配置。 @ConditionalOnMissingBean 当bean不存在时进行bean创建。 @ConditionalOnProperty 用属性对bean进行配置创建。它里面可以enabled可以控制是否启用我们的配置。 然后在resources/META-INF 文件夹下新建spring.factories文件，添加如下内容，用于Spring加载自动配置类。 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.zwt.elasticsearchspringbootstarter.factory.ElasticSearchStarterAutoConfigure 配置完成后，在application.properties里，就可以看到我们的自定义属性啦。 测试创建测试类进行测试。 application.properties配置如下： 1234## es地址spring.elasticserch.zwt.address=127.0.0.1:9300## 是否启用es客户端spring.elasticserch.zwt.enabled=true 可以看到我们的日志输出。 当我们把spring.elasticserch.zwt.enabled设置为false时，在运行可以看到bean创建失败了，即未注入属性配置bean。 提升我们把项目用maven打包发布下（clean install），可以看到大约30多M……. 原因是它里面有原始的springboot项目，还内置了tomcat等，我们如果想把该工具jar包引入到另一个springboot项目里，这显然是不合理的。 我们还需要对其进行优化。 我们删除无用的Application class（springboot启动使用，显然我们封装jar包不是为了封装web项目），application.properties（当我们把我们工具jar引入到新的springboot项目里，里面已经有了application.properties）及test类包。 然后处理下pom文件。 在pom文件里，spring-boot-starter这个依赖我们是不需要打到包里的。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt; 将其添加属性。 1&lt;optional&gt;true&lt;/optional&gt; 同时build插件做下调整（原来用的springboot的插件）。 build插件基本固定的，代码可以不用看。 12345678910111213141516171819202122232425262728293031&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;configuration&gt; &lt;attach&gt;true&lt;/attach&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 再次编译，这时候我们就可以看到jar包大小只有10几Kb啦。 应用我们随便创建一个新的springboot项目。引入我们封装的jar包。 配置application.properties文件 1234## es地址spring.elasticserch.zwt.address=127.0.0.1:9300## 是否启用es客户端spring.elasticserch.zwt.enabled=true 新建个测试类，对我们的jar包进行测试。可以看到正常运行。 总结通过一步步改造，我们成功创建了一个spring-boot-starter。 通过实践，我们也能看到springboot基于注解简化代码的配置。也是蛮不错的一次工具封装体验。 补充：上面说的@ConfigurationProperties注解也可以定义自己的配置文件。 如果我们想使用原来的es-config.properties则需要进行如下配置即可。 PS：SpringBoot官方已经有了对elasticsearch client的支持，建议使用官方相关支持工具。 自己实践时，可以简单的写一个HelloWorld去完成一个简单的spring-boot-starter的封装。 相关代码： https://github.com/JavaZWT/framework-base/tree/master/elasticsearch-spring-boot-starter]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringBoot</tag>
        <tag>spring-boot-starter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[接口管理平台YAPI]]></title>
    <url>%2Fblog%2F%E6%8E%A5%E5%8F%A3%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0YAPI.html</url>
    <content type="text"><![CDATA[前言有没有为维护接口文档而烦恼过？ 作为一个良好的开发人员，为使用者提供准确良好的接口文档也是开发的一部分。 也便于测试创建测试用例。 接口管理有很多种方式，很多是提供接口文档供使用者参考，也有一些是通过平台管理。 今天介绍一款非常好用的工具，YAPI，其主要功能是一款优秀的API接口管理平台。旨在为开发、产品、测试人员提供更优雅的接口管理服务。 我在使用这个API管理平台时，感觉还蛮不错的，特此介绍下。 官网地址：https://yapi.ymfe.org/ 我们搭建内网环境，把这款接口管理平台在内网部署下，并且简单使用下。 正文要搭建内网环境，首先要准备以下几个软件。 nodejs （必须） mongodb （必须） git （非必须） git主要是从GitHub上 clone项目的，如果采用可视化安装或者直接下载压缩包是用不到的。 没有以上软件的需要下载安装。 nodejs下载 mongodb下载 安装不做过多介绍，自己参考一些资料即可完成。 对于YAPI服务，我们采用可视化部署，执行命令： 1npm install -g yapi-cli --registry https://registry.npm.taobao.org 注：其他部署方式，详情可以查看官方文档 https://yapi.ymfe.org/devops/index.html 这儿不做讨论。 安装成功后启动 yapi server，执行命令： 1yapi server 通过浏览器打开页面 http://0.0.0.0:9090/ 会看到如下界面： 注意：需要启动mongdb数据库。 mongodb启动备注(路径未加入PATH):12cd /Users/zhangwentong/Applications/mongodb/mongodb-osx-x86_64-4.0.4/bin./mongod --dbpath /Users/zhangwentong/Applications/mongodb/data/db 部署引导完成后。我们进入到vendors 文件夹，启动YAPI服务。如下命令： 12cd my-yapi/vendors/node server/app.js 说明我们安装完成并启动成功了。浏览器打开图上地址。 使用管理员账号登陆，初始密码为 ymfe.org 其他人可以进行注册后登陆。 登陆成功后如下图： 我们可以在个人中心修改密码，这儿就不做过多介绍了。看一看它的主要功能吧。 首先我们创建一个分组。 然后我们在该分组里创建一个项目。 上图的成员列表里可以添加成员，把一起开发的开发者拉进来。 分组动态展示我们分组的一些操作等。 我们可以看到项目里内容如下： 接口主要就是用来管理我们的接口的界面，比如后台提供某些接口，全部放在里面，供其它开发人员查看使用。 动态代表着这个项目里的接口等改动动态，方便人员查看。 成员管理可以管理这个项目的成员，指定组长，开发者，外部接口使用者（访客）等。 然后我们可以添加一些模块接口分类。如下，不同接口放在不同模块下。 继续，我们新增加一个接口来看看吧。 新增OK后对接口进行修改。 当我们接口参数定义好，或者说后台接口完成后（可以测试时），我们应该把接口状态改为已完成。 一个写好的接口demo如下： 同时动态里也可以看到我们对于接口的改动。 同时YAPI还支持测试接口，mock，wiki等，类似于Postman等工具。 这儿就不过多介绍了，对于我们开发人员，最主要的应该属对接口文档的维护了，也就是刚才介绍的部分。 结语今天说到这里也差不多啦，YAPI还可以控制人员的权限、给指定人员发送邮件、设置是否可以注册等其它功能，有兴趣的可以摸索。 我们对YAPI的使用就是用其对接口文档进行管理。它是易用的简便的。 在项目开发过程中，在公司服务器上搭建一套环境，对组成员接口文档维护也是受益的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>YAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 新的日期API]]></title>
    <url>%2Fblog%2FJava-8-%E6%96%B0%E7%9A%84%E6%97%A5%E6%9C%9FAPI.html</url>
    <content type="text"><![CDATA[前言Java 8 中引入了新的一套日期API，相对于之前的Date或者Calendar类，这套API更好的解决了日期和时间的问题。 我们来简单看下。 正文LocalDateTime,LocalDate,LocalTime开始使用日期API时，最先碰到的也是这三个类。 他们的静态工厂of方法可以创建日期实例。如下代码： 1234567//1. 第一部分//创建一个日期LocalDate date=LocalDate.of(2018,11,22);//时分秒LocalTime time=LocalTime.of(13,45,20);//LocalDateTime对象LocalDateTime dateTime=LocalDateTime.of(2018,Month.NOVEMBER,22,13,45,20); 是不是很简单。 要注意这三个类都是final的，即不可被改变的。 LocalDate 表示年月日，LocalTime表示时分秒，LocalDateTime表示年月日时分秒。 他们三个之间的转换也是比较容易的。如下： 1234567891011//2. 第二部分//使用localDate和localTime构造一个LocalDateTimeLocalDateTime dateTime1=LocalDateTime.of(date,time);//使用LocalDate构造一个LocalDateTimeLocalDateTime dateTime2=date.atStartOfDay();//这一天的00:00:00LocalDateTime dateTime3=date.atTime(LocalTime.of(12,12,12));//指定这一天的时间//使用LocalTime构造LocalDateTimeLocalDateTime dateTime4=time.atDate(LocalDate.of(2018,11,22));//指定日期//通过LocalDateTime获取LocalDate和LocalTimeLocalDate date1=dateTime.toLocalDate();LocalTime time1=dateTime.toLocalTime(); 对于固定的日期，我们可以获取它的时间信息，也是比较容易的。如具体年月日的数值，等等。 123456789101112131415161718192021//3. 第三部分//获取日期年份int year=dateTime.getYear();int year1=dateTime.get(ChronoField.YEAR);//获取日期月份Month month=dateTime.getMonth();int month1=month.getValue();int month2=dateTime.get(ChronoField.MONTH_OF_YEAR);//获取当月第几天int day=dateTime.getDayOfMonth();int day1=dateTime.get(ChronoField.DAY_OF_MONTH);//获取星期几DayOfWeek dow=dateTime.getDayOfWeek();//获取该月有几天int len=date.lengthOfMonth();//获取小时数int hour=dateTime.getHour();//获取分钟int minute=dateTime.getMinute();//获取秒数int second=dateTime.getSecond(); 他们还有一些常用的方法，如时间和字符串时间之间的转化、判断闰年、获取当前时间信息、时间的比较、时间的加减天数（年数等）等方法。 相比于我们单独封装处理Date，或者Calendar类，更简便和安全了。 1234567891011121314151617181920212223242526272829//4. 第四部分//是不是闰年boolean leap=date.isLeapYear();//获取当前时间信息LocalDateTime localDateTimeNow=LocalDateTime.now();LocalDate localDateNow=LocalDate.now();LocalTime localTimeNow=LocalTime.now();//字符串转时间LocalDate localDate1=LocalDate.parse("2014-03-18");LocalTime localTime1=LocalTime.parse("13:45:20");LocalDateTime localDateTime1=LocalDateTime.parse("2018/11/22 11:22:33", DateTimeFormatter.ofPattern("yyyy/MM/dd HH:mm:ss"));//时间转换为字符串String localDateStr=localDate1.toString();String localTimeStr=localTime1.format(DateTimeFormatter.ofPattern("HH:mm:ss"));String localDateTimeStr=localDateTime1.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"));//当前时间减10天LocalDateTime localDateTime2=localDateTime1.minusDays(10);//当前时间加1年LocalDateTime localDateTime3=localDateTime1.plusYears(1);//当前时间加1个月LocalDateTime localDateTime4=localDateTime1.minus(-1, ChronoUnit.MONTHS);//当前时间加1个月LocalDateTime localDateTime5=localDateTime1.plus(1, ChronoUnit.MONTHS);//更改日期时间，返回新的对象，原对象不会变化LocalDate localDate=date1.with(ChronoField.MONTH_OF_YEAR,9);//日期时间比较boolean flag=localDateTime2.isAfter(localDateTime3);boolean flag1=localDateTime2.isBefore(localDateTime3);boolean flag2=localDateTime2.isEqual(localDateTime3); 这里面对于时间的加减（plus，minus方法），赋值（with方法）都会生成新的LocalDateTime对象，不会对原来的对象做修改。 Period、Duration 类这两个类都可以表示日期时间的差值，Period的话表示年月日，如两个时间差1年或者-1个月，Duration 的话用来表示天时分秒，比如它可以表示两个时间差相差34.5s这样的数据。 123456789//5. 第五部分//计算两个时间差Duration d1=Duration.between(localDateTime2,localDateTime3);long days1=d1.toDays();Duration d2=Duration.between(localDateTime4,localDateTime5);long hours=d2.toHours();//计算相差时间，结果10天Period period=Period.between(LocalDate.of(2014,3,8),LocalDate.of(2014,3,18));int days2=period.getDays(); Instant和 ZoneId从计算机的角度来看，建模时间最自然的格式是表示一个持续时间段上某个点的单一大整型数。这也是新的 java.time.Instant 类对时间建模的方式，基本上它是以Unix元年时间（传统的设定为UTC时区1970年1月1日午夜时分）开始所经历的秒数进行计算。而每个ZoneId由该地区的ID标识。地区ID都为“{区域}/{城市}”的格式，这些地区集合的设定都由英特网编号分配机构（IANA）的时区数据库提供。 看下它们的用法。 123456789101112131415161718//6. 第六部分//机器时间//以下均表示3s时间Instant.ofEpochSecond(3);Instant.ofEpochSecond(3,0);Instant.ofEpochSecond(2,1000000);Instant.ofEpochSecond(4,-1000000);//当前时间的时间戳Instant.now();//localDateTime转换为instantInstant instantFormDateTime=localDateTime1.toInstant(ZoneOffset.UTC);//获取本地区的zoneIdZoneId romeZone= TimeZone.getDefault().toZoneId();//instant转localDateTimeInstant instant1=Instant.now();LocalDateTime timeFromInstant=LocalDateTime.ofInstant(instant1,romeZone);//将时区设置为欧洲罗马城市。ZoneId romeZone1 = ZoneId.of("Europe/Rome"); 一般传统的Date和LocalDate之间的转换会用到它们（Instant，ZoneId）。 其他在一些项目中，应用Date还是很多的。我们想使用新的日期API，又不太想改动源代码。可以写一些转换的工具类，或者使用Java8的一些日期API对Date进行处理。 如Date转化为LocalDateTime，LocalDateTime转换为Date等。 12345678910111213141516171819202122232425/** * Java8 LocalDateTime对象转Date对象 * @param localDateTime * @return */public static Date localDateTime2Date(LocalDateTime localDateTime)&#123; //获取zoneId ZoneId zone = ZoneId.systemDefault(); //将localDateTime转换为Instant对象 Instant instant = localDateTime.atZone(zone).toInstant(); return Date.from(instant);&#125;/** * Date对象 转 Java8 LocalDateTime对象 * @param date * @return */public static LocalDateTime date2LocalDateTime(Date date)&#123; //根据date拿到Instant Instant instant = date.toInstant(); ZoneId zone = ZoneId.systemDefault(); //转换为LocalDateTime return LocalDateTime.ofInstant(instant, zone);&#125; 将字符串日期格式化为LocalDateTime，或者将LocalDateTime转换为字符串时间。 12345678910111213141516171819/** * 将日期格式化为指定的格式 * @param date * @return */ public static String localDateTime2String(Date date)&#123; LocalDateTime localDateTime=date2LocalDateTime(date); return DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss").format(localDateTime); &#125; /** * 将string时间格式转化为LocalDateTime * @param string * @return */ public static LocalDateTime string2LocalDateTime(String string)&#123; DateTimeFormatter df=DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"); return LocalDateTime.parse(string,df); &#125; 有了这些比较基础的方法，我们某些日期便可以使用Java8处理。比如项目使用Date，在判断闰年，或者时间的加减等，可以封装Java8工具类。 12345678910111213141516171819202122232425262728293031/** * 获取两个日期的时间间隔 * @param date1 * @param date2 * @return */public static long diffDays(Date date1,Date date2)&#123; LocalDateTime localDateTime1=date2LocalDateTime(date1); LocalDateTime localDateTime2=date2LocalDateTime(date2); return Duration.between(localDateTime1,localDateTime2).toDays();&#125;/** * 当期日期增加多少天后的日期，负数为减少多少天 * @param date * @param days * @return */public static Date addDays(Date date,int days)&#123; LocalDateTime localDateTime1=date2LocalDateTime(date); LocalDateTime localDateTime2=localDateTime1.minusDays(days); return localDateTime2Date(localDateTime2);&#125;/** * 判断是否是闰年 * @param date * @return */public static boolean isLeapYear(Date date)&#123; LocalDateTime localDateTime=date2LocalDateTime(date); return localDateTime.toLocalDate().isLeapYear();&#125; 这样对于Date的处理，均使用LocalDateTime处理，虽然整个项目把Date改造成LocalDateTime较困难，但是工具类相当于黑匣子，这样慢慢使用LocalDateTime去处理Date，也是蛮不错的一次体验。 总结Java8 新的日期API比较优秀的地方是更直观了，使用更简洁。而且不用担心变量污染问题，想想一般的Date，对日期进行操作，如果不小心没有创建新对象，会把传入的Date改变掉，是很不安全的。 而且它也没有时间从1900年起那种莫名其妙的限制，而且获取到的月份值是1-12，不是0-11，也是符合自然的。 其实Java 8 的日期API不单单完成了对于Java日期的优化，其更多的方法更像是一种工具API，如判断是不是闰年，求两日期之差等等常用方法，都被封装在了Java8的新的日期API里面了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码的优雅]]></title>
    <url>%2Fblog%2F%E4%BB%A3%E7%A0%81%E7%9A%84%E4%BC%98%E9%9B%85.html</url>
    <content type="text"><![CDATA[前言很早之前就想写这么一篇文章，苦于准备不足和时间有限吧，进度一拖再拖。 来到新公司差不多3个多月了，让我感到或者说受益比较大的可以说就是公司的代码风格吧。 估计也是由于有架构师的缘故，大家的代码质量感觉都比较高。 扯远了…… 今天结合实际聊一下关于如何让代码变得规范、优雅吧。 正文遵守规范这一点或许可以说很笼统，目前大家除了遵守公司制定的一些规范，针对于Java我还比较建议的是阅读下阿里的《阿里巴巴Java开发手册》，H5前端的可以看看Github 上的这个规范http://alloyteam.github.io/CodeGuide/ ，也是蛮不错的。 必要的封装当有一些重复，或者有共性的代码时，我们可以对它们进行包装。小到一些特殊的判断，大到常用的设计模式。 如下面的代码，实现相同的功能，明显第二种要好些。 这些必要的封装不仅可以减少一些额外的工作量，还能提高代码可阅读性。 一些工具包，如apache.commons 已经封装了一些通用校验，我们是可以直接拿来使用的。 学会占位符的使用这一点打印日志比较常见到。如下： 可以看到两者的输出是一样的，但是第一种是方便我们进行程序源码阅读的。 除了String.format里的%s占位符，log4j里的{}占位符也可以实现这种作用。 枚举和常量的使用正确使用一些枚举和常量可以提高代码的阅读性。 比如我们创建一个水果枚举。 进行某些方法，这些方法更容易被阅读和维护。 异常的正确使用或许你对异常不感冒，觉得有异常还要try catch是一件很费劲的事情，也影响代码美观，怎么会使代码优雅呢？ 我们可以举个例子看一下异常的一种好玩的用法。 比如有一个业务逻辑，需要通过分布式接口获取其它数据，比如分三步： 这种情况下每个Res我们需要判断成功，才能获取数据。 即retCode为0000时。其它为失败。 这时候我们可以创建异常类。用于对retCode和retInfo进行接收。 如果上面步骤中获取失败，直接上抛。 最后可以使用try catch统一处理返回。 还有一些异常的用法，在这里就不一一说明了。 必要时尝试使用多线程可以在必要时使用多线程来提高程序运行效率。 对于上面的例子，查询可用余额，查询提现次数，查询银行卡信息相互之间是不影响的（他们的结果只取决于userId或者customerId的值）。我们可以使用Future获取结果，减少程序运行时间。 使用多线程分别去请求获取结果，最后在进行处理。 结语规范的代码不仅自己赏心悦目，别人看后也有所启发，养成编写良好代码的习惯，对自己、对别人都是有所帮助的。 其实最重要的还是多学、多练、多思考吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[和朋友的一次关于数组问题讨论]]></title>
    <url>%2Fblog%2F%E5%92%8C%E6%9C%8B%E5%8F%8B%E7%9A%84%E4%B8%80%E6%AC%A1%E6%95%B0%E7%BB%84%E9%97%AE%E9%A2%98%E8%AE%A8%E8%AE%BA.html</url>
    <content type="text"><![CDATA[前言前两天下班和一朋友一起，被问到如下问题：如何移除一个数组中某些符合条件（或不符合条件）的元素形成新的数组，要求数组长度也应该变化（这是我后面加的）。 这两天研究下了这个问题，感觉比较有意思，也从中学到了一些其它该注意的东西。 特此来分享一下。 如有问题欢迎批评指正。 正文对于上面的问题，最简单也最应该想到的一个应该是借助List这个工具类。 是的，我写下大致如下代码： （1）使用List帮助类实现,借助remove方法优点：代码简洁，便于理解，不易出错 缺点：数组和List转换耗时，效率不算太高，使用List remove方法时应注意线程安全问题，如果是基本数据类型，需要转换为包装类，拆箱装箱也是影响效率的一个因素。 代码如下： 123456789101112131415/** * 过滤形成新的数组（转换为List，通过List里面remove(安全的)移除元素） * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray1(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //asList生成的是一个List的内部类，无法调用add remove delete方法会抛出异常 List&lt;T&gt; sourceList= new ArrayList&lt;&gt;(Arrays.asList(sourceArray)); //使用Java8 函数式接口，移除不符合条件的元素 sourceList.removeIf(predicate); return sourceList.toArray((T[]) Array.newInstance(type,0));&#125; 注： Predicate predicate 相当于一个判断条件（lambda表达式，Java8语法），具体的问题也可以写明，就是那种for list 在加上 if 条件判断的，这儿我就不啰嗦了。Java 8里面对于集合类，新增了 removeIf方法，我们可以看下它，其实就是我们上面说的那个。 关于这种写法，有需要注意的几点： for-each，for, iterator 这三种对于List的循环，for，iterator是可以遍历并进行remove操作的，但是for-each是不可以的。有兴趣的可以研究研究，这不是我们的重点。 看到我上面使用的泛型T，其实使用Object[]也是可以的，要注意一个问题，泛型T是不能包含int,long等基本数据类型的，使用的话只能转化为它们的包装类。 Object[]是没有限制的，但是Object[]转换为Integer[]或者int[]或者其他不能直接转换，如下写法是错误的，会出现问题。 12Object [] objects=new Object[10];Integer [] a=(Integer[])objects; 正确的转换方法应该循环里面的元素，并对它们强转添加进数组；或者使用下面的方法。123Object [] objects=new Object[10];//Integer [] a=(Integer[])objects;Integer [] a=(Integer[])Array.newInstance(Integer.class,objects.length); 当我们把数组转换成List的时候，为了不想循环遍历添加，可能会想到使用Arrays.asList(T t) 这个方法，这个方法生成的List是一个Arrays里面的一个内部类ArrayList。 看下这个内部类你会发现它没有对remove、add等方法的实现，也就是继承自AbstractList。 123public E remove(int index) &#123; throw new UnsupportedOperationException();&#125; 也就是执行它们时会抛出异常，如果想使用remove方法，应该把它转为具体实现，如ArrayList。 1List&lt;T&gt; sourceList= new ArrayList&lt;&gt;(Arrays.asList(sourceArray)); 为什么它要使用内部类处理这个方法呢？也是比较有趣的，我这儿还没做深入研究，有时间研究下。 (2)使用两次循环实现。原理是第一次数组循环查找符合条件（或不符合条件）的个数count，后面在创建一个指定长度（原数组总长度-count）的数组，然后在遍历循环一遍原数组，将符合条件（或不符合条件）的元素添加进新的数组。 优点：简单直接，易于理解，基本数据类型数组的处理应该比List方法有优势。 缺点：两次循环应该比较耗时，对于长Array应该显现的明显。 代码如下： 12345678910111213141516171819202122232425262728293031323334/** * 过滤形成新的数组（两次循环查找符合条件的，移动过去） * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray2(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; int count=0; for (T t:sourceArray)&#123; if(predicate.test(t))&#123; count++; &#125; &#125; //都不符合条件 if(count==0)&#123; return Arrays.copyOf(sourceArray,sourceArray.length); &#125; //都符合条件 if(count==sourceArray.length)&#123; return (T[]) Array.newInstance(type,0); &#125; T [] targetArray=(T[]) Array.newInstance(type,sourceArray.length-count); int index=0; for (T t:sourceArray)&#123; if(!predicate.test(t))&#123; targetArray[index]=t; index++; &#125; &#125; return targetArray;&#125; 注意：还有一种情况，如果过滤的数据（符合条件或者不符合条件的）出现次数较低，我们是不是可以考虑一次拿出一整段进行处理。当然，如果频率较高，比如在一堆自然数中取偶数，明显奇数偶数出现频率相近，都为50%，那么我们可能用上面这种方法效率也很好。 (3)预先设置等长数组，而后截取得到目标数组对比上一种方法，这是一种空间换时间的做法。 开始时创建一个和原数组相同大小的数组，遍历后把元素放进去，最后将数组截短。这种方法仅仅循环一次。 代码如下： 123456789101112131415161718192021222324252627/** * 对比第二种方法，这属于空间换时间的做法 * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray3(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //直接创建一个空的一样长的数组 T[] tempArray=(T[]) Array.newInstance(type,sourceArray.length); //不符合条件的数量 int count=0; for (T t:sourceArray)&#123; //拿到不符合过滤条件的，一个个赋值给新数组 if(!predicate.test(t))&#123; tempArray[count]=t; count++; &#125; &#125; //最后这个数组长度&lt;=原数组长度 //特殊处理下 if(count==0)&#123; return (T[]) Array.newInstance(type,0); &#125; return Arrays.copyOf(tempArray,count);&#125; (4)借助List的toArray方法借助List实现循环一次把符合条件的放到里面，再把List强转成数组。也是不错的实现方法。 代码如下： 123456789101112131415161718/** * 借助list拿到符合条件的，在强转成数组 * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray4(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; //记录符合条件的元素下标 List&lt;T&gt; targetList=new ArrayList&lt;&gt;(); for (T t:sourceArray)&#123; if(!predicate.test(t))&#123; targetList.add(t); &#125; &#125; return targetList.toArray((T[]) Array.newInstance(type,0));&#125; 无论上面哪种方法，其底层都使用了System.arraycopy方法。 12345678910/*** 数组复制核心方法* @param src 原数组* @param srcPos 原数组要复制的起始位置下标* @param dest 目标数组* @param destPos 目标数组的起始位置下标* @param length 要复制的长度* @return*/public static native void arraycopy(Object src, int srcPos,Object dest, int destPos,int length); (5)Java8串行处理方式Java8中使用Stream操作集合工具类来对其进行处理。分为串行和并行两种方式。先来看看串行。 代码如下： 1234567891011121314/** * Java8 串行流语法(收集符合条件的) * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray5(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; return Arrays.stream(sourceArray).sequential(). filter(predicate). collect(Collectors.toList()). toArray((T[]) Array.newInstance(type,0));&#125; 其底层也是元素的循环遍历。 （6）Java8并行处理方式我们应该知道，无论方法怎样，至少应该遍历数组一次以判断该元素是否符合条件。当数据量较大时，这儿会成为方法运行时间的瓶颈，由于List家族中ArrayList是有序的，我们可以使用多线程对它进行分割，每段进行遍历筛选结果，最后再把结果合起来。 并行流就是利用分支/合并框架实现的，使用了多线程。当数组数据量较大时效率是明显的。 Java8的相关API已经封装好，我们可以直接使用。 代码如下： 1234567891011121314/** * Java8 并行流语法(收集符合条件的) * @param sourceArray * @param predicate * @param type * @param &lt;T&gt; * @return */public static &lt;T&gt; T[] getArray6(T[] sourceArray,Predicate&lt;T&gt; predicate,Class&lt;T&gt; type)&#123; return Arrays.stream(sourceArray).parallel(). filter(predicate). collect(Collectors.toList()). toArray((T[]) Array.newInstance(type,0));&#125; (7)自己动手，丰衣足食我们其实是可以借助多线程自己实现一个相似工具类的。 可以使用分支/合并框架自己实现一个多线程的处理。 关于这一块，我有一篇文章 一道Java试题引发的思考中有具体例子及测试。 大家可以看下，数据量大的情况下并行效率还是比较明显的。 我这儿对这个例子就不在验证了。 注意：使用并行流（或者说多线程）要注意的点。 首先是数据量，数据量的主要意义就在于单线程处理的耗时（处理数据的时间）已经超过了多线程耗时（数据处理时间+拆分数据时间+合并结果时间），这一点是比较难把控的。其次一点是要确定这些数据可以使用多线程处理，不会产生意外的情况，比如我们这个问题，我想删除两个数相差1的所有元素，剩下的元素生成一个新的数组，多线程显然不易解决这种问题，或者解决起来较复杂。 测试测试的话今天就省了，（3）、（4）、（5）都是不错的写法，（6）话具体问题具体分析，（7）的话有想法的可以试试，（1），（2）不推荐。 因为数据量大小，数据类型都对方法有些影响。 比如较短的原数组，基本数据类型，（3）方法效率很快的，对比（4）、（5）是没有数据拆箱操作的。换成长数组，引用数据类型，（6）可能效率就高了。 结语开始写这篇文章的时候脑子不好使，根本没想到Java8的Stream，失误。我甚至一开始想的都是些可能不安全（（1）方法），或者比较繁琐（（2）方法）的方法，过了一天晾了晾脑子就好使多了。 在处理时，因为数组底层操作都是基于System.arraycopy嘛，我想到是不是循环一次记录符合条件（不符合条件）的元素下标（可用List记录），然后建立一个目标数组，使用System.arraycopy一段一段的将数据copy进去。 无奈才疏学浅，想了半天使用System.arraycopy时的两个起始位置，copy长度始终弄错了，仔细想了下，估计这种方法效率也不怎么高。 哈哈，于是就没写。 大家有什么好的、耳目一新的方法也可以说出来交流交流。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ简介]]></title>
    <url>%2Fblog%2FRocketMQ%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言 为什么要学习RocketMQ？ RocketMQ是一个队列模型的的消息中间件，具有高性能、高可靠、高实时、分布式等特点。 其主要特点如下： 生产者、消费者、队列都可以分布式。 能够保证严格的消息顺序。 提供丰富的消息拉取模式。 高效的订阅者水平扩展能力。 实时的消息订阅机制。 亿级消息堆积能力。 较少的依赖。 RocketMQ的基本原理就是生产者（Producer）向一些队列轮流发送消息，队列集合称为Topic，消费者（Consumer）如果做广播消费，则一个消费者实例消费这个Topic对应的所有队列，如果做集群消费，则多个消费者实例平均消费这个topic对应的队列集合。 消息中间件收发消息的典型模型如下图： 好吧，我承认前言太官方了。我们来看下正文关于RocketMQ的一些特点吧。 正文RocketMQ术语。先介绍RocketMQ的几个术语。 Producer 消息生产者，生产者的作用就是将消息发送到Broker（MQ），生产者本身既可以产生消息。也可以对外提供接口，由外部应用来调用接口，再由生产者将收到的消息发送到 Broker（MQ）。 Producer Group 生产者组，一般来说多个发送同一类消息的生产者称之为一个生产者组。 Consumer 消息消费者，消费 Broker（MQ） 上的消息的应用程序就是消费者，消息的处理取决于业务。 Consumer Group 消费者组，和生产者类似，消费同一类消息的多个 consumer 实例组成一个消费者组。 Topic Topic 是一种消息的逻辑分类，比如说你有支付订单类的消息，也有奖品抽取类的消息，那么就需要进行分类，一个是支付订单 Topic 存放支付订单相关的消息，一个是奖品抽取 Topic 存储抽取奖品结果相关的消息。 Message Message 是消息的载体。一个 Message 必须指定 topic。Message 还有一个可选的 tag 设置，以便消费者可以基于 tag 进行过滤消息。也可以添加额外的键值对等。 Tag 标签可以被认为是对 Topic 进一步细化。一般在相同业务模块中通过引入标签来标记不同用途的消息。 Broker Broker 是 RocketMQ系统的主要角色。Broker 接收来自生产者的消息，储存以及为消费者拉取消息的请求做好准备。 Name Server Name Server 为 producer 和 consumer 提供路由信息。 对于上面的几个概念的理解，可以从一封邮件的发送入手，认为发信人是Producer，收信人为Consumer，Message，Topic和Tag分别指信的内容，信的分类规则等，Broker就相当于邮局了。 RocketMQ集群架构再来看一下RocketMQ常用的物理部署结构。（生产环境常用的RocketMQ集群架构） RocketMQ的集群特点如下： Name Server是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。 Broker部署相对复杂，Broker分为Master与Slave，一个Master可以对应多个Slave，但是一个Slave只能对应一个Master，Master与Slave的对应关系通过指定相同的BrokerName，不同的BrokerId来定义，BrokerId为0表示Master，非0表示Slave。Master也可以部署多个。每个Broker与Name Server集群中的所有节点建立长连接，定时注册Topic信息到所有Name Server。 Producer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master建立长连接，且定时向Master发送心跳。Producer完全无状态，可集群部署。要注意Producer并不能和 Broker Slave建立连接。 Consumer与Name Server集群中的其中一个节点（随机选择）建立长连接，定期从Name Server取Topic路由信息，并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳。Consumer既可以从Master订阅消息，也可以从Slave订阅消息，订阅规则由Broker配置决定。 Broker Master和 Broker Slave之间会进行数据同步，即Data Sync。 数据复制主要有两种： ①同步复制 SYNC_MASTER master 和 slave 都写成功后返回成功状态。好处是如果master出故障，slave上有全部备份，容易恢复。缺点是增大延迟，降低吞吐量。 ②异步复制 ASYNC_MASTER 只要 master 写成功就返回成功状态。好处是低延迟、高吞吐，缺点是如果 master 出故障，数据没有写入 slave，就会有丢失。 master或者 slave在返回处理成功消息时，需要将数据持久化（写入磁盘）。称之为MQ的刷盘。 刷盘策略也有两种： ①同步刷盘 SYNC_FLUSH 返回成功状态时，消息已经被写入磁盘。 消息写入内存后，立即通知刷盘线程，刷盘完成后，返回消息写成功的状态。 ②异步刷盘 ASYNC_FLUSH 返回成功状态时，消息只是被写入内存，写操作返回快，吞吐量达，当内存里的消息积累到一定程度时，统一出发写磁盘动作，快速写入。 RocketMQ部署模式根据上面的描述，我们也大致可以知道RocketMQ通常有3种部署模式。 单master部署 简单来说就是环境上就一台RocketMQ，一般自己开发研究学习会用这种模式。这种部署明显的缺点就是MQ宕机后系统就会挂掉。所以不用于测试或生产环境。 多master部署 就是环境上有多台RocketMQ，都充当master的角色。这种方式部署，如果有一台MQ挂掉或者重启，并不会对整个应用系统造成影响。但是该节点MQ宕机期间，未消费的MQ消息是无法被消费的，只有等该节点故障排除后才能恢复正常。如果无法恢复，则会造成RocketMQ消息的永久丢失。 这种方式，如果是使用的异步刷盘，则故障发生并恢复正常后可能丢失部分消息（写入内存没来得及写入磁盘的消息）；如果同步刷盘，则不会出现此情况，但同步刷盘会对MQ的响应速度造成一些影响。 多master 多slave部署 根据刷盘策略和数据复制策略。我们知道这种部署方式基本有四种组合策略。同步复制同步刷盘明显是效率最差但最安全的，异步复制异步刷盘明显是效率最好但最不安全的。 一般情况下，我们比较常用的是异步刷盘+同步复制的模式。这样即使master 宕机，同步复制的slave也能保证把消息写入磁盘。同时使用异步刷盘策略，因为写入磁盘本身是应用系统中耗时的一个操作，先写入内存，随后写入磁盘，可以保证MQ的响应速度。 结语这一篇文章简单介绍了RocketMQ的一些性质和特点，了解这些特点，才能更好的掌握RocketMQ，后面我们会搭建一个RocketMQ服务进行下学习，并对我们的调用代码做些优化，封装为自己的一个小小工具类。 回答下最开始的问题，因为我们公司用到了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring Transaction 注解不生效bug引发的思考]]></title>
    <url>%2Fblog%2F%E4%B8%80%E4%B8%AASpring-Transaction-%E6%B3%A8%E8%A7%A3%E4%B8%8D%E7%94%9F%E6%95%88bug%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83.html</url>
    <content type="text"><![CDATA[前言某日，在项目测试代码过程中，发现一个问题，对于一个方法A（无事务），调用B方法（有事务），当A，B方法在同一个类中的时候，在B方法上的事务注解是不生效的！ 同事说将B方法写到新的Service类中就可以解决，遂试之，确实得以解决。但不解其原理，问同事、查资料均感觉不如意。故分析了下Spring 事务的部分源码。有所见解，特此记录。 下图就是我描述的这种情况，B事务不生效的问题。 1.测试类 2.实现类 我们经过测试可以发现，当insert方法有事务、但被该实现类内部方法doInsert调用后，即使insert方法出现异常，该方法的数据库操作也不会回滚。 3.数据没有回滚，已经入库 正文要理解研究这种情况，我们先来简单说下Spring 的注解方式的事务实现机制。 事务的一些基础我在一篇文章中有介绍 https://www.sakuratears.top/blog/Spring-Transactional%E6%B3%A8%E8%A7%A320181013/ 不懂得可以先大致看看。 Spring注解方式的事务实现机制在应用系统调用声明@Transactional 的目标方法时，Spring Framework 默认使用 AOP 代理，在代码运行时生成一个代理对象，根据@Transactional 的属性配置信息，这个代理对象决定该声明@Transactional 的目标方法是否由拦截器 TransactionInterceptor 来使用拦截，在 TransactionInterceptor 拦截时，会在在目标方法开始执行之前创建并加入事务，并执行目标方法的逻辑, 最后根据执行情况是否出现异常，利用抽象事务管理器AbstractPlatformTransactionManager 操作数据源 DataSource 提交或回滚事务, 如图。 Spring AOP 代理有 CglibAopProxy 和 JdkDynamicAopProxy 两种，上图以 CglibAopProxy 为例，对于 CglibAopProxy，需要调用其内部类的 DynamicAdvisedInterceptor 的 intercept 方法。对于 JdkDynamicAopProxy，需要调用其 invoke 方法。当然我们也可以使用AspectJ的方式实现AOP代理，这儿不做过多介绍。 事务管理的框架是由抽象事务管理器 AbstractPlatformTransactionManager 来提供的，而具体的底层事务处理实现，由 PlatformTransactionManager 的具体实现类来实现，如事务管理器 DataSourceTransactionManager。不同的事务管理器管理不同的数据资源 DataSource，比如 DataSourceTransactionManager 管理 JDBC 的 Connection。 PlatformTransactionManager，AbstractPlatformTransactionManager 及具体实现类关系图如下。 一次正常事务调试出现问题，debug是比较好的解决方法。我们大致跟下SpringTransaction的使用过程。先从正确流程入手吧。如下：直接将事务注解加在doInsert方法上，明显，这种情况下出现异常事务会回滚。我们debug下事务大致的回滚过程。 DefaultAopProxyFactory里的createAopProxy方法可以拿到看到该方法具体使用的哪种代理。 可以看到我们这个类使用了Cglib代理。使用了Cglib代理，上面讲到 对于 CglibAopProxy，需要调用其内部类的 DynamicAdvisedInterceptor 的 intercept 方法。我们继续断点跟踪下。 一步步进行，如事务图所示，进入了TransactionInterceptor的invoke方法，并执行invokeWithinTransaction方法。 继续跟踪。来到了TransactionAspectSupport，这是spring事务处理的关键类，谨记。 会进行事务的创建，createTransactionIfNecessary getTransaction方法会开启一个事务。 根据上面debug看到的事务管理器是DataSourceTransactionManager， 执行getTransaction会调用它的doBeigin方法。 可以看到把自动提交设置成了false，并且暂时保存了原来的自动提交属性状态。 而后可以看到他将当前事务信息绑定在了ThreadLocal里了。 执行我们添加事务注解的方法，抛出了异常被捕获。 执行completeTransactionAfterThrowing方法，我们的异常正好是这个异常（或者其父类）。 然后执行回滚操作，最终到达下图所示方法（DataSourceTransactionManager的doRollback） 回滚具体代码不在介绍，我们可以看到在回滚时它把原来数据库的自动提交属性改了过来。 最后他会把本次事务状态清除，相当于保存上一次的事务状态。 注意： 在spring启动时获取事务注解时我们可以看到下图。 这个说明Spring AOP 事务注解只能作用于public 方法。 关于事务回滚rollbackFor条件的问题，我们可以看到下图。 当我们事务注解配置具体的回滚条件，如rollbackFor = Exception.class，只要是Exception.class或者其子类，都可以实现事务回滚。它会通过RuleBasedTransactionAttribute.class这个类去校验抛出的异常是否符合条件。进而判断是否需要回滚。 但是当我们不声明rollbackFor 条件时，这儿应该注意一点。它会使用默认的条件，而不是不处理异常。主要由DefaultTransactionAttribute.class 里的rollbackOn方法实现。我们看下这个方法，可以发现，他只会处理RuntimeException和Error。也就是说，如果我们一个方法有事务，但抛出了非RuntimeException（如检查时异常等），且事务没有声明rollbackFor回滚条件，那么，它是不会触发事务回滚的。这一点要注意。 上图调用RuleBasedTransactionAttribute.class的方法，回滚规则为空，使用父类rollbackOn方法。 通过调试可以轻松看到这一情况，这儿不在做过多赘述。 异常事务调试我们来看下事务不成功的情况。就是题目开始的问题。 Spring刚启动时，会扫描需要进行代理的类，生成代理对象，在AdvisedSupport.class类中，把类中的方法缓存起来。 首先查询该方法是不是需要拦截（是不是有事务注解） 在TransactionAttributeSourcePointcut.class 类里的matches方法，查询事务注解情况。 查到了就缓存起来了。 当doInset方法进入时，同样的逻辑。也会缓存起来，但是cached是值为null。 该类的其他方法也会被缓存，没有事务注解的都放为null。 开始执行doInsert方法时，进入CglibAopProxy的intercept方法。 可以看到尝试拿缓存，但缓存的值为空。 尝试获取一下，显然也是没有值的。所以这时候认为不需要进行事务。事务链为空。 就直接执行了方法doInsert。并不会开启事务。（不为空的话会创建一个CglibMethodInvocation并开启事务执行方法，如上面开始的情况）。 当生成一个动态代理对象后，对这个对象引用中方法的调用就是对代理的调用，而这个代理能够代理所有跟特定方法调用相关的拦截器。不过，一旦调用最终抵达了目标对象 （此处为TransactionalTestImpl类的引用），任何对自身的调用例如insert将对this引用进行调用而非代理。这一点意义重大， 它意味着自我调用将不会导致和方法调用关联的通知得到执行的机会。 如果需要insert的事务生效，一种典型的方法就将方法insert放到新的类中，这便很好理解了。因为新的类会生成新的动态代理对象，调用源从而获得通知。 如果我非要在本类中实现通知呢？ 那我们就需要直接获取代理对象调用insert方法了。如下图。 要实现这个功能，需要开启Spring AspectJ支持，我使用的Springboot，启动类上加入如下注解，并引入如下依赖。 这个pom文件你进去可以看到就是引用了AspectJ 的相关jar包。 这个时候我们在测试一下，就会发现事务生效了。 在CglibAopProxy中可以看到如下代码，可以明白开启后它把代理对象绑定到ThreadLocal上等待insert方法执行的通知。 当然，如果这两个方法上都存在事务，它也会进行判断处理，也就是事务的传播属性，他们主要通过AbstractPlatformTransactionManager这个类（这个类也很重要）的getTransaction方法和handleExistingTransaction方法来进行事务传播属性的处理。这儿不做过多讲解，自己看看逻辑处理即可。 getTransaction部分代码： handleExistingTransaction部分代码： 结语总的来说，通过一个问题，我们大致看了下Spring Transactional注解的实现过程。并分析了产生这种问题的原因，通过有效的手段来进行验证。还是蛮不错的一次体验。 下面总结下： A：在Spring中，一个类中无事务注解的方法A调用有事务注解的方法B，默认情况下B出现异常事务是不会进行回滚的。 解决方法： 将B写到一个新的方法中。（原理上是生成不同类的动态代理对象，实际中比较常用的一种手段，但需要管理一个新的类） 如果业务（情形等）允许，可以将事务移动到A上，或者B的事务不动，给A也加一个事务。（根据具体情况讨论，有时候效果很好，有时候不适宜，使用此种方法可能影响程序效率或者产生莫名其妙的bug，慎用） 启用增强型事务，引入AspectJ。（不太常用的一种手段，但如果项目中本来已经引入了AspectJ并且开启了增强型事务管理，何乐而不为呢？） B：研究过程中发现的其他应该注意的坑。 事务注解应当作用在public方法上，需要注意。 如果不设置事务回滚条件（rollbackFor参数为空），它能捕获RuntimeException及其子类 和 Error及其子类 出现的异常情况并回滚，其他异常是无法捕获并回滚的。如IOException（检查型异常）等。 事务的传播属性的几个应该了解，不能乱用，虽然我们可能就用到过或者就用到了Propagation.REQUIRED ，但不代表其它不会用到。 C：本次研究学习我们应该理解掌握的。 Spring事务的处理过程。（Spring AOP的体现，应用反射和动态代理） 事务的一些性质。（事务的传播属性、事务的四大特性等） 其它一些需要学习的地方。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何以并发方式在同一个流上执行多种操作]]></title>
    <url>%2Fblog%2F%E5%A6%82%E4%BD%95%E4%BB%A5%E5%B9%B6%E5%8F%91%E6%96%B9%E5%BC%8F%E5%9C%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E6%B5%81%E4%B8%8A%E6%89%A7%E8%A1%8C%E5%A4%9A%E7%A7%8D%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[前言Java 8中，流有一个非常大的局限性，使用时，对它操作一次仅能得到一个处理结果。当流进行终端操作后，如果你在试图遍历它，就会出现异常。 1java.lang.IllegalStateException: stream has already been operated upon or closed 虽然流就是如此设计的，但是我们有时候就希望可以通过流获取多个结果。或者说，你希望一次性向流中传入多个Lambda表达式。 为了达到这一目标，我们应该需要一个fork类型的方法，对每个复制的流应用不同的函数。理想情况下，这些操作也应该支持并行去拿到运算结果。 这一特性在Java 8中是没有的，不过我们可以利用一个通用API，即Spliterator，尤其是它的延迟绑定能力，结合BlockingQueues和Futures来实现这一特性。 正文复制流要达到此效果，我们首先应该创建一个StreamForker，它会对原始的流进行封装，在此基础上在执行各种操作。我们来看下代码。 12345678910111213141516public class StreamForker&lt;T&gt; &#123; private final Stream&lt;T&gt; stream; private final Map&lt;Object, Function&lt;Stream&lt;T&gt;,?&gt;&gt; forks=new HashMap&lt;&gt;(); public StreamForker(Stream&lt;T&gt; stream) &#123; this.stream = stream; &#125; public StreamForker&lt;T&gt; fork(Object key,Function&lt;Stream&lt;T&gt;,?&gt; f)&#123; //使用一个键对流上的函数进行索引 forks.put(key,f); //返回this从而保证多次顺畅的调用fork方法 return this; &#125; public Results getResults()&#123; //TODO &#125;&#125; fork方法接受两个参数。 Function:对流进行处理，转变成这些操作结果的类型。 key: 通过它拿到结果，这些结果被放到内部的一个Map中。 fork方法需要返回自身，这样可以复制多个操作构成流水线。 如图： 上图不难理解。 而由fork方法添加的操作如何执行呢，就是通过getResults方法的调用触发，该方法返回一个Results接口的实现。接口定义如下： 123public interface Results &#123; public &lt;R&gt; R get(Object key);&#125; 实现Results接口我们使用ForkingStreamConsumer实现Results接口。 123456789public Results getResults()&#123; ForkingStreamConsumer&lt;T&gt; consumer=build(); try&#123; stream.sequential().forEach(consumer); &#125;finally &#123; consumer.finish(); &#125; return consumer;&#125; ForkingStreamConsumer同时实现了Results和Consumer接口。其主要任务就是来处理流元素，将他们分发到多个BlockingQuenes中处理，BlockingQuenes的数量和通过fork方法提交的操作数是一致的。这里的getResults的实现，流应该是顺序处理的，否则，forEach后元素的顺序就会变化。finish方法用来表明队列中没有更多要处理的元素了。build方法主要用于创建ForkingStreamConsumer。代码如下： 123456789101112131415161718private ForkingStreamConsumer&lt;T&gt; build()&#123; //创建由队列组成的列表，每一个队列对应一个操作 List&lt;BlockingQueue&lt;T&gt;&gt; queues=new ArrayList&lt;&gt;(); //建立用于标识操作的键与包含操作结果的Future之间的映射关系 Map&lt;Object,Future&lt;?&gt;&gt; actions= forks.entrySet().stream().reduce( new HashMap&lt;Object,Future&lt;?&gt;&gt;(), (map,e)-&gt;&#123; map.put(e.getKey(),getOperationResult(queues,e.getValue())); return map; &#125;, (m1,m2)-&gt;&#123; m1.putAll(m2); return m1; &#125; ); return new ForkingStreamConsumer&lt;&gt;(queues,actions);&#125; 可以看到，我们先创建了BlockingQuenes列表。接着创建了一个Map，Map的键就是用来标识不同操作的键，值包含着Future里。最终BlockingQuenes和Map会被传递给ForkingStreamConsumer的构造函数。每个Future通过关键方法getOperationResult创建。 来看看getOperationResult的实现。 1234567891011private Future&lt;?&gt; getOperationResult(List&lt;BlockingQueue&lt;T&gt;&gt; queues,Function&lt;Stream&lt;T&gt;,?&gt; f)&#123; //创建一个队列，并将其添加到队列的列表中 BlockingQueue&lt;T&gt; queue=new LinkedBlockingDeque&lt;&gt;(); queues.add(queue); //创建一个Spliterator，遍历队列中的元素 Spliterator&lt;T&gt; spliterator=new BlockingQueueSpliterator&lt;&gt;(queue); //创建一个流，将Spliterator作为数据源 Stream&lt;T&gt; source= StreamSupport.stream(spliterator,false); //创建一个Future对象，以异步方式计算在流上执行特定函数的结果 return CompletableFuture.supplyAsync(()-&gt;f.apply(source));&#125; 该方法创建一个新的BlockingQuene，并将其添加到队列列表。队列会被传递给一个新的BlockingQueueSpliterator对象，后者是一个延迟绑定的Spliterator。然后我们创建一个顺序流对Spliterator进行遍历，最终创建一个Future收集结果。 开发ForkingStreamConsumer代码如下： 1234567891011121314151617181920212223242526public class ForkingStreamConsumer&lt;T&gt; implements Consumer&lt;T&gt;,Results &#123; public static final Object END_OF_STREAM=new Object(); private final List&lt;BlockingQueue&lt;T&gt;&gt; queues; private final Map&lt;Object, Future&lt;?&gt;&gt; actions; public ForkingStreamConsumer(List&lt;BlockingQueue&lt;T&gt;&gt; queues, Map&lt;Object, Future&lt;?&gt;&gt; actions) &#123; this.queues = queues; this.actions = actions; &#125; @Override public &lt;R&gt; R get(Object key) &#123; try &#123; return ((Future&lt;R&gt;)actions.get(key)).get(); &#125;catch(Exception e)&#123; throw new RuntimeException(e); &#125; &#125; @Override public void accept(T t) &#123; //将流中遍历的元素添加到所有的队列中 queues.forEach(q-&gt;q.add(t)); &#125; void finish()&#123; //将最后一个元素添加到队列中，表明该流已经结束 accept((T)END_OF_STREAM); &#125;&#125; 这个类同时实现了Consumer接口和Results接口。 Consumer接口要求实现accept方法，每当ForkingStreamConsumer接受流中的一个元素，它就会将元素添加到所有BlockingQuenes中当所有元素都添加到所有队列后，finish方法将最后一个元素添加到所有队列。处理时碰上这个元素表明后面没有元素要处理了。 Results接口需要实现get方法。一旦处理结束，get方法会获取Map中由键索引的Future，解析到结果后返回。 每有一个操作，就会对应一个BlockingQueueSpliterator。我们来看下BlockingQueueSpliterator的实现。 开发BlockingQueueSpliterator12345678910111213141516171819202122232425262728293031323334public class BlockingQueueSpliterator&lt;T&gt; implements Spliterator&lt;T&gt; &#123; private final BlockingQueue&lt;T&gt; q; public BlockingQueueSpliterator(BlockingQueue&lt;T&gt; q) &#123; this.q = q; &#125; @Override public boolean tryAdvance(Consumer&lt;? super T&gt; action) &#123; T t; while (true)&#123; try &#123; t=q.take(); break; &#125;catch(InterruptedException e)&#123; &#125; &#125; if(t!=ForkingStreamConsumer.END_OF_STREAM)&#123; action.accept(t); return true; &#125; return false; &#125; @Override public Spliterator&lt;T&gt; trySplit() &#123; return null; &#125; @Override public long estimateSize() &#123; return 0; &#125; @Override public int characteristics() &#123; return 0; &#125;&#125; 可以看到该Spliterator未定义任何切割流的策略，仅仅利用了流的延迟绑定能力。也没有实现trySplit方法。由于我们的操作数是不确定的，故estimateSize不能提供任何有意义的数字，返回0.也没有体现Spliterator的特性，故characteristics返回0. 仅仅实现了tryAdvance方法，它从BlockingQueue中取得原始流元素，进一步传给Consumer对象。当返回true时表明还有元素要处理，直到发现最后一个元素时终止。 以上基本上是在一个流上执行多种操作的代码。 我们下面来检测一下正确性。 测试编写测试类。如下数据： 123456789101112131415161718public static void main(String[] args) &#123; //生成1到1000的数组 List&lt;Integer&gt; list1=IntStream.rangeClosed(1,1000).filter(n-&gt;n%2==0).boxed().collect(Collectors.toList()); List&lt;Integer&gt; list2=IntStream.rangeClosed(1,1000).filter(n-&gt;n%5==0).boxed().collect(Collectors.toList()); //同时对list1数据求和，统计list1数据数量，统计list1和list2相同元素，统计list1和list2相同元素的最大值和最小值 Results results=new StreamForker&lt;Integer&gt;(list1.stream()) .fork("sum",s-&gt;s.mapToInt(Integer::intValue).sum()) .fork("count",s-&gt;s.count()) .fork("list3",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).collect(Collectors.toList())) .fork("max",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).max(Comparator.naturalOrder())) .fork("min",s-&gt;s.flatMap(i-&gt;list2.stream().filter(j-&gt;i.equals(j))).min(Comparator.naturalOrder())) .getResults(); System.out.println("sum="+results.get("sum")); System.out.println("count="+results.get("count")); System.out.println("max="+((Optional) results.get("max")).get()); System.out.println("min="+((Optional)results.get("min")).get()); ((List&lt;Integer&gt;)results.get("list3")).stream().forEach(System.out::println);&#125; 输出结果： 可以看到，使用了一个流，通过我们实现的方法进行了多次终端操作返回正确结果。 性能问题这是我们用一个流实现多种终端操作的方式，当然这并不意味着会比普通的写法效率高，如果对于上述问题，我们可以分个构建若干个流进行一一实现。 这种一个流进行多个终端操作的情况使用，一定是生成流比较耗费资源性能时才会用到，比如操作一个较大文件时生成的字符流，我们想统计字数，检查某些单词出现的次数，统计行数等等操作，重复生成流显然是耗费资源的。这种情况可以考虑使用这种一个流进行多个终端操作的实现。 当然，具体到具体问题优化，建议认真分析两者的资源消耗。这是比较稳妥的做法。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[封装一个属于自己的Redis API]]></title>
    <url>%2Fblog%2F%E5%B0%81%E8%A3%85%E4%B8%80%E4%B8%AA%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84Redis-API.html</url>
    <content type="text"><![CDATA[前言Redis作为一款强大的key-value型数据库，其应用是十分广泛的。在Java语言中，常用来与Redis数据库建立连接用到的是Jedis Pool连接池。 今天我们来简单了解下它们然后实现一个可移植的操作Redis的API。 正文知识准备我们知道Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。并提供了一系列的命令操作这些数据类型。 Jedis相当于对这些操作进行了代码封装，及提供了一些其它常用操作。 我们先来了解下Jedis的连接池配置参数。 commons-pool2 有一个配置类GenericObjectPoolConfig里面的通用参数设置如下： 参数 说明 默认值 备注 maxTotal 说明一个pool最多可以有多少个Jedis实例 8 -1表示不限制 maxIdle 一个pool最多可以有多少个空闲的Jedis实例 8 minIdle 一个pool最少有多少个空闲的Jedis实例 0 可以看到它继承BaseObjectPoolConfig。我们可以看到BaseObjectPoolConfig的参数如下。 部分参数意义如下： 参数 说明 默认值 备注 lifo pool中的idle列表是双端队列，设定是否last in first out true maxWaitMillis 当active数量为max时,等待的时长 -1L(代表一直等) 配合blockWhenExhausted使用 blockWhenExhausted 当active数量为max时，是否阻塞等待一段时间 true testOnCreate 创建实例时有效性检测 false testOnReturn 归还实例时有效性检测 false testOnBorrow 借出实例时有效性检测 false 思路分析首先Redis连接池属性我们应当放置在配置文件里，解析并获得，连接池最好设计成单例的，每次不用在初始化过多连接资源。同时Redis有单机模式和集群模式区分，这两种模式我们也应该区分开来。单机模式下，可以选择多个database，集群模式下只能选择database0.集群模式下，如果redis地址过多，我们如何分开呢？ 我们可以考虑如下样式：address =127.0.0.1:6379;127.0.0.1:6380 每个redis地址用分号分隔，解析配置时把每个解析到并建立连接。 当然，最后完成JedisPool的创建后，我们应该编写工具类对一些常用操作方法进行封装，便于我们使用。 代码我们根据上述思路，构造了如下图所示的小项目。 其中： RedisException是用来统一处理程序过程中的异常的类。 JedisFactory可以认为是一个JedisPool工厂，用来提供单机模式的连接池或者集群模式的连接池。 RedisConfiguration是与配置文件对应的配置类，用于存放配置的数据。 RedisConstants用来放置一些项目中用到的常量。 RedisUtil工具类接口，提供了多种操作Redis的方法。 RedisSingleUtil工具接口的单机模式实现。 RedisClusterUtil工具接口的集群模式实现。 redis-config.properties Redis的配置文件存放 JedisFactory和RedisUtill为主要类。我们看下他们的具体实现。 JedisFactory的主要代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697private volatile JedisPool jedisPool; private volatile JedisCluster jedisCluster; private RedisConfiguration redisConfig; private Pattern addRessPattern = Pattern.compile("^.+[:]\\d&#123;1,5&#125;\\s*(;.+[:]\\d&#123;1,5&#125;\\s*)*[;]?\\s*$"); public JedisFactory(final RedisConfiguration redisConfiguration)&#123; this.redisConfig=redisConfiguration; &#125; public JedisPool getJedisPool()&#123; if(jedisPool==null)&#123; synchronized (JedisFactory.class)&#123; if(jedisPool==null)&#123; init(); &#125; &#125; &#125; return jedisPool; &#125; public JedisCluster getJedisCluster()&#123; if(jedisCluster==null)&#123; synchronized (JedisFactory.class)&#123; if(jedisCluster==null)&#123; init(); &#125; &#125; &#125; return jedisCluster; &#125; public void init()&#123; logger.info("JedisFactory init start..."); try&#123; if(StringUtils.isNotBlank(redisConfig.getLocalPropertiesPath()))&#123; fillData(); &#125; logger.info("redis config is: &#123;&#125;.", redisConfig.toString()); Set&lt;HostAndPort&gt; hostAndPortSet = this.parseHostAndPort(redisConfig.getAddress()); GenericObjectPoolConfig genericObjectPoolConfig = new GenericObjectPoolConfig(); genericObjectPoolConfig.setMaxWaitMillis(redisConfig.getMaxWaitMillis()); genericObjectPoolConfig.setMaxTotal(redisConfig.getMaxTotal()); genericObjectPoolConfig.setMinIdle(redisConfig.getMinIdle()); genericObjectPoolConfig.setMaxIdle(redisConfig.getMaxIdle()); if(redisConfig.getMode()== RedisConstants.REDIS_MODE_SINGLE)&#123; HostAndPort hostAndPort=(HostAndPort)hostAndPortSet.toArray()[0]; jedisPool=new JedisPool(genericObjectPoolConfig, hostAndPort.getHost(), hostAndPort.getPort(), redisConfig.getTimeout(), null,redisConfig.getDatabase()); logger.info("jedisPool init is finished"); &#125;else&#123; if(redisConfig.getDatabase()!=0)&#123; logger.warn("当前配置的database为："+redisConfig.getDatabase()+",集群模式下不能选择database，只能使用database0"); &#125; jedisCluster = new JedisCluster(hostAndPortSet, redisConfig.getTimeout(), redisConfig.getMaxRedirections(), genericObjectPoolConfig); logger.info("jedisCluster init is finished"); &#125; &#125;catch(Exception ex)&#123; throw new RedisException(ex); &#125; &#125; private void fillData() throws Exception &#123; Properties localProperties = PropertiesUtils.loadLocalProperties(redisConfig.getLocalPropertiesPath()); String address=localProperties.getProperty("address", ""); if (StringUtils.isBlank(address)) &#123; throw new RedisException("error:redis config address is blank!"); &#125; // 设置初始值 long maxWaitMillis=Long.parseLong(localProperties.getProperty("maxWaitMillis", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_WAIT_MILLIS))); int maxTotal=Integer.parseInt(localProperties.getProperty("maxTotal", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_TOTAL))); int minIdle=Integer.parseInt(localProperties.getProperty("minIdle", String.valueOf(GenericObjectPoolConfig.DEFAULT_MIN_IDLE))); int maxIdle=Integer.parseInt(localProperties.getProperty("maxIdle", String.valueOf(GenericObjectPoolConfig.DEFAULT_MAX_IDLE))); int timeout=Integer.parseInt((localProperties.getProperty("timeout", "2000"))); int maxRedirections=Integer.parseInt((localProperties.getProperty("maxRedirections", "6"))); int database=Integer.parseInt((localProperties.getProperty("database", "0"))); //1单机模式，2集群模式 int mode=Integer.parseInt((localProperties.getProperty("mode", String.valueOf(RedisConstants.REDIS_MODE_SINGLE)))); redisConfig.setAddress(address); redisConfig.setMaxWaitMillis(maxWaitMillis); redisConfig.setMaxTotal(maxTotal); redisConfig.setMinIdle(minIdle); redisConfig.setMaxIdle(maxIdle); redisConfig.setTimeout(timeout); redisConfig.setMaxRedirections(maxRedirections); redisConfig.setDatabase(database); redisConfig.setMode(mode); &#125; //部分代码略 对于RedisUtil接口，应有两个实现，单机和集群的，这里为了简化代码，只简单列举了一个方法。 1234public interface RedisUtil &#123; String setString(String key, String value); //部分代码略&#125; 单机模式的实现： 123456789101112public class RedisSingleUtil implements RedisUtil&#123; @Override public String setString(String key, String value) &#123; Jedis jedis = this.getResource(); try&#123; return jedis.set(key, value); &#125;finally&#123; this.closeResource(jedis); &#125; &#125; //部分代码略&#125; 集群模式的实现： 12345678public class RedisClusterUtil implements RedisUtil&#123; @Override public String setString(String key, String value) &#123; JedisCluster cluster=getResource(); return getResource().set(key, value); &#125; //部分代码略&#125; 其他方法及实现不在赘述，有兴趣的可以在 https://github.com/javazwt/framework-base 上查看相关代码。 测试1234567public class RedisTest &#123; private static RedisSingleUtil redisSingleUtil=new RedisSingleUtil(); public static void main(String[] args) &#123; redisSingleUtil.setString("str","123"); redisSingleUtil.getString("str"); &#125;&#125; 可以检测我们的正确性。 结束语经过封装后，我们可以把该工具类使用在任何项目上，提高开发效率，降低项目耦合性，同时对Redis有了更深入的认知。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[封装构建基于ES的Java API包]]></title>
    <url>%2Fblog%2F%E5%B0%81%E8%A3%85%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8EES%E7%9A%84Java-API%E5%8C%85.html</url>
    <content type="text"><![CDATA[前言前些日子，elastic公司成功上市了。旗下有一款优秀的开源搜索引擎ElasticSearch。 ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。 好了，扯远了，回归正题。今天我们使用Java对它的一些API进行封装，以使其API更具备灵活性。 完成对其简单的使用是比较简单的，我们今天把我们的一些调用代码封装，以便可以在各个项目中自由使用。这才是我们的根本目的。 今天，我们来构建一个工具包类。 环境准备安装ElasticSearch，主要就是为了测试我们的代码，对于有现成环境的，如开发环境，测试环境上的ElasticSearch，这一步可以忽略。 首先请安装ElasticSearch，这里就不介绍了，我这里是安装的ElasticSearch最新版，6.4.0版本。https://www.elastic.co/downloads/elasticsearch 。安装成功后可以将其添加到环境变量中，然后启动。启动成功浏览器访问http://localhost:9200/ 会看到json信息。 安装Kibana，Kibana是ElasticSearch的可视化工具，可以方便的查看ElasticSearch及其运行状态。https://www.elastic.co/products/kibana 。我安装的也是最新版本，6.4.0. 安装成功后加入环境变量，然后启动，访问http://localhost:5601/ 可以看到Kibana界面。 JDK版本为1.8 以上都为准备工作。 架构构建org.elasticsearch.client.transport jar包已经有相关关于ES API的操作。 我们把它封装为专用jar包，建议使用Maven构建。如下： Maven项目首先，我们先建一个名叫 framework-es的Maven项目。如下。 建好各个package。 exception里面定义我们的异常。 factory里面用来生成esclient。 util里面放一些工具类。 vo里面可以放查询对象等。 property为配置文件。 test里面存放测试类。 各个package的名字可以自己定义，描述清目的即可。如下图。 添加依赖添加依赖，我的pom.xml依赖如下。 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.31&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 思路我的思路是构建一个单例的ESClient，用于与ES建立连接，以后增删改查等逻辑均使用此Client，保证资源的高效利用。这个也应当支持ES集群，有多个ES服务端也应当支持。而且地址应为可配置的。 话不多说，上代码，主要代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class ElasticSearchClientFactory &#123; private final static Logger logger = LoggerFactory.getLogger(ElasticSearchClientFactory.class); private volatile TransportClient esClient; //ES配置 private ElasticSearchConfiguration esConfig; //校验多个ES地址的正则 private Pattern addRessPattern = Pattern.compile("^.+[:]\\d&#123;1,5&#125;\\s*(;.+[:]\\d&#123;1,5&#125;\\s*)*[;]?\\s*$"); public ElasticSearchClientFactory(final ElasticSearchConfiguration elasticSearchConfiguration)&#123; this.esConfig=elasticSearchConfiguration; &#125; /** * 获取一个单例的ESClient * @return */ public Client getEsClient()&#123; if(esClient==null)&#123; synchronized (ElasticSearchClientFactory.class)&#123; if(esClient==null)&#123; logger.info("ElasticSearchClientFactory init start..."); try&#123; if(StringUtils.isNotBlank(esConfig.getLocalPropertiesPath()))&#123; //获取ES配置信息 fillData(); &#125; logger.info("ESConfig is:&#123;&#125;",esConfig.toString()); //多个ES地址解析 List&lt;HostAndPort&gt; hostAndPortList = this.parseHostAndPortList(esConfig.getAddress()); TransportAddress [] transportAddress=new TransportAddress[hostAndPortList.size()]; for (int i = 0; i &lt; hostAndPortList.size(); i++) &#123; transportAddress[i] = new TransportAddress(InetAddress.getByName(hostAndPortList.get(i).getIp()),hostAndPortList.get(i).getPort()); &#125; //节点名 String nodeName=esConfig.getNodeName()+ UUID.randomUUID(); String clusterName=esConfig.getClusterName(); Settings.Builder settingsBuilder = Settings.builder(); settingsBuilder.put("node.name", nodeName); if(StringUtils.isNotBlank(clusterName))&#123; settingsBuilder.put("cluster.name", clusterName); &#125; settingsBuilder.put("client.transport.sniff", true); Settings settings = settingsBuilder.build(); TransportClient client = new PreBuiltTransportClient(settings); //创建ESClient esClient = client.addTransportAddresses(transportAddress); logger.info("EalsticSearchClientFactory init is finished"); &#125;catch(Exception e)&#123; logger.error("EalsticSearchClientFactory create failed",e); throw new ElasticSearchException("EalsticSearchClientFactory create faile",e); &#125; &#125; &#125; &#125; return esClient; &#125; //其他代码略&#125; 以上代码用于生成一个单例的ESClient类。 123public class ElasticSearchConstants &#123; public final static String DEFAULT_PROPERTIES_PATH="property/es-config.properties";&#125; 同时默认配置文件为property/es-config.properties可手动进行配置，且支持多个地址，地址写法为 127.0.0.1:9200;127.0.0.2:9300 这样，中间应使用;分割。 我们对外提供一个工具类，供使用者进行对数据的操作。如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class ElasticSearchUtil &#123; private final static Logger logger = LoggerFactory.getLogger(ElasticSearchUtil.class); private static final SerializerFeature[] featuresWithNullValue=&#123;SerializerFeature.WriteMapNullValue, SerializerFeature.WriteNullBooleanAsFalse, SerializerFeature.WriteNullListAsEmpty, SerializerFeature.WriteNullNumberAsZero, SerializerFeature.WriteNullStringAsEmpty&#125;; private ElasticSearchClientFactory elasticSearchClientFactory; public void setElasticSearchClientFactory(ElasticSearchClientFactory elasticSearchClientFactory) &#123; this.elasticSearchClientFactory = elasticSearchClientFactory; &#125; public ElasticSearchUtil(ElasticSearchClientFactory elasticSearchClientFactory)&#123; this.elasticSearchClientFactory=elasticSearchClientFactory; &#125; public ElasticSearchUtil(String localPropertiesPath)&#123; ElasticSearchConfiguration esConfig=new ElasticSearchConfiguration(); esConfig.setLocalPropertiesPath(localPropertiesPath); this.elasticSearchClientFactory=new ElasticSearchClientFactory(esConfig); &#125; public ElasticSearchUtil() &#123; ElasticSearchConfiguration esConfig = new ElasticSearchConfiguration(); esConfig.setLocalPropertiesPath(ElasticSearchConstants.DEFAULT_PROPERTIES_PATH); this.elasticSearchClientFactory = new ElasticSearchClientFactory(esConfig); &#125; public ElasticSearchUtil(ElasticSearchConfiguration esConfig) &#123; this.elasticSearchClientFactory = new ElasticSearchClientFactory(esConfig); &#125; public Client getEsClient() &#123; return elasticSearchClientFactory.getEsClient(); &#125; /** * 创建索引 * @param index * @param type * @param id * @param isOnlyCreate * @param jsonString * @return */ protected boolean createDocument(String index,String type,String id,boolean isOnlyCreate,String jsonString)&#123; IndexResponse indexResponse; if(StringUtils.isBlank(id))&#123; indexResponse=elasticSearchClientFactory.getEsClient().prepareIndex(index,type).setCreate(isOnlyCreate).setSource(jsonString, XContentType.JSON).get(); &#125;else&#123; indexResponse=elasticSearchClientFactory.getEsClient().prepareIndex(index,type,id).setCreate(isOnlyCreate).setSource(jsonString,XContentType.JSON).get(); &#125; if(logger.isDebugEnabled())&#123; String _index=indexResponse.getIndex(); String _type=indexResponse.getType(); String _id=indexResponse.getId(); long _version = indexResponse.getVersion(); boolean created = RestStatus.CREATED.equals(indexResponse.status()); logger.debug(String.format("createDocument index:%s,type:%s,id:%s,version:%s,created:%s", _index, _type, _id, _version, created)); &#125; return RestStatus.CREATED.equals(indexResponse.status()); &#125; //其他代码略&#125; 创建好客户端后，我们可以用它执行增删改查，我们在封装一个ESUtil类，用于执行该系列操作，暴露一些特定增删改查接口。如上代码。 注：其他代码略。 这样，完成代码后，工程项目如下图所示。 ElasticSearchException 为自定义异常类。 ElasticSearchClientFactory为主要方法，用来构建一个单例的ESClient ElasticSearchConfiguration为ES配置类 ElasticSearchConstants为常量类，里面存放配置文件的路径 HostAndPort为地址和端口的一个辅助Bean ElasticSearchUtil为主要方法，用于对外提供服务（CRUD） DocumentVo为辅助Bean es-config.properties为配置文件 当我们把项目打包成jar包时，配置文件可以不用打包，这样引入其他项目后，在其他项目里配置配置文件即可。 测试若ES的安装路径D:\Program Files\elasticsearch-6.4.0\bin成功配置到环境变量后，在命令行输入elasticsearch便可启动服务。 若Kibana的安装路径D:\Program Files\kibana-6.4.0-windows-x86_64成功配置到环境变量后，在命令行输入kibana便可启动服务。 访问http://127.0.0.1:5601/ 看到可视化界面。 编写测试类。 12345678910public class ESTest &#123; private static ElasticSearchUtil esUtil=new ElasticSearchUtil(); private static String index="user_index"; private static String type="user_type"; public static void main(String[] args) &#123; JSONObject js=new JSONObject(); js.put("1","2"); esUtil.insertDocument(index,type,"123",js); &#125;&#125; 查看Kibana界面可以看到我们的数据被添加进来了。 结论关于ElasticSearch及Kibana的具体使用我们以后再讲。 我们平时开发中，也可以创建类似的工具包，提高代码使用率，实现软件的高内聚低耦合，同时也是提高自己。 部分未展示代码请见我的GitHub地址: https://github.com/javazwt/]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 Spliterator接口]]></title>
    <url>%2Fblog%2FJava8-Spliterator%E6%8E%A5%E5%8F%A3.html</url>
    <content type="text"><![CDATA[前言Spliterator 是Java8中加入的一个新接口，是“可分迭代器”（splitable iterator）的意思。它也是用来遍历数据源中的元素的，但它是为并行执行而设计的。 其接口主要代码如下： 123456Public interface Spliterator&lt;T&gt;&#123; Boolean tryAdvance(Consumer &lt;? super T&gt;) action; Spliterator&lt;T&gt; trySplit(); Long estimateSize(); Int characteristics();&#125; T 是Spliterator要遍历的元素类型。 tryAdvance 方法的行为类似于普通的Iterator，因为它会按顺序一个一个使用Spliterator中的元素，并且如果还有其他元素要遍历就返回true。 trySplit 是专为Spliterator接口而设计的，因为它可以把一些元素划出去分给第二个Spliterator，让他们两个并行处理。 estimateSize方法估计还剩多少元素需要遍历，因为即使不那么精确，快速算出来的值也有助于让拆分均匀点。 注：将Stream流拆分成多个部分的算法是个递归过程，第一步第一个Spliterator调用trySplit，生成两个Spliterator，第二步这两个Spliterator调用trySplit，生成4个Spliterator，直到调用Spliterator的trySplit 方法后返回null，表示这部分Spliterator不能在分割。 这个拆分过程也受Spliterator本身特性的影响，而特性是通过characteristics方法声明的。 我们来简单看一下它的特性的常用值。 特性 含义 ORDERED 按元素的既定顺序遍历和划分 DISTINCT 对于任一遍历过的元素x，y,x.equals(y)返回false SORTED 遍历元素按照一个预定义顺序排序 SIZED Spliterator由一个已知大小的数据源建立，estimateSize会返回准确值 NONNULL 保证遍历元素不会为空 IMMUTABLE Spliterator的数据源不能被修改，（不能 添加、删除、修改任何元素） CONCURRENT Spliterator的数据源可以被其他线程同时修改而无需同步 SUBSIZED 该Spliterator和从它拆分出来的Spliterator都是SIZED的 例子为什么我们需要了解这个类，有的时候甚至要实现这个类呢？ 我们来看一个例子。 对于下面一个String，我想统计下单词数量。 1static final String WORD="Hello World Happy EveryDay Good good study day day up let us study Spliterator"; 我们需要创建一个counter来累计流中字符，以及在counter中把它们结合起来的逻辑，如下： 12345678910111213141516171819202122232425public class WordCounter &#123; private final int counter; private final boolean lastSpace; public WordCounter(int counter, boolean lastSpace) &#123; this.counter = counter; this.lastSpace = lastSpace; &#125; //遍历一个个的Character public WordCounter accumulate(Character c)&#123; if(Character.isWhitespace(c))&#123; return lastSpace ? this : new WordCounter(counter,true); &#125;else&#123; //上一个字符是空格，而当前遍历的字符不是空格时，将单词计数器加一 return lastSpace ? new WordCounter(counter+1,false):this; &#125; &#125; //合并两个WordCounter，将其计数器加起来 public WordCounter combine(WordCounter wordCounter)&#123; return new WordCounter(counter+wordCounter.counter,wordCounter.lastSpace); &#125; public int getCounter()&#123; return counter; &#125;&#125; 这时候，我们在书写一个规约Character流统计单词个数就很简单了。 1234public static int countWords(Stream&lt;Character&gt; stream)&#123; WordCounter wordCounter=stream.reduce(new WordCounter(0,true),WordCounter::accumulate,WordCounter::combine); return wordCounter.getCounter();&#125; 12Stream&lt;Character&gt; stream= IntStream.range(0,WORD.length()).mapToObj(WORD::charAt);System.out.println(countWords(stream)); 输出14。结果是正确的。 现在我们让他在并行流上进行工作： 1Stream&lt;Character&gt; stream= IntStream.range(0,WORD.length()).mapToObj(WORD::charAt).parallel(); 结果输出26。显然这是不正确的。一脸懵逼。 为什么会出现这种情况呢？ 因为在并行流进行Spliterator分割时，把一个单词拆分成两部分了，导致结果变大。这显然不是我们想看到的。 实践我们要处理这种情况，就要指定分割原则，不要让程序把整个单词切开。 因此我们需要编写自己的Spliterator才能让上述问题在并行流下工作。如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243public class WordCounterSpliterator implements Spliterator&lt;Character&gt; &#123; private final String string; private int currentChar=0; public WordCounterSpliterator(String string) &#123; this.string = string; &#125; @Override public boolean tryAdvance(Consumer&lt;? super Character&gt; action) &#123; //处理当前字符 action.accept(string.charAt(currentChar++)); //如果还有字符要处理，返回true return currentChar&lt;string.length();&#125; @Override public Spliterator&lt;Character&gt; trySplit() &#123; int currentSize=string.length()-currentChar; //返回null表示要处理的String已经足够小 if(currentSize&lt;10)&#123; return null; &#125; //将试探拆分位置设定到要解析的文字中间 for(int splitPos=currentSize/2+currentChar;splitPos&lt;string.length();splitPos++)&#123; //如果是空格就开始拆分，不是空格将拆分位置前进直到下一个空格 if(Character.isWhitespace(string.charAt(splitPos)))&#123; //创建一个新的WordCounterSpliterator来解析String从开始到拆分位置的部分 Spliterator&lt;Character&gt; spliterator= new WordCounterSpliterator(string.substring(currentChar,splitPos)); //将这个WordCounterSpliterator的起始位置设为拆分位置 currentChar=splitPos; return spliterator; &#125; &#125; return null; &#125; @Override public long estimateSize() &#123; return string.length()-currentChar; &#125; @Override public int characteristics() &#123; return ORDERED+SIZED+SUBSIZED+NONNULL+IMMUTABLE; &#125;&#125; tryAdvance方法把String中当前位置的Character传给了Consumer，并让位置加一。作为参数传递的内部类Consumer，在遍历流时将要处理的Character传递给要执行的函数。如果新的指针位置小于String总长度，说明没有遍历完，返回true继续遍历。 trySplit方法，首先我们设置了一个拆分下限——10个Character，实际应用中我们应尽量提高这个长度避免生成太多的任务。如果长度小于这个数，就返回空无需继续拆分。否则就把试探拆分位置放到要解析的String块中间，但不能直接使用此位置，应该看看是不是空格，如果是就拆分，如果不是，就向前找，找到空格进行拆分，避免把一个单词拆成两份。 estimatedSize方法返回的是这个Spliterator解析的String的总长度和当前遍历位置的差值。 characteristic方法告诉这个框架是ORDERED（String的每个Character的默认顺序），SIZED（estimatedSize方法返回值是精确的），SUBSIZED（trySplit分出来的Spliterator大小也是固定的），NONNULL（String里面的Character不可能为null），IMMUTABLE（String本身就不可变化）。 下面我们测试一下我们的WordCounterSpliterator 。 123Spliterator&lt;Character&gt; spliterator=new WordCounterSpliterator(WORD);Stream&lt;Character&gt; stream= StreamSupport.stream(spliterator,true);System.out.println(countWords(stream)); 可以看到输出结果为14. 结论可以看到，并行流不是所有情况都适用的，有些情况要定制自己的Spliterator才能使并行流正常工作。这个例子或许运行效率并行比不上串行，但是在大数据下，比如分析一个文本文件中的单词数量，就能明显看到并行带来的速度优势了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[和前同事闲聊小记]]></title>
    <url>%2Fblog%2F%E5%92%8C%E5%89%8D%E5%90%8C%E4%BA%8B%E9%97%B2%E8%81%8A%E5%B0%8F%E8%AE%B0.html</url>
    <content type="text"><![CDATA[前言这是一篇技术问题讨论文章~~~ 最近和前同事（朋友）聊天，帮他分析了这么一个需求，蛮有意思的，特来分享下其代码。 和他聊了很久，算是大致上听懂了他的需求，总结下他的需求： 假设有若干方法（任务）(A,B,C,D,E…….)，对于每个方法（任务），同一时间只能有一个用户（线程）访问，现一些用户需要执行若干任务（如用户1执行（ABC），用户2执行（BEF），用户3执行（ACEF）….），对每个用户，用户的任务不要求执行顺序，如何尽可能的提高程序运行的效率。 分析其实这之中比较重要的一点是每个方法同一时间只允许一个用户访问，我们如果给每个方法加上synchronized关键字呢？我从这方面入手没有想到太好的方法。我用了一种方法，就是从队列入手，对每个方法，可以设置一个等待队列，如果有任务正在执行该方法，则将其他的访问该方法的线程挂起，当然，对于每个用户，应该开多个线程去异步执行各个方法。 于是我想到了定长线程池，允许一个线程执行，其他线程进入后会放到等待队列中。对于方法，每有一个方法（任务），就创建一个定长线程池。应当在初始化时就将数据设置好。 代码以下是我的一些实现代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class DoTask &#123; private static Map&lt;String,ExecutorService&gt; poolMap=new HashMap&lt;&gt;(); static&#123; //获取指定class的public方法 Method[] methods=ThingsMethod.class.getDeclaredMethods(); //有多少个方法就创建多少个线程池,一个方法指定一个定长线程池 for(Method method:methods)&#123; ExecutorService pool= Executors.newFixedThreadPool(1); poolMap.put(method.getName(),pool); &#125; &#125; //list里面假设是要执行的任务，变化的 public void doTask(List&lt;String&gt; list)&#123; for(String taskStr:list)&#123; final String id="Thread"+Thread.currentThread().getId(); switch (taskStr)&#123; case "A": try&#123; executeMethod("doSomethingA",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "B": try&#123; executeMethod("doSomethingB",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "C": try&#123; executeMethod("doSomethingC",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; case "D": try&#123; executeMethod("doSomethingD",id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; break; default: break; &#125; &#125; &#125; public void executeMethod(String methodName,String id)&#123; poolMap.get(methodName).execute(()-&gt;&#123; try&#123; Method method=ThingsMethod.class.getMethod(methodName,String.class); method.invoke(ThingsMethod.class.newInstance(),id); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ThingsMethod &#123; public boolean doSomethingA(String id)&#123; System.out.println("线程"+id+"做A事情开始-----&gt;"); //假设做A事情花费1s try&#123; Thread.sleep(1000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做A事情结束-----&gt;"); return true; &#125; public boolean doSomethingB(String id)&#123; System.out.println("线程"+id+"做B事情开始-----&gt;"); //假设做B事情花费3s try&#123; Thread.sleep(3000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做B事情结束-----&gt;"); return true; &#125; public boolean doSomethingC(String id)&#123; System.out.println("线程"+id+"做C事情开始-----&gt;"); //假设做C事情花费2s try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做C事情结束-----&gt;"); return true; &#125; public boolean doSomethingD(String id)&#123; System.out.println("线程"+id+"做D事情开始-----&gt;"); //假设做D事情花费5s try&#123; Thread.sleep(5000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做D事情结束-----&gt;"); return true; &#125;&#125; 编写测试类对其进行测试。 123456789101112131415161718192021222324252627282930313233343536public class MyTest implements Runnable&#123; @Override public void run() &#123; //随机假设我们的任务是这样的 List&lt;String&gt; s=new ArrayList&lt;&gt;(); String [] strings=new String[]&#123;"A","B","C","D"&#125;; int l=(int)Math.ceil(Math.random()*strings.length); for(int i=0;i&lt;l;i++)&#123; s.add(strings[i]); &#125; //获取结果 System.out.println(); System.out.print("Thread"+Thread.currentThread().getId()+"随机生成的任务--》"); for(int i=0;i&lt;s.size();i++)&#123; System.out.print(s.get(i)); &#125; System.out.println(); DoTask doTask=new DoTask(); doTask.doTask(s); &#125; public static void main(String[] args) &#123; MyTest test = new MyTest(); Thread t1 = new Thread(test); Thread t2 = new Thread(test); Thread t3 = new Thread(test); Thread t4 = new Thread(test); Thread t5 = new Thread(test); Thread t6 = new Thread(test); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); t6.start(); &#125;&#125; 输出结果： 12345678910111213141516171819202122232425262728293031323334353637Thread11随机生成的任务--》AThread13随机生成的任务--》AThread15随机生成的任务--》ABThread14随机生成的任务--》ABCDThread12随机生成的任务--》ABCDThread16随机生成的任务--》A线程Thread16做A事情开始-----&gt;线程Thread14做C事情开始-----&gt;线程Thread12做B事情开始-----&gt;线程Thread12做D事情开始-----&gt;线程Thread16做A事情结束-----&gt;线程Thread13做A事情开始-----&gt;线程Thread14做C事情结束-----&gt;线程Thread12做C事情开始-----&gt;线程Thread13做A事情结束-----&gt;线程Thread12做A事情开始-----&gt;线程Thread12做B事情结束-----&gt;线程Thread14做B事情开始-----&gt;线程Thread12做A事情结束-----&gt;线程Thread14做A事情开始-----&gt;线程Thread12做C事情结束-----&gt;线程Thread14做A事情结束-----&gt;线程Thread11做A事情开始-----&gt;线程Thread12做D事情结束-----&gt;线程Thread14做D事情开始-----&gt;线程Thread11做A事情结束-----&gt;线程Thread15做A事情开始-----&gt;线程Thread14做B事情结束-----&gt;线程Thread15做B事情开始-----&gt;线程Thread15做A事情结束-----&gt;线程Thread15做B事情结束-----&gt;线程Thread14做D事情结束-----&gt; 可以看到运行结果是正确的。 讨论我还想到一种就是对各个方法（任务）添加synchronized关键字，这些方法对用户共享，这样每当有用户请求时，开线程池执行各个方法。如下： 123456789101112131415161718192021222324252627282930public class DoTask &#123; private static ThingsMethod method=new ThingsMethod(); public void doTask(List&lt;String&gt; list)&#123; for(String taskStr:list)&#123; ExecutorService executor= Executors.newCachedThreadPool(); executor.execute(()-&gt;&#123; final String id="Thread"+Thread.currentThread().getId(); doSomething(taskStr,id,method); &#125;); &#125; &#125;public void doSomething(String taskStr,String id,ThingsMethod method)&#123; switch (taskStr)&#123; case "A": method.doSomethingA(id); break; case "B": method.doSomethingA(id); break; case "C": method.doSomethingA(id); break; case "D": method.doSomethingA(id); break; default: break; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ThingsMethod &#123; public synchronized boolean doSomethingA(String id)&#123; System.out.println("线程"+id+"做A事情开始-----&gt;"); //假设做A事情花费1s try&#123; Thread.sleep(1000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做A事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingB(String id)&#123; System.out.println("线程"+id+"做B事情开始-----&gt;"); //假设做B事情花费3s try&#123; Thread.sleep(3000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做B事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingC(String id)&#123; System.out.println("线程"+id+"做C事情开始-----&gt;"); //假设做C事情花费2s try&#123; Thread.sleep(2000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做C事情结束-----&gt;"); return true; &#125; public synchronized boolean doSomethingD(String id)&#123; System.out.println("线程"+id+"做D事情开始-----&gt;"); //假设做D事情花费5s try&#123; Thread.sleep(5000); &#125;catch(InterruptedException e)&#123; e.printStackTrace(); &#125; System.out.println("线程"+id+"做D事情结束-----&gt;"); return true; &#125;&#125; 由于在主线程上开启了多线程，所以这种方法观看日志比较不方便。这种方法的正确性也还没来得及验证。 如果大家有什么好的其他想法也可以讨论交流。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java8 Stream API简介]]></title>
    <url>%2Fblog%2FJava8-Stream-API%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言今天来简单讲下Java8新增的Stream API。 Stream是Java8新增的对于集合框架操作的一个类，我们也称之为流操作。使用Stream，可以简化很多重复代码，使用并行流时有时还能提高程序运行效率。我们来看下。 正文要把一个集合，List、Set或其他集合类转换为一个流，需要调用stream方法（转换为串行流）或者parallelStream方法（转换为并行流）。 Stream的常用方法如下表格介绍： 常用方法 说明 参数 举例 filter(Predicate&lt;? super T&gt; predicate) 过滤，返回一个匹配给定布尔表达式的数据流 一个返回值为boolean的lambda表达式 例1 map(Function&lt;? super T, ? extends R&gt; mapper) 匹配，对流的每个元素都应用给定的函数以获得新流 任一符合条件的lambda表达式 例2 mapToInt(ToIntFunction&lt;? super T&gt; mapper) 将流数据转换为IntStream流 流元素转为int的lambda表达式 例3 mapToLong(ToLongFunction&lt;? super T&gt; mapper) 将流数据转换为LongStream流 流元素转为long的lambda表达式 略 mapToDouble(ToDoubleFunction&lt;? super T&gt; mapper) 将流数据转换为DoubleStream流 流元素转为Double的lambda表达式 略 flatMap(Function&lt;? super T, ? extends Stream&lt;? extends R&gt;&gt; mapper) 返回一个对流的每个元素都应用给定的函数的新流。这个入参lambda包含有过滤流，此方法类似于平常的双重循环 一个包含过滤流的lambda表达式 例4 flatMapToInt(Function&lt;? super T, ? extends IntStream&gt; mapper) 参考mapToInt 参考mapToInt 略 flatMapToLong(Function&lt;? super T, ? extends LongStream&gt; mapper) 参考mapToLong 参考mapToLong 略 flatMapToDouble(Function&lt;? super T, ? extends DoubleStream&gt; mapper) 参考mapToDouble 参考mapToDouble 略 distinct() 去除流中的重复元素 无参数 例5 sorted() 对流中元素进行排序，使用自然顺序。 无参数 例6 sorted(Comparator&lt;? super T&gt; comparator) 对流中的元素进行排序，按照指定的规则 流中元素的比较规则 例7 peek(Consumer&lt;? super T&gt; action) 截断流后进行某些操作 指定的操作，无返回值lambda 例8 limit(long maxSize) 获取流中的前n个元素的流 限定值 例9 skip(long n) 获取从n开始后面元素的流，不包括n，如果n超出流长度，返回一个空流 起始长度 例10 forEach(Consumer&lt;? super T&gt; action) 循环流中的每个元素，执行指定的lambda表达式 无返回值的lambda表达式 例11 forEachOrdered(Consumer&lt;? super T&gt; action) 按照默认顺序循环流中的元素，执行指定的lambda表达式 无返回值的lambda表达式 略 toArray() 返回流中元素组成的数组 无参数 例12 toArray(IntFunction&lt;A[]&gt; generator) 将流中元素转换为指定类型的数组，数组类型必须是流元素的本身或者超类 略 略 reduce(T identity, BinaryOperator&lt;T&gt; accumulator) 归约，将流中的元素按照指定表达式归并。 起始值（默认值），指定归约公式 例13 reduce(BinaryOperator&lt;T&gt; accumulator) 同上，不同的是这个没有设置起始值，故结果可能为空，返回的是Optional对象 指定的归约公式 略 reduce(U identity,BiFunction&lt;U,? super T,U&gt; accumulator,BinaryOperator&lt;U&gt; combiner) 归约全参方法，可以指定初始值，归约方法，归约方式 略 例14 collect(Collector&lt;? super T, A, R&gt; collector) 收集器，比较常用的是toList，将流元素变为集合 略 略 collect(Supplier&lt;R&gt; supplier,BiConsumer&lt;R, ? super T&gt; accumulator,BiConsumer&lt;R, R&gt; combiner) 收集器，可以指定收集规则，收集方法等 略 略 min(Comparator&lt;? super T&gt; comparator) 根据指定规则获取流中最小值 排序规则lambda表达式 例15 max(Comparator&lt;? super T&gt; comparator) 根据指定规则获取流中最大值 排序规则lambda表达式 例15 count() 统计流中元素数量 无参数 例16 anyMatch(Predicate&lt;? super T&gt; predicate) 查找流中是否有元素符合条件，一旦查到，就返回true 条件的lambda表达式 例17 allMatch(Predicate&lt;? super T&gt; predicate) 查找流中的元素是否全部匹配指定表达式，全部符合才返回true 条件的lambda表达式 例18 noneMatch(Predicate&lt;? super T&gt; predicate) 查找流中的元素是否全部不匹配指定表达式，全部不匹配才返回true 条件的lambda表达式 略 findFirst() 查找流的第一个元素，如果流为空，返回空（Optional的空） 无参数 略 findAny() 随便从流中拿一个元素，如果流为空，返回空（Optional的空） 无参数 略 empty() 静态方法，返回一个空流 略 例19 of(T t) 静态方法，生成一个元素的流 略 例19 of(T… values) 静态方法，生成若干元素的流 略 例19 iterate(final T seed, final UnaryOperator&lt;T&gt; f) 静态方法，生成一个有规律的流 起始值，指定的lambda表达式 例19 generate(Supplier&lt;T&gt; s) 静态方法，生成指定规则的数据流，流数据的生成完全取决于lambda表达式 生成流数据的lambda表达式 例19 concat(Stream&lt;? extends T&gt; a, Stream&lt;? extends T&gt; b) 静态方法，将两个数据流合并，两个流的类型须一致 两个数据流 例19 例子举例：假设有一个1-100的List集合list1,50-150的List集合list2. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//例1：需要一个1-100的偶数List集合。list1.stream().filter(n-&gt;n%2==0).collect(Collectors.toList());//例2：获取list1各元素加上100，再取基数的List集合。list1.stream().map(n-&gt;n+100).map(n-&gt;n%2!=0).collect(Collectors.toList());//例3：将list1的流转换为IntStream。list1.stream().mapToInt(n-&gt;n.intValue());//例4：list1和list2的交集集合。list1.stream().flatMap(i-&gt;list2.stream().map(j-&gt;i==j)).collect(Collectors.toList());//例5：list1和list2的并集集合。list1.addAll(list2);list1.stream().distinct();//例6：对list1进行排序。list1.stream().sorted().collect(Collectors.toList());//例7：对list1进行倒序排序。list1.stream().sorted(Comparator.reverseOrder()).collect(Collectors.toList());//例8：截断查看流的每一步得到的流元素。list1.stream().map(n-&gt;n+100).peek(System.out::println).map(n-&gt;n%2!=0).peek(System.out::println).collect(Collectors.toList());//例9：取list1的前10个元素。list1.stream().limit(10).collect(Collectors.toList());//例10：取list1的后10个元素。list1.stream().skip(list1.size()-10).collect(Collectors.toList());//例11：输出list1的各个元素。list1.stream().forEach(System.out::println);//例12：将list1元素转为数组。list1.stream().toArray();//例13：对list1元素求和，对list2元素求差。list1.stream().reduce(0,(a,b)-&gt;a+b);list2.stream().reduce(0,(a,b)-&gt;a-b);//例14：对list1按照最大值到最小值求和。list1.stream().reduce(0,(a,b)-&gt;a+b, BinaryOperator.maxBy(Comparator.naturalOrder()));//例15：取list1的最大最小值list1.stream().max(Comparator.naturalOrder());list1.stream().min(Comparator.naturalOrder());//例16：求list1的元素个数list1.stream().count();//例17：查询list1中有没有17的倍数（除了17）。list1.stream().anyMatch(n-&gt;n!=17&amp;&amp;n%17==0);//例18：查询list1中所有元素是不是都是2的倍数。list1.stream().allMatch(n-&gt;n%2==0);//例19：Stream.empty();//空流Stream.of(1);//生成一个元素的流Stream.of(1,2,3,4);//生成若干元素的流Stream.iterate(1,n-&gt;2*n);//生成指定规则的数据流Stream.generate(()-&gt;Math.random());//生成指定规则的数据流Stream.concat(list1.stream(),list2.stream());//将两个数据流合并 结语Java8 Stream API的一些常用方法基本如上，Stream API为操作集合类提供了更方便快捷的方式，值得我们掌握。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你该相信什么]]></title>
    <url>%2Fblog%2F%E4%BD%A0%E8%AF%A5%E7%9B%B8%E4%BF%A1%E4%BB%80%E4%B9%88.html</url>
    <content type="text"><![CDATA[前言写这篇文章，或是怀念过去的自己，或是憧憬自己未来。可是啊，无论如何，要活好当下。 正文工作总的来说差不多四年了，或许已从职场菜鸟变化为职场老手了。 可是，岁月的变迁确无法掩饰我这颗浮躁的心，哎，想想自己，确实很浮躁啊。 曾经，想静心，于是开始练字，练了很长一段时间，可是后面确不知不觉的放弃了，至今剩下的字帖和钢笔仍静静地躺在我的桌子上。 后来，心血来潮搭建了属于自己的博客，然后创建了微信公众号，平时写些文章来总结自己和工作，我希望自己可以坚持下去，也相信自己可以坚持下去。 世上大多数人是平凡的，可是，要有一颗不甘平凡的心。 最怕一生碌碌无为，还安慰平凡难能可贵。 任何事都不应成为懒惰的借口。 不诉困苦于环境，不寄期望于运气，不服挫折于命运。 还记得那个年少的自己吗？或许他变了，可是他还在，他一直陪着你。 相信自己。做自己的英雄。 努力吧，骚年！ 加油！ 总结好了，喝饱鸡汤了。 努力去“搬砖”吧！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 接口]]></title>
    <url>%2Fblog%2FJava8-%E6%8E%A5%E5%8F%A3.html</url>
    <content type="text"><![CDATA[前言Java 8已经推出相当长一段时间了，其中，接口部分有一些变化。我们来研究下它。 问题我们知道，对于一个接口，如果我们声明好后，如果再想对其增加新的方法是困难的，因为我们要改变所有其实现类，也就是每个实现类都要对其新方法进行实现。如下图： 这显然是不现实的，如果我们直接把方法写在实现类里，接口中没有此方法，就破坏了我们的多态性。 对于某些已经发布的应用，无论哪种做法都是比较繁重且不被推荐的。 接口默认实现还好，Java大神们已经意识到了这个问题，于是在Java8中，引入了对接口的默认方法实现。 什么是默认方法实现呢？ 简单来说，就是允许接口定义默认方法，在接口中需要有方法体的具体实现，实现类默认继承该方法（如果不重写默认方法的话）。同时为区分默认方法，默认方法在接口中采用default关键字标明。如下图： 这样，如果我们新增一个接口方法（对于已经发布的接口），可以使用默认实现，就不会出现我们上述的问题。 思考你一定会说，这和抽象类有什么区别呢？ 当然还是有区别的，Java8以后，接口和抽象类的几点总结如下： 新的问题接口引入了默认方法后，就会有新的问题，好在Java已经替我们解决了，我们来看下。 情况一123456789101112131415public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B extends A&#123; default void doSomething()&#123; System.out.println("Interface B"); &#125;&#125;public class C implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 结果：输出 Interface B 情况二1234567public class D implements A &#123;&#125;public class C extends D implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 结果：输出 Interface B 情况三如果D是这样呢？ 12345public class D implements A &#123; public void doSomething()&#123; System.out.println("Class D"); &#125;&#125; 结果：输出Class D 如果D不对doSomething提供实现（D为抽象的类），则C需要为doSomething提供实现。 情况四如果B接口不在继承A接口。如下： 12345678910public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B&#123; default void doSomething()&#123; System.out.println("Interface B"); &#125;&#125; 那么我们C类必须为doSomething提供实现，当然我们可以具体制定使用哪个接口的doSomething方法，如下： 123456public class C implements A, B &#123; @Override public void doSomething() &#123; B.super.doSomething(); &#125;&#125; 情况五如果两个函数不一样但差距很小呢？如下： 123456789101112131415public interface A &#123; default int doSomething()&#123; return 100; &#125;&#125;public interface B&#123; default long doSomething()&#123; return 200; &#125;&#125;public class C implements A, B &#123; public static void main(String[] args) &#123; new C().doSomething(); &#125;&#125; 在IDEA里我们可以看到，类C是无法编译的，这是不被允许的。 情况六1234567891011121314public interface A &#123; default void doSomething()&#123; System.out.println("Interface A"); &#125;&#125;public interface B extends A&#123;&#125;public interface C extends A&#123;&#125;public class D implements B,C &#123; public static void main(String[] args) &#123; new D().doSomething(); &#125;&#125; 输出结果Interface A 因为只有A声明了一个默认方法，这个接口是D的父接口，故输出Interface A。如果B也提供了一个默认方法，签名和A一致，那么编译器会选择B的默认方法，如果B添加一个相同签名的抽象方法，则D需要为其提供实现，如果B,C都有相同签名的默认方法doSomething，则会出现冲突，需要我们为doSomething提供实现或者指定使用B，C中的哪个方法。 结论解决问题的三条规则： 如果一个类使用相同的函数签名从多个地方（比如另一个类或者接口）继承了方法，通过三条规则可进行判断。 类中的方法优先级最高。类或者父类中声明的方法优先级高于任何声明为默认方法的优先级。 如果无法依据第一条判断，那么子接口的优先级更高：函数签名相同时，优先选择拥有最具体实现的默认方法的接口，及如果B继承了A，那么B就比A更加具体。 最后，如果还是无法判断，继承了多个接口的类必须通过显式覆盖和调用期望的方法，显式地选择使用哪一个默认方法的实现。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Callable原理分析]]></title>
    <url>%2Fblog%2FCallable%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[前言今天我们来分析实现多线程的一种方式，实现Callable接口。这种方式有种特殊的地方，就是可以拿到线程返回值。具体怎么实现的呢？我们来研究下。 栗子我们先举个简单的栗子来看下Callable接口吧。要首先明白，线程池提交实现Callable接口的线程后会返回一个Future对象，这个对象里包含程序的运行结果。 123456789101112public class CallableTest &#123; public static void main(String[] args) throws ExecutionException, InterruptedException,TimeoutException&#123; //创建一个线程池 ExecutorService executor = Executors.newCachedThreadPool(); Future&lt;String&gt; future = executor.submit(()-&gt; &#123; //System.out.println("CallableTest"); TimeUnit.SECONDS.sleep(5); return "CallableTest"; &#125;); System.out.println(future.get()); &#125;&#125; 可以看到我们可以通过future.get()拿到结果”CallableTest”。我们也可以设置指定时间后拿到结果，如指定6s后拿到结果。 1System.out.println(future.get(4,TimeUnit.SECONDS)); 可以看到也拿到了返回结果，如果我们设置4s拿到结果，小于程序运行时间5s，可以看到它抛出了超时异常。java.util.concurrent.TimeoutException。 原理是不是很神奇？ 我们来研究下Callable接口获取返回值的原理。 我们先来看看ExecutorService的submit方法，它接受一个Callable对象，返回一个Future对象。 1&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); 及它的实现。AbstractExecutorService的submit方法。 12345678910public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125;protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable);&#125;void execute(Runnable command); ThreadPoolExecutor的execute是对其实现。 可以看到创建了一个FutureTask对象并执行。 FutureTask对象实现了Runable接口，我们来看下它。 看一下它的run方法。 1234567891011121314151617181920212223242526272829303132333435public void run() &#123; if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //拿到结果设置ran为true result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; //异常设置结果为空ran为false并设置异常 result = null; ran = false; setException(ex); &#125; //ran为true时放入结果 if (ran) set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; 1234567public V get() throws InterruptedException, ExecutionException &#123; int s = state; //会一直挂起知道处理业务的线程完成，唤醒等待线程 if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125; 我们调用get方法时，他先查询线程状态，如果未完成，就调用awaitDone方法。 1234567891011121314151617181920212223242526272829303132333435363738private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; //如果设置了超时时间就获取截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; /循环监视线程的状态 for (;;) &#123; if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; else if (s == COMPLETING) // cannot time out yet Thread.yield(); else if (q == null) q = new WaitNode(); else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); else if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; else LockSupport.park(this); &#125; &#125; 带有超时时间的get，到达时间后，会判断线程状态，如果未完成，抛出超时异常。 12345678910public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; if (unit == null) throw new NullPointerException(); int s = state; if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s);&#125; 总结因此，带有返回值得异步线程基本上可以这样理解。 由于有返回值，如果未设置等待时间，会等线程执行完成后返回，基本类似同步。其原理是线程运行后，如果未完成，会放入等待队列。直到线程状态变化（完成或者异常等）。如果设置了等待时间，则到时间后无论线程状态是否完成都会返回线程状态。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道Java试题引发的思考]]></title>
    <url>%2Fblog%2F%E4%B8%80%E9%81%93Java%E8%AF%95%E9%A2%98%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83.html</url>
    <content type="text"><![CDATA[前言无意中看到了自己入门Java时的一本Java基础书，看到了其中一个小结的习题，颇为简单。求一个数组所有数据的和。 分析那时入门Java，看到以前自己在书上写下的for循环。大致如下： 1234567public static Long sum(int [] numbers)&#123; long result=0L; for(int i=0;i&lt;numbers.length;i++)&#123; result+=numbers[i]; &#125; return result;&#125; 现已入行Java颇有时日，遂想到当n逐渐变大时，其执行效率会逐渐降低的。 近日对多线程有些许研究，故想到可以将数据平均分为2份，3份或者多份，每份进行和值，最后相加得到结果。也是一个不错的想法。 好在Java里有这么一个框架，名字叫分支/合并框架。我们来配合例子研究下。 分支/合并框架分支/合并框架的目的是以递归的方式将可以并行的任务分为更小的任务，然后将每个子任务的结果合并起来生成整体的结果。它是ExecutorService接口的一个实现，它把子任务分配给线程池（ForkJoinPool）中的工作线程。那我们如何来定义任务和子任务呢？ 要把任务提交到ForkJoinPool，必须创建RecursiveTask&lt; R &gt;的一个子类，其中R是并行化任务产生的结果类型。它其中有唯一一个抽象方法compute，用来定义将任务拆分成子任务的逻辑，以及无法再拆分或不方便拆分时，生成单个子任务结果的逻辑。 伪代码如下： 1234567if(任务足够小或者不可分)&#123; 顺序执行该任务&#125;else&#123; 将任务分为两个子任务 递归调用本方法，拆分每个子任务，等待所有子任务完成 合并每个子任务结果&#125; 那如何定义一个任务是否可以在拆分呢？ 一般来说没有明确的标准决定一个任务是否可以在拆分，但是有几种试探方法可以帮助我们做出决定，分支/合并框架采用了一种称为工作窃取的技术来解决这个问题。每个线程的子任务都保存在一个双向链式队列里，每完成一个任务，就从队列头部获取一个任务，如果有线程先执行完成，它不会“闲”下来，而是去未完成的队列尾部“窃取”一个任务进行完成，直至所有子任务完成返回结果。 实践我们用分支/合并框架对数组数据进行并行求和。 代码如下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class ForkJoinSumCalculator extends RecursiveTask&lt;Long&gt; &#123; //要求和的数组 private final int[] numbers; //子任务处理的数组的起始位置 private final int start; //子任务处理的数组的终止位置 private final int end; //不再将任务划分的子任务数组大小 public static final long THRESHOLD=10000; public ForkJoinSumCalculator(int[] numbers)&#123; this(numbers,0,numbers.length); &#125; private ForkJoinSumCalculator(int[] numbers,int start,int end)&#123; this.numbers=numbers; this.start=start; this.end=end; &#125; @Override protected Long compute() &#123; int length=end-start; //小于等于阈值，计算结果 if(length&lt;=THRESHOLD)&#123; return computeSequentially(); &#125; //创建一个子任务来为数组的前一半求和 ForkJoinSumCalculator leftTask=new ForkJoinSumCalculator(numbers,start,start+length/2); //利用另一个ForkJoinPool线程异步执行新创建的子任务 leftTask.fork(); //创建一个任务为数组的后一半求和 ForkJoinSumCalculator rightTask=new ForkJoinSumCalculator(numbers,start+length/2,end); //同步执行第二个子任务，有可能允许进一步递归划分 Long rightResult=rightTask.compute(); //读取第一个子任务的结果，没有完成就等待 Long leftResult=leftTask.join(); //合并结果 return rightResult+leftResult; &#125; //子任务数组求和 private long computeSequentially()&#123; long sum=0; for(int i=start;i&lt;end;i++)&#123; sum+=numbers[i]; &#125; return sum; &#125;&#125; 这样，我们在编写一个方法并行对数组求和就很简单了。 1234public static long forkJoinSum(int [] numbers)&#123; ForkJoinTask&lt;Long&gt; task=new ForkJoinSumCalculator(numbers); return new ForkJoinPool().invoke(task);&#125; 我们可以写一个测试方法，测试这两种方法的执行效率。 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) &#123; //构建一个数组 int [] numbers=new int[100000000]; for(int i=0;i&lt;numbers.length;i++)&#123; numbers[i]=(int)(Math.random() * Integer.MAX_VALUE); &#125; //分支/合并框架计算执行速度 long fastest=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); forkJoinSum(numbers); long duration=(System.nanoTime()-start); if(duration&lt;fastest)&#123; fastest=duration; &#125; &#125; System.out.println("分支/合并最快执行速度为"+fastest+"ns"); //普通方法计算执行速度 long fastest1=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); sum(numbers); long duration=(System.nanoTime()-start); if(duration&lt;fastest1)&#123; fastest1=duration; &#125; &#125; System.out.println("普通算法最快执行速度为"+fastest1+"ns"); &#125; 输出如下： 12分支/合并最快执行速度为25894038ns普通算法最快执行速度为38811709ns 可以看到速度是有明显提升的。 其他源数据问题这个计算的数组之所以随机，是因为我之前测试了1-n的和计算，发现for循环居然比分支/合并框架快！！我加大了n值也是如此，所以对于这种比较特殊的计算，Java虚拟机或者编译器对它们的计算做了优化，因此用这些数据测试时，可能得不到我们想要的结果，这也是并发处理比较难的地方。有的时候我们多线程处理的代码可能还没有单线程快，或者快是快了，但结果错了。 拥抱Java8看到上面我们为了求个和冒着出错的风险写了一个sum类，是不是心里退却了？不过啊，Java 8已经给我们提供了类似的功能啦。 它就是全新的Stream API，这个我们有时间在介绍。先看看Stream API对于这个是如何求和的。 12Arrays.stream(numbers).sum();Arrays.stream(numbers).parallel().sum(); 是不是很简单？？ 第一个为串行求和，即单线程，第二个为并行求和。 我们来测试下他们的效率。 1234567891011121314151617181920212223242526//Stream API 串行方法计算执行速度 long fastest2=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); //sum(numbers); Arrays.stream(numbers).sum(); long duration=(System.nanoTime()-start); if(duration&lt;fastest2)&#123; fastest2=duration; &#125; &#125; System.out.println("Stream API 串行 最快执行速度为"+fastest2+"ns"); //Stream API 并行方法计算执行速度 long fastest3=Long.MAX_VALUE; for(int i=0;i&lt;10;i++)&#123; long start=System.nanoTime(); //sum(numbers); Arrays.stream(numbers).parallel().sum(); long duration=(System.nanoTime()-start); if(duration&lt;fastest3)&#123; fastest3=duration; &#125; &#125; System.out.println("Stream API 并行 最快执行速度为"+fastest3+"ns"); 输出如下结果： 1234分支/合并最快执行速度为25316712ns普通算法最快执行速度为38812671nsStream API 串行 最快执行速度为36572646nsStream API 并行 最快执行速度为24291637ns 可以看到，并行情况下时间与刚才写的分支/合并框架相近。 其实：并行流背后使用的基础框架就是分支/合并框架。 这只是最简单的求和例子，遇到实际问题，可能使用Stream流比普通实现简单快速，因此，Stream是值得我们学习的。 结论学习就是不断思考不断进步的过程，有的时候看看自己之前写的代码，应该少一些“我去，写的什么玩意，原来？”之类的抱怨，多一些深入的思考及优化。 相关代码均已上传Github ： https://github.com/JavaZWT/framework-base]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Transactional注解]]></title>
    <url>%2Fblog%2FSpring-Transactional%E6%B3%A8%E8%A7%A3.html</url>
    <content type="text"><![CDATA[前言使用@Transactional 注解有一段时间了，今天来对它进行下总结。 再说这个之前先说下事务。 事务的基本要素（ACID） 原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 说明Transactional注解的主要参数如下： 下面来简单介绍下它的参数： value：一般用来配置指定的事务管理器。 propagation：事务的传播属性，有七种。见枚举Propagation，默认REQUIRED。 1234567891011121314//支持当前事务，如果不存在就创建一个REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED),//支持当前事务，如果不存在就以非事务的方式运行SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS),//支持当前事务，不存在就抛出异常MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY),//创建一个新事务，如果当前有事务就暂停当前事务REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW),//以非事务方式运行，如果当前有事务就暂停当前事务NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED),//以非事务的方式运行，如果当前有事务就抛出异常NEVER(TransactionDefinition.PROPAGATION_NEVER),//如果当前有事务，就加入当前事务。NESTED(TransactionDefinition.PROPAGATION_NESTED); isolation：隔离级别。事务的隔离级别有4种。我们看一下Isolation枚举类。 12345678910//使用默认的隔离级别，取决于底层数据库的默认隔离级别DEFAULT(TransactionDefinition.ISOLATION_DEFAULT),//读未提交，这种隔离级别最低，会出现脏读，不可重复读，虚读（幻读）等情况。一般不用。READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED),//读已提交，这种隔离级别可以防止脏读的产生，但是无法避免不可重复读和虚读（幻读）的出现。READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED),//可重复读，这种隔离级别可以防止脏读、不可重复读的出现，但是无法避免虚读（幻读）的产生。REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ),//串行化，这种隔离级别最高，可以避免脏读、不可重复读和虚读（幻读）的产生。SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。 Mysql默认隔离级别为可重复读。 关于事务的并发问题 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 timeout : 事务的超时时间，默认为-1，即事务直到完成都不会超时。 readOnly：是否只读，一般用来标识。拥有这个标识，写入操作不一定会失败，取决于数据库系统。 rollbackFor：事务回滚条件。参数为继承Throwable的class类。 rollbackForClassName：事务回滚条件。可以接受String数组。 noRollbackFor： 事务不会回滚条件。 noRollbackForClassName：事务不会回滚条件。 原理Spring的Transaction注解是如何实现并生效的呢？我们来探究下。 我们首先来看看SpringTransactionAnnotationParser这个类，这个是解析transaction注解的类。调用AnnotationUtils类的getAnnotation方法拿到注解信息。 可以看到AnnotationTransactionAttributeSource这个类调用了parseTransactionAnnotation方法。 我们来看下AbstractFallbackTransactionAttributeSource这个类的computeTransactionAttribute方法。调用了上图的findTransactionAttribute方法。getTransactionAttribute方法，由于获取注解信息耗时，故spring先从缓存里获取注解事务信息，查不到在用程序获取。 TransactionAspectSupport这个类的createTransactionIfNecessary方法，调用getTransactionAttribute方法。 我们可以看到AbstractTransactionAspect.aj文件使用了createTransactionIfNecessary方法。这个应用了aspectj。Before指在事务开始之前获取注解信息。After throwing 指当方法抛出异常后的执行动作。这里一般会进行回滚操作。After returning指当方法返回前的执行动作。这里一般会提交事务。After 指当事务完成后的动作。这里会清空当前事务注解信息。 结论事务的隔离级别和传播属性都是我们应该掌握和学习的，对于Spring的@Transactional 注解，我们不但要会使用，而且应该深入去理解它的实现原理。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道面试题引发的思考（递归）]]></title>
    <url>%2Fblog%2F%E4%B8%80%E9%81%93%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83.html</url>
    <content type="text"><![CDATA[前言某日，去某互联网公司面试，被问到了如下一道面试题： 题目很简单，有一队人，已知第一个人8岁，后一个人比前一个人大两岁，以此类推，问第8个人多少岁？第N个人多少岁。 我拿过笔和纸，不假思索的写下了如下答案： 1234567static int getAge(int n)&#123; if(n==1)&#123; return 8; &#125;else&#123; return getAge(n-1)+2; &#125;&#125; 面试官又问我还有什么需要注意的吗？我说要注意参数不能小于1. 显然，面试官不是很满意，又问我如果N很大会怎么样？ 我想了想，说，会出现OOM异常吧，或者超了int的范围。 感觉他还是不满意。 思考自那过去一段时间，我自认为答得没有问题，也答到了点上（只可惜面试没过~O(∩_∩)O哈哈~）。 最近又想到了这个问题，决定研究研究。 实践我用自己的方法，进行了数据测试。当然我把为了测试效果明显，我加大了测试值。 123public static void main(String[] args) &#123; System.out.println(getAge(1000000));&#125; 结果使我惊讶，不是OOM异常，而是堆栈异常。 1Exception in thread "main" java.lang.StackOverflowError 后面我了解到，递归调用，可以假想成一个函数调用另一个函数，而每个函数相当于占用一个栈帧，这些栈帧以先进后出的方式排列起来形成栈。如下图： 这样，函数会追寻到栈顶，拿到getAge(1)的值后逐渐返回。如下。 1234getAge(1)=8getAge(2)=getAge(1)+2=10getAge(3)=getAge(2)+2=12getAge(4)=getAge(3)+2=14 可以知道，如果堆栈深度不够的话，就会出现异常。 我们上图所示的异常就是这个原因。 提升改进一在研究这个问题时，我发现了一种递归，尾递归。 如下所示： 1234567static int getAge1(int n,int result)&#123; if(n==1)&#123; return result; &#125;else&#123; return getAge1(n-1,result+2); &#125;&#125; 这种递归我们可以看到，带了一个参数result，当他运行到n==1时，直接返回了result，不用在一层层回退进行计算。如下： 12345 getAge(4,8)= getAge(3,8+2)= getAge(2,8+2+2)= getAge(1,8+2+2+2)= 14 一些编译器发现这些函数可以在一个栈帧里进行完成，就会复用栈帧，优化代码。 可惜的是，到目前为止，JAVA在HotSpot（Oracle的JVM）上使用时，并不支持尾递归优化。据说IBM的JVM支持尾递归优化，有兴趣的童鞋可以试下。 所以这种方案的测试结果肯定也是StackOverflowError啦。 改进二循环解决: 那时脑子笨，现在想想，这道题用循环也是可以解决的，且不用担心堆栈溢出问题。如下： While循环： 1234567static int getAge2(int n,int result,int step)&#123; while(n&gt;1)&#123; result+=step; n--; &#125; return result;&#125; For循环: 123456static int getAge3(int start,int end,int firstValue,int step)&#123; for(int i=start;i&lt;end;i++)&#123; firstValue+=step; &#125; return firstValue;&#125; 且速度也提高了不少。 改进三这么有规律的数据，当然乘法就可以解决，如下： 123static int getAge4(int start,int end,int firstValue,int step)&#123; return firstValue+(end-start)*step;&#125; 以上例子没有对入参做校验处理，结合实际场景，其实需要处理的，在此略掉了。 总结JDK源码中很少有递归，因为递归不能被优化，当数据过大时，很容易出现堆栈溢出，我们也应该慎用，基本能有递归解决的问题也可以用循环解决。 可以看出，对于一个问题，实现的方案可能不止一种。 看到自己想了及总结了多种方法，感觉很诧异，面试时却只想到了递归一种，而且实现很不好。还是自己基础不够扎实，应该多学多看多练。 其他上面都说要多学多看多练啦….. 这是我测试IBM JDK对于尾递归（getAge1方法）的结果。 说明IBM JDK确实对尾递归有优化，我又测试了getAge方法（普通递归），结果和Oracle JDK一样，抛出 java.lang.StackOverflowError 异常。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于注解的通用数据验证]]></title>
    <url>%2Fblog%2F%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E7%9A%84%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E9%AA%8C%E8%AF%81.html</url>
    <content type="text"><![CDATA[前言在项目开发交互过程中，难免会遇到一些数据校验。以校验客户端发送数据的合法性，对于一些非空校验，我们也许可以使用@NonNull，@NotNull 等注解，可是对于一些常规的，如手机号，身份证等等的校验，我们就还要判断处理每个请求的参数的合法性。 但是合法性的判断是难以避免的，我们是否可以精简工作量、提高工作效率呢。 思考我们或许应该从@NonNull @NotNull等其他注解那里受到些启发。 我们或许可以结合正则表达式及注解对某些通用数据进行验证。 注解可以设置参数，我们可以设置参数为校验规则，通过枚举列举出来，同时也应该允许用户自定义正则等校验。 我们知道，注解有三种类型 RetentionPolicy.SOURCE RetentionPolicy.CLASS RetentionPolicy.RUNTIME。 SOURCE主要用于编译之前，编译过程中会被丢弃如@Override注解。 CLASS主要用于编译，运行时会被丢弃。 RUNTIME在源码，编译，运行时始终会存在。 可以利用反射，拿到具有特定注解的bean，并处理。所以我们定义的注解应该是RUNTIME类型。同时声明注作用范围为FIELD及PARAMETER。 实践定义注解1234567891011121314151617181920/** * 数据验证注解 */@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.FIELD,ElementType.PARAMETER&#125;)public @interface DataValid &#123; //是否可以为空 boolean nullable() default false; //提供几种常用的正则验证 RegexType regexType() default RegexType.NONE; //自定义正则验证 String regexExpression() default ""; //参数或者字段描述 String description() default "";&#125; 定义如上注解，nullable用来校验参数是否可空，默认不可以为空，false。 同时提供几种通用的正则校验，用枚举列出，如手机号码校验，身份证信息校验等等。 同时如果没有规定的正则表达式，可以让用户自定义自己的正则表达式。 另增加描述字段，用来说明这个paramer的用途。 定义常用正则枚举12345678910111213141516/** * 正则类型枚举 */public enum RegexType &#123; NONE, SPECIALCHAR, CHINESE, EMAIL, IP, NUMBER, NUMBERORNIL, PHONENUMBER, ID; &#125; 列出几种常用枚举。非空，特殊字符，中文，邮箱，IP，数字等等 枚举规则定义了枚举，要定义它们的具体对应的方法，以便后续调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164/** * 常用正则表达式 */public class RegexUtils &#123; /** * 判断是否是正确的IP地址 * * @param ip * @return boolean true,通过，false，没通过 */ public static boolean isIp(String ip) &#123; if (null == ip || "".equals(ip)) return false; String regex = "^(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|[1-9])\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)\\." + "(1\\d&#123;2&#125;|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)$"; return ip.matches(regex); &#125; /** * 判断是否是正确的邮箱地址 * * @param email * @return boolean true,通过，false，没通过 */ public static boolean isEmail(String email) &#123; if (null == email || "".equals(email)) return false; String regex = "\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*"; return email.matches(regex); &#125; /** * 判断是否含有中文，仅适合中国汉字，不包括标点 * @param text * @return boolean true,通过，false，没通过 */ public static boolean isChinese(String text) &#123; if (null == text || "".equals(text)) return false; Pattern p = Pattern.compile("[\u4e00-\u9fa5]"); Matcher m = p.matcher(text); return m.find(); &#125; /** * 判断是否正整数 * * @param number * 数字 * @return boolean true,通过，false，没通过 */ public static boolean isNumber(String number) &#123; if (null == number || "".equals(number)) return false; String regex = "[0-9]*"; return number.matches(regex); &#125; /** * 判断是否正整数(可以为空) * * @param number * 数字 * @return boolean true,通过，false，没通过 */ public static boolean isNumberOrNil(String number) &#123; if(null == number) return true; if ("".equals(number.trim())) return true; String regex = "[0-9]*"; return number.matches(regex); &#125; /** * 判断几位小数(正数) * * @param decimal * 数字 * @param count * 小数位数 * @return boolean true,通过，false，没通过 */ public static boolean isDecimal(String decimal, int count) &#123; if (null == decimal || "".equals(decimal)) return false; String regex = "^(-)?(([1-9]&#123;1&#125;\\d*)|([0]&#123;1&#125;))(\\.(\\d)&#123;" + count + "&#125;)?$"; return decimal.matches(regex); &#125; /** * 判断是否是手机号码 * * @param phoneNumber * 手机号码 * @return boolean true,通过，false，没通过 */ public static boolean isPhoneNumber(String phoneNumber) &#123; if (null == phoneNumber || "".equals(phoneNumber)) return false; String regex = "^1[3|4|5|6|7|8|9][0-9]\\d&#123;8&#125;$"; return phoneNumber.matches(regex); &#125; /** * 判断身份证号格式正确性 * * @param ID * 身份证号 * @return boolean true,通过，false，没通过 */ public static boolean isID(String ID) &#123; if (null == ID || "".equals(ID)) return false; String regex = "^(\\d&#123;14&#125;[0-9a-zA-Z])|(\\d&#123;17&#125;[0-9a-zA-Z])$"; return ID.matches(regex); &#125; /** * 判断是否含有特殊字符 * * @param text * @return boolean true,通过，false，没通过 */ public static boolean hasSpecialChar(String text) &#123; if (null == text || "".equals(text)) return false; if (text.replaceAll("[a-z]*[A-Z]*\\d*-*_*\\s*", "").length() == 0) &#123; // 如果不包含特殊字符 return true; &#125; return false; &#125; /** * 适应CJK（中日韩）字符集，部分中日韩的字是一样的 */ public static boolean isChinese2(String strName) &#123; char[] ch = strName.toCharArray(); for (int i = 0; i &lt; ch.length; i++) &#123; char c = ch[i]; if (isChinese(c)) &#123; return true; &#125; &#125; return false; &#125; private static boolean isChinese(char c) &#123; Character.UnicodeBlock ub = Character.UnicodeBlock.of(c); if (ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_COMPATIBILITY_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_A || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_B || ub == Character.UnicodeBlock.CJK_SYMBOLS_AND_PUNCTUATION || ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS || ub == Character.UnicodeBlock.GENERAL_PUNCTUATION) &#123; return true; &#125; return false; &#125;&#125; 实现及调用基本数据都定义及处理好了，我们应该建立注解与方法之间的关联，RUNTIME类型的注解在程序运行时也会被保留，我们可以利用反射，拿到具体注解参数信息，进行相关处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113/** * 注解解析Service */public class ValidateService &#123; /*private static DataValid dataValid;*/ public ValidateService() &#123; super(); &#125; /** * 解析入口 * @param object * @throws Exception */ public static void valid(Object object) throws Exception&#123; //获取object的类型 Class&lt;? extends Object&gt; clazz=object.getClass(); //获取该类型声明的成员 Field[] fields=clazz.getDeclaredFields(); //遍历属性 for(Field field:fields)&#123; //对于private私有化的成员变量，通过setAccessible来修改器访问权限 field.setAccessible(true); validate(field,object); //重新设置会私有权限 field.setAccessible(false); &#125; &#125; public static void validate(Field field,Object object) throws Exception&#123; String description = null; Object value = null; DataValid dataValid = null; //获取对象的成员的注解信息 dataValid=field.getAnnotation(DataValid.class); value=field.get(object); if(dataValid==null)return; description=dataValid.description().equals("")?field.getName():dataValid.description(); /*************注解解析工作开始******************/ if(!dataValid.nullable() &amp;&amp; dataValid.regexType() != RegexType.NUMBERORNIL)&#123; if(value==null|| StringUtils.isBlank(value.toString()))&#123; throw new Exception(description+"不能为空"); &#125; &#125; if(dataValid.regexType()!=RegexType.NONE)&#123; switch (dataValid.regexType()) &#123; case NONE: break; case SPECIALCHAR: if(RegexUtils.hasSpecialChar(value.toString()))&#123; throw new Exception(description+"不能含有特殊字符"); &#125; break; case CHINESE: if(RegexUtils.isChinese2(value.toString()))&#123; throw new Exception(description+"不能含有中文字符"); &#125; break; case EMAIL: if(!RegexUtils.isEmail(value.toString()))&#123; throw new Exception(description+"邮箱地址格式不正确"); &#125; break; case IP: if(!RegexUtils.isIp(value.toString()))&#123; throw new Exception(description+"IP地址格式不正确"); &#125; break; case NUMBER: if(!RegexUtils.isNumber(value.toString()))&#123; throw new Exception(description+"不是数字"); &#125; break; case NUMBERORNIL: if(value == null)&#123; break; &#125; if(!RegexUtils.isNumberOrNil(value.toString()))&#123; throw new Exception(description+"格式不正确"); &#125; break; case PHONENUMBER: if(!RegexUtils.isPhoneNumber(value.toString()))&#123; throw new Exception("手机号格式不正确"); &#125; break; case ID: if(!RegexUtils.isID(value.toString()))&#123; throw new Exception("身份证号格式不正确"); &#125; break; default: break; &#125; &#125; if(!dataValid.regexExpression().equals(""))&#123; if(value.toString().matches(dataValid.regexExpression()))&#123; throw new Exception(description+"格式不正确"); &#125; &#125; /*************注解解析工作结束******************/ &#125;&#125; 如上代码。 当然，到具体业务层，应该调用这个Service的valid方法去校验参数。 结论可以看到，经过这样，我们可以把一些常用的校验通过这种方式封装，大大简化代码量，使业务层更注重业务。 这种也可以添加自己的通用类型，灵活性很强。 这个小小的简单工具最主要的就是利用了Java的反射机制。 以上。 今天就到这里啦，中秋节快乐～～]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK里那些有趣的代码（1）]]></title>
    <url>%2Fblog%2FJDK%E9%87%8C%E9%82%A3%E4%BA%9B%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%881%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言今天我们来看一下JDK里一些好玩的代码。我们来分析下。 在Integer源码里，我们可以看到这样一段代码： 12345678final static int [] sizeTable = &#123; 9, 99, 999, 9999, 99999, 999999, 9999999,99999999, 999999999, Integer.MAX_VALUE &#125;;// Requires positive xstatic int stringSize1(int x) &#123; for (int i=0; ; i++) if (x &lt;= sizeTable[i]) return i+1;&#125; 这段代码是在干啥？？？ 我们仔细读下，其实就会发现他其实是计算传入的int型x的位数，要求x为正数。 嗯，不难理解。 他为什么要这么写呢？ 思考对于我们来讲，当拿到一个int型正整数，如何用程序算出它的位数呢？ 一般不假思索的可能想到String的length方法计算。如下： 123static int myStringSize1(int x)&#123; return String.valueOf(x).length();&#125; 嗯，代码量很少。。。。。。 还有吗？ 对，还可以用除法，除以10，计算位数。 12345678static int myStringSize2(int x)&#123; int num=1; while(x&gt;10)&#123; x=x/10; num++; &#125; return num;&#125; 嗯。。。。不错不错～ 当然能用除法解决的基本上也可以用乘法解决，int的最大位数为10位，所以可以乘以10，判断大小。 12345678910// Requires positive xstatic int stringSize2(int x) &#123; int p = 10; for (int i=1; i&lt;11; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 10;&#125; 好吧，目前我就想到了除JDK以外的这三种方法。 我们测试一下我们的代码正确性。 12345678910public static void main(String[] args) &#123; int [] x=new int[]&#123;5,10,333,6666,77777,123456,1234567,87654321,999999999,1111111111,Integer.MAX_VALUE&#125;; for(int i=0;i&lt;x.length;i++)&#123; int a1=Test.stringSize1(x[i]); int a2=Test.stringSize2(x[i]); int a3=Test.myStringSize1(x[i]); int a4=Test.myStringSize2(x[i]); System.out.println(a1+"---"+a2+"---"+a3+"---"+a4); &#125;&#125; 运行结果： 结果还是木有问题哒。 提升JDK的这种写法有什么好处呢？？？ 可以看到，JDK的方法把一部分数据计算变成了数据比较，相当于优化吧。 那它的执行效率怎么样呢？ 我们写的方法的效率又如何呢？ 我们来测试下吧！ 实践我们可以制造一个随机的定长int数组，看看它们执行耗时，同时统计若干组数据，进行比较。 话不多说，直接写代码并记录到Excel里供分析。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public static void main(String[] args) throws Exception&#123; List&lt;List&lt;Long&gt;&gt; rowList= Lists.newArrayList(); List&lt;String&gt; titleList=Lists.newArrayList(); titleList.add("JDK方法"); titleList.add("乘法"); titleList.add("String方法"); titleList.add("除法"); for(int s=0;s&lt;50;s++)&#123; List&lt;Long&gt; cellList=Lists.newArrayList(); int [] xArrays=new int [10000]; for(int i=0;i&lt;xArrays.length;i++)&#123; xArrays[i]=1 + (int)(Math.random()*Integer.MAX_VALUE); &#125; //System.out.println("当前S值为"+s); long start1=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; stringSize1(xArrays[i]); &#125; long end1=System.nanoTime(); long time1=(end1-start1)/1000; System.out.println("JDK方法耗时---》"+time1+"ms"); cellList.add(time1); long start4=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; stringSize2(xArrays[i]); &#125; long end4=System.nanoTime(); long time4=(end4-start4)/1000; System.out.println("乘法耗时---》"+time4+"ms"); cellList.add(time4); long start2=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; myStringSize1(xArrays[i]); &#125; long end2=System.nanoTime(); long time2=(end2-start2)/1000; System.out.println("String方法耗时---》"+time2+"ms"); cellList.add(time2); long start3=System.nanoTime(); for(int i=0;i&lt;xArrays.length;i++) &#123; myStringSize2(xArrays[i]); &#125; long end3=System.nanoTime(); long time3=(end3-start3)/1000; System.out.println("除法耗时---》"+time3+"ms"); cellList.add(time3); rowList.add(cellList); &#125; WriteExcelUtil.writeExecl(titleList,rowList,"/Users/zhangwentong/Desktop/workbook.xlsx"); &#125; 我记录了50组数据，每组里面每个计算位数的方法执行10000次，得到如下结果。 把它绘制成折线图。 可以看到，String的length方法效率是最差的。。。。 我们其实看一下String.valueOf(x).length()这个源码，就知道为什么这么慢了。 然后除法也稍微逊色一点。。。 乘法和JDK的效率都可以说不错。。。。。但JDK方法其实要好一点。。。。 而且开始创建的sizeTable是占据较少空间，但却降低了CPU的计算次数（乘法需要每次乘以十在比较计算）。 当然，因为int的位数只有10位，可以写一个sizeTable，当数据量大时，比如long，最大19位，写一个19个数的sizeTable？ 一堆9？？？ 哈哈，我们可以看看Long里面的计算位数的方法。 12345678910// Requires positive xstatic int stringSize(long x) &#123; long p = 10; for (int i=1; i&lt;19; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 19;&#125; 人家当然用的乘法啦。。。。。 结论我们可以看到，我们最容易想到的String.length方法确是效率最低的。。。。 JDK源码里一些方法会被大量调用，当然要做到最好的优化啦。。。 多读些源码，深入思考，多加练习，有助于提升自己。 今天就到这儿吧。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）线上某应用的FULLGC分析]]></title>
    <url>%2Fblog%2F%E8%BD%AC-%E7%BA%BF%E4%B8%8A%E6%9F%90%E5%BA%94%E7%94%A8%E7%9A%84FULLGC%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[原文链接：http://jm.taobao.org/2016/03/23/3782/ 这2天，排除线上某应用启动内存变化频繁的问题时，额外发现了一个fullgc的问题，分享给大家。 过程如下：抽了台线上机器，想看下这段时间机器的gc情况，发现里面有好几个FullGc的日志： T23:23:02.009+0800: 21860.015: [Full GC 21860.015: [CMS: 2361237K-&amp;gt;1111804K(4718592K), 4.9917540 secs] 2532961K-&amp;gt;1111804K(5190464K), [CMS Perm : 17397K-&amp;gt;17240K(131072K)], 4.9918770 secs] [Times: user=4.96 sys=0.03, real=4.99 secs] JVM参数设置如下： -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=60 参数的意思是：在旧区到60％的时候，会触发一次cmsgc，应该出现如下日志： T20:10:37.803+0800: 3246087.559: [CMS-concurrent-mark-start] T20:10:38.463+0800: 3246088.220: [CMS-concurrent-mark: 0.661/0.661 secs] [Times: user=3.17 sys=0.56, real=0.66 secs] T20:10:38.463+0800: 3246088.220: [CMS-concurrent-preclean-start] T20:10:38.552+0800: 3246088.309: [CMS-concurrent-preclean: 0.069/0.089 secs] [Times: user=0.14 sys=0.04, real=0.09 secs]_&lt;/span&gt; T20:10:38.552+0800: 3246088.309: [CMS-concurrent-abortable-preclean-start] 而现在日志里面都是old区到2.3G（50%）的时候，就会触发一次FullGc，而且gc日志里面没有一次正常的cmsgc，现在是什么原因在半路截胡了？ 开始怀疑JVM参数是否设置生效，通过jinfo进行查看： jinfo -flag UseCMSInitiatingOccupancyOnly 20195 jinfo -flag CMSInitiatingOccupancyFraction 20195 一切正常。 出现Fullgc，当时我想可能的原因有以下几个情况： cmsgc失败导致（GC日志中没有相关cmsgc失败的日志） JMAP -histo：现场（人为执行肯定不是） 大对象分配时，空间不够导致（当时还剩下50%内存，并且如果大对象分配，gc日志里面是会有如下WARN的） 内存碎片导致？（由于系统会经常分配一些大数组，这个会加剧碎片化） 第四点是最可能的原因了。于是，接下来怎么验证是否是它导致的呢？加上PrintGCReason，先打印出fullgc的原因, 命令如下： /java/bin/jinfo -flag +PrintGCReason 第二天，查看日志，如下： GC Cause: Heap Inspection Initiated GC T16:16:01.880+0800: 687439.886: [Full GC 687439.886: [CMS: 2362138K-&amp;gt;1180717K(4718592K), 5.6573690 secs] 2700275K-&amp;gt;1180717K(5190464K), [C MS Perm : 17531K-&amp;gt;17488K(131072K)], 5.6574950 secs] [Times: user=5.59 sys=0.06, real=5.65 secs] GC原因：堆检查启动GC，FullGc的原因是这个，看不明白，咨询过后，说这个很可能是因为JAMP -hist继：活导致的FullGc。 那如果是这样，就有可能是有脚本或者定时任务，也可能是什么其他东西，去执行了这个命令，反正据我了解的cs没有做这事。接下来就是找这个“凶手”了，这事情没做过，没啥头绪，看进程也看不出什么，想grep所有脚本，懒癌又发作了，还是先去群里咨询下有啥简单又省力的办法吧，一下搞定： [ ~]$ crontab -l */1 * * * * /home/bin/config-monitor.sh &amp;gt;&amp;gt; /home/logs/config-monitor.log 2&amp;gt;&amp;amp;1 [logs]$ cat /home/bin/config-monitor.sh |grep &quot;jmap&quot; jmaplog=&quot;/home/jmap.log&quot;; if (count == 3) { / run jmap print &quot;run jmap command : /java/bin/jmap -histo:live &quot;pid&quot; |head -n 20&quot;; system(&quot;/java/bin/jmap -histo:live &quot;pid&quot; |head -n 20&quot;)&amp;gt;jmaplog; print &quot;#######Server has recovered after running jmap######&quot;; 有个定时任务跑一个叫config-monitor.sh的脚本，里面做的事情，基本就是监视内存各个区的比例，超过一定比例，就通过jamp -histo：现场触发下fullgc，防止溢出===》这个定时任务是cs以前遗留下来的，一直没发现，后续就是评估是否去掉这个定时任务，整个过程告一段落。 总结： 问题可能出现的原因，要尽快动手去验证，不要只停留在思考的层面; 出现fullgc的时候，可以通过加上PrintGCReason，查看具体GC原因。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）为什么 String hashCode 方法选择数字31作为乘子]]></title>
    <url>%2Fblog%2F%E4%B8%BA%E4%BB%80%E4%B9%88-String-hashCode-%E6%96%B9%E6%B3%95%E9%80%89%E6%8B%A9%E6%95%B0%E5%AD%9731%E4%BD%9C%E4%B8%BA%E4%B9%98%E5%AD%90.html</url>
    <content type="text"><![CDATA[原文地址：https://segmentfault.com/a/1190000010799123 背景某天，我在写代码的时候，无意中点开了 String hashCode 方法。然后大致看了一下 hashCode 的实现，发现并不是很复杂。但是我从源码中发现了一个奇怪的数字，也就是本文的主角31。这个数字居然不是用常量声明的，所以没法从字面意思上推断这个数字的用途。后来带着疑问和好奇心，到网上去找资料查询一下。在看完资料后，默默的感叹了一句，原来是这样啊。那么到底是哪样呢？在接下来章节里，请大家带着好奇心和我揭开数字31的用途之谜。 选择数字31的原因在详细说明 String hashCode 方法选择数字31的作为乘子的原因之前，我们先来看看 String hashCode 方法是怎样实现的，如下： 123456789101112public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; 上面的代码就是 String hashCode 方法的实现，是不是很简单。实际上 hashCode 方法核心的计算逻辑只有三行，也就是代码中的 for 循环。我们可以由上面的 for 循环推导出一个计算公式，hashCode 方法注释中已经给出。如下： s[0]31^(n-1) + s131^(n-2) + … + s[n-1] 这里说明一下，上面的 s 数组即源码中的 val 数组，是 String 内部维护的一个 char 类型数组。这里我来简单推导一下这个公式： 假设 n=3 i=0 -&gt; h = 31 * 0 + val[0] i=1 -&gt; h = 31 * (31 * 0 + val[0]) + val[1] i=2 -&gt; h = 31 * (31 * (31 * 0 + val[0]) + val[1]) + val[2] h = 31*31*31*0 + 31*31*val[0] + 31*val[1] + val[2] h = 31^(n-1)*val[0] + 31^(n-2)*val[1] + val[2] 上面的公式包括公式的推导并不是本文的重点，大家了解了解即可。接下来来说说本文的重点，即选择31的理由。从网上的资料来看，一般有如下两个原因： 第一、31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。另外一些相近的质数，比如37、41、43等等，也都是不错的选择。那么为啥偏偏选中了31呢？请看第二个原因。 第二、31可以被 JVM 优化，31 * i = (i &lt;&lt; 5) - i。 上面两个原因中，第一个需要解释一下，第二个比较简单，就不说了。下面我来解释第一个理由。一般在设计哈希算法时，会选择一个特殊的质数。至于为啥选择质数，我想应该是可以降低哈希算法的冲突率。至于原因，这个就要问数学家了，我几乎可以忽略的数学水平解释不了这个原因。上面说到，31是一个不大不小的质数，是优选乘子。那为啥同是质数的2和101（或者更大的质数）就不是优选乘子呢，分析如下。 这里先分析质数2。首先，假设 n = 6，然后把质数2和 n 带入上面的计算公式。并仅计算公式中次数最高的那一项，结果是2^5 = 32，是不是很小。所以这里可以断定，当字符串长度不是很长时，用质数2做为乘子算出的哈希值，数值不会很大。也就是说，哈希值会分布在一个较小的数值区间内，分布性不佳，最终可能会导致冲突率上升。 上面说了，质数2做为乘子会导致哈希值分布在一个较小区间内，那么如果用一个较大的大质数101会产生什么样的结果呢？根据上面的分析，我想大家应该可以猜出结果了。就是不用再担心哈希值会分布在一个小的区间内了，因为101^5 = 10,510,100,501。但是要注意的是，这个计算结果太大了。如果用 int 类型表示哈希值，结果会溢出，最终导致数值信息丢失。尽管数值信息丢失并不一定会导致冲突率上升，但是我们暂且先认为质数101（或者更大的质数）也不是很好的选择。最后，我们再来看看质数31的计算结果： 31^5 = 28629151，结果值相对于32和10,510,100,501来说。是不是很nice，不大不小。 上面用了比较简陋的数学手段证明了数字31是一个不大不小的质数，是作为 hashCode 乘子的优选质数之一。接下来我会用详细的实验来验证上面的结论，不过在验证前，我们先看看 Stack Overflow 上关于这个问题的讨论，Why does Java’s hashCode() in String use 31 as a multiplier?。其中排名第一的答案引用了《Effective Java》中的一段话，这里也引用一下： The value 31 was chosen because it is an odd prime. If it were even and the multiplication overflowed, information would be lost, as multiplication by 2 is equivalent to shifting. The advantage of using a prime is less clear, but it is traditional. A nice property of 31 is that the multiplication can be replaced by a shift and a subtraction for better performance: 31 * i == (i &lt;&lt; 5) - i. Modern VMs do this sort of optimization automatically. 简单翻译一下： 选择数字31是因为它是一个奇质数，如果选择一个偶数会在乘法运算中产生溢出，导致数值信息丢失，因为乘二相当于移位运算。选择质数的优势并不是特别的明显，但这是一个传统。同时，数字31有一个很好的特性，即乘法运算可以被移位和减法运算取代，来获取更好的性能：31 * i == (i &lt;&lt; 5) - i，现代的 Java 虚拟机可以自动的完成这个优化。 排名第二的答案设这样说的： As Goodrich and Tamassia point out, If you take over 50,000 English words (formed as the union of the word lists provided in two variants of Unix), using the constants 31, 33, 37, 39, and 41 will produce less than 7 collisions in each case. Knowing this, it should come as no surprise that many Java implementations choose one of these constants. 这段话也翻译一下： 正如 Goodrich 和 Tamassia 指出的那样，如果你对超过 50,000 个英文单词（由两个不同版本的 Unix 字典合并而成）进行 hash code 运算，并使用常数 31, 33, 37, 39 和 41 作为乘子，每个常数算出的哈希值冲突数都小于7个，所以在上面几个常数中，常数 31 被 Java 实现所选用也就不足为奇了。 上面的两个答案完美的解释了 Java 源码中选用数字 31 的原因。接下来，我将针对第二个答案就行验证，请大家继续往下看。 实验及数据可视化本节，我将使用不同的数字作为乘子，对超过23万个英文单词进行哈希运算，并计算哈希算法的冲突率。同时，我也将针对不同乘子算出的哈希值分布情况进行可视化处理，让大家可以直观的看到数据分布情况。本次实验所使用的数据是 Unix/Linux 平台中的英文字典文件，文件路径为 /usr/share/dict/words。 哈希值冲突率计算计算哈希算法冲突率并不难，比如可以一次性将所有单词的 hash code 算出，并放入 Set 中去除重复值。之后拿单词数减去 set.size() 即可得出冲突数，有了冲突数，冲突率就可以算出来了。当然，如果使用 JDK8 提供的流式计算 API，则可更方便算出，代码片段如下： 123456789101112131415161718192021222324252627public static Integer hashCode(String str, Integer multiplier) &#123; int hash = 0; for (int i = 0; i &lt; str.length(); i++) &#123; hash = multiplier * hash + str.charAt(i); &#125; return hash;&#125; /** * 计算 hash code 冲突率，顺便分析一下 hash code 最大值和最小值，并输出 * @param multiplier * @param hashs */public static void calculateConflictRate(Integer multiplier, List&lt;Integer&gt; hashs) &#123; Comparator&lt;Integer&gt; cp = (x, y) -&gt; x &gt; y ? 1 : (x &lt; y ? -1 : 0); int maxHash = hashs.stream().max(cp).get(); int minHash = hashs.stream().min(cp).get(); // 计算冲突数及冲突率 int uniqueHashNum = (int) hashs.stream().distinct().count(); int conflictNum = hashs.size() - uniqueHashNum; double conflictRate = (conflictNum * 1.0) / hashs.size(); System.out.println(String.format("multiplier=%4d, minHash=%11d, maxHash=%10d, conflictNum=%6d, conflictRate=%.4f%%", multiplier, minHash, maxHash, conflictNum, conflictRate * 100));&#125; 结果如下： 从上图可以看出，使用较小的质数做为乘子时，冲突率会很高。尤其是质数2，冲突率达到了 55.14%。同时我们注意观察质数2作为乘子时，哈希值的分布情况。可以看得出来，哈希值分布并不是很广，仅仅分布在了整个哈希空间的正半轴部分，即 0 ~ 2^31-1。而负半轴 -2^31 ~ -1，则无分布。这也证明了我们上面断言，即质数2作为乘子时，对于短字符串，生成的哈希值分布性不佳。然后再来看看我们之前所说的 31、37、41 这三个不大不小的质数，表现都不错，冲突数都低于7个。而质数 101 和 199 表现的也很不错，冲突率很低，这也说明哈希值溢出并不一定会导致冲突率上升。但是这两个家伙一言不合就溢出，我们认为他们不是哈希算法的优选乘子。最后我们再来看看 32 和 36 这两个偶数的表现，结果并不好，尤其是 32，冲突率超过了了50%。尽管 36 表现的要好一点，不过和 31，37相比，冲突率还是比较高的。当然并非所有的偶数作为乘子时，冲突率都会比较高，大家有兴趣可以自己验证。 哈希值分布可视化上一节分析了不同数字作为乘子时的冲突率情况，这一节来分析一下不同数字作为乘子时，哈希值的分布情况。在详细分析之前，我先说说哈希值可视化的过程。我原本是打算将所有的哈希值用一维散点图进行可视化，但是后来找了一圈，也没找到合适的画图工具。加之后来想了想，一维散点图可能不合适做哈希值可视化，因为这里有超过23万个哈希值。也就意味着会在图上显示超过23万个散点，如果不出意外的话，这23万个散点会聚集的很密，有可能会变成一个大黑块，就失去了可视化的意义了。所以这里选择了另一种可视化效果更好的图表，也就是 excel 中的平滑曲线的二维散点图（下面简称散点曲线图）。当然这里同样没有把23万散点都显示在图表上，太多了。所以在实际绘图过程中，我将哈希空间等分成了64个子区间，并统计每个区间内的哈希值数量。最后将分区编号做为X轴，哈希值数量为Y轴，就绘制出了我想要的二维散点曲线图了。这里举个例子说明一下吧，以第0分区为例。第0分区数值区间是[-2147483648, -2080374784)，我们统计落在该数值区间内哈希值的数量，得到 &lt;分区编号, 哈希值数量&gt; 数值对，这样就可以绘图了。分区代码如下： 1234567891011121314151617181920212223242526 /** * 将整个哈希空间等分成64份，统计每个空间内的哈希值数量 * @param hashs */public static Map&lt;Integer, Integer&gt; partition(List&lt;Integer&gt; hashs) &#123; // step = 2^32 / 64 = 2^26 final int step = 67108864; List&lt;Integer&gt; nums = new ArrayList&lt;&gt;(); Map&lt;Integer, Integer&gt; statistics = new LinkedHashMap&lt;&gt;(); int start = 0; for (long i = Integer.MIN_VALUE; i &lt;= Integer.MAX_VALUE; i += step) &#123; final long min = i; final long max = min + step; int num = (int) hashs.parallelStream() .filter(x -&gt; x &gt;= min &amp;&amp; x &lt; max).count(); statistics.put(start++, num); nums.add(num); &#125; // 为了防止计算出错，这里验证一下 int hashNum = nums.stream().reduce((x, y) -&gt; x + y).get(); assert hashNum == hashs.size(); return statistics;&#125; 本文中的哈希值是用整形表示的，整形的数值区间是 [-2147483648, 2147483647]，区间大小为 2^32。所以这里可以将区间等分成64个子区间，每个自子区间大小为 2^26。详细的分区对照表如下： 分区编号 分区下限 分区上限 分区编号 分区下限 分区上限 0 -2147483648 -2080374784 32 0 67108864 1 -2080374784 -2013265920 33 67108864 134217728 2 -2013265920 -1946157056 34 134217728 201326592 3 -1946157056 -1879048192 35 201326592 268435456 4 -1879048192 -1811939328 36 268435456 335544320 5 -1811939328 -1744830464 37 335544320 402653184 6 -1744830464 -1677721600 38 402653184 469762048 7 -1677721600 -1610612736 39 469762048 536870912 8 -1610612736 -1543503872 40 536870912 603979776 9 -1543503872 -1476395008 41 603979776 671088640 10 -1476395008 -1409286144 42 671088640 738197504 11 -1409286144 -1342177280 43 738197504 805306368 12 -1342177280 -1275068416 44 805306368 872415232 13 -1275068416 -1207959552 45 872415232 939524096 14 -1207959552 -1140850688 46 939524096 1006632960 15 -1140850688 -1073741824 47 1006632960 1073741824 16 -1073741824 -1006632960 48 1073741824 1140850688 17 -1006632960 -939524096 49 1140850688 1207959552 18 -939524096 -872415232 50 1207959552 1275068416 19 -872415232 -805306368 51 1275068416 1342177280 20 -805306368 -738197504 52 1342177280 1409286144 21 -738197504 -671088640 53 1409286144 1476395008 22 -671088640 -603979776 54 1476395008 1543503872 23 -603979776 -536870912 55 1543503872 1610612736 24 -536870912 -469762048 56 1610612736 1677721600 25 -469762048 -402653184 57 1677721600 1744830464 26 -402653184 -335544320 58 1744830464 1811939328 27 -335544320 -268435456 59 1811939328 1879048192 28 -268435456 -201326592 60 1879048192 1946157056 29 -201326592 -134217728 61 1946157056 2013265920 30 -134217728 -67108864 62 2013265920 2080374784 31 -67108864 0 63 2080374784 2147483648 接下来，让我们对照上面的分区表，对数字2、3、17、31、101的散点曲线图进行简单的分析。先从数字2开始，数字2对于的散点曲线图如下： 上面的图还是很一幕了然的，乘子2算出的哈希值几乎全部落在第32分区，也就是 [0, 67108864)数值区间内，落在其他区间内的哈希值数量几乎可以忽略不计。这也就不难解释为什么数字2作为乘子时，算出哈希值的冲突率如此之高的原因了。所以这样的哈希算法要它有何用啊，拖出去斩了吧。接下来看看数字3作为乘子时的表现： 3作为乘子时，算出的哈希值分布情况和2很像，只不过稍微好了那么一点点。从图中可以看出绝大部分的哈希值最终都落在了第32分区里，哈希值的分布性很差。这个也没啥用，拖出去枪毙5分钟吧。在看看数字17的情况怎么样： 数字17作为乘子时的表现，明显比上面两个数字好点了。虽然哈希值在第32分区和第34分区有一定的聚集，但是相比较上面2和3，情况明显好好了很多。除此之外，17作为乘子算出的哈希值在其他区也均有分布，且较为均匀，还算是一个不错的乘子吧。 接下来来看看我们本文的主角31了，31作为乘子算出的哈希值在第33分区有一定的小聚集。不过相比于数字17，主角31的表现又好了一些。首先是哈希值的聚集程度没有17那么严重，其次哈希值在其他区分布的情况也要好于17。总之，选31，准没错啊。 最后再来看看大质数101的表现，不难看出，质数101作为乘子时，算出的哈希值分布情况要好于主角31，有点喧宾夺主的意思。不过不可否认的是，质数101的作为乘子时，哈希值的分布性确实更加均匀。所以如果不在意质数101容易导致数据信息丢失问题，或许其是一个更好的选择。 写在最后经过上面的分析与实践，我想大家应该明白了 String hashCode 方法中选择使用数字31作为乘子的原因了。 本文为转载，原文地址：https://segmentfault.com/a/1190000010799123]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReentrantLock那些事]]></title>
    <url>%2Fblog%2F%E8%B0%88%E8%B0%88ReentrantLock.html</url>
    <content type="text"><![CDATA[前言在说ReentrantLock之前，我们先说说并发吧。 在JDK1.5之前，并发处理常用的关键字synchronized。使用synchronized关键字，锁的获取和释放是隐式的，synchronized主要通过系统的monitorenter指令实现的。 那时候synchronized可以称为重量级锁，执行效率不是很高。 而Doug Lea编写的util.concurrent 包被纳入JSR-166标准。这里面就包含了ReentrantLock。 ReentrantLock为编写并发提供了更多选择。 使用ReentrantLock的通常用法如下：123456789101112public class X &#123; private final ReentrantLock lock = new ReentrantLock(); public void m() &#123; lock.lock(); try &#123; //TODO &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 原理ReentrantLock主要是通过AbstractQueuedSynchronizer实现的，是一个重入锁，即一个线程加锁后仍然可以获得锁，不会出现自己阻塞自己的情况。 UML图我们看一下它们的UML图。 可以看到ReentrantLock实现了Lock接口。 锁类型ReentrantLock的两种锁类型，公平锁和非公平锁。 源码分析我们先来看下ReentrantLock的构造方法。 123456public ReentrantLock() &#123; sync = new NonfairSync();&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 可以看到默认无参构造方法为非公平锁实现。如果想定义公平锁实现，可以传入true来控制。 它的lock方法： 12345678public void lock() &#123; sync.acquire(1); &#125; public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; 公平锁和非公平锁各有自己的实现方式。我们来看下他们的tryAcquire方法。 非公平锁源码： 123456789101112131415161718192021222324static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125;final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125; 可以看到，非公平锁首先判断AQS（AbstractQueuedSynchronizer）中的state是否为0，0表示没有线程持有该锁，当前线程就尝试获取锁。 如果不是0，那在判断是不是当前线程持有该锁，如果是，就会增加state，改变state状态。（因此ReentranLock支持重入）。 公平锁源码： 1234567891011121314151617181920212223static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; @ReservedStackAccess protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 1234567public final boolean hasQueuedPredecessors() &#123; Node t = tail; Node h = head; Node s; return h != t &amp;&amp; ((s = h.next) == null || s.thread != Thread.currentThread());&#125; 公平锁的tryAcquire方法，可以看到，相比非公平锁，多了hasQueuedPredecessors方法，这个方法是判断队列中是否有其他线程，如果没有，线程才会尝试获取锁，如果有，会先把锁分配给队列的线程，因此称为公平锁。 这儿可以看到，非公平锁的效率比公平锁要高。 这是tryAcquire方法，如果尝试获取锁失败了呢？ 那就会执行acquireQueued(addWaiter(Node.EXCLUSIVE), arg)方法啦。 我们先来看一下addWaiter方法。 12345678910111213141516private Node addWaiter(Node mode) &#123; Node node = new Node(mode); for (;;) &#123; Node oldTail = tail; if (oldTail != null) &#123; node.setPrevRelaxed(oldTail); if (compareAndSetTail(oldTail, node)) &#123; oldTail.next = node; return node; &#125; &#125; else &#123; initializeSyncQueue(); &#125; &#125;&#125; 可以看到，这个方法会把线程添加到队列尾，同时，for(;;)循环保证添加成功，直到return出去。 添加后，调用acquireQueued方法，这个方法为挂起等待线程。 看下该方法源码： 1234567891011121314151617181920212223final boolean acquireQueued(final Node node, int arg) &#123; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; catch (Throwable t) &#123; cancelAcquire(node); throw t; &#125;&#125;private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 可以看到，如果节点为头节点，就尝试获取一次锁，如果成功，就返回。 否则判断该线程是否需要挂起，如果需要的化就调用parkAndCheckInterrupt挂起。 调用LockSupport.park方法挂起线程，直到被唤醒。 selfInterrupt方法： 12345678910111213141516static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125;public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); b.interrupt(this); return; &#125; &#125; interrupt0();&#125; 调用interrupt方法，中断正在执行的线程（如果不是当前线程的话）。 释放锁unlock方法： 公平锁和非公平锁释放锁的方法是一样的。 123456789101112131415161718192021222324public void unlock() &#123; sync.release(1); &#125; public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; 可以看到首先会判断当前线程是否是获得锁的线程，如果是重入锁需要将state减完才算是完全释放锁。 释放后调用unparkSuccessor唤起挂起线程。 总结 非公平锁的效率是比公平锁要高的。 ReentranLock支持重入，因为增加了对自身线程的处理，通过state可以控制。 解锁操作应放到finally块里，避免使用锁时出现资源无法释放的问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[越努力越幸运]]></title>
    <url>%2Fblog%2F%E8%B6%8A%E5%8A%AA%E5%8A%9B%E8%B6%8A%E5%B9%B8%E8%BF%90.html</url>
    <content type="text"><![CDATA[前言感觉最近自己浮躁的很，不知为什么。 准备写些东西源自3个月前了，那时候想自己记录些知识，一方面便于加深记忆，一方面不让自己太“闲”。 或许心态本来就不太正，浮浮躁躁，有的时候写点东西感觉就是为了应付内心那个自己。 兴趣与工作从事Java开发已经3-4年了，也算是个半路出家的吧，总应该比别人多付出些努力。 人如何克服惰性？ 关键是培养自己的兴趣爱好吧。 3-4年工作期间，换了2、3家公司，每次面试的时候，总感觉自己欠点什么，说不出来的感觉。大概是基础比较薄弱吧。 回想起自己曾经从采矿专业转到软件开发，培训的时间，那时候是为了什么？那时候不是对软件的兴趣吗？ 初心呢？初衷呢？ 自己也渴望成为一个大牛，无论是架构师、全栈还是资深大咖。 阅读与写作曾说写作可以平复心态，阅读可以使人优秀。 是的呀，无论是阅读或者写作，都可以使人思考。 思考是什么、为什么、怎么办。 提升任何的辉煌与成功都是一步步慢慢实现的。 越努力越幸运。 创造性的思维也是必须的。 时刻牢记你想要的是什么。 结语兄台，就和我一起干了这碗鸡汤吧！]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList、LinkedList和Vector分析]]></title>
    <url>%2Fblog%2FArrayList%E5%92%8CVector%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[前言List家族常用的类有3个，ArrayList、LinkedList、Vector。 ArrayList和Vector的底层是基于数组实现的，LinkedList的底层是基于链表实现的。 三者的比较如下： ArrayList和LinkedList不是线程安全的，Vector是线程安全的。 对于随机访问（get和set），ArrayList的性能要优于LinkedList。 对于add和remove操作，LinkedList和ArrayList性能差距不是很大。 三者均实现了Collection接口。 分析UML图三者与其他类的继承实现关系UML图如下。 ArrayList： LinkedList： Vector： 源码分析核心实现ArrayList是基于数组实现。 12transient Object[] elementData;private int size; LinkedList是基于链表实现。 123transient int size = 0;transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; Vector是基于数组实现。 123protected Object[] elementData;protected int elementCount;protected int capacityIncrement; 核心参数ArrayList 可以设置初始大小（由于数组实现的原因），不设置默认为10。 LinkedList不需要设置参数（由于使用链表实现，无界）。 Vector不仅可以设置初始大小，还可以设置容量增幅。 1234567891011121314151617181920212223//ArrayList传参构造函数 public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125; &#125; //LinkedList构造函数 public LinkedList() &#123; &#125; //Vector传参构造函数 public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement; &#125; 关于初始化大小和扩容机制下面讲。 核心方法我们主要分析get、set、add、remove这几个方法。对于ArrayList和Vector，还要分析扩容方法。 get、set方法ArrayList get，set方法： 12345678910public E get(int index) &#123; Objects.checkIndex(index, size); return elementData(index);&#125;public E set(int index, E element) &#123; Objects.checkIndex(index, size); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; Vector get，set方法： 1234567891011121314public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125;public synchronized E set(int index, E element) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 二者的逻辑一样，get方法判断是否下标越界，不越界返回index下的数值。set方法判断是否越界，不越界将新值放到指定下标上。它俩的区别在与synchronized关键字，正好说明了Vector是线程安全的。 LinkedList get，set方法： 1234567891011121314151617181920212223242526public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;public E set(int index, E element) &#123; checkElementIndex(index); Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal;&#125;Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 可以看到LinkedList的get方法会先检查是否越界，不越界返回指定下标node的item值。set方法也是先检查越界情况，不越界将该点的node的item赋为新值。取node指定位置上的值时要循环遍历，所以对于随机的get，set，ArrayList的性能要优于LinkedList的。 add、remove方法ArrayList add、remove方法： 1234567891011121314151617181920212223242526272829303132333435363738public boolean add(E e) &#123; modCount++; add(e, elementData, size); return true;&#125;private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) elementData = grow(); elementData[s] = e; size = s + 1;&#125;public void add(int index, E element) &#123; rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); System.arraycopy(elementData, index, elementData, index + 1, s - index); elementData[index] = element; size = s + 1;&#125;public E remove(int index) &#123; Objects.checkIndex(index, size); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; Vector add、remove方法： 12345678910111213141516171819202122232425public synchronized boolean add(E e) &#123; modCount++; add(e, elementData, elementCount); return true;&#125;private void add(E e, Object[] elementData, int s) &#123; if (s == elementData.length) elementData = grow(); elementData[s] = e; elementCount = s + 1;&#125;public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue;&#125; 它们里面有add、remove方法不止一个，我们只拿一个来举例。 可以看到，Vector和ArrayList十分相近了，除了synchronized关键字。 add方法当elementData.length和elementCount相等时（容量满），会执行扩容操作，并将元素放到指定位置。 remove方法先判断下标是否越界，不越界会删除指定位置的元素，并且将数组重新拷贝合并。 同时它们有一个计数器modCount，在HashMap那边已经讲过，是用来fast-fail的，当多个线程同时操作，modCount不一致，就会抛出异常。 LinkedList的add、remove方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125;E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 可以看到，LinkedList的add方法开始也会校验指针位置，然后如果在末尾，就在链表最后面添加节点，否则就插入到链表指定位置上。 remove方法校验指针位置后，会删除指定位置上的node。 上面可以看到，对于add和remove，ArrayList数组要进行扩容或者删除部分长度，执行Sysetm.arraycopy方法，这是要消耗一些性能的，对于LinkedList，不需要维护容量问题，但是每次新增或者删除时，都会创建或删除一个Node对象，也是要消耗一些性能的。 扩容方法对于ArrayList或者Vector，扩容方法如下： ArrayList 扩容方法：1234567891011121314151617181920212223private static final int DEFAULT_CAPACITY = 10; private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt;= 0) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; Vector扩容方法： 12345678910111213141516171819202122232425public Vector() &#123; this(10); &#125; private int newCapacity(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt;= 0) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return minCapacity; &#125; return (newCapacity - MAX_ARRAY_SIZE &lt;= 0) ? newCapacity : hugeCapacity(minCapacity); &#125; private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 上面代码可以看到，对于ArrayList，如果不传入初始容量，默认为10。容量达到最值，执行扩容，每次扩容 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 默认原容量的1.5倍。 Vector，如果不传入初始容量和自增容量，默认初始容量也为10.扩容时执行int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ?capacityIncrement : oldCapacity); 默认为原容量的2倍。 两者的最大值容量均为Integer.MAX_VALUE. LinkedList由于是链表实现，没有容量限制。无需扩容。 代码我们从代码的角度比较下ArrayList和LinkedList，Vector。 我们构建一个有200W数据的ArrayList和LinkedList。 1234567891011121314151617181920212223242526public static void main(String[] args)&#123; List&lt;Integer&gt; list = new ArrayList(); //List&lt;Integer&gt; list = new LinkedList&lt;Integer&gt;(); //Vector&lt;Integer&gt; list=new Vector&lt;&gt;(); for (int i = 0; i &lt; 2000000; i++) &#123; list.add(i); &#125; Integer tmp; long start=System.currentTimeMillis() ; //ForEach for(Integer s:list)&#123; tmp=s; &#125; System.out.println("foreach spend:"+(System.currentTimeMillis()-start)); start = System.currentTimeMillis(); for(Iterator&lt;Integer&gt; it = list.iterator(); it.hasNext();)&#123; tmp=it.next(); &#125; System.out.println("Iterator spend;"+(System.currentTimeMillis()-start)); start=System.currentTimeMillis(); int size=list.size(); for(int i=0;i&lt;size;i++)&#123; tmp=list.get(i); &#125; System.out.println("for spend;"+(System.currentTimeMillis()-start));&#125; 某一次的结果： 数组 forEach遍历 Iterator遍历 for遍历 ArrayList 9ms 9ms 12ms LinkedList 17ms 16ms ??? Vector 44ms 55ms 41ms 由于for循环遍历是随机访问，故LinkedList在数据量很大的情况下时间消耗会很长，基本不能接受。由于Vector线程安全，synchronized，故其整体效率会比ArrayList低些。在实际开发中，应用的ArrayList还是比较多的。 结语以上就是对ArrayList、LinkedList、Vector的全部分析。对于不同的应用场景，合理的选择List的类型也是至关重要的。三种List都是比较基础的知识，应当学习和掌握。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StringBuffer和StringBuilder]]></title>
    <url>%2Fblog%2FStringBuffer%E5%92%8CStringBuilder.html</url>
    <content type="text"><![CDATA[前言我们知道，对于一般大量频繁的String操作，我们不建议也不应该直接用String进行相加操作，而我们应借助StringBuffer或者StringBuilder来实现。 StringBuffer是线程安全的，而StringBuilder是线程不安全的。 由此看来，StringBuilder对String的操作快，不安全，适合单线程；StringBuilder对String的操作较StringBuilder慢，安全，适合多线程和单线程。 我们今天分析一下二者的源码。 分析class定义两者的class定义。 1234567891011//StringBuffer public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125;//StringBuilderpublic final class StringBuilder extends AbstractStringBuilder implements java.io.Serializable, CharSequence&#123;&#125; 二者的UML图如下： 可以看到二者均继承AbstractStringBuilder类，且都实现了Serializable和CharSequence接口。即二者分别是AbstractStringBuilder类的安全和不安全的一种实现。 构造器我们先来分析下StringBuffer。 12345678910public StringBuffer() &#123; super(16);&#125;public StringBuffer(int capacity) &#123; super(capacity);&#125;public StringBuffer(String str) &#123; super(str.length() + 16); append(str);&#125; 可以看到，当我们new StringBuffer时，如果什么也不传，默认赋予16数组长度，如果传入一个String，则长度为String.length()+16。 append方法在看一下append方法。关键字synchronized 对该方法进行了加锁，保证安全，toStringCache 赋值为空。然后调用AbstractStringBuilder的append方法。 StringBuffer append方法。123456@Overridepublic synchronized StringBuffer append(String str) &#123; toStringCache = null; super.append(str); return this;&#125; AbstractStringBuilder里的方法。 123456789101112131415161718192021222324252627public AbstractStringBuilder append(String str) &#123; if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this;&#125;private AbstractStringBuilder appendNull() &#123; int c = count; ensureCapacityInternal(c + 4); final char[] value = this.value; value[c++] = 'n'; value[c++] = 'u'; value[c++] = 'l'; value[c++] = 'l'; count = c; return this;&#125;private void ensureCapacityInternal(int minimumCapacity) &#123; // overflow-conscious code if (minimumCapacity - value.length &gt; 0) &#123; value = Arrays.copyOf(value, newCapacity(minimumCapacity)); &#125;&#125; String里的getChars方法。 123456789101112public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) &#123; if (srcBegin &lt; 0) &#123; throw new StringIndexOutOfBoundsException(srcBegin); &#125; if (srcEnd &gt; value.length) &#123; throw new StringIndexOutOfBoundsException(srcEnd); &#125; if (srcBegin &gt; srcEnd) &#123; throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); &#125; System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin);&#125; 对于这段代码的理解。追加一个Str时，如果是null，则调用appendNull方法，在后面直接加一个null字符串。如果不为空，拿到字符串长度，进行容量扩容为当前容量+str的长度，调用String的getChars方法，将字符串数组加在后面，这最后是个char数组。 通过上图可以看到 AbstractStringBuilder是基于char数组实现的，count用于统计当前长度。 toString方法我们看一下toString方法。可以发现他把字符串数组先放到了缓存数组，然后在返回一个String。当StringBuffer变化时，如append，则直接把toStringCache 赋值为空。 1234567@Overridepublic synchronized String toString() &#123; if (toStringCache == null) &#123; toStringCache = Arrays.copyOfRange(value, 0, count); &#125; return new String(toStringCache, true);&#125; insert方法我们再来分析一下StringBuffer的insert，由于重载方法较多，我们只分析insert String的代码。 StringBuffer insert方法。123456@Overridepublic synchronized StringBuffer insert(int offset, String str) &#123; toStringCache = null; super.insert(offset, str); return this;&#125; AbstractStringBuilder里的insert方法。 123456789101112public AbstractStringBuilder insert(int offset, String str) &#123; if ((offset &lt; 0) || (offset &gt; length())) throw new StringIndexOutOfBoundsException(offset); if (str == null) str = "null"; int len = str.length(); ensureCapacityInternal(count + len); System.arraycopy(value, offset, value, offset + len, count - offset); str.getChars(value, offset); count += len; return this;&#125; 可以看到与append大致相同，就是调用System.arraycopy的时候插入的位置发生了变化。 其他方法（delete，replace等）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//StringBuffer @Override public synchronized StringBuffer delete(int start, int end) &#123; toStringCache = null; super.delete(start, end); return this; &#125; //AbstractStringBuilder public AbstractStringBuilder delete(int start, int end) &#123; if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (end &gt; count) end = count; if (start &gt; end) throw new StringIndexOutOfBoundsException(); int len = end - start; if (len &gt; 0) &#123; System.arraycopy(value, start+len, value, start, count-end); count -= len; &#125; return this; &#125; //StringBuffer @Override public synchronized StringBuffer replace(int start, int end, String str) &#123; toStringCache = null; super.replace(start, end, str); return this; &#125; //AbstractStringBuilder public AbstractStringBuilder replace(int start, int end, String str) &#123; if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (start &gt; count) throw new StringIndexOutOfBoundsException("start &gt; length()"); if (start &gt; end) throw new StringIndexOutOfBoundsException("start &gt; end"); if (end &gt; count) end = count; int len = str.length(); int newCount = count + len - (end - start); ensureCapacityInternal(newCount); System.arraycopy(value, end, value, start + len, count - end); str.getChars(value, start); count = newCount; return this; &#125; 可以看到他们均使用了System.arraycopy方法。这儿不再一一赘述。 其他我们再看一下StringBuilder的源码。发现它与StringBuffer差别很小。 不同点： 增删改操作上没有synchronized关键字。 没有private transient char[] toStringCache;的定义。 第一点正好验证了StringBuilder不是线程安全的，第二点StringBuffer中toStringCache的引入是为了在多线程并发下读取写入数据起到一定的缓存缓冲作用。 结论我们可以看到，无论StringBuilder还是StringBuffer，都是AbstractStringBuilder的实现类。 AbstractStringBuilder对字符串的操作，实质是将它存储在一个char数组中，这样减小了内存开销。我们知道，如果使用String连加，会创造大量String对象，GC来不及回收，导致OOM异常或内存开销增大。StringBuffer和StringBuilder对String的操作完美的解决了这个问题，且相当于提供了操作字符串更加直观的方法（如insert，delete，append等）。这是值得我们借鉴和学习的。 以上就是StringBuffer和StringBuilder的源码分析。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer源码解析]]></title>
    <url>%2Fblog%2FInteger%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.html</url>
    <content type="text"><![CDATA[前言今天我们来分析一下Integer源码。 Integer是八种包装类里面的比较常用的一种。那在使用时有什么注意及学习的地方呢？ 让我们一起来看一下 分析Integer是包java.lang下的一个类。 123public final class Integer extends Number implements Comparable&lt;Integer&gt; &#123;//code&#125; 其被定义成final类型，继承Number类实现Comparable接口。 123@Native public static final int MIN_VALUE = 0x80000000;@Native public static final int MAX_VALUE = 0x7fffffff;public static final Class&lt;Integer&gt; TYPE = (Class&lt;Integer&gt;) Class.getPrimitiveClass("int"); 可以看出，其定义了Integer的最大值为2^31-1，最小值为-2^31。Integer的基本数据类型为int。 我们来看一下Integer的toString方法，是比较有趣的。 12345678public static String toString(int i) &#123; if (i == Integer.MIN_VALUE) return "-2147483648"; int size = (i &lt; 0) ? stringSize(-i) + 1 : stringSize(i); char[] buf = new char[size]; getChars(i, size, buf); return new String(buf, true);&#125; 方法中使用到了stringSize函数，就是求这个Integer数的长度，我们来看看他是如何实现的。 123456789final static int [] sizeTable = &#123; 9, 99, 999, 9999, 99999, 999999, 9999999, 99999999, 999999999, Integer.MAX_VALUE &#125;;// Requires positive xstatic int stringSize(int x) &#123; for (int i=0; ; i++) if (x &lt;= sizeTable[i]) return i+1;&#125; 可以看到这段代码在计算Integer数长度时，构建了一个一维数组，然后拿x与数组每个值进行比较。而未使用我们经常说的除法或乘法计算长度。我们可以看下源码里的注释。 1234567891011121314151617// I use the "invariant division by multiplication" trick to// accelerate Integer.toString. In particular we want to// avoid division by 10.//// The "trick" has roughly the same performance characteristics// as the "classic" Integer.toString code on a non-JIT VM.// The trick avoids .rem and .div calls but has a longer code// path and is thus dominated by dispatch overhead. In the// JIT case the dispatch overhead doesn't exist and the// "trick" is considerably faster than the classic code.//// TODO-FIXME: convert (x * 52429) into the equiv shift-add// sequence.//// RE: Division by Invariant Integers using Multiplication// T Gralund, P Montgomery// ACM PLDI 1994 我们知道计算机在计算除法效率要比加减乘法低。所以为了避免除法，提高计算效率，采用此种方法。 正好我们可以看看Long的toString方法里的stringSize方法。 12345678910// Requires positive xstatic int stringSize(long x) &#123; long p = 10; for (int i=1; i&lt;19; i++) &#123; if (x &lt; p) return i; p = 10*p; &#125; return 19;&#125; 可以看到使用了乘法。你或许会问为什么没有像Integer那样构建一个数组去比较？额，如果要构造数组，那要构造一个19位的数组，里面有1-19位的数，代码写起来很多很臃肿吧，而且构造好的数组会长期放在内存中，我们知道，在实际应用中，Integer的使用频率要比Long高多了，长期让Long里面的一个数组占据内存空间也不太合理。以上是我个人见解。 我们再来看一下parseInt方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public static int parseInt(String s, int radix) throws NumberFormatException&#123; /* * WARNING: This method may be invoked early during VM initialization * before IntegerCache is initialized. Care must be taken to not use * the valueOf method. */ if (s == null) &#123; throw new NumberFormatException("null"); &#125; if (radix &lt; Character.MIN_RADIX) &#123; throw new NumberFormatException("radix " + radix + " less than Character.MIN_RADIX"); &#125; if (radix &gt; Character.MAX_RADIX) &#123; throw new NumberFormatException("radix " + radix + " greater than Character.MAX_RADIX"); &#125; int result = 0; boolean negative = false; int i = 0, len = s.length(); int limit = -Integer.MAX_VALUE; int multmin; int digit; if (len &gt; 0) &#123; char firstChar = s.charAt(0); if (firstChar &lt; '0') &#123; // Possible leading "+" or "-" if (firstChar == '-') &#123; negative = true; limit = Integer.MIN_VALUE; &#125; else if (firstChar != '+') throw NumberFormatException.forInputString(s); if (len == 1) // Cannot have lone "+" or "-" throw NumberFormatException.forInputString(s); i++; &#125; multmin = limit / radix; while (i &lt; len) &#123; // Accumulating negatively avoids surprises near MAX_VALUE digit = Character.digit(s.charAt(i++),radix); if (digit &lt; 0) &#123; throw NumberFormatException.forInputString(s); &#125; if (result &lt; multmin) &#123; throw NumberFormatException.forInputString(s); &#125; result *= radix; if (result &lt; limit + digit) &#123; throw NumberFormatException.forInputString(s); &#125; result -= digit; &#125; &#125; else &#123; throw NumberFormatException.forInputString(s); &#125; return negative ? result : -result;&#125; 我们可以看到此方法首先进行异常处理，然后判断传入String是否有正负号，然后截取位数，使用乘法，用减法得到int值，然后判断正负并返回结果。 我们再来看下Integer的内部类IntegerCache。 这儿可以说是一个坑，也是比较有意思的地方。 123456789101112131415161718192021222324252627282930313233private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 很容易理解这段代码，初始化Integer后，IntegerCache会缓存[-128,127]之间的数据，这个区间的上限可以配置，取决于java.lang.Integer.IntegerCache.high这个属性，这个属性在VM参数里为-XX:AutoBoxCacheMax=2000进行设置调整或者VM里设置-Djava.lang.Integer.IntegerCache.high=2000。所以Integer在初始化完成后会缓存[-128,max]之间的数据。 并且我们可以看到valueOf方法。 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 可以看到valueOf方法，在cache范围内，返回的是缓存的值，是相同的对象，不在cache范围内，才会新建Integer。 由于有了Integer缓存，我们可以测试以下代码。 12345678Integer a=1;Integer b=1;Integer c=new Integer(1);Integer d=1000;Integer e=1000;System.out.println(a==b);System.out.println(b==c);System.out.println(d==e); 可以看到结果为true,false,false。 这样，我们在比较Integer时，如果仅仅比较值相等，建议使用equals方法比较。 我们可以看下Integer的equals方法。 123456public boolean equals(Object obj) &#123; if (obj instanceof Integer) &#123; return value == ((Integer)obj).intValue(); &#125; return false;&#125; 可以看到它会先判断类型是否符合，然后进行拆箱比较操作。 同样，在Long，Byte，Short，我们也可以看到缓存，其缓存数据长度均是-128到127。 123456789101112131415161718192021222324252627282930313233//Long private static class LongCache &#123; private LongCache()&#123;&#125; static final Long cache[] = new Long[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Long(i - 128); &#125; &#125; //Byte private static class ByteCache &#123; private ByteCache()&#123;&#125; static final Byte cache[] = new Byte[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Byte((byte)(i - 128)); &#125; &#125; //Short private static class ShortCache &#123; private ShortCache()&#123;&#125; static final Short cache[] = new Short[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Short((short)(i - 128)); &#125; &#125; 关于缓存的意义：在该范围内数据比较常用，添加缓存提高性能。不用每次都新建，浪费系统资源。 同时根据Integer的hashCode方法，我们可以看到，Integer的hashCode返回本身的int值。 1234@Overridepublic int hashCode() &#123; return Integer.hashCode(value);&#125; 结论以上就是Integer的源码分析，可以看到，对于偏底层的一些调用频繁的类，Java都做了很多方面的优化。包括从性能及内存开销等诸多方面。是值得我们学习和理解的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 数据结构之链表]]></title>
    <url>%2Fblog%2FJava-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[前言链表是一种物理存储单元上非连续、非顺序的存储结构，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储数据元素的数据域，另一个是存储下一个结点地址的指针域。 由于链表不必按照顺序存储，故在插入数据时可以达到O(1)的复杂度，但是查找的时候就需要遍历，时间复杂度为O(n)。 分类链表根据实现方式一般有三种分类：单向链表、循环链表、双向链表。 单向链表单向链表指的是链表中的元素的指向只能指向链表中的下一个元素或者为空，元素之间不能相互指向。也就是一种线性链表。 图示： 普通单向链表用Java代码实现一普通的单向链表。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class SingleLinkList &#123; private int size;//链表节点的个数 private Node head;//头节点 public SingleLinkList()&#123; size = 0; head = null; &#125; //链表的每个节点类 private class Node&#123; private Object data;//每个节点的数据 private Node next;//每个节点指向下一个节点的连接 public Node(Object data)&#123; this.data = data; &#125; &#125; //在链表头添加元素 public Object addHead(Object obj)&#123; Node newHead = new Node(obj); if(size == 0)&#123; head = newHead; &#125;else&#123; newHead.next = head; head = newHead; &#125; size++; return obj; &#125; //在链表头删除元素 public Object deleteHead()&#123; Object obj = head.data; head = head.next; size--; return obj; &#125; //查找指定元素，找到了返回节点Node，找不到返回null public Node find(Object obj)&#123; Node current = head; int tempSize = size; while(tempSize &gt; 0)&#123; if(obj.equals(current.data))&#123; return current; &#125;else&#123; current = current.next; &#125; tempSize--; &#125; return null; &#125; //删除指定的元素，删除成功返回true public boolean delete(Object value)&#123; if(size == 0)&#123; return false; &#125; Node current = head; Node previous = head; while(current.data != value)&#123; if(current.next == null)&#123; return false; &#125;else&#123; previous = current; current = current.next; &#125; &#125; //如果删除的节点是第一个节点 if(current == head)&#123; head = current.next; size--; &#125;else&#123;//删除的节点不是第一个节点 previous.next = current.next; size--; &#125; return true; &#125; //判断链表是否为空 public boolean isEmpty()&#123; return (size == 0); &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 栈具有先进后出的原则，所以单向链表可以用来实现栈。Java代码如下： 12345678910111213141516171819202122232425262728293031public class StackSingleLinkList &#123; public class StackSingleLink &#123; private SingleLinkList link; public StackSingleLink()&#123; link = new SingleLinkList(); &#125; //添加元素 public void push(Object obj)&#123; link.addHead(obj); &#125; //移除栈顶元素 public Object pop()&#123; Object obj = link.deleteHead(); return obj; &#125; //判断是否为空 public boolean isEmpty()&#123; return link.isEmpty(); &#125; //打印栈内元素信息 public String display()&#123; return link.display(); &#125; &#125;&#125; 我们可以看出，如果对链表的最后一个元素进行操作，需要遍历到链表尾部，在进行操作，十分消耗资源。 双端链表还有一种单向链表称为双端链表。这种链表有一个特点，即在链表内添加了对链表尾部的引用。这使得链表可以方便的操作尾部元素。 Java代码实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class DoublePointLinkList &#123; private Node head;//头节点 private Node tail;//尾节点 private int size;//节点的个数 private class Node&#123; private Object data; private Node next; public Node(Object data)&#123; this.data = data; &#125; &#125; public DoublePointLinkList()&#123; size = 0; head = null; tail = null; &#125; //链表头新增节点 public void addHead(Object data)&#123; Node node = new Node(data); if(size == 0)&#123;//如果链表为空，那么头节点和尾节点都是该新增节点 head = node; tail = node; size++; &#125;else&#123; node.next = head; head = node; size++; &#125; &#125; //链表尾新增节点 public void addTail(Object data)&#123; Node node = new Node(data); if(size == 0)&#123;//如果链表为空，那么头节点和尾节点都是该新增节点 head = node; tail = node; size++; &#125;else&#123; tail.next = node; tail = node; size++; &#125; &#125; //删除头部节点，成功返回true，失败返回false public boolean deleteHead()&#123; if(size == 0)&#123;//当前链表节点数为0 return false; &#125; if(head.next == null)&#123;//当前链表节点数为1 head = null; tail = null; &#125;else&#123; head = head.next; &#125; size--; return true; &#125; //判断是否为空 public boolean isEmpty()&#123; return (size ==0); &#125; //获得链表的节点个数 public int getSize()&#123; return size; &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 双端链表可以用来实现队列，相关实现如下： 123456789101112131415161718192021222324252627public class QueueLinkList &#123; private DoublePointLinkList dp; public QueueLinkList()&#123; dp = new DoublePointLinkList(); &#125; public void insert(Object data)&#123; dp.addTail(data); &#125; public void delete()&#123; dp.deleteHead(); &#125; public boolean isEmpty()&#123; return dp.isEmpty(); &#125; public int getSize()&#123; return dp.getSize(); &#125; public String display()&#123; return dp.display(); &#125;&#125; 有序链表上面所说的单链表数据都是无序的，我们可以构建一个有序的单向链表。即有序链表。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class OrderLinkList &#123; private Node head; private int size; private class Node&#123; private int data; private Node next; public Node(int data)&#123; this.data = data; &#125; &#125; public OrderLinkList()&#123; size=0; head = null; &#125; //插入节点，并按照从小打到的顺序排列 public void insert(int value)&#123; Node node = new Node(value); Node pre = null; Node current = head; while(current != null &amp;&amp; value &gt; current.data)&#123; pre = current; current = current.next; &#125; if(pre == null)&#123; head = node; head.next = current; &#125;else&#123; pre.next = node; node.next = current; &#125; size++; &#125; //删除头节点 public void deleteHead()&#123; head = head.next; size--; &#125; //判断是否为空 public boolean isEmpty()&#123; return (size ==0); &#125; //获取长度 public int getSize() &#123; return size; &#125; public String display()&#123; StringBuilder sb=new StringBuilder(); Node current = head; while(current != null)&#123; sb.append(current.data+" "); current = current.next; &#125; return sb.toString(); &#125;&#125; 对于有序链表，可以看出，插入或删除某一项最多需要O(n)的时间复杂度（遍历），但如果我们每次只删除最小值，且对插入没有过高要求的话，有序链表是一个不错的选择，比如优先级队列就可以利用有序链表实现。 比如我们插入int数并以最小值为优先级，每次取最小的int值的队列。 123456789101112131415161718192021222324252627public class QueueOrderLinkList &#123; private OrderLinkList dp; public QueueOrderLinkList()&#123; dp = new OrderLinkList(); &#125; public void insert(int data)&#123; dp.insert(data); &#125; public void delete()&#123; dp.deleteHead(); &#125; public int getSize() &#123; return dp.getSize(); &#125; public boolean isEmpty() &#123; return dp.isEmpty(); &#125; public String display()&#123; return dp.display(); &#125;&#125; 单向链表的用途可以说是十分广泛的。 双向链表双向链表即是这样一个有序的结点序列，每个链表元素既有指向下一个元素的指针，又有指向前一个元素的指针，其中每个结点都有两种指针，即left和right。left指针指向左边结点，right指针指向右边结点。所以双向链表是可以从两个方向进行遍历的。 图示： 双向链表的Java实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class DoubleWayLinkList &#123; private Node head;//表示链表头 private Node tail;//表示链表尾 private int size;//表示链表的节点个数 private class Node&#123; private Object data; private Node next; private Node prev; public Node(Object data)&#123; this.data = data; &#125; &#125; public DoubleWayLinkList()&#123; size = 0; head = null; tail = null; &#125; //在链表头增加节点 public void addHead(Object value)&#123; Node newNode = new Node(value); if(size == 0)&#123; head = newNode; tail = newNode; size++; &#125;else&#123; head.prev = newNode; newNode.next = head; head = newNode; size++; &#125; &#125; //在链表尾增加节点 public void addTail(Object value)&#123; Node newNode = new Node(value); if(size == 0)&#123; head = newNode; tail = newNode; size++; &#125;else&#123; newNode.prev = tail; tail.next = newNode; tail = newNode; size++; &#125; &#125; //删除链表头 public Node deleteHead()&#123; Node temp = head; if(size != 0)&#123; head = head.next; head.prev = null; size--; &#125; return temp; &#125; //删除链表尾 public Node deleteTail()&#123; Node temp = tail; if(size != 0)&#123; tail = tail.prev; tail.next = null; size--; &#125; return temp; &#125; //获得链表的节点个数 public int getSize()&#123; return size; &#125; //判断链表是否为空 public boolean isEmpty()&#123; return (size == 0); &#125; //显示节点信息 public String display()&#123; StringBuilder sb=new StringBuilder(); if(size &gt;0)&#123; Node node = head; int tempSize = size; if(tempSize == 1)&#123;//当前链表只有一个节点 sb.append("["+node.data+"]"); return sb.toString(); &#125; while(tempSize&gt;0)&#123; if(node.equals(head))&#123; sb.append("["+node.data+"-&gt;"); &#125;else if(node.next == null)&#123; sb.append(node.data+"]"); &#125;else&#123; sb.append(node.data+"-&gt;"); &#125; node = node.next; tempSize--; &#125; return sb.toString(); &#125;else&#123;//如果链表一个节点都没有，直接打印[] sb.append("[]"); return sb.toString(); &#125; &#125;&#125; 使用双向链表可以构建双端队列。在这儿就不上代码了，和之前的队列构造类似。 循环链表循环链表指的是在单向链表和双向链表的基础上，将两种链表的最后一个结点指向第一个结点从而实现循环。 图示： 循环链表的Java实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class CircleLinkList&lt;T&gt; &#123; // 链表的每个节点类 private class Node&lt;T&gt; &#123; private Object data;// 每个节点的数据 private Node&lt;T&gt; next;// 每个节点指向下一个节点的连接 public Node(Object data) &#123; this.data = data; &#125; &#125; Node&lt;T&gt; head, tail; Node&lt;T&gt; p; int size = 0; public CircleLinkList() &#123; this.head = null; tail = head; p = head; &#125; public int length() &#123; return size; &#125; /** * 添加节点 * * @param data */ public void add(T data) &#123; Node node = new Node&lt;T&gt;(data); if (head == null) &#123; head = node; tail = head; p = head; size++; &#125; else &#123; node.next = head; head = node; tail.next = head; p = head; size++; &#125; &#125; /** * 得到数据 * * @param index * @return */ public T get(int index) &#123; int i = 0; p = head; while (i != index &amp;&amp; p != tail) &#123; i++; p = p.next; &#125; return (T) p.data; &#125; /** * @return */ public boolean isEmpty() &#123; if (head != null) return false; else return true; &#125;&#125; 同样，使用循环链表可以实现循环队列。 总结链表作为数据结构的一部分，应用是十分广泛的，我们上面说明了几种链表在不同情况下的应用，链表是我们应当学会掌握和使用的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一款基于JQuery和bootstrap的单页面WebApp框架]]></title>
    <url>%2Fblog%2F%E4%B8%80%E6%AC%BE%E5%9F%BA%E4%BA%8EJQuery%E5%92%8Cbootstrap%E7%9A%84%E5%8D%95%E9%A1%B5%E9%9D%A2WebApp%E6%A1%86%E6%9E%B6.html</url>
    <content type="text"><![CDATA[前言现在React.js，Vue.js等MVVM框架逐渐火了起来，今天，先暂时不说这些。 我在之前呆过一家公司，他们前端封装了一套基于JQuery和bootstrap的单页面WebApp框架，虽然技术栈方面可能比较落后了，且是基于CSS，Html，JavaScript分层的。与现在的模块化开发有些不同。但是它的一些封装思想是值得我们学习的。 而且这个框架，让我这个前端小白（我是后端开发）对前端JS产生了很大兴趣，特此写下这篇文章来分析和纪念这个框架。 框架主页：https://github.com/JavaZWT/JQueryApp 简介package.json这款框架也是基于Node.js搭建的，主要是用了gulp，还包含了css,html,js压缩，js校验等包。其环境启动包依赖如下图： 在项目的package.json里可以看到。 关于为什么引用这些插件的说明： js，img，png等的压缩，可大幅度减小生成的资源包体积，在生成APK后用户当然是希望安装包要小越好。如果gulp到服务器WebRoot目录下，当用户访问项目资源时，体积小的静态资源更能减小用户流量开支，提高系统响应速度，增强用户体验。 创建一个前端webserver服务器，相当于提供了一个前端APK环境（可以这么理解），大部分bug在浏览器上就可以复现和解决，减少了打包到apk里的繁琐流程，提高开发效率。 根据以上介绍，大家应该知道node_modules,gulpfile.js,package.json,package-lock.json都不会被打到前台资源包中。 package.json里的依赖，都可以通过npm install （同级目录执行）的方式安装，安装后就会生成node_modules文件夹。 安装启动前端环境主要有以下三个步骤。 npm install -g gulp 安装全局gulp，使gulp可以在任何目录下使用，和java环境变量差不多。 npm install 安装环境依赖，需要到有package.json的文件夹下执行。 gulp 启动前端webserver gulpfile.js再来说下gulpfile.js 光有插件也不行啊，这个文件就是把插件们拿来用的。 关于这个js，我们简单说下配置及使用。 参数配置envConfig env：启动模式，有三种，DEV，DEBUG，PRO。DEV和DEBUG模式下不会压缩js和图片，节省时间，便于调试。 distPath: 生成的前端资源文件路径，有的gulpfile.js里面没有它，其他是分开的，那个可以生成发布在webapp目录下的资源文件。 webserverConfig 服务器启动配置参数 port ：默认打开的端口号 open：默认代开的文件名 host：默认打开的地址，不配置的话默认localhost livereload:即时刷新，主要用来检测文件变化。 项目架构图 架构说明： public 前端项目文件夹。 css 用于存放页面css样式的文件夹，理论上里面的文件都会被放入生成的前端资源包。 html 用于放置前端页面的文件夹，理论上里面的文件都会被放入生成的前端资源包。 js 用于放置前端js的文件夹，理论上里面的文件都会被放入生成的前端资源包。 node_modules npm install 后生成的文件夹，是前端资源包的打包环境及在Web浏览器上查看及debug App提供支持。 resources 用于放置前端静态资源的文件夹，如图片，字体，静态json等，理论上里面的文件都会被放入前端资源包。 gulpfile.js 构建前端环境，生成前端资源包的主要文件。 index.html 前端主要页面，所有资源理论上都会被按需加载进来。 package-lock.json npm install 之后生成的文件，存放着一些npm之后的包的信息。 package.json 构建前端环境，生成前端资源包的主要文件，里面放置着构建环境及生成资源包所需的依赖。 分析分析这个框架，我们先从index.html入手，单页面的操作实现主要是将目标html移到这个index.html里实现的。 可以看下index.html里面的几个主要id，如上图。 index.html引入了相关js及css。 我们看下api.js和app.js 项目启动后会调用到此方法，详见index.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function loadViewport()&#123; var path; if(location.hash.indexOf("?")&gt;-1)&#123; path = location.hash.substr(0,location.hash.indexOf("?")); &#125;else&#123; path = location.hash; &#125; path = (path || '#' + api.route.default.path).slice(1); //如果没有配置路径则不做操作 if(!api.route[path])&#123; api.globalLoading.hide(); console.log('[APP] 没有查询到路径:'+ path + ',不进行加载'); &#125;else if(api.curr.clean)&#123; switch(api.curr.clean.length)&#123; case 0://如果不包含参数 则认为这是一个同步 直接执行完成后 加载页面 api.curr.clean(); load(); break; case 1://如果只接收一个参数,则传入回调函数 等待页面完成后自行处理 api.curr.clean(load); break; case 2://如果接收两个参数 则分别是 要跳转页面的hash,回调函数 api.curr.clean(path,load); break; default://如果接收大于2个参数 则只传入三个 分别是 要跳转页面的hash,要跳转页面的配置在route中的数据,回调函数 api.curr.clean(path,api.route[path],load); &#125; &#125;else&#123; load(); &#125; function load()&#123; api.globalLoading.show(); console.log('[APP] 开始加载页面['+path+']'); api.loadPath(path,function(data)&#123; console.log('[APP] 加载页面&lt;p&gt;['+path+']&lt;/p&gt;完成'); api.plugins.header(data,path); //如果页面没有设置 则默认隐藏全局loading if(!data || !data.selfHideLoading) api.globalLoading.hide(); &#125;); &#125; &#125; api.loadViewport = loadViewport; /** * 当浏览器的hash发生变化时进行页面的加载 */ window.onhashchange = loadViewport; 其原理就是利用location.hash的？后半段，进行地址跳转，加载在route里配置的相关html及js，加载之前移除原来的html和js，以实现页面跳转。 方法及使用api.js1、route,modal 新增一个页面，可以添加在route里，新增一个模态框，可以添加在modal里。 2、api.globalLoading 全局loading，有两个方法: api.globalLoading.show(); api.globalLoading.hide(); 一般应用在ajax发送前后，当需要等待操作处理时，也可以直接调用。 3、api.globalShadow 全局shadow，是一个z-index为2000的透明遮罩层。 有两个方法： api.globalShadow.hide(); api.globalShadow.show(); 一般调用模态框时会被加载，自己写一些插件集成在里面时可以自己控制处理下。 4、api.modal.exec 调用模态框时会使用此方法，由于模态框和页面不太一样，模态框关闭后再打开可以保证数据不用重新加载。 5、api.loadJS 框架中加载js用到的方法，分为串行加载和并行加载，各有利弊。 6、api.loadModal modal加载方法，刚才提到的那个是这一个的进一步封装，这里面的方法显示了modal是如何被加载的，这里应注意，modal默认只会被加载一次。 7、api.loadPath 页面跳转实现的主要方法，这个方法制定跳转path后，会在route里寻找，若没有此页面，则不会跳转。 8、loadViewport 刚才提到了，这个可以控制页面hash路由跳转。也是页面跳转的主要方法。 app.js1、app.alert 消息提示框方法，只有一个确认键。 2、app.confirm 消息提示框，有确认和取消两个按钮。 3、app.ajax 基于JQuery ajax后自己封装的一套ajax，增加里请求头签名等等。 cordova.js自己封装的一个cordova工具，根据设备类型不同（Android，iOS）调用不同的cordova插件。 GlobalValue.js全局变量控制插件，通过sessionStorage存储变量。 总结还有很多部分没有介绍到，但框架的关键部分都介绍了，要想清楚明白，可以自己在框架上写个页面或者完善个插件来试试。加深对它的理解。 虽然在现在前端框架日新月异的今天，虚拟dom流行的今天，感觉这个框架已经过时了，但是思想永不过时，框架设计上的一些思想还是值得我们学习和借鉴的。 关于这个框架有什么问题欢迎留言，也可以直接在GitHub上提问题。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Stack类简介]]></title>
    <url>%2Fblog%2FJava-Stack%E7%B1%BB%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[前言栈（Stack）是限定只能在一段进行插入和删除操作的线性表。 进行插入和删除操作的一端称为“栈顶”（top），另一端称为“栈底”（bottom）。 栈的插入操作称为“入栈”(push)，栈的删除 操作称为“出栈”（pop）。 栈具有后进先出（LIFO），先进后出(FILO)的特性。 Stack类Java工具包下的Stack类继承于Vector，由此可见Stack底层是由数组实现的。 Stack和Collection的关系如下图： 我们来看下Stack的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package java.util;publicclass Stack&lt;E&gt; extends Vector&lt;E&gt; &#123; /** * 构造函数 */ public Stack() &#123; &#125; /** * 将一个元素压入栈顶 */ public E push(E item) &#123; addElement(item); return item; &#125; /** * 取出栈顶的一个元素，并删除 */ public synchronized E pop() &#123; E obj; int len = size(); obj = peek(); removeElementAt(len - 1); return obj; &#125; /** * 取出栈顶元素，不删除 */ public synchronized E peek() &#123; int len = size(); if (len == 0) throw new EmptyStackException(); return elementAt(len - 1); &#125; /** * 判断栈是不是空 */ public boolean empty() &#123; return size() == 0; &#125; /** * 查找元素在栈的位置，没有返回-1 */ public synchronized int search(Object o) &#123; int i = lastIndexOf(o); if (i &gt;= 0) &#123; return size() - i; &#125; return -1; &#125; private static final long serialVersionUID = 1224463164541339165L;&#125; 根据源码，可以发现Stack的方法调用了Vector类的方法，实现了线程安全。 我们主要看一下Vector里的下面三个方法： 12345678910111213141516171819202122232425262728293031323334//添加一个元素 public synchronized void addElement(E obj) &#123; modCount++; //确认容量，不够会扩容 ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = obj; &#125; //移除指定位置的元素 public synchronized void removeElementAt(int index) &#123; modCount++; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt;= " + elementCount); &#125; else if (index &lt; 0) &#123; throw new ArrayIndexOutOfBoundsException(index); &#125; int j = elementCount - index - 1; if (j &gt; 0) &#123; System.arraycopy(elementData, index + 1, elementData, index, j); &#125; elementCount--; //赋空，便于垃圾回收 elementData[elementCount] = null; &#125; //找出在指定位置的元素 public synchronized E elementAt(int index) &#123; if (index &gt;= elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + " &gt;= " + elementCount); &#125; return elementData(index); &#125; 关联方法如下： 1234567891011121314151617181920212223242526272829// private void ensureCapacityHelper(int minCapacity) &#123; 如果长度超了就扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); &#125; //扩容方法 private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; //新的容量定义：如果有容量增量且大于0，取增量，否则执行2倍扩容 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); //扩容后容量比传入容量还小，就取传入容量。 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //新容量比数组最大长度还大 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) //计算容量 newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); &#125; //计算容量 private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; 实践我们如何用数组实现自己的一个stack呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Stack &#123; //栈元素组 private Object[] members; //指针 private int size; //自定义初始栈长度 public Stack(int initCapacity) throws Exception&#123; if(initCapacity&lt;=0) &#123; throw new Exception(); &#125; this.members=new Object[initCapacity]; &#125; //默认栈长度为10 public Stack() &#123; this.members=new Object[10]; &#125; //元素入栈 public synchronized void push(Object o)&#123; ensureCapacity(size+1); members[size++]=o; &#125; //元素出栈 public synchronized Object pop() throws Exception&#123; if(size&lt;=0) &#123; throw new Exception(); &#125; return members[--size]; &#125; //查看栈顶元素 public synchronized Object peek() throws Exception&#123; if(size&lt;=0) &#123; throw new Exception(); &#125; return members[size-1]; &#125; //确认容量 private synchronized void ensureCapacity(int minCapacity) &#123; //size+1比数组长度要长，扩容 if(minCapacity-members.length&gt;0) &#123; int oldCapacity = members.length; Object oldMembers=members; //扩容到二倍 int newCapacity = 2 * oldCapacity ; //扩容后还不够或者超过int最大值，就直接赋值size+1 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; members=new Object[newCapacity]; //拷贝数组 System.arraycopy(oldMembers, 0, members, 0, size); oldMembers=null; &#125; &#125;&#125; 以上代码就是一个简易的Stack的实现方式。 代码见： https://github.com/JavaZWT/sakuratears 总结Stack类在编程过程中用到的不是很多，但是计算机栈内存机制遵循先进后出原则，学习Stack类，可以帮助我们加深对程序及数据结构的理解。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java正则表达式使用]]></title>
    <url>%2Fblog%2FJava%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言1.什么是正则表达式？ 正则表达式是一种可以用于模式匹配和替换的强有力的工具。 2.为什么要使用正则表达式？有什么优点缺点？ 正则表达式在参数校验、参数匹配方面可以发挥重大作用。 优点：更少的代码，更高的效率。 缺点：需要一定时间的学习及记忆，不然理解晦涩。 正则表达式符号及其含义下表是正则表达式的所有符号及含义。 常用的符号.表示任意字符\s空格字符(空格键, tab, 换行, 换页, 回车)\S非空格字符([^\s])\d 一个数字，(相当于[0-9] )\D一个非数字的字符，(相当于[^0-9] )\w一个单词字符(word character) (相当于 [a-zA-Z_0-9] )\W一个非单词的字符，[^\w]^一行的开始$一行的结尾\b一个单词的边界 \B一个非单词的边界\G前一个匹配的结束 []匹配方括号内的一个字符例如:[abc] 表示字符 a，b，c 中的任意一个(与 a|b|c 相同) [a-zA-Z]表示从a到z或A到Z当中的任意一个字符表示次数的符号*重复零次或更多次例如:a* 匹配零个或者多个 a+重复一次或更多次例如:a+ 匹配一个或者多个 a?重复零次或一次例如:a? 匹配零个或一个 a{n}重复n次例如:a{4}匹配4个a {n,}重复 n 次或更多次例如:a{4,} 匹配至少 4 个 a{n,m}重复n到m次例如:a{4,10} 匹配 4~10 个 a 正则表达式实例java 正则表达式通过 java.util.regex 包下的 Pattern 类与 Matcher 类实现: 正则表达式使用总共分三步: 构造一个模式. Pattern p=Pattern.compile(“[a-z]*”); 建造一个匹配器 Matcher m = p.matcher(str); 进行判断，得到结果 boolean b = m.matches(); Matcher 类提供三个匹配操作方法,三个方法均返回 boolean 类型,当匹配到 时返回 true,没匹配到则返回 false m.matches() matches()对整个字符串进行匹配,只有整个字符串都匹配了才返回 true m.lookingAt() lookingAt()对前面的字符串进行匹配,只有匹配到的字符串在最前面才返 回 true m.find() find()对字符串进行匹配,匹配到的字符串可以在任何位置 Matcher 类的其他方法: int groupcount() 返回此匹配器模式中的捕获组数。 String replaceAll(String replacement) 用给定的 replacement 全部替代匹配的 部分 String repalceFirst(String replacement) 用给定的 replacement 替代第一次匹 配的部分 appendReplacement(StringBuffer sb,String replacement) 根 据 模 式 用 replacement 替换相应内容,并将匹配的结果添加到 sb 当前位置之后 StringBuffer appendTail(StringBuffer sb) 将输入序列中匹配之后的末尾字串 添加到 sb 当前位置之后. group(n) 0 代表永远都是匹配整个表达式的字符串的那部分 n&lt;&gt;0 时代表 第 n 组匹配的部分 正则表达式的用法 字符匹配 1234Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(str); // 操作的字符串boolean b = m.matches(); //返回是否匹配的结果 System.out.println(b); 分割字符串 12Pattern pattern = Pattern.compile(expression); //正则表达式String[] strs = pattern.split(str); //操作字符串 得到返回的字符串数组 替换字符串 123Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串String s = m.replaceAll(str); //替换后的字符串 查找替换指定字符串 12345678910Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串 StringBuffer sb = new StringBuffer();int i = 0;while (m.find()) &#123; m.appendReplacement(sb, str); i++; //字符串出现次数&#125;m.appendTail(sb);//从截取点将后面的字符串接上 String s = sb.toString(); 查找输出字符串 1234567Pattern p = Pattern.compile(expression); // 正则表达式 Matcher m = p.matcher(text); // 操作的字符串while (m.find()) &#123; //m.start() 返回匹配到的子字符串在字符串中的索引位置. //m.end()返回匹配到的子字符串的最后一个字符在字符串中的索引位置. //m.group()返回匹配到的子字符串&#125; 常用的正则表达式 手机号码校验 12345 public static boolean isPhone(String str) &#123; Pattern p = Pattern.compile("^[1][3,5,7,8]+\\d&#123;9&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 邮箱地址校验 12345 public static boolean isEmail(String str) &#123; Pattern p = Pattern.compile("^[a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\\.[a-zA-Z0-9-]+)*\\.[a-zA-Z0-9]&#123;2,6&#125;$"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; IP地址校验 12345 public static boolean isIPv4(String str) &#123; Pattern p = Pattern.compile("\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;+\\.\\d&#123;1,3&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 是不是汉字 12345 public static boolean isChinese(String str) &#123; Pattern p = Pattern.compile("^[\\u4e00-\\u9fa5]+$"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; QQ号校验 12345 public static boolean isQQ(String str)&#123; Pattern p = Pattern.compile("[1-9][0-9]&#123;4,13&#125;"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 身份证号校验 1234567891011public static boolean isIDNumber(String IDNumber) &#123; if (IDNumber == null || "".equals(IDNumber)) &#123; return false; &#125; // 定义判别用户身份证号的正则表达式（15位或者18位，最后一位可以为字母） String regularExpression = "(^[1-9]\\d&#123;5&#125;(18|19|20)\\d&#123;2&#125;((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d&#123;3&#125;[0-9Xx]$)|" + "(^[1-9]\\d&#123;5&#125;\\d&#123;2&#125;((0[1-9])|(10|11|12))(([0-2][1-9])|10|20|30|31)\\d&#123;3&#125;$)"; boolean matches = IDNumber.matches(regularExpression); //详细校验略 return matches;&#125; 邮编校验 12345 public static boolean isMail(String str) &#123; Pattern p = Pattern.compile("[1-9]\\d&#123;5&#125;(?!\\d)"); //正则表达式 Matcher m = p.matcher(str); //操作的字符串 return m.matches();&#125; 其他 其他常用校验不在一一说明。 可以通过 https://github.com/JavaZWT/sakuratears 获取。 结语如果我们能够熟练掌握正则表达式，一定会对我们的开发提供很大的帮助。 正则表达式整体上相对于代码来说是比较晦涩的，用好它需要一定的功力及学习能力。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（2）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2%EF%BC%882%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言关于之前的部分请参考我的《搭建自己的Hexo博客（1）》这篇文章。 创建Github账户我们把博客发布到Github上。 首先申请Github账号：https://github.com/ 新建一个Repository 红框内为项目名称，应写成 yourname.github.io 这种形式。 如我注册的用户名是JavaZWT，则项目名应写为 JavaZWT.github.io 新建好后，我们开始发布我们的博客。 博客发布进入到原来的blog文件夹。 我们要记住以下指令： 博客本地调试启动：hexo s 博客部署文件生成：hexo g 博客发布：hexo d 博客部署文件夹清空： hexo clean hexo g 后，要发布的博客文件会生成在public文件夹下。 hexo d 会把这个文件夹下的文件deploy到Github上。 _config.yml配置文件里，指明自己的Github地址上新建的yourname.github.io的项目。 12345# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:JavaZWT/JavaZWT.github.io.git 正常的发布流程： 新写了一篇博文 通过hexo s本地调试通过后 hexo clean 清空public文件夹及db文件 hexo g 重新生成部署文件 hexo d 发布到Github 当我们发布成功后，通过 https://yourname.github.io/ 便可以在外网访问自己的blog啦。 添加域名成功访问之后，我们觉得这个网址不好，想换个自己喜欢的域名怎么办？ 这里我使用了阿里云申请了域名。 首先登陆阿里云，没有账号请注册申请。 找到域名与网站（万网）菜单。 找到自己喜欢的域名进行购买。 得到域名后，我们需要配置域名解析。把域名指向我们的博客。 在域名解析列表里我们新增两条记录，如下图红框部分。 其中记录值可以写 https://yourname.github.io/ 相当于把这个域名指向了https://yourname.github.io/这个域名。 或者我们可以查到 https://yourname.github.io/ 这个域名的IP，直接将新的域名指向这个IP即可。 配置好后等一段时间，我们访问新的域名，就可以跳转到我们的博客啦。 其他优化添加音乐播放功能找到 /themes/next/layout/_custom/sidebar.swig 文件 登陆网易云音乐，对于一首歌，生成外链播放器。 将上述代码复制到sidebar.swig文件里。 生成如下图所示效果。 添加博客编写功能写博客时是不是很不方便？ 我们安装hexo-admin插件吧。这个插件可以是写博客更快捷方便。 npm install hexo-admin –save 安装好后，启动调试博客 hexo s 输入 http://localhost:4000/admin/ 是不是看到进入了博客管理菜单？ 通过这个插件可以方便的管理和书写博客。 压缩博客博客里有一些图片或者css，js过大怎么办，这样会导致博客加载缓慢。 我们在发布博客时可以先对其进行压缩。 在blog目录下新建gulpfile.js 有如下内容： 123456789101112131415161718192021222324252627282930313233343536373839404142var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');var imagemin = require('gulp-imagemin');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public/images 目录 imagesgulp.task('images', function() &#123; return gulp.src(['./public/**/*.png', './public/**/*.jpg', './public/**/*.gif', './public/**/*.svg']) .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html', 'minify-css', 'minify-js', 'images']); 同时在package.json里添加这几个包。 &quot;gulp-minify-css&quot;: &quot;^1.2.4&quot;, &quot;gulp-uglify&quot;: &quot;^3.0.0&quot;, &quot;gulp-htmlmin&quot;: &quot;^4.0.0&quot;, &quot;gulp-htmlclean&quot;: &quot;^2.7.22&quot;, &quot;gulp&quot;: &quot;^3.9.1&quot;, &quot;gulp-imagemin&quot;: &quot;^4.1.0&quot;, 添加完后执行 npm install。 或者直接一个个的 npm install 包名 –save 也是可以的。 弄好后，在hexo g 指令后进行 压缩操作，执行 gulp 命令，可对public文件夹下的资源进行压缩。 结语一个博客的基本搭建到这里基本就结束了，如果想提高性能及速度，可以自己申请个服务器，或者使用CDN进行加速，基本都涉及到一定的费用。 博客还有一些比较小的美化的地方就不一一说明，大家可以自己尝试，自己动手，把自己的博客搭建的更漂亮。 毕竟，搭建博客这件事我们应本着学习，兴趣，快乐的本质去出发，自己摸索自己研究一些比较好玩的东西，这样才是做这件事的意义。 附上我的博客地址： https://www.sakuratears.top/ 大家可以参考下，有什么问题可以随时联系我，一定会为大家解答。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建自己的Hexo博客（1）]]></title>
    <url>%2Fblog%2F%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[前言我的Hexo博客很早就建好了，但一直没时间把搭建博客的步骤和经验好好总结。今天，我把在搭建博客中注意到的点及经验总结下来。一方面与大家分享，另一方面也加深自己的记忆，总结经验。 环境安装Node.js及npm要搭建Hexo博客，首先需要安装Node.js环境，去Node.js官网下载Node并安装。 NodeJs官网：https://nodejs.org/ 在命令行界面输入node -v 查询版本号。 输入npm -v 可查询npm版本号。 安装Hexo使用npm指令安装hexo npm install hexo-cli -g 安装成功后输入hexo -v可查询hexo版本信息 博客初始化安装好hexo后，我们找到一个自己文件夹，通过指令进入该文件夹。 使用指令： ①hexo init blog ②cd blog ③npm install ④hexo s 会看到命令行输出如下信息 打开浏览器，输入地址便可以看到blog搭建成功了。 美化博客虽然blog搭建好了，但是总感觉不是很好看。 推荐一款hexo主题Next，其有很多种blog主题供我们选择。 安装Next主题首先cd到刚刚建好的blog目录下，执行命令： git clone https://github.com/iissnan/hexo-theme-next themes/next 如果没有安装git，那么安装下吧。 请去git官网下载git： https://git-scm.com/downloads 如果命令执行成功，在blog下的themes包下，我们可以看到next文件夹，那就是刚刚下好的next主题啦。themes包下那个landscape包，就是hexo自带的默认主题。 如果不成功，那么我们可以直接下载next主题，https://github.com/iissnan/hexo-theme-next/releases，下载source code，然后把文件解压，重命名为next放到themes包下就可以了啦。 完成后，在blog包下，有个_config.yml文件，打开它，里面配置为theme: next即可加载next主题。 Hexo的_config.yml文件上文提到的这个配置文件是Hexo的核心配置文件，我们来说下这个文件吧 请先对yml语法有一定了解，本文不做说明。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: SakuraTears的博客 //博客的标题subtitle: 越努力越幸运 //博客简介description: 越努力越幸运 //博客描述keywords: 'Java,努力,随笔,学习,SakuraTears,博客,生活' //博客关键字author: SakuraTears //博客作者language: zh-Hans //博客语言timezone: //时区avatar: /uploads/avatar.jpg //头像#google_analytics: your-analytics-id //谷歌统计，开启的话请输入自己的id#baidu_analytics: your-analytics-id //百度统计，开启的话请输入自己的id# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://www.sakuratears.top //自己的网站地址root: / //根目录permalink: blog/:title:year:month:day/ permalink_defaults:# Directorysource_dir: source //源码位置，请默认public_dir: public //生成的文件位置，请默认tag_dir: tags //标签文件位置，请默认archive_dir: archives //归档文件位置，请默认category_dir: categories //分类文件位置，请默认code_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new posts //新文章默认名字default_layout: post //默认样式titlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: //高亮 enable: true //启用 line_number: true //linenumber启用 auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 3 //每页三篇文章 order_by: -date //按日期排序 # Category &amp; Tagdefault_category: uncategorized //按默认策略分类category_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD //日期时间格式time_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10 //归档每页10篇文章pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/plugins: hexo-generate-feed## Themes: https://hexo.io/themes/theme: next //hexo主题# hexo-admin authentificationadmin: username: sakuratears password_hash: $2a$10$HhGu7w5fYH6e7wmb2tXta.FvJvGWlzosdUgImJxGhxU0K3YAkXIkm secret: what's your name?# Live2D //live2D插件，后面介绍## https://github.com/xiazeyu/live2d-widget.js## https://l2dwidget.js.org/docs/class/src/index.js~L2Dwidget.html#instance-method-initlive2d: model: scale: 1 hHeadPos: 0.5 vHeadPos: 0.618 display: superSample: 2 width: 150 height: 300 position: right hOffset: 0 vOffset: -20 mobile: show: false scale: 0.1 motion: true react: opacityDefault: 0.7 opacityOnHover: 0.2##local_search //搜索插件search: path: search.xml field: post format: html limit: 10000# Deployment //部署## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:JavaZWT/JavaZWT.github.io.git Next主题的_config.yml文件在next包下，也有一个_config.yml文件，这个文件是配置next主题的，也简单介绍下。 12345678910menu: //next主题菜单 home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap commonweal: /404/ || heartbeat support: /support/ || support 1scheme: Pisces //主题类型 我选的Pisces主题。 12345social: //社交信息 GitHub: https://github.com/JavaZWT || github Email: mailto:971258230@qq.com || envelope QQ: http://wpa.qq.com/msgrd?v=3&amp;uin=971258230&amp;site=qq&amp;menu=yes || qq CSDN: https://blog.csdn.net/JavaZWT || copyright 上面都是一些基本配置，下面我们对blog进行更多的美化。 添加动态旋转头像打开/themes/next/source/css/_common/components/sidebar/sidebar-author.styl文件 将代码替换为： 1234567891011121314151617181920212223242526272829.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; border-radius: 60%; transition: 2.5s all; &#125;.site-author-image:hover &#123; transform: rotate(360deg);&#125;.site-author-name &#123; margin: $site-author-name-margin; text-align: $site-author-name-align; color: $site-author-name-color; font-weight: $site-author-name-weight;&#125;.site-description &#123; margin-top: $site-description-margin-top; text-align: $site-description-align; font-size: $site-description-font-size; color: $site-description-color;&#125; 后重新发布，可以看到头像可以旋转啦。 添加本地搜索插件安装hexo-generator-searchdb： npm install hexo-generator-searchdb –save 同时在hexo的_config.yml里配置如下参数： 123456##local_searchsearch: path: search.xml field: post format: html limit: 10000 Next的_config.yml配置如下： 1234567local_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 重新发布，可以看到新增了搜索功能。 更换背景图片找到 /themes/next/source/css/_custom/custom.styl文件 加入如下代码： 123456789body &#123; background:url("/images/background.jpg"); background-attachment: fixed; // 不随屏幕滚动而滚动 background-repeat: no-repeat; // 如果背景图不够屏幕大小则重复铺，改为no-repeat则表示不重复铺 background-size: 100%; // 等比例铺满屏幕&#125;.main-inner &#123; opacity: 0.8;&#125; 重新发布，可以看到背景图片变化了。 添加live2D功能安装live2D插件hexo-helper-live2d npm install hexo-helper-live2d –save Hexo的 _config.yml文件里配置如下参数 12345678910111213141516171819live2d: model: scale: 1 hHeadPos: 0.5 vHeadPos: 0.618 display: superSample: 2 width: 150 height: 300 position: right hOffset: 0 vOffset: -20 mobile: show: false scale: 0.1 motion: true react: opacityDefault: 0.7 opacityOnHover: 0.2 重新发布，便看到萌萌的二次元人物出现啦。 添加评论插件这里用的Valine评论插件，Valine插件需要leancloud支持，请先注册leancloud，注册成功拿到id和key。 12345678910valine: enable: true appid: //你的id appkey: //你的key notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: (*^_^*)看了这么多，可能你有话想说 # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 配置成功后，重新发布便能看到文章可以评论啦。 开启访客统计当我们注册好leancloud后。 1234leancloud_visitors: enable: true app_id: //你的id app_key: //你的key 找到代码，填入自己的id和key。 重新发布，在页面上可以看到博客的访客数量。 开启字数统计安装字数统计插件：hexo-wordcount npm install hexo-wordcount –save Next配置文件如下参数开启： 123456post_wordcount: item_text: true wordcount: true min2read: true totalcount: true separated_meta: true 重新发布便拥有了字数统计功能。 续文章还没写完，后面会继续更新。 欢迎大家关注我的博客 https://www.sakuratears.top/]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java通过反射获取class信息]]></title>
    <url>%2Fblog%2FJava%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E8%8E%B7%E5%8F%96class%E4%BF%A1%E6%81%AF.html</url>
    <content type="text"><![CDATA[前言在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为反射机制。 我们知道，Java语言不是动态语言，但是他却有非常突出的动态相关机制，反射机制。 代码我们可以通过反射机制获取一个class的相关信息。 1. 利用Javassist获取class信息 Javassist是一个动态类库，可以用来检查、”动态”修改以及创建 Java类。其功能与jdk自带的反射功能类似，但比反射功能更强大。 123456789101112131415161718192021222324252627282930313233343536373839404142public static void getClassInfoByJavassist(Class clazz) &#123; ClassPool classPool = ClassPool.getDefault(); classPool.insertClassPath(new ClassClassPath(clazz)); try &#123; System.out.println("class--&gt;"+clazz.getName()); CtClass ctClass = classPool.get(clazz.getName()); //获取常量信息 CtField[] fields=ctClass.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].getConstantValue()+"--&gt;type="+fields[i].getType().getName()); &#125; //获取方法信息 CtMethod[] ctMethods = ctClass.getMethods(); for (CtMethod ctMethod : ctMethods) &#123; //排除equals，hash，toString等方法 if (!clazz.getName().equals(ctMethod.getDeclaringClass().getName())) &#123; continue; &#125; MethodInfo methodInfo = ctMethod.getMethodInfo(); CodeAttribute codeAttribute = methodInfo.getCodeAttribute(); LocalVariableAttribute attr = (LocalVariableAttribute) codeAttribute.getAttribute(LocalVariableAttribute.tag); if (attr == null) &#123; // exception &#125; String[] paramNames = new String[ctMethod.getParameterTypes().length]; TreeMap&lt;Integer, String&gt; sortMap = new TreeMap&lt;Integer, String&gt;(); for (int i = 0; i &lt; attr.tableLength(); i++) sortMap.put(attr.index(i), attr.variableName(i)); int pos = Modifier.isStatic(ctMethod.getModifiers()) ? 0 : 1; paramNames = Arrays.copyOfRange(sortMap.values().toArray(new String[0]), pos, paramNames.length + pos); CtClass[] types=ctMethod.getParameterTypes(); for(int i=0;i&lt;paramNames.length;i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+ctMethod.getName()+"--&gt;isStatic="+Modifier.isStatic(ctMethod.getModifiers())+"--&gt;paramsType="+types[i].getName()+"--&gt;paramsName="+paramNames[i]); &#125; &#125; &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 我们新建一个Demo.class 1234567891011121314151617181920 public class Demo &#123; public static String getName(String str) &#123; String s="123"; return str+s; &#125; public Integer doSomething(String str,double a,Map&lt;String, String&gt; map,List&lt;String&gt; list) &#123; //doSomething Integer i=0; return i; &#125; private static final int NUM=1; private static String s="1234"; static &#123; s="5678"; &#125;&#125; 调用方法 getClassInfoByJavassist(Demo.class)，输出class信息。 12345678class--&gt;com.zwt.reflect.Demoobject=NUM--&gt;value=1--&gt;type=intobject=s--&gt;value=null--&gt;type=java.lang.Stringclass=Demo--&gt;method=getName--&gt;isStatic=true--&gt;paramsType=java.lang.String--&gt;paramsName=strclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.lang.String--&gt;paramsName=strclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=double--&gt;paramsName=aclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.util.Map--&gt;paramsName=mapclass=Demo--&gt;method=doSomething--&gt;isStatic=false--&gt;paramsType=java.util.List--&gt;paramsName=list 2. 通过spring里的LocalVariableTableParameterNameDiscoverer获取paramsName，使用jdk自带reflect反射类获取class其他信息 在jdk1.8以下java版本中，根据jdk自带reflect包，可以拿到大部分class信息，唯一拿不到的是参数name，我们可以借助spring包里的LocalVariableTableParameterNameDiscoverer去获取paramsName。 1234567891011121314151617181920212223public static void getClassInfoBySpringAndReflect(Class clazz) &#123; try &#123; LocalVariableTableParameterNameDiscoverer u = new LocalVariableTableParameterNameDiscoverer(); Method[] methods =clazz.getDeclaredMethods(); Field[] fields=clazz.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; //设置成可以access的，否则get(clazz)报错，无法读取private属性 fields[i].setAccessible(true); System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].get(clazz)+"--&gt;type="+fields[i].getType().getName()); &#125; for(Method method:methods) &#123; //使用spring LocalVariableTableParameterNameDiscoverer 获取paramsName String[] params = u.getParameterNames(method); Class&lt;?&gt; [] classType=method.getParameterTypes(); for (int i = 0; i &lt; params.length; i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+method.getName()+"--&gt;isStatic="+Modifier.isStatic(method.getModifiers())+"--&gt;paramsType="+classType[i].getName()+"--&gt;paramsName="+params[i]); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 调用getClassInfoBySpringAndReflect(Demo.class)，也可以拿到class信息。 3.使用jdk1.8及以上java版本获取class信息 若jdk版本较高，完全可以不用借助第三方jar包而获取class信息。 jdk1.8以上，添加了Parameter类，可以获取参数paramsName. 123456789101112131415161718192021public static void getClassInfoByJava8AndReflect(Class clazz)&#123; try &#123; Method[] methods = clazz.getDeclaredMethods(); Field[] fields=clazz.getDeclaredFields(); for(int i=0;i&lt;fields.length;i++) &#123; //设置成可以access的，否则get(clazz)报错，无法读取private属性 fields[i].setAccessible(true); System.out.println("object="+fields[i].getName()+"--&gt;value="+fields[i].get(clazz)+"--&gt;type="+fields[i].getType().getName()); &#125; for (Method method : methods) &#123; //使用jdk1.8Parameter获取paramsNames Parameter[] params = method.getParameters(); Class&lt;?&gt; [] classType=method.getParameterTypes(); for (int i = 0; i &lt; params.length; i++) &#123; System.out.println("class="+clazz.getSimpleName()+"--&gt;method="+method.getName()+"--&gt;isStatic="+Modifier.isStatic(method.getModifiers())+"--&gt;paramsType="+classType[i].getName()+"--&gt;paramsName="+params[i]); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 调用getClassInfoByJava8AndReflect(Demo.class)，获取class信息。 其他 反射优点： 合理的使用反射机制可以有效降低代码冗余及代码量。并且可以让应用程序实现一些几乎不可能做到的事情。反射属于Java语言里比较高级的一个特性。 反射缺点： 如果不合理的使用反射，可能降低系统性能。 而且非常重要的一点，我们看如上代码，有一句fields[i].setAccessible(true); 这是在设置私有属性可以访问，显然，这破坏了代码的抽象性，而且可能导致安全问题的产生。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据传输安全性保证]]></title>
    <url>%2Fblog%2F%E6%B5%85%E8%B0%88HTTP%E4%B8%8EHTTPS.html</url>
    <content type="text"><![CDATA[前言数据传输过程中，安全性十分重要，尤其是对于一些敏感的用户信息，其安全性保证更应当被重视。 现代软件开发中，无论是基于敏捷式的软件开发，或者基于前后端分离的软件开发，都离不开提供对外接口，而请求这些接口数据的安全性应当得到保证。 方法保证传输数据的安全性，主要有以下方法： 数据加密 数据加密一直是保密数据的重要部分，常见的加密算法有可逆加密算法和不可逆加密算法，可逆加密算法又分为对称加密算法和非对称加密算法。 比如一个系统的登陆操作，客户输入用户名登陆，如果不进行任何保障措施，用户名和密码明文传输，被不法分子截获数据后，显然是不安全的。如果我们这时对密码进行不可逆加密，如md5，对用户名进行可逆加密，如des，这时候在截获数据时，得到的将是一串密文，显然，即使要破解，也需要相当时间。 但这样，有一个明显问题，就是接口吞吐量下降，明显，加密情况下，由于需要解密数据，接口的响应速度会下降。 可能，对于一些非重要数据，我们这样牺牲系统性能换取来的安全可能有些过了。 数据签名 数据签名又是什么呢？它和数据加密的区别呢？ 数据签名，相当于对传输的数据，进行一些不可逆加密算法后，如md5，生成一段签名字符串sign。 比如上述列子中，登陆操作中如果还要传输IP，地点等等数据，这些数据明显没那么重要，这时可以对全部传输数据进行签名，生成sign，将其传入后端，后端用同样算法及密钥计算比较sign，如果一致认为数据正确，直接拿到IP，地点等数据（不用解密，相对于解密各个信息，理论上所有信息计算签名要节省时间），不一致则认为被修改过，返回错误信息。 session，token机制 session（cookie）和token机制的出现是为了校验用户状态的。 比如不法分子知道了我们的后台接口，恶意伪造大量数据攻击，即使这些数据不正确，而服务器每次都需要校验这些数据的正确性，显然带来大量性能消耗。 我们当然可以进行一些优化操作，如对于同一个IP，短时间大量请求则封掉该IP一段时间，但这不是太合理的。 设想，如果用户登陆后，保存状态，只有登陆的用户可以访问这些接口，每次请求到来，均先校验用户登陆状态，对于session，如果没有sessionid或者sessionid错误或者过期则直接返回登陆界面。对于token，与session同理，没有token或者token错误或者过期的直接返回登陆页面。 这样，我们开始校验token或者session，就可以拒绝大量伪造请求。 Https（数字证书机制） 上面，无论数据加密还是签名，我们发现最重要的就是加密方法和加密密钥。 对于两台服务器交互，可能不用太担心，但是如果是webapp或者原生app，不法分子反编译前端代码后，就有可能拿到加密方法和加密key，怎么办呢？ 这就属于Https要解决的事情，下篇文章会介绍https，这儿先简单说下： 在加密算法中，有一种叫做非对称加密的算法，有公钥和私钥组成，他有个特点：公钥加密的数据，只有私钥能解密；私钥加密的数据，只有公钥能解密。 https就是需要让客户端与服务器端安全地协商出一个对称加密算法。剩下的就是通信时双方使用这个对称加密算法进行加密解密。 ①客户端启动，发送请求到服务端，服务端通过非对称加密算法（如RSA）生成公钥pubkey1和私钥prikey1。 ②服务端将公钥pubkey1发给客户端，客户端用自己的非对称加密算法也生成一套公钥pubkey2和私钥prikey2，并将公钥pubkey2通过pubkey1加密后返回服务端。 ③服务端用私钥prikey1解密后拿到pubkey2，并将确定好的未来交互的对称加密算法和密钥通过pubkey2加密，返回客户端。 ④客户端用私钥pubkey2解密数据，拿到服务器给定的加密算法和密钥，双方开始用其数据通信。 这样仍有一个问题，如何证明公钥pubkey1加密的这串数字是客户端来的，即证明他就是他。。。 这就是https的数字证书，相当于网络中心的部分，证明他就是他。数字证书就是来干这个的。 其他 安全性保证还有其他可以自己操作的方法。 如对于两台稳定的服务器交互，直接进行IP校验或许比token，session机制更好更方便。及一些其他的操作，如同一IP短时间大量错误报文，可以将其暂时拉入黑名单。等等。 其他数据安全一直是数据交互的永恒话题，保证数据安全，肯定是需要消耗一定的系统性能的。如何选择合适的安全保证机制，是每一个软件开发工程师应该了解和掌握的。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UTF-8和GBK等编码格式转换问题]]></title>
    <url>%2Fblog%2FUTF-8%E5%92%8CGBK%E7%AD%89%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言在开发过程中，我们可能会遇到Java各种编码格式之间的转换问题。下面我们来研究下UTF-8和GBK等编码格式之间的相互转化。 实践在进行编码转换时，我们用ISO-8859-1编码来接受和保存数据，并转换为相应编码。 为什么采用ISO-8859-1编码作为中间转存方案呢？ 下面我们通过程序验证： 通过ISO-8859-1转存： 123456789101112131415161718192021public static void test(String str1,String encode) throws UnsupportedEncodingException &#123; System.out.println("字符串："+str1); //将str转为原编码字节流 byte[] byteArray1=str1.getBytes(encode); System.out.println(byteArray1.length); //转换为一个ISO-8859-1的字符串 String str2=new String(byteArray1,"ISO-8859-1"); System.out.println("转成ISO-8859-1："+str2); //转回为byte数组 byte[] byteArray2=str2.getBytes("ISO-8859-1"); System.out.println(byteArray2.length); //重新用目标编码格式编码 String str3=new String(byteArray2,encode); System.out.println("字符串："+str3); &#125; public static void main(String[] args) throws UnsupportedEncodingException &#123; String str1="你好"; String str2="你好呀"; test(str1,"UTF-8"); test(str2,"UTF-8"); &#125; 运行结果： 字符串：你好 6 转成ISO-8859-1：ä½ å¥½ 6 字符串：你好 字符串：你好呀 9 转成ISO-8859-1：ä½ å¥½å 9 字符串：你好呀 通过GBK转存： 123456789101112131415161718192021 public static void test(String str1,String encode) throws UnsupportedEncodingException &#123; System.out.println("字符串："+str1); //将str转为原编码字节流 byte[] byteArray1=str1.getBytes(encode); System.out.println(byteArray1.length); //转换为一个GBK的字符串 String str2=new String(byteArray1,"GBK"); System.out.println("转成GBK："+str2); //转回为byte数组 byte[] byteArray2=str2.getBytes("GBK"); System.out.println(byteArray2.length); //重新用目标编码格式编码 String str3=new String(byteArray2,encode); System.out.println("字符串："+str3); &#125; public static void main(String[] args) throws UnsupportedEncodingException &#123; String str1="你好"; String str2="你好呀"; test(str1,"UTF-8"); test(str2,"UTF-8");&#125; 运行结果： 字符串：你好 6 转成GBK：浣犲ソ 6 字符串：你好 字符串：你好呀 9 转成GBK：浣犲ソ鍛� 9 字符串：你好�? 可以看到，当用GBK暂存UTF-8编码字符串时，字符串汉字出现了乱码。 为什么会这样？ 分析我们新增一个方法，将byte数组打印出来： 123456789public static void printHex(byte[] byteArray) &#123; StringBuffer sb = new StringBuffer(); for (byte b : byteArray) &#123; sb.append(Integer.toHexString((b &gt;&gt; 4) &amp; 0xF)); sb.append(Integer.toHexString(b &amp; 0xF)); sb.append(" "); &#125; System.out.println(sb.toString());&#125;; 这样上面两个的运行结果分别如下：ISO-8859-1: 字符串：你好 e4 bd a0 e5 a5 bd 转成ISO-8859-1：ä½ å¥½ e4 bd a0 e5 a5 bd 字符串：你好 字符串：你好呀 e4 bd a0 e5 a5 bd e5 91 80 转成ISO-8859-1：ä½ å¥½å e4 bd a0 e5 a5 bd e5 91 80 字符串：你好呀 GBK: 字符串：你好 e4 bd a0 e5 a5 bd 转成GBK：浣犲ソ e4 bd a0 e5 a5 bd 字符串：你好 字符串：你好呀 e4 bd a0 e5 a5 bd e5 91 80 转成GBK：浣犲ソ鍛� e4 bd a0 e5 a5 bd e5 91 3f 字符串：你好�? 可以看到，UTF-8转换为GBK在转换回来时，最后的80变成了3f，为什么会这样？ 我们使用”你好呀” 三个字来分析，它的UTF-8 的字节流为： [e4 bd a0] [e5 a5 bd] [e5 91 80] 我们按照三个字节一组分组，用GBK处理，因为GBK是双字节编码，如下按照两两一组进行分组： [e4 bd] [a0 e5] [a5 bd] [e5 91] [80 ?] 不够了，怎么办？它把 0x8d当做一个未知字符，用一个半角Ascii字符的 “？” 代替，变成了： [e4 bd] [a0 e5] [a5 bd] [e5 91] \3f 数据被破坏了。 为什么 ISO-8859-1 没问题呢？ 因为 ISO-8859-1 是单字节编码，因此它的分组方案是： [e4] [bd] [a0] [e5] [a5] [bd] [e5] [91] [80] 因此中间不做任何操作，因此数据没有变化。 问题你也许会问到，比如将“你好呀”三个字先由UTF-8转为ISO-8859-1，再由ISO-8859-1转为GBK，结果也是乱码啊，不是和下面的代码一样么，性质上？12String isoFont = new String(chinese.getBytes("UTF-8"),"ISO-8859-1");String gbkFont = new String(isoFont.getBytes("ISO-8859-1"),"GBK"); 1String gbkFont = new String(chinese.getBytes("UTF-8"),"GBK"); 两者的性质确实是一样的。 那与上面说的不矛盾吗？ 不矛盾。上面的代码，第一步你指定了字符串编码格式为UTF-8，第二步你将其转换为GBK，肯定会乱码。可以认为你拿一个UTF-8的字符串去转GBK字符串，其实在程序里这种写法本身是错误的！ 我们来看下面一段代码： 12345678910111213141516public static void test2() throws UnsupportedEncodingException &#123; String chinese = "你好呀"; //GBK 测试 String gbkChinese = new String(chinese.getBytes("GBK"),"ISO-8859-1"); System.out.println(gbkChinese); printHex(gbkChinese.getBytes("ISO-8859-1")); String gbkTest = new String(gbkChinese.getBytes("ISO-8859-1"),"GBK"); System.out.println(gbkTest); //UTF-8测试 String utf8Chinese = new String(chinese.getBytes("UTF-8"),"ISO-8859-1"); System.out.println(utf8Chinese); printHex(utf8Chinese.getBytes("ISO-8859-1")); String utfTest = new String(utf8Chinese.getBytes("ISO-8859-1"),"UTF-8"); System.out.println(utfTest); &#125; 输出结果： ÄãºÃÑ½ c4 e3 ba c3 d1 bd 你好呀 ä½ å¥½å e4 bd a0 e5 a5 bd e5 91 80 你好呀 可以看到， GBK分组 ：[c4 e3]–&gt;你 [ba c3]–&gt;好 [d1 bd]–&gt;呀 UTF-8分组：[e4 bd a0]–&gt;你 [e5 a5 bd]–&gt;好 [e5 91 80]–&gt;呀 字符串“你好呀”在GBK编码和UTF-8编码里生成的byte数据流是不一样的。 结论所以如何正确将两种编码格式数据进行转换？ 注意：这儿的转换指的是这样，比如一个GBK编码文件，里面有“你好呀”字符串，写入到UTF-8编码文件里仍然是“你好呀”。 我们新建一个GBK编码文件，里面有你好呀，三个字符，同时将三个字用UTF-8，写入到另一个文件里。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Test2 &#123; public static void main(String[] args) throws Exception &#123; String line = readInFile("/Users/zhangwentong/junrongdai/gbk.txt", "GBK"); System.out.println(line); writeInFile("/Users/zhangwentong/junrongdai/utf8.txt", line, "UTF-8"); &#125; public static String readInFile(String fileName, String charset) &#123; File gbkfile = new File(fileName); String line = ""; FileInputStream gbkIO = null; InputStreamReader gbkISR = null; BufferedReader br = null; try &#123; gbkIO = new FileInputStream(gbkfile); gbkISR = new InputStreamReader(gbkIO, charset); br = new BufferedReader(gbkISR); String rline = ""; while ((rline = br.readLine()) != null) &#123; line += rline; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if(br!=null) fos.close(); if(gbkISR!=null) gbkISR.close(); if(gbkIO!=null) gbkIO.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return line; &#125; public static void writeInFile(String fileName, String content, String charset) &#123; File f = new File(fileName); FileOutputStream fos = null; try &#123; if (!f.exists()) &#123; f.createNewFile(); &#125; fos = new FileOutputStream(f); fos.write(content.getBytes(charset)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 可以测试下上段代码，GBK文字被转为了UTF-8文字。反过来一个UTF-8文件写入到GBK也是可以实现的。 所以，在读取和写入文字时，指定文字的编码格式，再进行读取和写入操作，便不会有乱码的产生。否则读取和写入时会按照执行操作的class文件的编码格式进行写入和读取。 结语欢迎光临我的博客 https://www.sakuratears.top 我的GitHub地址 https://github.com/javazwt]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap实现原理]]></title>
    <url>%2Fblog%2FHashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86.html</url>
    <content type="text"><![CDATA[前言今天我们来聊聊HashMap。 Java集合类里的HashMap，实现Map接口，是个非线程安全的类。HashMap允许key和value有null值，且循环遍历为无序的，HashMap底层主要是通过数组+链表实现的，同时JDK8引入红黑树优化，提高HashMap的性能。 要了解HashMap，我们可以从几方面下手。 分析我们先来看下HashMap的两个参数： initialCapacity：初始容量，默认16 loadFactor：负载因子，默认0.75 现在我们执行了下面一段代码，根据代码来分析HashMap： 12Map&lt;String,String&gt; map=new HashMap&lt;&gt;();map.put("0","0"); 创建一个HashMap，其会初始化以下数据： 123456static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64; DEFAULT_INITIAL_CAPACITY: 初始容量，也就是默认会创建 16 个箱子，箱子的个数不能太多或太少。如果太少，很容易触发扩容，如果太多，遍历哈希表会比较慢。 MAXIMUM_CAPACITY: 哈希表最大容量，一般情况下只要内存够用，哈希表不会出现问题。 DEFAULT_LOAD_FACTOR: 默认的负载因子。因此初始情况下，当键值对的数量大于 16 * 0.75 = 12 时，就会触发扩容。 TREEIFY_THRESHOLD: 如果哈希函数不合理，即使扩容也无法减少箱子中链表的长度，因此处理方案是当链表太长时，转换成红黑树。这个值表示当某个箱子中，链表长度大于 8 时，有可能会转化成树。 UNTREEIFY_THRESHOLD: 在哈希表扩容时，如果发现链表长度小于 6，则会由树重新退化为链表。 MIN_TREEIFY_CAPACITY: 在转变成树之前，还会有一次判断，只有键值对数量大于 64 才会发生转换。这是为了避免在哈希表建立初期，多个键值对恰好被放入了同一个链表中而导致不必要的转化。 根据HashMap源码，可以看到源码中有两个static final class Node &lt; K,V &gt; 和 TreeNode &lt; K,V &gt;分别为链表和红黑树链表。 本文不对红黑树链表的实现做过多分析。 我们来看下HashMap的put方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//put方法，调用putVal方法 public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; //HashMap放值方法 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //构建一个链表数组tab，链表p，长度n，索引i Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //把table的值赋给tab，如果tab是空或者长度为0 if ((tab = table) == null || (n = tab.length) == 0) //调用resize方法，并获得tab长度 n = (tab = resize()).length; //计算索引并获得tab索引下的值，如果为空直接将值添加 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //如果key值相同，直接替换value值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果key不相同，判断p是不是TreeNode，是的话就执行红黑树放入值操作 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; //如果key上没值就放入普通链表 p.next = newNode(hash, key, value, null); //如果链表长度超了8 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //尝试将链表转化为红黑树（不一定会转化） treeifyBin(tab, hash); break; &#125; //如果key上有值就覆盖掉value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果节点value不为空，即key上有值，把这个值返回去 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //记录改变次数（fast-fail机制） ++modCount; //如果长度超过当前，就进行扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 我们再来看下，resize方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081final Node&lt;K,V&gt;[] resize() &#123; //获取旧的tab Node&lt;K,V&gt;[] oldTab = table; //旧的容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; //旧的阀值 int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; //如果容量超了MAXIMUM_CAPACITY，最大阀值定为Integer.MAX_VALUE if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //如果新容量赋值后小于MAXIMUM_CAPACITY并且旧容量不小于初始值 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果定义了初始容量 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults //否则为初始化，所有均为默认值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //为新的阀值赋值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap的get方法 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; //调用getNode方法 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;//获取HashMap Value值final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //先判断链表第一个值是不是结果 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //否则循环链表找值 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 我们看一下hash获取方法： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 代码的意思是：如果Key值为null，返回0；如果Key值不为空，返回原hash值和原hash值无符号右移16位的值按位异或的结果。可以看到当key=null时，hash为0. 问题 为什么HashMap的初始长度默认16，负载因子默认0.75，且长度建议取2的倍数？ &ensp;&ensp;&ensp;&ensp; 通过以上的代码我们可以知道这两个值主要影响的threshold的大小，这个值的数值是当前桶数组需不需要扩容的边界大小，我们都知道桶数组如果扩容，会申请内存空间，然后把原桶中的元素复制进新的桶数组中，这是一个比较耗时的过程。既然这样，那为何不把这两个值都设置大一些呢，threshold是两个数的乘积，设置的大些不就减小了扩容次数吗？ &ensp;&ensp;&ensp;&ensp; 原因是这样的，如果桶初始化桶数组设置太大，就会浪费内存空间，16是一个折中的大小，既不会像1，2，3那样放几个元素就扩容，也不会像几千几万那样可以只会利用一点点空间从而造成大量的浪费。 &ensp;&ensp;&ensp;&ensp; 加载因子设置为0.75而不是1，是因为设置过大，桶中键值对碰撞的几率就会越大，同一个桶位置可能会存放好几个value值，这样就会增加搜索的时间，性能下降，设置过小也不合适，如果是0.1，那么10个桶，threshold为1，你放两个键值对就要扩容，太浪费空间了。 &ensp;&ensp;&ensp;&ensp; HashMap为了存取高效，要尽量较少碰撞，就是要尽量把数据分配均匀，每个链表长度大致相同，这个实现就在把数据存到哪个链表中的算法；这个算法实际就是取模，hash%length，计算机中直接求余效率不如位移运算，源码中做了优化hash&amp;(length-1)，hash%length==hash&amp;(length-1)的前提是length是2的n次方；为什么这样能均匀分布减少碰撞呢？&ensp;&ensp;&ensp;&ensp; 2的n次方实际就是1后面n个0，2的n次方-1 实际就是n个1；例如长度为9时候，3&amp;(9-1)=0 2&amp;(9-1)=0 ，都在0上，碰撞了；例如长度为8时候，3&amp;(8-1)=3 2&amp;(8-1)=2 ，不同位置上，不碰撞； HashMap的fast-fail机制？ &ensp;&ensp;&ensp;&ensp; 在put方法里，我们注意到一个参数，modCount，每当HashMap改变，modCount都会改变,在HashMap的remove，clear等对HashMap的变动操作中，都可以看到对此参数的操作，同时我们还能看到如下代码： 12if (modCount != expectedModCount) throw new ConcurrentModificationException(); &ensp;&ensp;&ensp;&ensp; 这就是HashMap的fast-fail机制，HashMap会记录Map的改变次数，如果多个线程操作HashMap，会导致modCount和expectedModCount不一致，就会抛出ConcurrentModificationException异常，说明你的代码里有多线程对HashMap的操作，这是不被允许的，这样也可以快速检索程序错误，但是我们不能指望HashMap的fast-fail机制来检索错误，我们更应该在编码中注意HashMap是线程不安全的，多线程情况可以考虑ConcurrentHashMap,Hashtable或者使用Collections.synchronizedMap(map)等操作。 深度分析 关于hash桶索引计算函数 i=(n-1)&amp;hash 好处。 &ensp;&ensp;&ensp;&ensp; 观察HashMap将元素放入tab操作如下图，n在1处被赋值为tab.length，在2处，有一个关键算法，i=(n-1)&amp;hash,i为tab下标，这样做有什么好处呢？ （1）保证不会发生数组越界 &ensp;&ensp;&ensp;&ensp; 首先我们要知道的是，在HashMap，数组的长度按规定是2的幂。因此，数组的长度的二进制形式是：10000…000, 1后面有偶数个0。 那么，length - 1 的二进制形式就是01111…111, 0后面有偶数个1。最高位是0, 和hash值相“与”，结果值一定不会比数组的长度值大，因此也就不会发生数组越界。 （2）保证元素尽可能的均匀分布 &ensp;&ensp;&ensp;&ensp; 由上边的分析可知，length若是一个偶数，length - 1一定是一个奇数。假设现在数组的长度length为16，减去1后length - 1就是15，15对应的二进制是：1111。现在假设有两个元素需要插入，一个哈希值是8，二进制是1000，一个哈希值是9，二进制是1001。和1111“与”运算后，结果分别是1000和1001，它们被分配在了数组的不同位置，这样，哈希的分布非常均匀。那么，如果数组长度是奇数呢？减去1后length - 1就是偶数了，偶数对应的二进制最低位一定是 0，例如14二进制1110。对上面两个数子分别“与”运算，得到1000和1000。结果都是一样的值。那么，哈希值8和9的元素都被存储在数组同一个index位置的链表中。在操作的时候，链表中的元素越多，效率越低，因为要不停的对链表循环比较。 关于”扰动函数”，(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16) ？ &ensp;&ensp;&ensp;&ensp; 在对数据进行hash计算时，可以看到，不仅仅是取了数据的hashCode，而是将hashCode和hashCode无符号右移16位的值进行异或运算。 &ensp;&ensp;&ensp;&ensp; 我们知道，key.hashCode返回一个int值，这个值一般比hash桶数组长度要大，比如一个长度为16的hash桶，放入String abc (hashCode为96354)，直接进行桶索引计算，i=(n-1)&amp;hashCode 可以得出(15&amp;96354)=2,索引值为2，如果是abcd，计算（15&amp;2987074）=2，索引值也为2。 &ensp;&ensp;&ensp;&ensp; 可以看出，即使hashCode散列再离散，计算索引值时低位才是主要影响原因，而特征较大的高位（96354和2987074高位特征较大）根本不参与运算，这样hash冲突也会较高。而右移16位（32位的一半，int最大32位），正好为32位一半，这样可以把前16位认为高位，后16位认为低位，然后进行异或操作，高16位的信息被变相保存了下来，增大了随机性。 &ensp;&ensp;&ensp;&ensp; 可以看出这样操作后abc的下标为3（二进制11），abcd的下标为15（二进制1111）。 &ensp;&ensp;&ensp;&ensp; Peter Lawrey有一篇关于hash冲突率比较的文章《An introduction to optimising a hashing strategy》，大家可以看看。 &ensp;&ensp;&ensp;&ensp; https://www.javacodegeeks.com/2015/09/an-introduction-to-optimising-a-hashing-strategy.html 为什么引入负载因子这个概念？ &ensp;&ensp;&ensp;&ensp; 负载因子的引入，可以来说是时间复杂度和空间复杂度的折中。（大数据统计下）负载因子越低，一般认为空间开销越大，查询时间开销越低（hash碰撞低），大量hash数组，少量链表；负载因子越高，一般认为空间开销越低，查询时间开销越高（hash碰撞高），少量hash桶数组，大量链表。负载因子的引入恰可以增加HashMap不同场景使用的灵活性。 结语欢迎光临我的博客 https://www.sakuratears.top 我的GitHub地址 https://github.com/javazwt]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池]]></title>
    <url>%2Fblog%2FJava%E7%BA%BF%E7%A8%8B%E6%B1%A0.html</url>
    <content type="text"><![CDATA[前言Java中，创建线程的方式一般有三种方法： 继承Thread类创建线程 实现Runnable接口创建线程 使用Callable和Future创建线程 关于三种创建方法本文不再赘述。 可以看出，以上创建线程的方式，都缺乏对线程的管理，我们设想，如果线程在调用过程中使用了某一资源，当该资源处理缓慢或异常时，可能产生大量线程等待的情况，严重时可能造成OOM异常。 针对以上情况，应该对创建线程进行管理，这样线程池便产生了，好在在jdk1.5时，Doug Lea大神已经帮我们实现了这些功能，它们均在java.util.concurrent包下。建议大家想学习多线程，把该包下的源码理解，一定对多线程会有更深入的理解。 本文重点讲述线程池，会对以下这几个类（接口）进行重点讲解。 Executor，ExecutorService，Executors，AbstractExecutorService，ThreadPoolExecutor 线程池的创建我们先来简单说下线程池的使用： 缓存型线程池 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 Executors.newCachedThreadPool 简单使用： 123456789101112131415ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; final int index = i; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); &#125; &#125;); &#125; 定长线程池 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 Executors.newFixedThreadPool 简单使用： 123456789101112131415ExecutorService fixedThreadPool = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 定时及周期性任务性线程池 创建一个定长线程池，支持定时及周期性任务执行。 Executors.newScheduledThreadPool 简单使用： 1234567ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id="+Thread.currentThread().getId()+";5s后,每2s执行一次"); &#125; &#125;, 5, 2, TimeUnit.SECONDS); 单线程型线程池 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 Executors.newSingleThreadExecutor 简单使用： 123456789101112131415ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println("Thread id="+Thread.currentThread().getId()+";index="+index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; 自定义线程池 创建一个自定义线程池，以优化线程池。 根据Executors源码，可以看出： 12345678910111213141516171819 public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 线程池的实现使用ThreadPoolExecutor这个类实现的。这个类全参参数有以下几个： 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ① corePoolSize：核心线程池大小 取值范围 0-Integer.MaxValue ② maximumPoolSize：最大线程池大小 取值范围 0-Integer.MaxValue ③ keepAliveTime：线程空闲时存活时间 ④ unit：线程空闲时存活时间单位 ⑤ workQueue：工作队列类型，线程队列类型 队列分类： 直接提交策略：SynchronousQueue，其无法设置队列长度，所有线程均直接提交给线程池。 无界队列：LinkedBlockingQueue，如果默认不设置初始长度，这个队列是无界的，可缓存大量等待线程。 有界队列：ArrayBlockingQueue，必须设置初始长度，线程池满，且达到队列最大长度后执行拒绝策略。 ⑥ threadFactory：线程工厂 ⑦ handler：线程池饱和后的拒绝策略 ThreadPoolExecutor定义了四种，我们也可以自己定义： ThreadPoolExecutor.AbortPolicy：拒绝该任务并抛出异常 ThreadPoolExecutor.CallerRunsPolicy：直接在 execute 方法的调用线程中运行被拒绝的任务；如果执行程序已关闭，则会丢弃该任务 ThreadPoolExecutor.DiscardOldestPolicy：直接丢弃正在执行的任务，并执行该任务 ThreadPoolExecutor.DiscardPolicy：丢弃该任务 可以看出，当业务情况复杂时，Executors里提供的几种基本的线程池已经不能满足我们的要求，需要我们根据情况自定义线程池，而且可以举个例子，比如对于newCachedThreadPool创建线程池的方法，它传入的maximumPoolSize为Integer的Max值，如果业务资源异常，创建大量线程而不释放，newCachedThreadPool这种创建线程池的方法也能导致OOM异常。 而我们声明最大线程池大小，并声明拒绝策略。如下： 1234567891011121314151617181920ExecutorService myExecutor = new ThreadPoolExecutor(5, 10, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(5), new AbortPolicy()); for (int i = 0; i &lt; 11; i++) &#123; final int index = i; try &#123; myExecutor.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println("Thread id=" + Thread.currentThread().getId() + ";index=" + index); try &#123; Thread.sleep(10000000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 可以有效防止OOM异常以及及时发现系统运行问题。 自定义线程池也是被推荐的创建线程池的方法。 源码分析下面我们主要对ThreadPoolExecutor这个类进行分析。 我们先看下它的execute方法： 123456789101112131415161718192021222324252627282930public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //获取线程数量 int c = ctl.get(); //如果线程池线程数量小于核心线程数，那么试着向核心线程池添加一个线程 if (workerCountOf(c) &lt; corePoolSize) &#123; //添加成功就返回 if (addWorker(command, true)) return; //添加不成功就再次拿到线程数量 c = ctl.get(); &#125; //如果添加失败了，或者线程池数量达到了核心线程池数量 //那么判断下运行状态，然后试着向工作等待队列里添加此线程 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); //如果不是运行状态，那么试着从工作队列移除此线程 if (! isRunning(recheck) &amp;&amp; remove(command)) //成功就进行拒绝策略处理 reject(command); //如果核心线程池和队列都满了，达到CAPACITY else if (workerCountOf(recheck) == 0) //那么尝试将任务添加至非核心线程池 addWorker(null, false); &#125; //如果添加失败，执行拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 再看下addWorker方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; //sakuratears int c = ctl.get(); //拿下线程运行状态 int rs = runStateOf(c); //如果程序停止或者状态不是暂停并且任务不为空并且任务队列不为空，添加失败 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //数量大于CAPACITY或者数量大于corePoolSize（向核心线程池添加时）或者maximumPoolSize（向非核心线程池添加时），返回添加失败 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //试着计算下当前线程数量 if (compareAndIncrementWorkerCount(c)) //成功跳出循环 break retry; //不成功就重读ctl c = ctl.get(); //如果当前状态与开始线程状态不一致 if (runStateOf(c) != rs) //重试循环 continue retry; &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; //获得锁 final ReentrantLock mainLock = this.mainLock; //创建一个新的Worker w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; //加锁 mainLock.lock(); try &#123; int c = ctl.get(); int rs = runStateOf(c); //如果是运行状态，或者核心线程池暂停，但要将线程添加到非核心线程池中 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) throw new IllegalThreadStateException(); //添加线程 workers.add(w); int s = workers.size(); //记录曾经有过的最大线程数 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; //解锁 mainLock.unlock(); &#125; if (workerAdded) &#123; //启动 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) //线程启动失败，执行失败操作 addWorkerFailed(w); &#125; return workerStarted;&#125; 更多代码不一一赘述。上面代码基本是线程池的核心原理。 通俗点讲，线程池工作分为下面几步： 根据传入参数，设置核心线程池数量，最大线程池数量，拒绝策略，线程工作队列 当添加一个线程时，如果线程池线程数小于核心线程数，直接开启一个新线程执行任务。 如果核心线程池满了，那么把它添加到工作队列中。 如果核心线程池和工作队列都满了，则开启非核心线程执行任务。 如果全部都满了，执行拒绝策略。 以上就是对线程池的全部分析。 关于我的个人博客： https://www.sakuratears.top GitHub地址： https://github.com/javazwt 欢迎关注。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未闻花名]]></title>
    <url>%2Fblog%2F%E6%9C%AA%E9%97%BB%E8%8A%B1%E5%90%8D.html</url>
    <content type="text"><![CDATA[已知花意未见其花已见其花未闻花名再见其花落泪千溟未闻花名但识花香已知花名花已不在未闻花名但识花香再遇花时泪已千行 某天，你无端想起一个人，她曾让你对明天有所期许，但她却完全没有出现在你的明天里。————《再见金华站》]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MarkDown语法]]></title>
    <url>%2Fblog%2FMarkDown%E8%AF%AD%E6%B3%95.html</url>
    <content type="text"><![CDATA[前言最近自己搭建了一个博客网站，写文章时用到了markdown语法，之前虽未接触过markdown语法，但在两三天的使用过程中感觉markdown语法很棒，写法简洁，而且支持Html语言。特总结一些常用的语法如下： 语法汇总标题markdown有6种标题，写法如下： 级别 写法 1级 # 大标题 2级 ## 前言 3级 ### 语法汇总 4级 #### 标题 5级 ##### 标题5 6级 ###### 标题6 注意：#号和文字之间有空格 段落写完一段话，一般空出一行便可以分为两个段落，如果不主动空行，其便会认为为一个段落。也可以在一段话的结束输入两个空格，markdown也会换行。 注意：这两种换行，第一种间距大。我现在这段话分别用了空格换行和空行换行。 tab键这是一段用了tab缩进的话。 这是用了两个tab缩进的话。 这是一段很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长很长的话。 注意：当一段话过长时，这样生成的句子可能不换行，而是在下面有一个拉动条。 资源引用 引用静态资源时，如img，png，gif等 语法：![图片名称](图片的绝对路径或者相对路径) ![upload successful](/images/pasted-10.png) 引用网址时，直接贴上网址就能识别 我的博客：https://javazwt.github.io/ 表格语法： | Name | Value | |-------------|:-----------: | | sakuratears | 999999 | 效果： Name Value sakuratears 999999 注意：在表头下方的分隔线标记中加入 :，即可标记下方单元格内容的对齐方式，默认左对齐： :— 代表左对齐:–: 代表居中对齐—: 代表右对齐 left（:— ） center（:—-: ） right（ —-:） left center right 加粗 斜体 删除线加粗语法： **要加粗的字** 加粗效果： 要加粗的字 斜体语法：*斜体的字* 斜体效果： 斜体的字 删除线语法： ~~要删除的字~~ 删除线效果： 要删除的字 引用如果需要引用文献资料或者别的引用，可以用大于号解决： 语法：&gt;要引用的文字 效果： 要引用的文字 列表有序列表语法：在指定序号后面加上点（.） 并空格 1. aaa 2. bbb 3. ccc 效果： aaa bbb ccc 无序列表语法：在文字前面加上* 或者 + 或者 - 并空格 * aaa * bbb * ccc 效果： aaa bbb ccc 注意： 如果生成多级列表，可以这样： - 1.a - 1.1.aa - 1.1.1.aaa - 2.b - 3.c 效果图： 1.a 1.1.aa 1.1.1.aaa 2.b 3.c 代码支持语法： 1//代码片段 效果：一段java代码 1System.out.println("Hello World"); 其他由于Markdown支持Html语法，故Markdown以上无法实现的可以借助Html实现，如居中实现： &lt;center&gt;这段话要居中&lt;/center&gt; 效果：这段话要居中]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Html5+JQuery页面标签的禁用问题]]></title>
    <url>%2Fblog%2FHtml5-JQuery%E9%A1%B5%E9%9D%A2%E6%A0%87%E7%AD%BE%E7%9A%84%E7%A6%81%E7%94%A8%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[前言在做移动应用项目时遇到的问题，完成某些操作后返回到前面页面要保证前面页面的标签内容不可被编辑，修改。有如下方法： 方法 对于input，select框，设置disabled=’disabled’。 如： 12$('#testid1 input').attr('disabled','disabled');$('#testid2 select').attr('disabled','disabled'); 对于一些点击触发事件的button，可以用unbind()移除事件方法。 如： 1$(".testclass3").unbind(); 注：在某些情况下unbind慎用，如页面初始化时调用click事件加载其他事件属性，unbind()可能会导致初始化时无法加载。 插件禁用，有些页面可能使用某些插件，对于插件的禁用，可以设置插件的事件方法return false; 如： 123$("#test #test4").on("touchmove",function()&#123; return false;&#125;); 这个是我们项目一个滑动插件触摸禁用的例子。 对于一些比较特殊的，如li标签，可以使用class样式进行处理。 这是一个两个li标签切换禁用的例子。 例： 1234$("#test_li").addClass('disabledTab');.disabledTab&#123; pointer-events: none;//这样鼠标或触摸就无法选中li标签&#125;]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云主机环境搭建]]></title>
    <url>%2Fblog%2F%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html</url>
    <content type="text"><![CDATA[前言现在很多服务都部署在云服务上，最近项目也要发布到云上，特写此文章。有需要部署云服务的小伙伴可以参考下。 云主机参数在部署云服务时，一般需要知道如下参数： 云ECS主机服务器远程连接： url XXXXXXXX 用户名：XXXXXXX 密码：XXXXXXX 超级管理员：用户名：XXXX 密码 ：XXXX 硬件配置： 实例规格：2 核 4GB 网络：带宽 5Mbps 操作系统：Ubuntu 16.04 64位 系统盘：高效云盘（100 GB） 工具准备远程连接工具 XShell连接工具 Xftp文件上传管理工具 两工具下载地址：https://www.netsarang.com/products/xsh_overview.html 其他说明：由于当前Ubuntu 16.04 64位 Linux系统弃用了一些旧的远程ssh加密方式，故原先的ssh2工具不能连接，当然也可以修改linux配置文件使其支持旧的ssh加密方式，详见http://blog.csdn.net/liucky/article/details/51889112 等文章。本服务器未修改也不建议修改linux配置，进而选择使用XSHell等工具连接。 服务器搭建工具 Linux 64位 jdk压缩包 jdk-7u79-linux-x64.tar.gz Tomcat压缩包 For Linux 64 apache-tomcat-7.0.79.tar.gz Linux 64位 Mysql 安装包 （在线安装可不使用） mysql-5.7.19-linux-glibc2.12-x86_64.tar.gz 编译好的项目 demo 数据库表 demo 连接云主机 连接到Linux服务器 打开XShell，输入前面的云主机配置，连接到Linux远程服务器。如图： 登录成功后如图 使用 pwd 命令可查看当前路径 目前/home 文件夹下已使xxxxxx用户拥有读、写、执行的权限 上传文件 进入/home文件夹 我们在其目录下建三个文件夹jdk,mysql,tomcat用于存放文件 创建文件夹，可以使用 mkdir 文件夹名 这种方式，也可以直接打开xftp 创建，如图： 上传并解压 通过Xftp将 jdk，tomcat，mysql，项目包demo，数据表全部传上去 对于tar.gz文件进行解压 tar -xzvf jdk-7u79-linux-x64.tar.gz 注意：项目demo要放到tomcat webapps目录下，故建议将tomcat 解压后再上传demo项目 配置环境变量 找到jdk路径 同时找到 /etc/profile 使用vi /etc/profile 编辑该文件，设置环境变量 export JAVA_HOME=”/home/jdk/jdk1.7.0_79” export PATH=”$PATH:$JAVA_HOME/bin” export JRE_HOME=”$JAVA_HOME/jre” export CLASSPATH=”.:$JAVA_HOME/lib:$JRE_HOME/lib” 或者通过Xftp把profile文件down下来，修改好后再上传回去 弄好后要使用source /etc/profile 命令进行更新使其变量生效 同时 使用命令 java -version java javac 正常则表示配置成功了 启动Tomcat 由于tomcat自带的startup.sh脚本看不到日志输出，因此写了个脚本startTomcat.sh脚本 进入到tomcat bin目录下，使用命令 sh startTomcat.sh 启动tomcat，看到starup毫秒数表示启动成功。 启动后，在本地机器上访问 http://xxxxxxxxxxx:8080/ 可以看到久违的tomcat页面 bin里的shutdown.sh用于停止tomcat服务，sh shutdown.sh 成功后上传项目 demo到 tomcat webapps文件夹 安装MySQL接下来开始安装Mysql 对于阿里云，我们可以采取在线安装的方式快速安装MySQL 使用Linux root用户登录， 使用命令： sudo apt-get install mysql-server apt-get isntall mysql-client sudo apt-get install libmysqlclient-dev 安装过程中会提示输入root用户密码，设置所需的root用户密码即可。 使用以下命令查看MySQL是否安装成功 sudo netstat -tap | grep mysql 可以看到MySQL处于listen状态，配置成功。 关于离线安装，暂时没弄成功，有时间在补上。 导入数据表将数据表数据导入到MySQL数据库。mysql命令service mysql start 启动service mysql stop 停止 service mysql restart 重启 使用root用户登录MySQ mysql -u root -p 输入密码登录成功。 设置必要数据 进入mysql命令行 查看数据库编码还有表名大小写情况（Linux系统下MySQL表名会区分大小写） 查看字符编码 mysql&gt; show variables like ‘character%’; 查看表名大小写 mysql&gt; show Variables like ‘%table_names’; 如果不是utf-8编码（右图所示为utf-8编码格式），表名默认区分大小写 lower_case_table_names=0 则需要修改默认配置 cd etc/mysql/mysql.conf.d/ vi mysqld.cnf 加入如下配置： character-set-server=utf8 collation-server=utf8_general_ci skip-character-set-client-handshake lower_case_table_names=1 重启mysql服务 开启访问权限 我们可以在开启外网MySQL访问权限在Windows机器上用GUI工具导入MySQL表数据 开启外网权限： mysql&gt; SELECT User, Host FROM mysql.user; mysql&gt; update user set host = ‘%’ where user =’root’; mysql&gt; flush privileges; 同时将etc/mysql/mysql.conf.d/mysqld.cnf 配置文件里的 bind-address = 127.0.0.1注释掉。 这样在MySQL可视化工具如Navicat上远程连接MySQL，设置utf-8编码格式后将数据表导入即可。 注意事项及其他 启动tomcat及MySQL后，通过外网便可以正常访问项目。输入用户名密码后登陆。 注意事项。 使用root用户时，应尽量减少敏感操作，如 rm ， mv ，cp -r 等命令的执行。 tomcat请使用tomcat用户启动，不建议使用root用户启动。 MySQL开启外网访问后，如果无法访问，可能是阿里云禁用了3306端口的外网访问，需要登录阿里云开启，当成功导入数据后，十分建议关闭MySQL远程访问权限。 如果要设置tomcat默认端口为80，也需要登录阿里云设置。 如果操作过程中出现异常破坏了Linux系统，可以登录阿里云进行系统还原。 部署项目时，项目的一些配置文件如数据库连接，请求地址一定要配置正确。 如果要设置tomcat默认端口为80，则需要用root启动，因为linux只有root才能启动1024以内的端口，同时需要修改tomcat server.xml配置文件及demo访问地址。 文档PPT简单阿里云主机搭建文档资源：https://download.csdn.net/download/javazwt/10448764]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>云部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java排序算法]]></title>
    <url>%2Fblog%2FJava%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[排序算法分类十种常见排序算法可以分为两大类： 非线性时间比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此称为非线性时间比较类排序。 线性时间非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此称为线性时间非比较类排序。 如下图： 排序算法复杂度及稳定性如下图： 相关概念： 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数。 排序算法冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 动图演示 代码实现JavaScript代码 12345678910111213function bubbleSort(arr) &#123; var len = arr.length; for (var i = 0; i &lt; len; i++) &#123; for (var j = 0; j &lt; len - 1 - i; j++) &#123; if (arr[j] &gt; arr[j+1]) &#123; // 相邻元素两两对比 var temp = arr[j+1]; // 元素交换 arr[j+1] = arr[j]; arr[j] = temp; &#125; &#125; &#125; return arr; &#125; Java代码 123456789101112public static int[] bubbleSort(int[] a)&#123; for(int i=0;i&lt;a.length-1;i++)&#123; for(int j=i+1;j&lt;a.length;j++)&#123; if(a[i]&gt;a[j])&#123; int temp=a[i]; a[i]=a[j]; a[j]=temp; &#125; &#125; &#125; return a; &#125; 简单选择排序（Selection Sort）选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 算法描述n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n)。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n]分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； n-1趟结束，数组有序化了。 动图演示 代码实现JavaScript代码 12345678910111213141516function selectionSort(arr) &#123; var len = arr.length; var minIndex, temp; for (var i = 0; i &lt; len - 1; i++) &#123; minIndex = i; for (var j = i + 1; j &lt; len; j++) &#123; if (arr[j] &lt; arr[minIndex]) &#123; // 寻找最小的数 minIndex = j; // 将最小数的索引保存 &#125; &#125; temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; &#125; return arr; &#125; Java代码 12345678910111213141516public static int[] simpleSelectSort(int[] a)&#123; for(int i=0;i&lt;a.length-1;i++)&#123; int min=i; for(int j=i+1;j&lt;a.length;j++)&#123; if(a[min]&gt;a[j])&#123; min=j; &#125; &#125; if(min!=i)&#123; int temp=a[i]; a[i]=a[min]; a[min]=temp; &#125; &#125; return a; &#125; 算法分析表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 插入排序（Insertion Sort）插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 动图演示 代码实现JavaScript代码 1234567891011121314function insertionSort(arr) &#123; var len = arr.length; var preIndex, current; for (var i = 1; i &lt; len; i++) &#123; preIndex = i - 1; current = arr[i]; while (preIndex &gt;= 0 &amp;&amp; arr[preIndex] &gt; current) &#123; arr[preIndex + 1] = arr[preIndex]; preIndex--; &#125; arr[preIndex + 1] = current; &#125; return arr; &#125; Java代码 1234567891011121314public static int[] straightInsertionSort(int[] a)&#123; for(int i=1;i&lt;a.length;i++)&#123; for(int j=i;j&gt;0;j--)&#123; if(a[j]&lt;a[j-1])&#123; int temp=a[j-1]; a[j-1]=a[j]; a[j]=temp; &#125;else&#123; break; &#125; &#125; &#125; return a; &#125; 算法分析插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 希尔排序（Shell Sort）1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。 算法描述先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 动图演示 代码实现JavaScript代码 123456789101112131415161718function shellSort(arr) &#123; var len = arr.length, temp, gap = 1; while (gap &lt; len / 3) &#123; // 动态定义间隔序列 gap = gap * 3 + 1; &#125; for (gap; gap &gt; 0; gap = Math.floor(gap / 3)) &#123; for (var i = gap; i &lt; len; i++) &#123; temp = arr[i]; for (var j = i-gap; j &gt; 0 &amp;&amp; arr[j]&gt; temp; j-=gap) &#123; arr[j + gap] = arr[j]; &#125; arr[j + gap] = temp; &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920public static int[] shellsSort(int a[]) &#123; int d = a.length; while (true) &#123; d = d / 2; for (int x = 0; x &lt; d; x++) &#123; for (int i = x + d; i &lt; a.length; i = i + d) &#123; int temp = a[i]; int j; for (j = i - d; j &gt;= 0 &amp;&amp; a[j] &gt; temp; j = j - d) &#123; a[j + d] = a[j]; &#125; a[j + d] = temp; &#125; &#125; if (d == 1) &#123; break; &#125; &#125; return a; &#125; 算法分析希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的。 归并排序（Merge Sort）归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 算法描述 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 动图演示 代码实现JavaScript代码 123456789101112131415161718192021222324252627282930function mergeSort(arr) &#123; // 采用自上而下的递归方法 var len = arr.length; if (len &lt; 2) &#123; return arr; &#125; var middle = Math.floor(len / 2), left = arr.slice(0, middle), right = arr.slice(middle); return merge(mergeSort(left), mergeSort(right)); &#125; function merge(left, right) &#123; var result = []; while (left.length&gt;0 &amp;&amp; right.length&gt;0) &#123; if (left[0] &lt;= right[0]) &#123; result.push(left.shift()); &#125; else &#123; result.push(right.shift()); &#125; &#125; while (left.length) result.push(left.shift()); while (right.length) result.push(right.shift()); return result; &#125; Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static int[] mergeSort(int a[])&#123; sort(a,0,1); return a; &#125; private static void sort(int[] a, int s, int len) &#123; int size = a.length; int mid = size / (len &lt;&lt; 1); int c = size &amp; ((len &lt;&lt; 1) - 1); // -------归并到只剩一个有序集合的时候结束算法-------// if (mid == 0) return; // ------进行一趟归并排序-------// for (int i = 0; i &lt; mid; ++i) &#123; s = i * 2 * len; merge(a, s, s + len, (len &lt;&lt; 1) + s - 1); &#125; // -------将剩下的数和倒数一个有序集合归并-------// if (c != 0) merge(a, size - c - 2 * len, size - c, size - 1); // -------递归执行下一趟归并排序------// sort(a, 0, 2 * len); &#125; private static void merge(int[] a, int s, int m, int t) &#123; int[] tmp = new int[t - s + 1]; int i = s, j = m, k = 0; while (i &lt; m &amp;&amp; j &lt;= t) &#123; if (a[i] &lt;= a[j]) &#123; tmp[k] = a[i]; k++; i++; &#125; else &#123; tmp[k] = a[j]; j++; k++; &#125; &#125; while (i &lt; m) &#123; tmp[k] = a[i]; i++; k++; &#125; while (j &lt;= t) &#123; tmp[k] = a[j]; j++; k++; &#125; System.arraycopy(tmp, 0, a, s, tmp.length); &#125; 算法分析归并排序是一种稳定的排序方法。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(nlogn）的时间复杂度。代价是需要额外的内存空间。 快速排序（Quick Sort）快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法描述快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 动图演示 代码实现JavaScript代码 1234567891011121314151617181920212223242526272829303132function quickSort(arr, left, right) &#123; var len = arr.length, partitionIndex, left = typeof left != 'number' ? 0 : left, right = typeof right != 'number' ? len - 1 : right; if (left &lt; right) &#123; partitionIndex = partition(arr, left, right); quickSort(arr, left, partitionIndex-1); quickSort(arr, partitionIndex+1, right); &#125; return arr; &#125; function partition(arr, left ,right) &#123; // 分区操作 var pivot = left, // 设定基准值（pivot） index = pivot + 1; for (var i = index; i &lt;= right; i++) &#123; if (arr[i] &lt; arr[pivot]) &#123; swap(arr, i, index); index++; &#125; &#125; swap(arr, pivot, index - 1); return index-1; &#125; function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; Java代码 12345678910111213141516171819202122232425262728public static int[] quickSort(int a[],int low,int high)&#123; if(low&lt;high)&#123; int privotLoc=partition(a,low,high);//将表一分为二 quickSort(a,low,privotLoc-1); quickSort(a,privotLoc+1,high); &#125; return a; &#125; private static int partition(int a[],int low,int high)&#123; int privotKey=a[low];//基准元素 while(low&lt;high)&#123; //从表的两端交替向中间扫描 while(low&lt;high&amp;&amp;a[high]&gt;=privotKey)&#123; //从high 所指位置向前搜索，至多到low+1 位置。将比基准元素小的交换到低端 --high; //从右找比基准元素少的 &#125; a[low]=a[high]; //如果比基准元素小，交换位置 a[high]=privotKey; while(low&lt;high&amp;&amp;a[low]&lt;=privotKey)&#123; ++low; //从右找比基准元素大的 &#125; a[high]=a[low]; //如果比基准元素大，交换位置 a[low]=privotKey; &#125; return low; &#125; 堆排序（Heap Sort）堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 算法描述 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 动图演示 代码实现JavaScript代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344var len; // 因为声明的多个函数都需要数据长度，所以把len设置成为全局变量 function buildMaxHeap(arr) &#123; // 建立大顶堆 len = arr.length; for (var i = Math.floor(len/2); i &gt;= 0; i--) &#123; heapify(arr, i); &#125; &#125; function heapify(arr, i) &#123; // 堆调整 var left = 2 * i + 1, right = 2 * i + 2, largest = i; if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) &#123; largest = left; &#125; if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) &#123; largest = right; &#125; if (largest != i) &#123; swap(arr, i, largest); heapify(arr, largest); &#125; &#125; function swap(arr, i, j) &#123; var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; function heapSort(arr) &#123; buildMaxHeap(arr); for (var i = arr.length - 1; i &gt; 0; i--) &#123; swap(arr, 0, i); len--; heapify(arr, 0); &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public static int[] heapSort(int a[])&#123; buildMaxHeapTree(a); sort(a); return a; &#125; private static void buildMaxHeapTree(int a[])&#123; //没有子节点的才需要创建最大堆，从最后一个的父节点开始 int startIndex=getParentIndex(a.length-1); //从尾端开始创建最大堆，每次都是正确的堆 for(int i=startIndex;i&gt;=0;i--)&#123; maxHeap(a,a.length,i); &#125; &#125; private static void maxHeap(int[] data, int heapSize, int index) &#123; //当前点与左右子节点比较 int left = getChildLeftIndex(index); int right = getChildRightIndex(index); int largest = index; if (left &lt; heapSize &amp;&amp; data[index] &lt; data[left]) &#123; largest = left; &#125; if (right &lt; heapSize &amp;&amp; data[largest] &lt; data[right]) &#123; largest = right; &#125; //得到最大值后可能需要交换，如果交换了，其子节点可能就不是最大堆了，需要重新调整 if (largest != index) &#123; int temp = data[index]; data[index] = data[largest]; data[largest] = temp; maxHeap(data, heapSize, largest); &#125; &#125; private static int getParentIndex(int current)&#123; return (current-1)&gt;&gt;2; &#125; private static int getChildLeftIndex(int current)&#123; return(current&lt;&lt;1)+1; &#125; private static int getChildRightIndex(int current)&#123; return(current&lt;&lt;1)+2; &#125; private static void sort(int[] data)&#123; //末尾与头交换，交换后调整最大堆 for(int i=data.length-1;i&gt;0;i--)&#123; int temp=data[0]; data[0]=data[i]; data[i]=temp; maxHeap(data,i,0); &#125; &#125; 计数排序（Counting Sort）计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 动图演示 代码实现JavaScript代码 12345678910111213141516171819202122function countingSort(arr, maxValue) &#123; var bucket = new Array(maxValue + 1), sortedIndex = 0; arrLen = arr.length, bucketLen = maxValue + 1; for (var i = 0; i &lt; arrLen; i++) &#123; if (!bucket[arr[i]]) &#123; bucket[arr[i]] = 0; &#125; bucket[arr[i]]++; &#125; for (var j = 0; j &lt; bucketLen; j++) &#123; while(bucket[j] &gt; 0) &#123; arr[sortedIndex++] = j; bucket[j]--; &#125; &#125; return arr; &#125; Java代码 123456789101112131415161718192021222324public static int[] countSort(int[]a)&#123; int b[] = new int[a.length]; int max = a[0],min = a[0]; for(int i:a)&#123; if(i&gt;max)&#123; max=i; &#125; if(i&lt;min)&#123; min=i; &#125; &#125;//这里k的大小是要排序的数组中，元素大小的极值差+1 int k=max-min+1; int c[]=new int[k]; for(int i=0;i&lt;a.length;++i)&#123; c[a[i]-min]+=1;//优化过的地方，减小了数组c的大小 &#125; for(int i=1;i&lt;c.length;++i)&#123; c[i]=c[i]+c[i-1]; &#125; for(int i=a.length-1;i&gt;=0;--i)&#123; b[--c[a[i]-min]]=a[i];//按存取的方式取出c的元素 &#125; return b; &#125; 算法分析计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 桶排序（Bucket Sort）桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 算法描述 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来。 图片演示 代码实现JavaScript代码 12345678910111213141516171819202122232425262728293031323334353637383940function bucketSort(arr, bucketSize) &#123; if (arr.length === 0) &#123; return arr; &#125; var i; var minValue = arr[0]; var maxValue = arr[0]; for (i = 1; i &lt; arr.length; i++) &#123; if (arr[i] &lt; minValue) &#123; minValue = arr[i]; // 输入数据的最小值 &#125; else if (arr[i] &gt; maxValue) &#123; maxValue = arr[i]; // 输入数据的最大值 &#125; &#125; // 桶的初始化 var DEFAULT_BUCKET_SIZE = 5; // 设置桶的默认数量为5 bucketSize = bucketSize || DEFAULT_BUCKET_SIZE; var bucketCount = Math.floor((maxValue - minValue) / bucketSize) + 1; var buckets = new Array(bucketCount); for (i = 0; i &lt; buckets.length; i++) &#123; buckets[i] = []; &#125; // 利用映射函数将数据分配到各个桶中 for (i = 0; i &lt; arr.length; i++) &#123; buckets[Math.floor((arr[i] - minValue) / bucketSize)].push(arr[i]); &#125; arr.length = 0; for (i = 0; i &lt; buckets.length; i++) &#123; insertionSort(buckets[i]); // 对每个桶进行排序，这里使用了插入排序 for (var j = 0; j &lt; buckets[i].length; j++) &#123; arr.push(buckets[i][j]); &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526272829public static void basket(int data[])//data为待排序数组 &#123; int n = data.length; int bask[][] = new int[10][n]; int index[] = new int[10]; int max = Integer.MIN_VALUE; for (int i = 0; i &lt; n; i++) &#123; max = max &gt; (Integer.toString(data[i]).length()) ? max : (Integer.toString(data[i]).length()); &#125; String str; for (int i = max - 1; i &gt;= 0; i--) &#123; for (int j = 0; j &lt; n; j++) &#123; str = ""; if (Integer.toString(data[j]).length() &lt; max) &#123; for (int k = 0; k &lt; max - Integer.toString(data[j]).length(); k++) str += "0"; &#125; str += Integer.toString(data[j]); bask[str.charAt(i) - '0'][index[str.charAt(i) - '0']++] = data[j]; &#125; int pos = 0; for (int j = 0; j &lt; 10; j++) &#123; for (int k = 0; k &lt; index[j]; k++) &#123; data[pos++] = bask[j][k]; &#125; &#125; for (int x = 0; x &lt; 10; x++) index[x] = 0; &#125; &#125; 算法分析桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 基数排序（Radix Sort）基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 算法描述 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 动图演示 代码实现JavaScript代码 123456789101112131415161718192021222324var counter = []; function radixSort(arr, maxDigit) &#123; var mod = 10; var dev = 1; for (var i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) &#123; for(var j = 0; j &lt; arr.length; j++) &#123; var bucket = parseInt((arr[j] % mod) / dev); if(counter[bucket]==null) &#123; counter[bucket] = []; &#125; counter[bucket].push(arr[j]); &#125; var pos = 0; for(var j = 0; j &lt; counter.length; j++) &#123; var value = null; if(counter[j]!=null) &#123; while ((value = counter[j].shift()) != null) &#123; arr[pos++] = value; &#125; &#125; &#125; &#125; return arr; &#125; Java代码 1234567891011121314151617181920212223242526public static void sort(int[] number, int d) //d表示最大的数有多少位 &#123; int k = 0; int n = 1; int m = 1; //控制键值排序依据在哪一位 int[][] temp = new int[10][number.length]; //数组的第一维表示可能的余数0-9 int[] order = new int[10]; //数组orderp[i]用来表示该位是i的数的个数 while (m &lt;= d) &#123; for (int i = 0; i &lt; number.length; i++) &#123; int lsd = ((number[i] / n) % 10); temp[lsd][order[lsd]] = number[i]; order[lsd]++; &#125; for (int i = 0; i &lt; 10; i++) &#123; if (order[i] != 0) for (int j = 0; j &lt; order[i]; j++) &#123; number[k] = temp[i][j]; k++; &#125; order[i] = 0; &#125; n *= 10; k = 0; m++; &#125; &#125; 算法分析基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n&gt;&gt;k，因此额外空间需要大概n个左右。 测试新建Test类，测试十种排序算法时间消耗。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119 public static void main(String[] args) &#123; // int arrayLength=10; // int range=100; int arrayLength=100000; int range=100000; //生成一个指定长度数据的int数组 int[] a=new int[arrayLength]; for (int i=0;i&lt;arrayLength;i++)&#123; a[i]=(int)(Math.random()*range); &#125; //创建十个数组用于测试，否则会冲突 int[] a1= a.clone(); int[] a2= a.clone(); int[] a3= a.clone(); int[] a4= a.clone(); int[] a5= a.clone(); int[] a6= a.clone(); int[] a7= a.clone(); int[] a8= a.clone(); int[] a9= a.clone(); int[] a10= a.clone(); System.out.println(a1.toString()); System.out.println(a2.toString()); System.out.println(a3.toString()); System.out.println(a4.toString()); System.out.println(a5.toString()); System.out.println(a6.toString()); System.out.println(a7.toString()); System.out.println(a8.toString()); System.out.println(a9.toString()); System.out.println(a10.toString()); System.gc(); //直接插入排序法 long starta1=System.currentTimeMillis(); a1= StraightInsertionSort.straightInsertionSort(a1); long enda1=System.currentTimeMillis(); System.out.println("插入排序法耗时："+(enda1-starta1)+"ms"); //冒泡排序法 long starta2=System.currentTimeMillis(); a2= BubbleSort.bubbleSort(a2); long enda2=System.currentTimeMillis(); System.out.println("冒泡排序法耗时："+(enda2-starta2)+"ms"); //直接(简单)选择排序法 long starta3=System.currentTimeMillis(); a3= SimpleSelectionSort.simpleSelectSort(a3); long enda3=System.currentTimeMillis(); System.out.println("直接选择排序法耗时："+(enda3-starta3)+"ms"); //堆排序 long starta4=System.currentTimeMillis(); a4= HeapSort.heapSort(a4); long enda4=System.currentTimeMillis(); System.out.println("堆排序法耗时："+(enda4-starta4)+"ms"); //归并排序 long starta5=System.currentTimeMillis(); a5= MergeSort.mergeSort(a5); long enda5=System.currentTimeMillis(); System.out.println("归并排序法耗时："+(enda5-starta5)+"ms"); //快速排序算法 long starta6=System.currentTimeMillis(); a6= QuickSort.quickSort(a6,0,a6.length-1); long enda6=System.currentTimeMillis(); System.out.println("快速排序耗时："+(enda6-starta6)+"ms"); //计数排序算法 long starta7=System.currentTimeMillis(); a7= CountingSort.countSort(a7); long enda7=System.currentTimeMillis(); System.out.println("计数排序耗时："+(enda7-starta7)+"ms"); //希尔排序算法 long starta8=System.currentTimeMillis(); a8= ShellsSort.shellsSort(a8); long enda8=System.currentTimeMillis(); System.out.println("希尔排序耗时："+(enda8-starta8)+"ms"); //桶排序算法 long starta9=System.currentTimeMillis(); a9= BucketSort.basket(a9); long enda9=System.currentTimeMillis(); System.out.println("桶排序耗时："+(enda9-starta9)+"ms"); //基数排序算法 long starta10=System.currentTimeMillis(); a10= RadixSort.sort(a10,6); long enda10=System.currentTimeMillis(); System.out.println("基数排序耗时："+(enda10-starta10)+"ms"); // ArrayHelper.printLine(a1); // System.out.println(); // ArrayHelper.printLine(a2); // System.out.println(); // ArrayHelper.printLine(a3); // System.out.println(); // ArrayHelper.printLine(a4); // System.out.println(); // ArrayHelper.printLine(a5); // System.out.println(); // ArrayHelper.printLine(a6); // System.out.println(); // ArrayHelper.printLine(a7); // System.out.println(); // ArrayHelper.printLine(a8); // System.out.println(); // ArrayHelper.printLine(a9); // System.out.println(); // ArrayHelper.printLine(a10); System.gc(); &#125; 运行结果： 其他 本文为转载，十种算法的Java实现方式及测试为原创。 本文转载自：https://www.cnblogs.com/onepixel/articles/7674659.html 文中涉及到的Java代码地址：https://github.com/JavaZWT/sort]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之代理模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言代理模式：即通过代理对象访问目标对象，实现目标对象的方法。这样做的好处是：可以在目标对象实现的基础上，增强额外的功能操作，实现对目标功能的扩展。 这涉及到一个编程思想：不要随意去修改别人已经写好的代码或者方法（有坑）。如果需要修改，可以通过代理模式实现。 写法实现代理模式通常有三种实现写法：静态代理、动态代理、Cglib代理。 我们依次说下这三种代理模式： 我们现有 AnimalsDao接口和其实现AnimalsDaoImpl实现类，有两个方法，run和eat。 12345public interface AnimalsDao &#123; //提供两个方法 void run(); void eat(); &#125; 12345678910public class AnimalsDaoImpl implements AnimalsDao&#123; @Override public void run() &#123; System.out.println("run"); &#125; @Override public void eat() &#123; System.out.println("eat"); &#125; &#125; 现在我们想在run或者eat方法里之前做一些操作。 静态代理12345678910111213141516171819public class StaticProxy implements AnimalsDao&#123; @Override public void run() &#123; System.out.println("StaticProxy---------&gt;"); dao.run(); &#125; @Override public void eat() &#123; System.out.println("StaticProxy---------&gt;"); dao.eat(); &#125; private AnimalsDao dao; public StaticProxy(AnimalsDao dao)&#123; this.dao=dao; &#125; &#125; 说明：静态代理通过实现目标对象接口，然后调用相同方法来实现代理。这种方式的缺点显而易见，当目标对象接口方法变动时，直接影响到代理类，需要对代理类进行修改，十分不方便。而且如果目标对象接口方法较多时，代理类也十分臃肿，不便维护。 动态代理1234567891011121314151617181920public class DynamicProxy &#123; //要代理的对象 private Object obj; public DynamicProxy(Object obj)&#123; this.obj=obj; &#125; public Object getObjProxyIntance()&#123; return Proxy.newProxyInstance(obj.getClass().getClassLoader(), obj.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //执行目标对象方法 System.out.println("DynamicProxy---------&gt;"); return method.invoke(obj,args); &#125; &#125;); &#125; &#125; 说明：动态代理模式主要借助JDK代理对象API java.lang.reflect.Proxy来实现的，所以也称作JDK代理。我们看一下JDK这个类，其中重要的一个方法如下： 这个方法的三个参数： ClassLoader loader 目标对象类加载器 Class&lt;?&gt;[] interfaces 目标对象接口类型 InvocationHandler h 事物处理，在这里面可以实现自己想要的逻辑 根据以上，可以看出动态代理实现要求目标对象必须有实现接口。代理类不必实现接口。 Cglib代理要实现Cglib代理，必须引入cglib.jar 包，由于Spring-core包中已经包含了cglib功能，且大部分Java项目均引入了spring 相关jar包，这边使用spring的cglib来讲解。（他俩实现方式都是一样的） 1234567891011121314151617181920212223242526public class CglibProxy implements MethodInterceptor &#123; //目标对象 private Object obj; public CglibProxy(Object obj)&#123; this.obj=obj; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(obj.getClass()); //3.设置回调函数 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("CglibProxy---------&gt;"); return method.invoke(obj,objects); &#125; &#125; 说明：可以看出，Cglib代理模式实现不需要目标对象一定实现接口，故目标对象如果没有实现接口，可以使用cglib代理模式。其实Spring的代理模式也是这么实现的。 应用实例Spring的代理模式：如果容器目标对象有接口，使用JDK动态代理，如果没有接口，使用cglib动态代理。 参考资料 https://www.cnblogs.com/cenyu/p/6289209.html Spring 4.0 源代码 GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之工厂模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言工厂模式，顾名思义，即用来实例化一个个对象的模式。把相同类别的对象用工厂模式代替new生成，提高代码质量和系统扩展性。 写法分类 工厂模式的写法通常分为四类，简单工厂、多方法工厂、普通工厂、抽象工厂。 在分别说明他们之前，我们先创建几个对象。 抽象的Animals类，Plants类 1234public abstract class Animals &#123; //动物的描述 public abstract String desc(); &#125; 及Animals的三个基本实现，Tiger类、Bird类、Fish类，Plants的实现Tree类。 1234567public class Tiger extends Animals&#123; @Override public String desc() &#123; System.out.println("这是一只老虎"); return "这是一只老虎"; &#125; &#125; 1234567public class Bird extends Animals &#123; @Override public String desc() &#123; System.out.println("这是一只鸟"); return "这是一只鸟"; &#125; &#125; 1234567public class Fish extends Animals &#123; @Override public String desc() &#123; System.out.println("这是一条鱼"); return "这是一条鱼"; &#125; &#125; 1234public abstract class Plants &#123; //植物的描述 public abstract String desc(); &#125; 123456public class Tree extends Plants &#123; @Override public String desc() &#123; return "这是一棵树"; &#125; &#125; 下面来说说这四种工厂模式。 简单工厂模式1234567891011121314151617 public class AnimalsFactory &#123; public static final String Tiger="Tiger"; public static final String Bird="Bird"; public static final String Fish="Fish"; public static Animals getAnimal(String name)&#123; switch (name)&#123; case Tiger: return new Tiger(); case Bird: return new Bird(); case Fish: return new Fish(); default: return null; &#125; &#125; &#125; 调用：12Animals a=AnimalsFactory.getAnimal("Tiger"); a.desc(); 说明：简单工厂模式易于理解，但扩展性差，如我想在增加一种动物，则还需要修改逻辑内容。而且如果设置额外入参，对参数的管理也较为不便。 多方法工厂模式1234567891011public class AnimalsFactory &#123; public static Animals getTiger()&#123; return new Tiger(); &#125; public static Animals getBird()&#123; return new Bird(); &#125; public static Animals getFish()&#123; return new Fish(); &#125; &#125; 调用：12Animals b= AnimalsFactory.getFish(); b.desc(); 说明：这种模式使用方便，容错率高，且可适应不同需求，如我现在想创建一个羽毛为蓝色的小鸟，明显多方法工厂模式比简单工厂模式实现简单。 普通工厂模式123public abstract class PlantsFactory &#123; public abstract Plants getPlant(); &#125; 123456public class TreeFactory extends PlantsFactory&#123; @Override public Plants getPlant() &#123; return new Tree(); &#125; &#125; 调用：12PlantsFactory p=new TreeFactory(); p.getPlant().desc(); 说明：这种工厂模式划分成两层：抽象工厂层+具体的工厂子类层。 当我们不需要Tree，或者需要其他时，都可以新增或删除一个类实现，这种工厂模式看着很简洁明了易于理解。 抽象工厂模式12345678910public abstract class AbstractFactory&lt;T&gt; &#123; static&#123; // do something &#125; public void doSomeThing()&#123; //do something &#125; private int age; public abstract T getT(); &#125; 123456public class BirdFactory extends AbstractFactory&lt;Animals&gt; &#123; @Override public Animals getT() &#123; return new Bird(); &#125; &#125; 123456public class TreeFactory extends AbstractFactory&lt;Plants&gt; &#123; @Override public Plants getT() &#123; return new Tree(); &#125; &#125; 调用：123AbstractFactory factory= new TreeFactory(); Tree t=(Tree)factory.getT(); t.desc(); 说明：可以看出，这种方法把工厂本身也抽象了，如果对于两个或多个简单工厂模式，相似点较多，比如PlantsFactory和AnimalsFactory，作为生物，它们也有相似之处，dosomething部分的代码。其实抽象工厂模式跟普通工厂模式的区别就在于把工厂也抽象了一层。 工厂模式实例 Spring Bean对象的创建 抽象工厂模式的使用 线程池 Executors 多方法工厂模式的使用 其他：Spring框架中大量使用了工厂模式，单例模式等设计模式，有时间阅读些源代码，对技术能力的提升有很大帮助。 参考资料 https://blog.csdn.net/zxt0601/article/details/52798423 Spring 4.0 源码 JDK8源码 GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式之单例模式]]></title>
    <url>%2Fblog%2FJava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[前言单例模式，顾名思义，即只产生唯一实例的设计模式。一个类只有一个实例,即一个类只有一个对象实例。 优点1. 减少系统开销及资源消耗。 2. 设置全局访问点，优化共享资源的使用。 注：对于以上优点，我们可以拿Java配置读取举例，对于固定的配置文件，我们完全可以设计一个单例类，读取配置文件，而避免每次都读取文件造成的系统开销，同时也优化了配置文件的使用。 应用场景 Servlet 开源框架Spring中的默认bean配置模式 数据库连接池 项目中对于配置文件的处理等等 写法分类懒汉式单例模式（非线程安全）123456789101112public class Demo_Singleton_01 &#123; private static Demo_Singleton_01 instance=null; private Demo_Singleton_01()&#123; &#125; public static Demo_Singleton_01 getInstance()&#123; if(instance==null)&#123; instance=new Demo_Singleton_01(); &#125; return instance; &#125; &#125; 说明：这种写法不常用，虽然实现了懒加载（lazy-loading），但未考虑线程安全问题，不适用于多线程情况。 懒汉式单例模式（线程安全）123456789101112public class Demo_Singleton_02 &#123; private static Demo_Singleton_02 instance=null; public static synchronized Demo_Singleton_02 getInstance()&#123; if(instance==null)&#123; instance=new Demo_Singleton_02(); &#125; return instance; &#125; private Demo_Singleton_02()&#123; &#125; &#125; 说明：相对于demo1，这种写法实现了懒加载（lazy-loading），也考虑到线程安全问题，可适用于多线程情况。 饿汉式单例模式123456789public class Demo_Singleton_03 &#123; private static final Demo_Singleton_03 instance=new Demo_Singleton_03(); private Demo_Singleton_03()&#123; &#125; public static Demo_Singleton_03 getInstance()&#123; return instance; &#125; &#125; 说明：饿汉式单例模式的明显缺点是无法实现懒加载，在类被创建后就创建了唯一实例。 饿汉式单例模式（变种）123456789101112public class Demo_Singleton_04 &#123; private static Demo_Singleton_04 instance=null; static&#123; instance=new Demo_Singleton_04(); &#125; private Demo_Singleton_04()&#123; &#125; public static Demo_Singleton_04 getInstance()&#123; return instance; &#125; &#125; 说明：可以看到这种方法和demo3的性质是一样的，只不过改为了静态块加载唯一实例。 静态内部类模式1234567891011public class Demo_Singleton_05 &#123; private static class Singleton&#123; private static final Demo_Singleton_05 instance=new Demo_Singleton_05(); &#125; private Demo_Singleton_05()&#123; &#125; public static final Demo_Singleton_05 getInstance()&#123; return Singleton.instance; &#125; &#125; 说明：这种方法既可以实现懒加载，也能保证线程安全。 双重锁单例模式12345678910111213141516 public class Demo_Singleton_06 &#123; private static volatile Demo_Singleton_06 instance=null; private Demo_Singleton_06()&#123; &#125; public static Demo_Singleton_06 getInstance()&#123; if(instance==null)&#123; synchronized(Demo_Singleton_06.class)&#123; if(instance==null)&#123; instance=new Demo_Singleton_06(); &#125; &#125; &#125; return instance; &#125; &#125; 说明：可以看出，双重锁单例模式只有在第一次创建对象的时候才同步，因为创建对象后就没必要了，（实例唯一）相比懒汉式，效率明显提高，相比饿汉式，不用在类加载的时候就创建实例，而是等到什么时候想调用的时候在创建实例。线程安全，效率高。 枚举法123456public enum Demo_Singleton_07 &#123; instance; public void doSomething()&#123; &#125; &#125; 说明：枚举法代码十分直观，线程安全，但是无法实现懒加载。 参考资料 https://blog.csdn.net/goodlixueyong/article/details/51935526 https://www.cnblogs.com/Ycheng/p/7169381.html GitHub代码地址https://github.com/JavaZWT/designPatterns]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2Fhello-world.html</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>学习</category>
      </categories>
  </entry>
</search>
